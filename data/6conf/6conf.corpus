leonardos laptop leonardos laptop the old computing was about what computers could do the new computing is about what people can doto accelerate the shift from the old to the new computing designers need toreduce computer user frustration recent studies show of time is lost to crashes confusing instructions navigation problems etc public pressure for change could promote design improvements and increase reliability thereby dramatically enhancing user experiencespromote universal usability interfaces must be tailorable to a wide range of hardware software and networks and users when broad services such as voting healthcare and education are envisioned the challenge to designers is substantialenvision a future in which human needs more directly shape technology evolution four circles of human relationships and four human activities map out the human needs for mobility ubiquity creativity and community the world wide med and millionperson communities will be accessible through desktop palmtop and fingertip devices to support elearning ebusiness ehealthcare and egovernmentleonardo da vinci could help as an inspirational muse for the new computing his example could push designers to improve quality through scientific study and more elegant visual design leonardos example can guide us to the new computing which emphasizes empowerment creativity and collaboration information visualization and personal photo interfaces will be shown photomesa wwwcsumdeduhcilphotomesa and photofinder wwwcsumdeduhcilphotolibfor more httpmitpressmiteduleonardoslaptop and httpwwwcsumdeduhcilnewcomputing 
emerging data management systems emerging data management systems conventional data management occurs primarily in centralized servers or in wellinterconnected distributed systems these are removed from their end users who interact with the systems mostly through static devices to obtain generic services around mainstream applications banking retail business management etc several recent advances in technologies however give rise to a new breed of applications which change altogether the user experience and sense of data management very soon several such systems will be in our pockets many more in our homes the kitchen appliances our clothes etc how would these systems operate many system and user aspects must be approached in novel ways while several new issues come up and need to be addressed for the first time highlights include personalization privacy information trading annotation new interaction devices and corresponding interfaces visualization etc in this talk we take a close look at and give a very personal guided tour to this emerging world of data management offering some thoughts on how the new technical challenges might be approached 
from bits and bytes to information and knowledge from bits and bytes to information and knowledge unstructured data is a valuable source of information and implicit knowledge yet the bits and bytes of eg text image or clickstream data need to be interpreted in order to transform them into business intelligence and actionable information clearly this process needs to be automated to the largest possible extend in order to be scalable to the typical volumes of data one way to accomplish this is through the use of machine learning and statistical modelling techniques this talk will provide an overview of recent progress and new trends in machine learning and discuss their relevance for developing intelligent tools for search information filtering categorization and knowledge extraction 
structured queries in xml retrieval structured queries in xml retrieval documentcentric xml is a mixture of text and structure with the increased availability of documentcentric xml content comes a need for query facilities in which both structural constraints and constraints on the content of the documents can be expressed how does the expressiveness of languages for querying xml documents help users to express their information needs we address this question from both an experimental and a theoretical point of view our experimental analysis compares a structureignorant with a structureaware retrieval approach using the testsuite of the edition of the inex xml retrieval evaluation initiative theoretically we create mathematical models of users knowledge of a set of documents and define query languages which exactly fit these models one of these languages corresponds to an xml version of fielded search the other to the inex query language our main findings are first while structure is used in varying degrees of complexity over half of the queries can be expressed in a fieldedsearch like format which does not use the hierarchical structure of the documents second structure is used as a search hint and not a strict requirement when judged against the underlying information need third the use of structure in queries functions as a precision enhancing device 
score region algebra score region algebra a unified database framework that will enable better comprehension of ranked xml retrieval is still a challenge in the xml database field we propose a logical algebra named score region algebra that enables transparent specification of information retrieval ir models for xml databases the transparency is achieved by a possibility to instantiate various retrieval models using abstract score functions within algebra operators while logical query plan and operator definitions remain unchanged our algebra operators model three important aspects of xml retrieval element relevance score computation element score propagation and element score combination to illustrate the usefulness of our algebra we instantiate four different well known ir scoring models and combine them with different score propagation and combination functions we implemented the algebra operators in a prototype system on top of a lowlevel database kernel the evaluation of the system is performed on a collection of ieee articles in xml format provided by inex we argue that state of the art xml ir models can be transparently implemented using our score region algebra framework on top of any lowlevel physical database engine or existing rdbms allowing a more systematic investigation of retrieval model behavior 
generalized contextualization method for xml information retrieval generalized contextualization method for xml information retrieval a general reweighting method called contextualization for more efficient element ranking in xml retrieval is introduced reweighting is based on the idea of using the ancestors of an element as a context if the element appears in a good context good interpreted as probability of relevance its weight is increased in relevance scoring if the element appears in a bad context its weight is decreased the formal presentation of contextualization is given in a general xml representation and manipulation frame which is based on utilization of structural indices this provides a general approach independent of weighting schemas or query languagescontextualization is evaluated with the inex test collection we tested four runs no contextualization parent root and tower contextualizations the contextualization runs were significantly better than no contextualization the root contextualization was the best among the reweighted runs 
decentralized coordination of transactional processes in peertopeer environments decentralized coordination of transactional processes in peertopeer environments business processes executing in peertopeer environments usually invoke web services on different independent peers although peertopeer environments inherently lack global control some business processes nevertheless require global transactional guarantees ie atomicity and isolation applied at the level of processes this paper introduces a new decentralized serialization graph testing protocol to ensure concurrency control and recovery in peertopeer environments the uniqueness of the proposed protocol is that it ensures global correctness without relying on a global serialization graph essentially each transactional process is equipped with partial knowledge that allows the transactional processes to coordinate globally correct execution is achieved by communication among dependent transactional processes and the peers they have accessed in case of failures a combination of partial backward and forward recovery is applied experimental results exhibit a significant performance gain over traditional distributed lockingbased protocols with respect to the execution of transactions encompassing web service requests 
on the complexity of computing peer agreements for consistent query answering in peertopeer data integration systems on the complexity of computing peer agreements for consistent query answering in peertopeer data integration systems peertopeer pp data integration systems have recently attracted significant attention for their ability to manage and share data dispersed over different peer sources while integrating data for answering user queries it often happens that inconsistencies arise because some integrity constraints specified on peers global schemas may be violated in these cases we may give semantics to the inconsistent system by suitably repairing the retrieved data as typically done in the context of traditional data integration systems however some specific features of pp systems such as peer autonomy and peer preferences eg different source trusting should be properly addressed to make the whole approach effective in this paper we face these issues that were only marginally considered in the literature we first present a formal framework for reasoning about autonomous peers that exploit individual preference criteria in repairing the data the idea is that queries should be answered over the best possible database repairs with respect to the preferences of all peers ie the states on which they are able to find an agreement then we investigate the computational complexity of dealing with peer agreements and of answering queries in pp data integration systems it turns out that considering peer preferences makes these problems only mildly harder than in traditional data integration systems 
internet scale string attribute publishsubscribe data networks internet scale string attribute publishsubscribe data networks with this work we aim to make a threefold contribution we first address the issue of supporting efficiently queries over stringattributes involving prefix suffix containment and equality operators in largescale data networks our first design decision is to employ distributed hash tables dhts for the data networks topology harnessing their desirable properties our next design decision is to derive dhtindependent solutions treating dht as a black box second we exploit this infrastructure to develop efficient content based publishsubscribe systems the main contribution here are algorithms for the efficient processing of queries subscriptions and events publications specifically we show that our subscription processing algorithms require ologn messages for a nnode network and our event processing algorithms require ol x logn messages with l being the average string lengththird we develop algorithms for optimizing the processing of multidimensional events involving several string attributes further to our analysis we provide simulationbased experiments showing promising performance results in terms of number of messages required bandwidth load balancing and response times 
intelligent creation of notification events in information systems intelligent creation of notification events in information systems an important feature of information systems is the ability to inform users about changes of the stored information therefore systems have to know what changes a user wants to be informed about this is well known from the field of publishsubscribe architectures in this paper we propose a solution for information system designers of how to extend their information model in a way that the notification mechanism can consider semantic knowledge when determining which parties to inform two different kinds of implementations are introduced and evaluated one based on aspect oriented programming aop the other one based on traditional database triggers the evaluation of both approaches leads to a combined approach preserving the advantages of both techniques using model driven architecture mda to create the triggers from a uml model enhanced with stereotypes 
opportunity map opportunity map data mining techniques frequently find a large number of patterns or rules which make it very difficult for a human analyst to interpret the results and to find the truly interesting and actionable rules due to the subjective nature of interestingness human involvement in the analysis process is crucial in this paper we propose a novel visual data mining framework for the purpose of identifying actionable knowledge quickly and easily from discovered rules and data this framework is called the opportunity map it is inspired by some interesting ideas from quality engineering in particular quality function deployment qfd and the house of quality it associates summarized data or discovered rules with the application objective using an interactive matrix which enables the user to quickly identify where the opportunities are the proposed system can be used to visually analyze discovered rules and other statistical properties of the data the user can also interactively group actionable attributes and values and see how they affect the targets of interest combined with drilldown and comparative analysis the user can analyze rules and data at different levels of detail the proposed visualization framework thus represents a systematic and yet flexible method of rule analysis applications of the system to largescale data sets from our industrial partner have yielded promising results 
establishing value mappings using statistical models and user feedback establishing value mappings using statistical models and user feedback in this paper we present a value mapping algorithm that does not rely on syntactic similarity or semantic interpretation of the values the algorithm first constructs a statistical model eg cooccurrence frequency or entropy vector that captures the unique characteristics of values and their cooccurrence it then finds the matching values by computing the distances between the models while refining the models using user feedback through iterations our experimental results suggest that our approach successfully establishes value mappings even in the presence of opaque data values and thus can be a useful addition to the existing data integration techniques 
retrieving answers from frequently asked questions pages on the web retrieving answers from frequently asked questions pages on the web we address the task of answering natural language questions by using the large number of frequently asked questions faq pages available on the web the task involves three steps fetching faq pages from the web automatic extraction of questionanswer qa pairs from the collected pages and answering users questions by retrieving appropriate qa pairs we discuss our solutions for each of the three tasks and give detailed evaluation results on a collected corpus of about gb of text data k pages m qa pairs with real users questions sampled from a web search engine log specifically we propose simple but effective methods for qa extraction and investigate taskspecific retrieval models for answering questions our best model finds answers for of the test questions in the top results our overall conclusion is that faq pages on the web provide an excellent resource for addressing real users information needs in a highly focused manner 
finding similar questions in large question and answer archives finding similar questions in large question and answer archives there has recently been a significant increase in the number of communitybased question and answer services on the web where people answer other peoples questions these services rapidly build up large archives of questions and answers and these archives are a valuable linguistic resource one of the major tasks in a question and answer service is to find questions in the archive that a semantically similar to a users question this enables high quality answers from the archive to be retrieved and removes the time lag associated with a communitybased system in this paper we discuss methods for question retrieval that are based on using the similarity between answers in the archive to estimate probabilities for a translationbased retrieval model we show that with this model it is possible to find semantically similar questions with relatively little word overlap 
connecting topics in document collections with stepping stones and pathways connecting topics in document collections with stepping stones and pathways in this paper we present stepping stones and pathways ssp an alternative model of building and presenting answers for the cases when queries on document collections cannot be answered just by a ranked list stepping stones can handle questions like what is the relation of topics x and y ssp addresses when the contents of a small set of related documents is needed as an answer rather than a single document or when query splitting is required to satisfactorily explore a document space query results are networks of document groups representing topics each group relating to and connecting by documents to other groups in the network thus a network answers the users information need we devise new and more effective representations and techniques to visualize such answers and to involve users as part of the answerfinding process in order to verify the validity of our approach and since the questions we aim to answer involve multiple topics we performed a study involving a custom built broad collection of operating systems research papers and evaluated the results with interested computer science students using multiple measures 
securing xml data in thirdparty distribution systems securing xml data in thirdparty distribution systems webbased thirdparty architectures for data publishing are today receiving growing attention due to their scalability and the ability to efficiently manage large numbers of users and great amounts of data a thirdparty architecture relies on a distinction between the owner and the publisher of information the owner is the producer of information whereas publisher provides data management services and query processing functions for a portion of the owners information in such architecture there are important security concerns especially if we do not want to make any assumption on the trustworthy of the publishers although approaches have been proposed providing partial solutions to this problem no comprehensive framework has been so far developed able to support all the most important security properties in the presence of an untrusted publisher in this paper we develop an xmlbased solution to such problem which makes use of nonconventional digital signature techniques and queries over encrypted data 
the case for access control on xml relationships the case for access control on xml relationships with the emergence of xml as the de facto standard to exchange and disseminate information the problem of regulating access to xml documents has attracted a considerable attention in recent years existing models attach authorizations to nodes of an xml document but disregard relationships between them however ancestor and sibling relationships may reveal information as sensitive as the one carried out by the nodes themselves eg classification this paper advocates the integration of relationships as first class citizen in the access control models for xml and makes the following contributions first it characterizes important relationship authorizations and identifies the mechanisms required to translate them accurately in an authorized view of a source document second it introduces a rulebased formulation for expressing these classes of relationship authorizations and defines an associated conflict resolution strategy rather than being yetanother xml access control model the proposed approach allows a seamless integration of relationship authorizations in existing xml access control model 
a functionbased access control model for xml databases a functionbased access control model for xml databases xml documents are frequently used in applications such as business transactions and medical records involving sensitive information typically parts of documents should be visible to users depending on their roles for instance an insurance agent may see the billing information part of a medical document but not the details of the patients medical history access control on the basis of data location or value in an xml document is therefore essential in practice the number of access control rules is on the order of millions which is a product of the number of document types in s and the number of user roles in s therefore the solution requires high scalability and performance current approaches to access control over xml documents have suffered from scalability problems because they tend to work on individual documents in this paper we propose a novel approach to xml access control through rule functions that are managed separately from the documents a rule function is an executable code fragment that encapsulates the access rules paths and predicates and is shared by all documents of the same document type at runtime the rule functions corresponding to the access request are executed to determine the accessibility of document fragments using synthetic and real data we show the scalability of the scheme by comparing the accessibility evaluation cost of two rule function models we show that the rule functions generated on user basis is more efficient for xml databases 
exact match search in sequence data using suffix trees exact match search in sequence data using suffix trees we study suitable indexing techniques to support efficient exact match search in large biological sequence databases we propose a suffix tree st representation called stadf as an alternative to the array representation of st sta proposed in and utilized in to study the performance of sta and stadf we develop a memory efficient stbased exact match stem search algorithm we implemented stem and both representations of st and conducted extensive experiments our results indicate that the sta and stadf representations are very similar in construction time storage utilization and search time using stem in terms of the access patterns by stem our results show that compared to sta the stadf representation exhibits better spatial and sequential locality of reference this suggests that stadf would require less number of disk ios and hence is more amenable to efficient and scalable diskbased computation 
rotation invariant indexing of shapes and line drawings rotation invariant indexing of shapes and line drawings we present data representations distance measures and organizational structures for fast and efficient retrieval of similar shapes in image databases using the hough transform we extract shape signatures that correspond to important features of an image the new shape descriptor is robust against line discontinuities and takes into consideration not only the shape boundaries but also the content inside the object perimeter the object signatures are eventually projected into a space that renders them invariant to translation scaling and rotation in order to provide support for realtime querybycontent we also introduce an index structure that hierarchically organizes compressed versions of the extracted object signatures in this manner we can achieve a significant performance boost for multimedia retrieval our experiments suggest that by exploiting the proposed framework similarity search in a database of images would require under sec using an offtheshelf personal computer 
dist dist we consider the general problem of tracking moving objects in sensor networks the specific application we consider is that of tracking a chemical plume moving over a large infrastructure network we present a distributed index structure dist that stores and updates distributed summaries as the plume moves we present algorithms for range queries on the history of the plume dist localizes information with respect to time and space using a hierarchy that scales with the plume size the highlight of our work is an analytical model to predict the cost of query algorithms based on the query location query size and plumes spatiotemporal distribution using this model our adaptive scheme chooses the optimal scheme experimental results show that dist outperforms alternative techniques in query update and storage costs and scales well with the number of plumes 
focused crawling for both topical relevance and quality of medical information focused crawling for both topical relevance and quality of medical information subjectspecific search facilities on health sites are usually built using manual inclusion and exclusion rules these can be expensive to maintain and often provide incomplete coverage of web resources on the other hand health information obtained through wholeofweb search may not be scientifically based and can be potentially harmfulto address problems of cost coverage and quality we built a focused crawler for the mental health topic of depression which was able to selectively fetch higher quality relevant information we found that the relevance of unfetched pages can be predicted based on link anchor context but the quality cannot we therefore estimated quality of the entire linking page using a learned irstyle query of weighted single words and word pairs and used this to predict the quality of its links the overall crawler priority was determined by the product of link relevance and source qualitywe evaluated our crawler against baseline crawls using both relevance judgments and objective site quality scores obtained using an evidencebased rating scale both a relevance focused crawler and the quality focused crawler retrieved twice as many relevant pages as a breadthfirst control the quality focused crawler was quite effective in reducing the amount of low quality material fetched while crawling more high quality content relative to the relevance focused crawleranalysis suggests that quality of content might be improved by postfiltering a very big breadthfirst crawl at the cost of substantially increased network traffic 
hybrid index structures for locationbased web search hybrid index structures for locationbased web search there is more and more commercial and research interest in locationbased web search ie finding web content whose topic is related to a particular place or region in this type of search location information should be indexed as well as text information however the index of conventional text search engine is setoriented while location information is twodimensional and in euclidean space this brings new research problems on how to efficiently represent the location attributes of web pages and how to combine two types of indexes in this paper we propose to use a hybrid index structure which integrates inverted files and rtrees to handle both textual and location aware queries three different combining schemes are studied inverted file and rtree double index first inverted file then rtree first rtree then inverted file to validate the performance of proposed index structures we design and implement a complete locationbased web search engine which mainly consists of four parts an extractor which detects geographical scopes of web pages and represents geographical scopes as multiple mbrs based on geographical coordinates an indexer which builds hybrid index structures to integrate text and location information a ranker which ranks results by geographical relevance as well as nongeographical relevance an interface which is friendly for users to input locationbased search queries and to obtain geographical and textual relevant results experiments on large realworld web dataset show that both the second and the third structures are superior in query time and the second is slightly better than the third additionally indexes based on rtrees are proven to be more efficient than indexes based on grid structures 
person resolution in person search results person resolution in person search results finding information about people on the web using a search engine is difficult because there is a manytomany mapping between person names and specific persons ie referents this paper describes a person resolution system called webhawk given a list of pages obtained by submitting a person query to a search engine webhawk facilitates person search in three steps first of all a filter removes those pages that contain no information about any person secondly a cluster groups the remaining pages into different clusters each for one specific person to make the resulting clusters more meaningful an extractor is used to induce queryoriented personal information from each page finally a namer generates an informative description for each cluster so that users can find any specific person easily the architecture of webhawk is presented and the four components are discussed in detail with a separate evaluation of each component presented where appropriate a user study shows that webhawk complements most existing search engines and successfully improves users experience of person search on the web 
adaptive load shedding for windowed stream joins adaptive load shedding for windowed stream joins we present an adaptive load shedding approach for windowed stream joins in contrast to the conventional approach of dropping tuples from the input streams we explore the concept of selective processing for load shedding we allow stream tuples to be stored in the windows and shed excessive cpu load by performing the join operations not on the entire set of tuples within the windows but on a dynamically changing subset of tuples that are learned to be highly beneficial we support such dynamic selective processing through three forms of runtime adaptations adaptation to input stream rates adaptation to time correlation between the streams and adaptation to join directions indexes are used to further speed up the execution of stream joins experiments are conducted to evaluate our adaptive load shedding in terms of output rate the results show that our selective processing approach to load shedding is very effective and significantly outperforms the approach that drops tuples from the input streams 
integrating dct and dwt for approximating cube streams integrating dct and dwt for approximating cube streams for timerelevant multidimensional data sets mds users usually pose a huge amount of data due to the large dimensionality and approximating query processing has emerged as a viable solution specifically the cube streams handle mdss in a continuous manner traditional cube approximation focuses on generating single snapshots rather than continuous ones to address this issue the application of generating snapshots for cube streams called scs is investigated in this paper such an application collects data events for cube streams online and generates snapshots with limited resources in order to keep the approximated information in synopsis memory for further analysis as compared to olap applications the scs ones are subject to much more resource constraints for both processing time and memory and cannot be dealt with by existing methods due to the limited resources in this paper the dawa algorithm standing for a hybrid algorithm of dct for data and discrete wavelet transform is proposed to approximate the cube streams the dawa algorithm combines the advantage of high compression rate from dwt and that of low memory cost from dct consequently dawa costs much smaller working buffer and outperforms both dwtbased and dctbased methods in execution efficiency also it is shown that dawa provides answers of good quality for scs applications with a small working buffer and short execution time the optimality of algorithm dawa is theoretically proved and also empirically demonstrated by our experiments 
exploiting redundancy in sensor networks for energy efficient processing of spatiotemporal region queries exploiting redundancy in sensor networks for energy efficient processing of spatiotemporal region queries sensor networks are made of autonomous devices that are able to collect store process and share data with other devices spatiotemporal region queries can be used for retrieving information of interest from such networks such queries require the answers only from the subset of the network nodes that fall into the query region if the network is redundant in the sense that the measurements of some nodes can be substituted by those of other nodes with a certain degree of confidence then a much smaller subset of nodes may be sufficient to answer the query at a lower energy cost we investigate how to take advantage of such data redundancy and propose two techniques to process spatiotemporal region queries under these conditions our techniques reduce up to twenty times the energy cost of query processing compared to the typical network flooding thus prolonging the lifetime of the sensor network 
collective multilabel classification collective multilabel classification common approaches to multilabel classification learn independent classifiers for each category and employ ranking or thresholding schemes for classification because they do not exploit dependencies between labels such techniques are only wellsuited to problems in which categories are independent however in many domains labels are highly interdependent this paper explores multilabel conditional random field crfclassification models that directly parameterize label cooccurrences in multilabel classification experiments show that the models outperform their singlelabel counterparts on standard text corpora even when multilabels are sparse the models improve subset classification error by as much as 
clustering highdimensional data using an efficient and effective data space reduction clustering highdimensional data using an efficient and effective data space reduction this paper introduces a new algorithm for clustering data in highdimensional feature spaces called gardenhd the algorithm is organized around the notion of data space reduction ie the process of detecting dense areas dense cells in the space it performs effective and efficient elimination of empty areas that characterize typical highdimensional spaces and an efficient adjacencyconnected agglomeration of dense cells into larger clusters it produces a compact representation that can effectively capture the essence of data gardenhd is a hybrid of cellbased and densitybased clustering however unlike typical clustering methods in its class it applies a recursive partition of sparse regions in the space using a new spacepartitioning strategy the properties of this partitioning strategy greatly facilitate data space reduction the experiments on synthetic and real data sets reveal that gardenhd and its data space reduction are effective efficient and scalable 
versatile structural disambiguation for semanticaware applications versatile structural disambiguation for semanticaware applications in this paper we propose a versatile disambiguation approach which can be used to make explicit the meaning of structure based information such as xml schemas xml document structures web directories and ontologies it can be of support to the semanticawareness of a wide range of applications from schema matching and query rewriting to peer data management systems from xml data clustering to ontologybased automatic annotation of web pages and query expansion the effectiveness of the achieved results has been experimentally proved and is founded both on a flexible exploitation of the structure context whose extraction can be tailored on the specific application needs and of the information provided by commonly available thesauri such as wordnet 
dcape dcape note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
mining conserved xml query paths for dynamicconscious caching mining conserved xml query paths for dynamicconscious caching existing xml query patternbased caching strategies focus on extracting the set of frequently issued query pattern trees based on the number of occurrences of the query pattern trees in the history each occurrence of the same query pattern tree is considered equally important for the caching strategy however the same query pattern tree may occur at different timepoints in the history of xml queries this temporal feature can be used to improve the caching strategy in this paper we propose a novel type of query pattern called conserved query paths for efficient caching by integrating the support and temporal features together conserved query paths are paths in query pattern trees that never change or do not change significantly most of the time if not always in terms of their support values during a specific time period we proposed an algorithm to extract those conserved query paths by ranking those conserved query paths a dynamicconscious caching dcc strategy is proposed for efficient xml query processing experiments show that the dcc caching strategy outperforms the existing xml query pattern treebased caching strategies 
optimizing continuous multijoin queries over distributed streams optimizing continuous multijoin queries over distributed streams note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
processing xpath queries with xml summaries processing xpath queries with xml summaries range labeling and structural joins are wellstudied techniques for efficiently processing xpath queries however when xpath queries become long many times of structural joins are required to solve this problem we developed a method to reduce the number of joins and nodes read from the disk using strong dataguides our method can process single paths without any joins and twig patterns with joins amongst branching nodes and leaves in queries experimental results verified that our approach outperforms the best optimization technique for structural joins by factors of up to several hundreds of times 
on reducing redundancy and improving efficiency of xml labeling schemes on reducing redundancy and improving efficiency of xml labeling schemes the basic relationships to be determined in xml query processing are ancestordescendant ad parentchild pc sibling and ordering relationships the containment labeling scheme can determine the ad pc and ordering relationships fast but it is very expensive in determining the sibling relationship the prefix labeling scheme can determine all the four basic relationships fast if the xml tree is shallow however if the xml tree is deep the prefix scheme is inefficient since the prefix is long furthermore the prefix label is repeated by all the siblings only the self labels of these siblings are different thus in this paper we propose the pcontainment and pprefix schemes which can determine all the four basic relationships faster no matter what the xml structure is meanwhile pprefix can reduce the redundancies in the prefix labeling scheme 
applying cosine series to join size estimation applying cosine series to join size estimation this paper provides a general overview of two innovative applications of cosine series in xml joins and data stream joins 
database selection in intranet mediators for natural language queries database selection in intranet mediators for natural language queries note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
structurebased queryspecific document summarization structurebased queryspecific document summarization summarization of text documents is increasingly important with the amount of data available on the internet the large majority of current approaches view documents as linear sequences of words and create queryindependent summaries however ignoring the structure of the document degrades the quality of summaries furthermore the popularity of web search engines requires queryspecific summaries we present a method to create queryspecific summaries by adding structure to documents by extracting associations between their fragments 
typed functional query languages with equational specifications typed functional query languages with equational specifications we present a framework for functionally modeling query languages and data models data and queries are uniformly represented by firstorder functions and querylanguage constructs by polymorphic higherorder functions the functions are typed by a databaseoriented type system that supports polymorphism and nesting of types thus one can perform static typechecking and typeinferencing of queryexpressions the query language can be freely extended by introducing new querying constructs as polymorphic higherorder functionswhile type information gives the inputoutput description of the functions the semantic information is captured by equational specifications knowledge about the functions is represented as equalities of functional expressions in the form of equations by equational axiomatization of the query language database problems of query equivalence and answeringquery with views can be posed as equational wordproblems and equational matching 
dsac dsac database outsourcing is an important trend which involves data owners farming out their data management needs to an external service provider one important requirement is to maintain the integrity and authenticity of outsourced data whenever an outsourced database is queried the corresponding query reply must be demonstrably authentic furthermore a reply must include a proof of completeness to convince the querier that no data matching the query predicates has been omitted in this paper we suggest new techniques in support of efficient authenticity and completeness guarantees of such query replies 
answering aggregation queries on hierarchical web sites using adaptive sampling answering aggregation queries on hierarchical web sites using adaptive sampling we study how to answer aggregation queries over hierarchical web sites using adaptive sampling 
osqr osqr note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
infer infer the infer query language allows users to express queries without referencing relations or specifying joins since the infer syntax is similar to but less restrictive than sql users can easily write highly expressive queries that are automatically completed by infers inference engine infers sqlbased syntax is familiar to current database users and its improved ranking and query explanation system makes it easier to use 
efficient data dissemination using locale covers efficient data dissemination using locale covers locationdependent data are central to many emerging applications ranging from traffic information services to sensor networks the standard pull and pushbased data dissemination models become unworkable since the data volumes and number of clients are highwe address this problem using locale covers a subset of the original set of locations of interest chosen to include at least one location in a suitably defined neighborhood of any client since locationdependent values are highly correlated with location a query can be answered using a location close to the query pointwe show that locationdependent queries may be answered satisfactorily using locale covers with small loss of accuracy our approach is independent of locations and speeds of clients and is applicable to mobile clients 
incremental evaluation of a monotone xpath fragment incremental evaluation of a monotone xpath fragment this paper shows a scheme for incremental evaluation of xpath queries here we focus on a monotone fragment of xpath ie when a data is deleted from or inserted to the database only deletion insertion resp may occur to query answers for efficiently processing deletions we store information on partial matchings ie which elements were participating in matchings for which query answers and also store counters showing how many matchings each query answer had we use the information on the partial matchings also for skipping a part of computation upon data insertion we investigate properties of the xpath fragment in order to keep the amount of information we store as small as possible 
discovering strong skyline points in high dimensional spaces discovering strong skyline points in high dimensional spaces current interests in skyline computation arise due to their relation to preference queries since it is guaraneed that a skyline point will not lose out in all dimensions when compared to any other point in the data set this means that for each skyline point there exists a set of weight assignments to the dimensions such that the point will become the top user preferencewe believe that the usefulness of skyline points is not limited to such application and can be extended to data analysis and knowledge discovery as well however since the skyline of high dimensional datasets which are common in data analysis applications can contain too many points various means must be developed to filter off the less interesting skyline points in high dimensions in this paper we will propose algorithms to find a set of interesting skyline points called strong skyline points extensive experiments show that our proposal is both effective and efficient 
mining undiscovered public knowledge from complementary and noninteractive biomedical literature through semantic pruning mining undiscovered public knowledge from complementary and noninteractive biomedical literature through semantic pruning two complementary and noninteractive literature sets of articles when they are considered together can reveal useful information of scientific interest not apparent in either of the two document sets swanson called the existence of such knowledge undiscovered public knowledge udpk this paper proposes a semanticbased mining model for udpk our method replaces manual adhoc pruning with using semantic knowledge from the biomedical ontologies using the semantic types and semantic relationships of the biomedical concepts our prototype system can identify the relevant concepts collected from medline and generate the novel hypothesis between these concepts the system successfully replicates swansons two famous discoveries raynaud diseasefish oils and migrainemagnesium compared with previous approaches our methods generate much fewer but more relevant novel hypotheses and require much less human intervention in the discovery procedure 
access control for xml access control for xml being able to express and enforce rolebased access control on xml data is a critical component of xml data management however given the semistructured nature of xml this is nontrivial as access control can be applied on the values of nodes as well as on the structural relationship between nodes in this context we adopt and extend a graph editing language for specifying rolebased access constraints in the form of security views a security annotated schema sas is proposed as the internal representation for the security views and can be automatically constructed from the original schema and the security view specification to enforce the access constraints on user queries we propose secure query rewrite sqr a set of rules that can be used to rewrite a user xpath query on the security view into an equivalent xquery expression against the original data with the guarantee that the users only see information in the view but not any data that was blocked experimental evaluation demonstrates the efficiency and the expressiveness of our approach 
relational computation for mining association rules from xml data relational computation for mining association rules from xml data we develop a fixpoint operator for computing large item sets and demonstrate three query paradigm solutions for association rule mining that use the idea of least fixpoint computation and indicates some optimisation issues the results of our research provide theoretical foundation for relational computation of association rules and its application on xml mining 
mining all maximal frequent word sequences in a set of sentences mining all maximal frequent word sequences in a set of sentences we present an efficient algorithm for finding all maximal frequent word sequences in a set of sentences a word sequence s is considered frequent if all its words occur in at least sentences and the words occur in each of these sentences in the same order as in s given a frequency threshold hence the words of a sequence s do not have to occur consecutively in the sentences 
joint deduplication of multiple record types in relational data joint deduplication of multiple record types in relational data record deduplication is the task of merging database records that refer to the same underlying entity in relational databases accurate deduplication for records of one type is often dependent on the decisions made for records of other types whereas nearly all previous approaches have merged records of different types independently this work models these interdependencies explicitly to collectively deduplicate records of multiple types we construct a conditional random field model of deduplication that captures these relational dependencies and then employ a novel relational partitioning algorithm to jointly deduplicate records for two citation matching datasets we show that collectively deduplicating paper and venue records results in up to a error reduction in venue deduplication and up to a error reduction in paper deduplication 
localized routing trees for query processing in sensor networks localized routing trees for query processing in sensor networks in this paper we propose a novel energyefficient approach a localized routing tree lrt coupled with a route redirection rr strategy to support various types of queries lrts take care of the sensors near the sink and reduce the energy consumption of these sensors and rr reduces the energy cost of data receptions compared to the existing approaches simulation studies show that lrt together with rr has significant improvement on the query capacity 
a latent semantic classification model a latent semantic classification model latent semantic indexing lsi has been successfully applied to information retrieval and text classification however when lsi is used in classification some important features for small classes may be ignored because of their small feature values to solve this problem we propose the latent semantic classification lsc model which extends the lsi model in the following way the classification information of the training documents is introduced into the latent semantic structure via a second set of latent variables so that both indexing and classification information can be taken into account during the classification process our experiments on reuters show that our new model performs better than the existing classification methods such as knn and svm 
supporting ranked search in parallel search cluster networks supporting ranked search in parallel search cluster networks we investigate how to support ranked keyword search in a parallel search cluster network which is a newly proposed peertopeer network overlay in particular we study how to efficiently acquire and distribute the global information required by ranked keyword search by taking advantage of the architectural features of pscns 
web opinion poll web opinion poll note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
statistical relationship determination in automatic thesaurus construction statistical relationship determination in automatic thesaurus construction statistical relationship determination among terms is one of the key issues in automatic thesaurus construction we systematically analyze existing relevant approaches based on their underlying probabilistic assumptions and propose a combined approach that overcomes their limitations 
modelguided information discovery for intelligence analysis modelguided information discovery for intelligence analysis intelligence analysis can be aided and guided by models of the analysts interests and priorities this paper describes our approach to analyst modeling as part of the ant caf project in which analyst models are used to guide the searching behavior of a swarm of intelligent agents structural elements of our analyst model include concepts and relations both of which help to capture the analysts current interest and concerns in addition the concepts and relationships have associated scalar parameters to provide a quantitative measure of the users level of interest we have developed algorithms for dynamically adapting the weights and evolving the elements of the model itself to evaluate these algorithms we have built an analyst modeling environment workbench we have tested our approach on this workbench using traces generated by human analysts and have demonstrated improvements over current state of the art search engines 
biasing web search results for topic familiarity biasing web search results for topic familiarity depending on a web searchers familiarity with a querys target topic it may be more appropriate to show her introductory or advanced documents the trec hard track defined topic familiarity as metadata associated with a users query we instead define a userindependent and queryindependent model of topicfamiliarity required to read a document so it can be matched to a given user in response to a query an introductory web page is defined as a web page that doesnt presuppose any background knowledge of the topic it is on and to an extent introduces or defines the key terms in the topic while an advanced web page is defined as a web page that assumes sufficient background knowledge of the topic it is on and familiarity with the key technical important terms in the topic and potentially builds on them we develop a method for biasing the initial mix of documents returned by a search engine to increase the number of documents of desired familiarity level up to position and up to position our method involves building a supervised text classifier incorporating features based on reading level the distribution of stopwords in the text and nontext features such as average linelength using this familiarity classifier we achieve statistically significant improvements at reranking the result set to show introductory documents higher up the ranked list our classifier can be seamlessly integrated into current search engine technology without involving any major modifications to existing architectures 
accurate language model estimation with document expansion accurate language model estimation with document expansion note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
mining community structure of named entities from free text mining community structure of named entities from free text although community discovery has been studied extensively in the web environment limited research has been done in the case of free text cooccurrence of words and entities in sentences and documents usually implies connections among them in this paper we investigate the cooccurrences of named entities in text and mine communities among these entities we show that identifying communities from free text can be transformed into a graph clustering problem a hierarchical clustering algorithm is then proposed our experiment shows that the algorithm is effective to discover named entity communities from text documents 
a practical system of keyphrase extraction for web pages a practical system of keyphrase extraction for web pages keyphrases can be used to facilitate web users grasping the main topics of a web page we present a practical system of automatic keyphrase extraction for web pages in this system a regression model was first trained based on a set of humanlabeled documents then it was used to extract keyphrases from new pages automatically this paper makes three contributions first the structure information in a web page was investigated for keyphrase extraction task second the query log data associated with a web page collected by a search engine server were used to help keyphrase extraction third a method was put forward in this paper in order to evaluate the similarity of phrases 
incremental stock time series data delivery and visualization incremental stock time series data delivery and visualization sbtree is a binary tree data structure proposed to represent time series according to the importance of data points its use in stock data management is distinguished by preserving the critical data points attribute values retrieving time series data according to the importance of data points and facilitating multiresolution time series retrieval as new stock data are available continuously an effective updating mechanism for sbtree is needed in this paper a study of different updating approaches is reported three families of updating methods are proposed they are periodic rebuild batch update and pointbypoint update their efficiency effectiveness and characteristics are compared and reported 
generating better concept hierarchies using automatic document classification generating better concept hierarchies using automatic document classification this paper presents a hybrid concept hierarchy development technique for web returned documents retrieved by a metasearch engine the aim of the technique is to separate the initial retrieved documents into topical oriented categories prior to the actual concept hierarchy generation the topical categories correspond to different semantic aspects of the query this is done using a ofn automatic document classification on the initial set of returned documents then an individual topical concept hierarchy is automatically generated inside each of the resulted categories both steps are executed on the fly at retrieval time due to the efficiency constraints imposed by the web retrieval context the algorithm only uses document snippets rather than full web pages for both document classification and concept hierarchy generation experimental results show that the algorithm is able to improve the quality of the concept hierarchy presented to the searcher at the same time the efficiency parameters are kept within reasonable intervals 
domainspecific keyphrase extraction domainspecific keyphrase extraction document keyphrases provide semantic metadata characterizing documents and producing an overview of the content of a document they can be used in many textmining and knowledge management related applications this paper describes a keyphrase identification program kip which extracts document keyphrases by using prior positive samples of human identified domain keyphrases to assign weights to the candidate keyphrases the logic of our algorithm is the more keywords a candidate keyphrase contains and the more significant these keywords are the more likely this candidate phrase is a keyphrase to obtain prior positive inputs kip first populates its glossary database using manually identified keyphrases and keywords it then checks the composition of all noun phrases of a document looks up the database and calculates scores for all these noun phrases the ones having higher scores will be extracted as keyphrases 
an rsabased timebound hierarchical key assignment scheme for electronic article subscription an rsabased timebound hierarchical key assignment scheme for electronic article subscription the timebound hierarchical key assignment problem is to assign time sensitive keys to security classes in a partially ordered hierarchy so that legal data accesses among classes can be enforced two timebound hierarchical key assignment schemes have been proposed in the literature but both of them were proved insecure against collusive attacks in this paper we will propose an rsabased timebound hierarchical key assignment scheme and describe its possible application the security analysis shows that the new scheme is safe against the collusive attacks 
maximal termsets as a query structuring mechanism maximal termsets as a query structuring mechanism search engines process queries conjunctively to restrict the size of the answer set further it is not rare to observe a mismatch between the vocabulary used in the text of web pages and the terms used to compose the web queries the combination of these two features might lead to irrelevant query results particularly in the case of more specific queries composed of three or more terms to deal with this problem we propose a new technique for automatically structuring web queries as a set of smaller subqueries to select representative subqueries we use information on their distributions in the document collection this can be adequately modeled using the concept of maximal termsets derived from the formalism of association rules theory experimentation shows that our technique leads to improved results for the trec test collection for instance our technique led to gains in average precision of roughly with regard to a bm ranking formula 
accurately extracting coherent relevant passages using hidden markov models accurately extracting coherent relevant passages using hidden markov models in this paper we present a principled method for accurately extracting coherent relevant passages of variable lengths using hmms we show that with appropriate parameter estimation the hmm method outperforms a number of strong baseline methods on two data sets 
structural features in content oriented xml retrieval structural features in content oriented xml retrieval the structural features of xml components are an extra source of information that should be used in a contentoriented retrieval task on this type of documents in this paper we explore one of the structural features from the inex collection that could be used in contentoriented search we analyse the gain this knowledge could add to the performance of an information retrieval system and present a first approach on how this structural information could be extracted from a relevance feedback process to be used as priors in a language modelling framework 
text document clustering based on frequent word sequences text document clustering based on frequent word sequences in this paper we propose a new text clustering algorithm named clustering based on frequent word sequences cfws a word sequence is frequent if it occurs in more than certain percentage of the documents in the text database in the past the vector space model was commonly used for information retrieval but it treats documents as bags of words ignoring the sequential pattern of word occurrences in the documents however the meaning of natural languages strongly depends on the word sequences and the frequent word sequences can provide compact and valuable information about the text database bisecting kmeans and fihc algorithms are evaluated on the performance of text clustering and are compared with the proposed cfws algorithm it has been shown that cfws has much better performance 
information retrieval and machine learning for probabilistic schema matching information retrieval and machine learning for probabilistic schema matching schema matching is the problem of finding correspondences mapping rules eg logical formulae between heterogeneous schemas this paper presents a probabilistic framework called splmap for automatically learning schema mapping rules similar to lsd different techniques mostly from the ir field are combinedour approach however is also able to give a probabilistic interpretation of the prediction weights of the candidates and to select the rule set with highest matching probability 
learning to summarise xml documents using content and structure learning to summarise xml documents using content and structure documents formatted in extensible markup language xml are becoming increasingly available in collections of various document types in this paper we present an approach for the summarisation of xml documents the novelty of this approach lies in that it is based on features not only from the content of documents but also from their logical structure we follow a machine learning like sentence extractionbased summarisation technique to find which features are more effective for producing summaries this approach views sentence extraction as an ordering task we evaluated our summarisation model using the inex dataset the results demonstrate that the inclusion of features from the logical structure of documents increases the effectiveness of the summariser and that the learnable system is also effective and wellsuited to the task of summarisation in the context of xml documents 
trustbased collaborative filtering trustbased collaborative filtering note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
the earth movers distance as a semantic measure for document similarity the earth movers distance as a semantic measure for document similarity different words are usually assumed to be semantically independent in most existing similarity measures which is not often true in practice the semantic relatedness between words cannot be conveniently employed in the existing measures we propose a novel similarity measure based on the earth movers distance emd in the proposed measure the semantic distances between words are computed based on the electronic lexical databasewordnet and then the emd is employed to calculate the document similarity with a manytomany matching between words experiments and results demonstrate the effectiveness of the proposed similarity measure 
slicingtree based web page transformation for small displays slicingtree based web page transformation for small displays we propose a new web page transformation method for browsing on mobile devices with small displays in our approach an original web page that does not fit into the screen is transformed into a set of pages each of which fits into the screen this transformation is done through slicing the original page the resulting set of transformed pages form a multilevel tree structure called a slicingtree in which an internal node consists of a thumbnail image with hyperlinks and a leaf node is a block from the original web page our slicingtree based web page transformation eases web browsing on small displays by providing screenfitting visual context and reducing page scrolling effort 
an evaluation of evolved termweighting schemes in information retrieval an evaluation of evolved termweighting schemes in information retrieval this paper presents an evaluation of evolved termweighting schemes on short medium and long trec queries a previously evolved global collectionwide termweighting scheme is evaluated on unseen trec data and is shown to increase mean average precision over idf a local withindocument evolved termweighting scheme is presented which is dependent on the best performing global scheme the full evolved scheme ie the combined local and global scheme is compared to both the bm scheme and the pivoted normalisation schemeour results show that the local evolved solution does not perform well on some collections due to its document normalisation properties and we conclude that okapitf can be tuned to interact effectively with the evolved global weighting scheme presented and increase mean average precision over the standard bm scheme 
webcentric language models webcentric language models we investigate language models for informational and navigational web search retrieval on the web is a task that differs substantially from ordinary ad hoc retrieval we perform an analysis of prior probability of relevance for a wide range of noncontent features shedding further light on the importance of noncontent features for web retrieval this directly explains the success or failure of various techniques eg why the link topology is particularly helpful to single out important sites language models can naturally incorporate multiple document representations as well as noncontent information for the former we employ mixture language models based on document fulltext incoming anchortext and document titles for the latter we study a range of priors based on document length url structure and link topology we look at three types of topicsdistillation home page and named pageas well as for a mixed query set we find that the mixture models lead to considerable improvement of retrieval effectiveness for all topic types the webcentric priors generally lead to further improvement of retrieval effectiveness 
using rankboost to compare retrieval systems using rankboost to compare retrieval systems this paper presents a new pooling method for constructing the assessment sets used in the evaluation of retrieval systems our proposal is based on rankboost a machine learning voting algorithm it leads to smaller pools than classical pooling and thus reduces the manual assessment workload for building test collections experimental results obtained on an xml document collection demonstrate the effectiveness of the approach according to different evaluation criteria 
static score bucketing in inverted indexes static score bucketing in inverted indexes maintaining strict static score order of inverted lists is a heuristic used by search engines to improve the quality of query results when the entire inverted lists cannot be processed this heuristic however increases the cost of index generation and requires complex index build algorithms in this paper we study a new index organization based on static score bucketing we show that this new technique significantly improves in index build performance while having minimal impact on the quality of search results 
scalable ranking for preference queries scalable ranking for preference queries topk preference queries with multiple attributes are critical for decisionmaking applications previous research has concentrated on improving the computational efficiency mainly by using novel index structures and search strategies since current applications need to scale to terabytes of data and thousands of users performance of such systems is strongly impacted by the amount of available memory this paper proposes a scalable approach for memorybounded topk query processing 
finding experts in communitybased questionanswering services finding experts in communitybased questionanswering services note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
indexing time vs query time indexing time vs query time we examine issues in the design of fully dynamic information retrieval systems supporting both document insertions and deletions the two main components of such a system index maintenance and query processing affect each other as high query performance is usually paid for by additional work during update operations two aspects of the system incremental updates and garbage collection for delayed document deletions are discussed with a focus on the respective indexing vs query performance tradeoffs depending on the relative number of queries and update operations different strategies lead to optimal overall performance 
poison pills poison pills note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
discretization based learning approach to information retrieval discretization based learning approach to information retrieval we have designed a representation scheme which is based on the discrete representation of a document ranking function which is capable of reproducing and enhancing the properties of such popular ranking functions as tfidf bm or those based on language models our tests have demonstrated the capability of our approach to achieve the performance of the best known scoring functions solely through training without using any known heuristic or analytic formulas 
semantic verification for fact seeking engines semantic verification for fact seeking engines we present the architecture of our web question answering fact seeking system and introduce a novel algorithm to validate semantic categories of the expected answers when tested on the questions used by the prior research our system demonstrated the performance comparable to the current state of the art systems our semantic verification algorithm has improved the accuracy of answers of the affected questions by 
fast webpage classification using url features fast webpage classification using url features we demonstrate the usefulness of the uniform resource locator url alone in performing web page classification this approach is faster than typical web page classification as the pages do not have to be fetched and analyzed our approach segments the url into meaningful chunks and adds component sequential and orthographic features to model salient patterns the resulting features are used in supervised maximum entropy modeling we analyze our approachs effectiveness on two standardized domains our results show that in certain scenarios urlbased methods approach the performance of current stateoftheart fulltext and linkbased methods 
on the estimation of frequent itemsets for data streams on the estimation of frequent itemsets for data streams in this paper we devise a method for the estimation of the true support of itemsets on data streams with the objective to maximize one chosen criterion among precision recall while ensuring a degradation as reduced as possible for the other criterion we discuss the strengths weaknesses and range of applicability of this method that relies on conventional uniform convergence results yet guarantees statistical optimality from different standpoints 
unapparent information revelation unapparent information revelation information generated by multiple authors working independently at different times when analyzed synergistically reveals more information than apparent for example a traditional search for connections between the trucking industry and iraqi banks may not produce any documents mentioning both however a search that follows trails of associations across documents may suggest a connection between an auto parts manufacturer who exports to iraq and an iraqi bank providing loans to buy cars the work described here extends link analysis based on named entities and labeled relationships to general concepts and unnamed associations unapparent information revelation involves finding chains connecting concepts across documents it uses a new representation formalism called concept chain graphs 
document quality models for web ad hoc retrieval document quality models for web ad hoc retrieval the quality of document content which is an issue that is usually ignored for the traditional ad hoc retrieval task is a critical issue for web search web pages have a huge variation in quality relative to for example newswire articles to address this problem we propose a document quality language model approach that is incorporated into the basic query likelihood retrieval model in the form of a prior probability our results demonstrate that on average the new model is significantly better than the baseline query likelihood model in terms of precision at the top ranks 
cooperative caching for knn search in ad hoc networks cooperative caching for knn search in ad hoc networks mobile ad hoc networks have multiple limitations in performing similaritybased nearest neighbor search dynamic topology frequent disconnections limited power and restricted bandwidth cooperative caching is an effective technique to reduce network traffic and increase accessibility in this paper we propose to solve the knearestneighbor search problem in ad hoc networks using a semanticbased caching scheme which reflects the content distribution in the network the proposed scheme describes the semantic similarity among data objects using constraints and employs cooperative caching to estimate the content distribution in the network the query resolution based on the cooperative caching scheme is nonflooding and hierarchyfree 
a new framework to combine descriptors for contentbased image retrieval a new framework to combine descriptors for contentbased image retrieval in this paper we propose a novel framework using genetic programming to combine image database descriptors for contentbased image retrieval cbir our framework is validated through several experiments involving two image databases and specific domains where the images are retrieved based on the shape of their objects 
a structuresensitive framework for text categorization a structuresensitive framework for text categorization this paper presents a framework called structure sensitive categorizationsscat that exploits document structure for improved categorization there are two parts to this framework viz documents often have layout structure such that logically coherent text is grouped together into fields using some markup language we use a loglinear model which associates one or more features with each field weights associated with the field features are learnt from training data and these weights quantify the perclass importance of the field features in determining the category for the document we employ a technique that exploits the parse tree of fields that are phrasal constructs such as title and associates weights with words in these constructs while boosting weights of important words called focus words these weights are learnt from example instances of phrasal constructs marked with the corresponding focus words the learning is accomplished by training a classifier that uses linguistic features obtained from the texts parse structure the weighted words in fields with phrasal constructs are used in obtaining features for the corresponding fields in the overall framework sscat was tested on the supervised categorization task of over one million products from yahoos online shopping data with an accuracy of over our classifier outperforms naive bayes and support vector machines this not only shows the effectiveness of sscat but also strengthens our belief that linguistic features based on natural language structure can improve tasks such as text categorization 
efficient and effective serversided distributed clustering efficient and effective serversided distributed clustering clustering has become an increasingly important task in modern application domains where the data are originally located at different sites in order to create a central clustering all clients have to transmit their data to a central server due to technical limitations and security aspects at the central site often only vague object descriptions are available the server then has to carry out the clustering based on vague and uncertain data in a recent paper an approach for clustering uncertain data was proposed based on the concept of medoid clusterings the idea of this approach is to create first several sample clusterings then based on suitable distance functions between clusterings the most average clustering ie the medoid clustering was determined in this paper we extend this approach for partitioning clustering algorithms and propose to compute a centroid clustering based on these input sample clusterings these centroid clusterings are new artificial clusterings which minimize the distance to all the sample clusterings 
evaluation of a mcabased approach to organize data cubes evaluation of a mcabased approach to organize data cubes in the olap context exploration of huge and sparse data cubes is a tedious task that does not always lead to efficient results we propose to use a multiple correspondence analysis mca in order to enhance data cube representations and make them more suitable for visualization and thus easier to analyze we also provide an original quality criterion to measure the relevance of the obtained data representations experimental results we led on real data samples have shown the interest and the efficiency of our approach 
semantic similarity over the gene ontology semantic similarity over the gene ontology many bioinformatics applications would benefit from comparing proteins based on their biological role rather than their sequence in most biological databases proteins are already annotated with ontology terms previous studies identified a correlation between the sequence similarity and the semantic similarity of proteins the semantic similarity of proteins was computed from their annotated go terms however proteins sharing a biological role do not necessarily have a similar sequencethis paper introduces our study of the correlation between go and family similarity family similarity overcomes some of the limitations of sequence similarity thus we obtained a strong correlation between go and family similarity additionally this paper introduces grasm a novel method that uses all the information in the graph structure of the go instead of considering it as a hierarchical tree when calculating the semantic similarity of two concepts grasm selects the disjunctive common ancestors rather than only using the most informative common ancestor grasm produced a higher family similarity correlation than the original semantic similarity measures 
frequent pattern discovery with memory constraint frequent pattern discovery with memory constraint we explore in this paper a practicably interesting mining task to retrieve frequent itemsets with memory constraint as opposed to most previous works that concentrate on improving the mining efficiency or on reducing the memory size by best effort we first attempt to constrain the upper memory size that can be utilized by mining frequent itemsets in this paper 
extracting a websites content structure from its link structure extracting a websites content structure from its link structure hierarchical models are commonly used to organize a websites content a websites content structure can be represented by a topic hierarchy a directed tree rooted at a websites homepage in which the vertices and edges correspond to web pages and hyperlinks in this work we propose an algorithm for extracting a websites topic hierarchy from its link structure the proposed algorithm consists of a construction stage and a refining stage in which we analyze the semantic relationships between web pages based on link structure web page content and directory structure weve done extensive experiments using different websites and obtained very promising results 
improving intranet searchengines using context information from databases improving intranet searchengines using context information from databases information in enterprises comes in documents and data bases from a semantic viewpoint both kinds of information are usually tightly connected in this paper we propose to enhance common searchengines with contextual information retrieved from databases we establish system requirements and anecdotally demonstrate how documents and database information can be represented as the nodes of a graph then we give an example how we exploit this graph information for document retrieval 
a new permutation approach for distributed association rule mining a new permutation approach for distributed association rule mining privacy preserving distributed data mining has become a promising research area this paper addresses the problem of association rule mining where the global database is vertically partitioned when transactions are distributed in different sites scalar product is a feasible tool to discover frequent itemsets we present a new protocol to compute scalar product between two parties with a permutation approach we analyze the protocol in detail and demonstrate its effectiveness and high privacy properties and compare it to other published protocols 
on offtopic access detection in information systems on offtopic access detection in information systems we focus on detecting insider access violations to offtopic documents previously we utilized information retrieval techniques eg clustering and relevance feedback to warn of potential misuse for the relevance feedback approach we minimize the indicative features needed for detection using data mining techniques we show that the derived reduced feature subset achieves equivalent performance to that of the previously derived full set of features 
privacy leakage in multirelational databases via pattern based semisupervised learning privacy leakage in multirelational databases via pattern based semisupervised learning in multirelational databases a view which is a context and contentdependent subset of one or more tables or other views is often used to preserve privacy by hiding sensitive information however recent developments in data mining present a new challenge for database security even when traditional database security techniques such as database access control are employed this paper presents a data mining framework using semisupervised learning that demonstrates the potential for privacy leakage in multirelational databases many different types of semisupervised learning techniques such as the knearest neighbor knn method can be used to demonstrate privacy leakage however we also introduce a new approach to semisupervised learning hyperclique pattern based semisupervised learning hpsl which differs from traditional semisupervised learning approaches in that it considers the similarity among groups of objects instead of only pairs of objects our experimental results show that both the knn and hpsl methods have the ability to compromise database security although hpsl is better at this privacy violation than the knn method 
document clustering using character ngrams document clustering using character ngrams we propose a novel method for document clustering using character ngrams in the traditional vectorspace model the documents are represented as vectors in which each dimension corresponds to a word we propose a document representation based on the most frequent character ngrams with window size of up to characters we derive a new distance measure which produces uniformly better results when compared to the wordbased and termbased methods the result becomes more significant in the light of the robustness of the ngram method with no languagedependent preprocessing experiments on the performance of a clustering algorithm on a variety of test document corpora demonstrate that the ngram representation with n outperforms both word and term representations the comparison between word and term representations depends on the data set and the selected dimensionality 
inferring document similarity from hyperlinks inferring document similarity from hyperlinks assessing semantic similarity between text documents is a crucial aspect in information retrieval systems in this work we propose to use hyperlink information to derive a similarity measure that can then be applied to compare any text documents with or without hyperlinks as linked documents are generally semantically closer than unlinked documents we use a training corpus with hyperlinks to infer a function ab simab that assigns a higher value to linked documents than to unlinked ones two sets of experiments on different corpora show that this function compares favorably with okapi matching on document retrieval tasks 
a hybrid approach to ner by memm and manual rules a hybrid approach to ner by memm and manual rules this paper describes a framework for defining domain specific feature functions in a user friendly form to be used in a maximum entropy markov model memm for the named entity recognition ner task our system called merge allows defining general feature function templates as well as linguistic rules incorporated into the classifier the simple way of translating these rules into specific feature functions are shown we show that merge can perform better from both purely machine learning based systems and purelyknowledge based approaches by some small expert interaction of ruletuning 
situationaware risk management in autonomous agents situationaware risk management in autonomous agents we present a novel approach to enable decisionmaking in a highly distributed multiagent environment where individual agents need to act in an autonomous fashion our architecture framework integrates risk management knowledge management and agent deliberation to enable sophisticated autonomous decisionmaking instead of a centralized knowledge repository our approach supports a highly distributed knowledge base in which each agent manages a fraction of the knowledge needed by the entire system 
mining officially unrecognized side effects of drugs by combining web search and machine learning mining officially unrecognized side effects of drugs by combining web search and machine learning we consider the problem of finding officially unrecognized side effects of drugs by submitting queries to the web involving a given drug name it is possible to retrieve pages concerning the drug however many retrieved pages are irrelevant and some relevant pages are not retrieved more relevant pages can be obtained by adding the active ingredient of the drug to the query in order to eliminate irrelevant pages we propose a machine learning process to filter out the undesirable pages the process is shown experimentally to be very effective since obtaining training data for the machine learning process can be time consuming and expensive we provide an automatic method to generate the training data the method is also shown to be very accurate the side effects of three drugs which are not recognized by fda are validated by an expert we believe that the same approach can be applied to many real life problems and will yield high precision thus this could lead a new way to perform retrieval with high accuracy 
mailrank mailrank can we use social networks to combat spam this paper investigates the feasibility of mailrank a new email ranking and classification scheme exploiting the social communication network created via email interactions the underlying email network data is collected from the email contacts of all mailrank users and updated automatically based on their email activities to achieve an easy maintenance mailrank is used to rate the sender address of arriving emails such that emails from trustworthy senders can be ranked and classified as spam or nonspam the paper presents two variants basic mailrank computes a global reputation score for each email address whereas in personalized mailrank the score of each email address is different for each mailrank user the evaluation shows that mailrank is highly resistant against spammer attacks which obviously have to be considered right from the beginning in such an application scenario mailrank also performs well even for rather sparse networks ie where only a small set of peers actually take part in the ranking of email addresses 
viper viper in this paper we address the problem of unsupervised web data extraction we show that unsupervised web data extraction becomes feasible when supposing pages that are made up of repetitive patterns as it is the case eg for search engine result pages hereby the extraction rules are generated automatically without any training or human interaction by means of operating on the dom tree respectively the flat tag token sequence of a single pageour contribution to automatic data extraction through this paper is twofold first we identify and rank potential repetitive patterns with respect to the users visual perception of the web page well aware that location and size of matching elements within a web page constitute important criteria for defining relevance second matching subsequences of the pattern with the highest weightiness are aligned with global multiple sequence alignment techniques experimental results show that our system is able to achieve high accuracy in distilling and aligning regularly structured objects inside complex web pages 
interconnection semantics for keyword search in xml interconnection semantics for keyword search in xml a framework for describing semantic relationships among nodes in xml documents is presented in contrast to earlier work the xml documents may have id references ie they correspond to graphs and not just trees a specific interconnection semantics in this framework can be defined explicitly or derived automatically the main advantage of interconnection semantics is the ability to pose queries on xml data in the style of keyword search several methods for automatically deriving interconnection semantics are presented the complexity of the evaluation and the satisfiability problems under the derived semantics is analyzed for many important cases the complexity is tractable and hence the proposed interconnection semantics can be efficiently applied to realworld xml documents 
efficient indexing and querying of xml data using modified prfer sequences efficient indexing and querying of xml data using modified prfer sequences with the advent of xml as the new standard for information representation and exchange indexing and querying of xml data is of major concern in this paper we propose a method for representing an xml document as a sequence based on a variation of prfer sequences we incorporate new components in the node encodings such as level number of a certain kind of descendants and develop methods for holistic processing of tree pattern queries the query processing involves converting the query also into a sequence and performing subsequence matching on the document sequence we establish certain interesting properties of the proposed method of sequencing that give rise to a new efficient pattern matching algorithm the sequence data is stored in a two level btrees to support query processing we also propose an optimization for parentchild axis to speed up the query processing our approach does not require any postprocessing and guarantees results that are free of false positives and duplicates experimental results show that our system performs significantly better than previous systems in a large number of cases 
towards automatic association of relevant unstructured content with structured query results towards automatic association of relevant unstructured content with structured query results faced with growing knowledge management needs enterprises are increasingly realizing the importance of seamlessly integrating critical business information distributed across both structured and unstructured data sources in existing information integration solutions the application needs to formulate the sql logic to retrieve the needed structured data on one hand and identify a set of keywords to retrieve the related unstructured data on the other this paper proposes a novel approach wherein the application specifies its information needs using only a sql query on the structured data and this query is automatically translated into a set of keywords that can be used to retrieve relevant unstructured data we describe the techniques used for obtaining these keywords from i the query result and ii additional related information in the underlying database we further show that these techniques achieve high accuracy with very reasonable overheads 
predicting accuracy of extracting information from unstructured text collections predicting accuracy of extracting information from unstructured text collections exploiting lexical and semantic relationships in large unstructured text collections can significantly enhance managing integrating and querying information locked in unstructured text most notably named entities and relations between entities are crucial for effective question answering and other information retrieval and knowledge management tasks unfortunately the success in extracting these relationships can vary for different domains languages and document collections predicting extraction performance is an important step towards scalable and intelligent knowledge management information retrieval and information integration we present a general language modeling method for quantifying the difficulty of information extraction tasks we demonstrate the viability of our approach by predicting performance of real world information extraction tasks named entity recognition and relation extraction 
wamminer wamminer existing web usage mining techniques focus only on discovering knowledge based on the statistical measures obtained from the static characteristics of web usage data they do not consider the dynamic nature of web usage data in this paper we focus on discovering novel knowledge by analyzing the change patterns of historical web access sequence data we present an algorithm called wltsmallgtamltsmallgtmltsmallgtinerltsmallgt to discover web access motifs wams wams are web access patterns that never change or do not change significantly most of the time if not always in terms of their support values during a specific time period wams are useful for many applications such as intelligent web advertisement web site restructuring business intelligence and intelligent web caching 
a framework for mining topological patterns in spatiotemporal databases a framework for mining topological patterns in spatiotemporal databases mining topological patterns in spatial databases has received a lot of attention however existing work typically ignores the temporal aspect and suffers from certain efficiency problems they are not scalable for mining topological patterns in spatiotemporal databases in this paper we study the problem for mining topological patterns by incorporating the temporal aspect in the mining process we introduce a summarystructure that records the instances count information of a feature in a region within a time window using this structure we design an algorithm topologyminer to find interesting topological patterns without the need to generate candidates experimental results show that topologyminer is effective and scalable in finding topological patterns and outperforms apriorilike algorithm by a few orders of magnitudes 
automated cleansing for spend analytics automated cleansing for spend analytics the development of an aggregate view of the procurement spend across an enterprise using transactional data is increasingly becoming a very important and strategic activity not only does it provide a complete and accurate picture of what the enterprise is buying and from whom it also allows it to consolidate suppliers as well as negotiate better prices the importance as well as the complexity of this cleansing exercise is further magnified by the increasing popularity of business transformation outsourcing bto wherein enterprises are turning over noncore activities such as indirect procurement to third parties who now need to develop an integrated view of spend across multiple enterprises in order to optimize procurement and generate maximum savings however the creation of such an integrated view of procurement spend requires the creation of a homogeneous data repository from disparate heterogeneous data sources across various geographic and functional organizations throughout the enterprises such repositories get transactional data from various sources such as invoices purchase orders account ledgers as such the transactions are not crossindexed refer to the same suppliers by different names and use different ways of representing information about the same commodities before an aggregated spend view can be developed this data needs to be cleansed primarily to normalize the supplier names and correctly map each transaction to the appropriate commodity code commodity mapping in particular is made more difficult by the fact that it has to be done on the basis of unstructured text descriptions found in the various data sources we describe an ondemand system to automatically perform this cleansing activity using techniques from information retrieval and machine learning built on standard integration and application infrastructure software this system provides enterprises with a fast reliable accurate and ondemand way of cleansing transactional data and generating an integrated view of spend this system is currently in the process of being deployed by ibm for use in its bto practice 
featurebased recommendation system featurebased recommendation system the explosive growth of the worldwideweb and the emergence of ecommerce has led to the development of recommender systemsa personalized information filtering technology used to identify a set of n items that will be of interest to a certain user userbased and modelbased collaborative filtering are the most successful technology for building recommender systems to date and is extensively used in many commercial recommender systems the basic assumption in these algorithms is that there are sufficient historical data for measuring similarity between products or users however this assumption does not hold in various application domains such as electronics retail home shopping network online retail where new products are introduced and existing products disappear from the catalog another such application domains is home improvement retail industry where a lot of products such as window treatments bathroom kitchen or deck are custom made each product is unique and there are very little duplicate products in this domain the probability of the same exact two products bought together is close to zero in this paper we discuss the challenges of providing recommendation in the domains where no sufficient historical data exist for measuring similarity between products or users we present featurebased recommendation algorithms that overcome the limitations of the existing topn recommendation algorithms the experimental evaluation of the proposed algorithms in the real life data sets shows a great promise the pilot project deploying the proposed featurebased recommendation algorithms in the online retail web site shows increase in the recommendation revenue for the first month period 
automatic analysis of callcenter conversations automatic analysis of callcenter conversations we describe a system for automating callcenter analysis and monitoring our system integrates transcription of incoming calls with analysis of their content for the analysis we introduce a novel method of estimating the domainspecific importance of conversation fragments based on divergence of corpus statistics combining this method with information retrieval approaches we provide knowledgemining tools both for the callcenter agents and for administrators of the center 
a new approach to intranet search based on information extraction a new approach to intranet search based on information extraction this paper is concerned with intranet search by intranet search we mean searching for information on an intranet within an organization we have found that search needs on an intranet can be categorized into types through an analysis of survey results and an analysis of search log data the types include searching for definitions persons experts and homepages traditional information retrieval only focuses on search of relevant documents but not on search of special types of information we propose a new approach to intranet search in which we search for information in each of the special types in addition to the traditional relevance search information extraction technologies can play key roles in such kind of search by type approach because we must first extract from the documents the necessary information in each type we have developed an intranet search system called information desk in the system we try to address the most important types of search first finding term definitions homepages of groups or topics employees personal information and experts on topics for each type of search we use information extraction technologies to extract fuse and summarize information in advance the system is in operation on the intranet of microsoft and receives accesses from about employees per month feedbacks from users and system logs show that users consider the approach useful and the system can really help people to find information this paper describes the architecture features component technologies and evaluation results of the system 
a novel refinement approach for text categorization a novel refinement approach for text categorization in this paper we present a novel strategy dragpushing for improving the performance of text classifiers the strategy is generic and takes advantage of training errors to successively refine the classification model of a base classifier we describe how it is applied to generate two new classification algorithms a refined centroid classifier and a refined nave bayes classifier we present an extensive experimental evaluation of both algorithms on three english collections and one chinese corpus the results indicate that in each case the refined classifiers achieve significant performance improvement over the base classifiers used furthermore the performance of the refined centroid classifier implemented is comparable if not better to that of stateoftheart support vector machine svmbased classifier but offers a much lower computational cost 
intelligent gp fusion from multiple sources for text classification intelligent gp fusion from multiple sources for text classification this paper shows how citationbased information and structural content eg title abstract can be combined to improve classification of text documents into predefined categories we evaluate different measures of similarity five derived from the citation information of the collection and three derived from the structural content and determine how they can be fused to improve classification effectiveness to discover the best fusion framework we apply genetic programming gp techniques our experiments with the acm computing classification scheme using documents from the acm digital library indicate that gp can discover similarity functions superior to those based solely on a single type of evidence effectiveness of the similarity functions discovered through simple majority voting is better than that of contentbased as well as combinationbased support vector machine classifiers experiments also were conducted to compare the performance between gp techniques and other fusion techniques such as genetic algorithms ga and linear fusion empirical results show that gp was able to discover better similarity functions than ga or other fusion techniques 
time weight collaborative filtering time weight collaborative filtering collaborative filtering is regarded as one of the most promising recommendation algorithms the itembased approaches for collaborative filtering identify the similarity between two items by comparing users ratings on them in these approaches ratings produced at different times are weighted equally that is to say changes in user purchase interest are not taken into consideration for example an item that was rated recently by a user should have a bigger impact on the prediction of future user behaviour than an item that was rated a long time ago in this paper we present a novel algorithm to compute the time weights for different items in a manner that will assign a decreasing weight to old data more specifically the users purchase habits vary even the same user has quite different attitudes towards different items our proposed algorithm uses clustering to discriminate between different kinds of items to each item cluster we trace each users purchase interest change and introduce a personalized decay factor according to the user own purchase behaviour empirical studies have shown that our new algorithm substantially improves the precision of itembased collaborative filtering without introducing higher order computational complexity 
handling frequent updates of moving objects handling frequent updates of moving objects a critical issue in moving object databases is to develop appropriate indexing structures for continuously moving object locations so that queries can still be performed efficiently however such location changes typically cause a high volume of updates which in turn poses serious problems on maintaining index structures in this paper we propose a lazy group update lgu algorithm for diskbased index structures of moving objects lgu contains two key additional structures to group similar updates so that they can be performed together a diskbased insertion buffer ibuffer for each internal node and a memorybased deletion table dtable for the entire tree different strategies of pushing down an overflow ibuffer to the next level are studied comprehensive empirical studies over uniform and skewed datasets as well as simulated street traffic data show that lgu achieves a significant improvement on update throughput while allowing a reasonable performance for queries 
qed qed the method of assigning labels to the nodes of the xml tree is called a labeling scheme based on the labels only both ordered and unordered queries can be processed without accessing the original xml file one more important point for the labeling scheme is the label update cost in inserting or deleting a node into or from the xml tree all the current labeling schemes have high update cost therefore in this paper we propose a novel quaternary encoding approach for the labeling schemes based on this encoding approach we need not relabel any existing nodes when the update is performed extensive experimental results on the xml datasets illustrate that our qed works much better than the existing labeling schemes on the label updates when considering either the number of nodes or the time for relabeling 
detecting changes on unordered xml documents using relational databases detecting changes on unordered xml documents using relational databases several relational approaches have been proposed to detect the changes to xml documents by using relational databases these approaches store the xml documents in the relational database and issue sql queries whenever appropriate to detect the changes all of these relationalbased approaches use the schemaoblivious xml storage strategy for detecting the changes however there is growing evidence that schemaconscious storage approaches perform significantly better than schemaoblivious approaches as far as xml query processing is concerned in this paper we study a relationalbased unordered xml change detection technique called hltsmallgteliosltsmallgt that uses a schemaconscious approach sharedinlining as the underlying storage strategy hltsmallgteliosltsmallgt is up to times faster than xdiff for large datasets more than nodes it is also up to times faster than xltsmallgtandyltsmallgt the result quality of deltas detected by hltsmallgteliosltsmallgt is comparable to the result quality of deltas detected by xandy 
similarity measures for tracking information flow similarity measures for tracking information flow text similarity spans a spectrum with broad topical similarity near one extreme and document identity at the other intermediate levels of similarity resulting from summarization paraphrasing copying and stronger forms of topical relevance are useful for applications such as information flow analysis and questionanswering tasks in this paper we explore mechanisms for measuring such intermediate kinds of similarity focusing on the task of identifying where a particular piece of information originated we consider both sentencetosentence and documenttodocument comparison and have incorporated these algorithms into ltsmallgtrecapltsmallgt a prototype information flow analysis tool our experimental results with ltsmallgtrecapltsmallgt indicate that new mechanisms such as those we propose are likely to be more appropriate than existing methods for identifying the intermediate forms of similarity 
word sense disambiguation in queries word sense disambiguation in queries this paper presents a new approach to determine the senses of words in queries by using wordnet in our approach noun phrases in a query are determined first for each word in the query information associated with it including its synonyms hyponyms hypernyms definitions of its synonyms and hyponyms and its domains can be used for word sense disambiguation by comparing these pieces of information associated with the words which form a phrase it may be possible to assign senses to these words if the above disambiguation fails then other query words if exist are used by going through exactly the same process if the sense of a query word cannot be determined in this manner then a guess of the sense of the word is made if the guess has at least chance of being correct if no sense of the word has or higher chance of being used then we apply a web search to assist in the word sense disambiguation process experimental results show that our approach has applicability and accuracy on the most recent robust track of trec collection of queries we combine this disambiguation algorithm to our retrieval system to examine the effect of word sense disambiguation in text retrieval experimental results show that the disambiguation algorithm together with other components of our retrieval system yield a result which is above that produced by the same system but without the disambiguation and above that produced by using lesks algorithm our retrieval effectiveness is better than the best reported result in the literature 
erknn erknn the reverse knearest neighbors rknn queries are important in profilebased marketing information retrieval decision support and data mining systems however they are very expensive and existing algorithms are not scalable to queries in high dimensional spaces or of large values of k this paper describes an efficient estimationbased rknn search algorithm erknn which answers rknn queries based on local knndistance estimation methods the proposed approach utilizes estimationbased filtering strategy to lower the computation cost of rknn queries the results of extensive experiments on both synthetic and real life datasets demonstrate that erknn algorithm retrieves rknn efficiently and is scalable with respect to data dimensionality k and data size 
kalchas kalchas this paper outlines the system architecture and the core data structures of kalchas a fulltext search engine for xml data with emphasis on dynamic indexing and identifies features worth demonstrating the concept of dynamic index implies that the aim is to re ect the creation of deletion of and updates to relevant files in the search index as early as possible this is achieved by a number of techniques including ideas drawn from partitioned btrees and inverted indices the actual ranked retrieval of document is implemented with xmlspecific query operators for lowest common ancestor queriesa live demonstration will discuss kalchas behaviour in typical use cases such as interactive editing sessions and bulk loading large amounts of static files as well as querying the contents of the indexed files it tries to clarify both the shortcomings and the advantages of the method 
order checking in a cpoe using event analyzer order checking in a cpoe using event analyzer in this paper we present our experience in applying event analyzer a processing engine we have developed to extract patterns from a sequence of events in the checking of medical orders of a cpoe system we present some extensions we have implemented in event analyzer in order to fulfill the needs of those orders checking as well as some performance evaluation results we also outline some problems we are facing now to adapt event analyzers pattern detection engine to support streaming orders in an online cpoe checking system 
syynx solutions syynx solutions in this paper we describe the knowledge management approach for the biomedical scientific community developed by syynx solutions gmbh 
leveraging collective knowledge leveraging collective knowledge as more organizations begin to deploy taxonomies for categorization and faceted search the cost of producing these knowledge models is becoming the largest expense on a project at a cost of dollars per topic manually developing subject area taxonomies does not scale for any but the smallest of projects this paper will discuss an approach called orthogonal corpus indexing oci oci leverages existing published knowledge in the subject area of the taxonomy model this knowledge is algorithmically mapped into multiple taxonomies via the oci algorithm the resulting taxonomy costs are th of the cost of manual methods and are created with embedded rule sets for categorization engines this paper will discuss the theory of oci its practical use as well as examples of knowledge management techniques that are possible when taxonomies are large detailed and inexpensive 
taxonomies by the numbers taxonomies by the numbers in this paper we describe a system for the construction of taxonomies which yield high accuracies with automated categorization systems even on web and intranet documents in particular we describe the way in which measurement of five key features of the system can be used to predict when categories are sufficiently well defined to yield high accuracy categorization we describe the use of this system to construct a large category generalpurpose taxonomy and categorization system 
distributed pagerank computation based on iterative aggregationdisaggregation methods distributed pagerank computation based on iterative aggregationdisaggregation methods pagerank has been widely used as a major factor in search engine ranking systems however global link graph information is required when computing pagerank which causes prohibitive communication cost to achieve accurate results in distributed solution in this paper we propose a distributed pagerank computation algorithm based on iterative aggregationdisaggregation iad method with block jacobi smoothing the basic idea is divideandconquer we treat each web site as a node to explore the block structure of hyperlinks local pagerank is computed by each node itself and then updated with a low communication cost with a coordinator we prove the global convergence of the block jacobi method and then analyze the communication overhead and major advantages of our algorithm experiments on three real web graphs show that our method converges times faster than the traditional power method we believe our work provides an efficient and practical distributed solution for pagerank on large scale web graphs 
scalable summary based retrieval in pp networks scalable summary based retrieval in pp networks much of the present ppir literature is focused on distributed indexing structures within this paper we present an approach based on the replication of peer data summaries via rumor spreading and multicast in a structured overlaywe will describe rumorama a pp framework for similarity queries inspired by gloss and cori and their ppadaptation planetp rumorama achieves a hierarchization of planetplike summarybased ppir networks in a rumorama network each peer views the network as a small planetp network with connections to peers that see other small planetp networks one important aspect is that each peer can choose the size of the planetp network it wants to see according to its local processing power and bandwidth even in this adaptive environment rumorama manages to process a query such that the summary of each peer is considered exactly once in a network without churn however the actual number of peers to be contacted for a query is a small fraction of the total number of peers in the networkwithin this article we present the rumorama base protocol as well as experiments demonstrating the scalability and viability of the approach under churn 
compact reachability labeling for graphstructured data compact reachability labeling for graphstructured data testing reachability between nodes in a graph is a wellknown problem with many important applications including knowledge representation program analysis and more recently biological and ontology databases inferencing as well as xml query processing various approaches have been proposed to encode graph reachability information using node labeling schemes but most existing schemes only work well for specific types of graphs in this paper we propose a novel approach hlsshierarchical labeling of substructures which identifies different types of substructures within a graph and encodes them using techniques suitable to the characteristics of each of them we implement hlss with an efficient twophase algorithm where the first phase identifies and encodes strongly connected components as well as tree substructures and the second phase encodes the remaining reachability relationships by compressing dense rectangular submatrices in the transitive closure matrix for the important subproblem of finding densest submatrices we demonstrate the hardness of the problem and propose several practical algorithms experiments show that hlss handles different types of graphs well while existing approaches fall prey to graphs with substructures they are not designed to handle 
a formal characterization of pivotunpivot a formal characterization of pivotunpivot pivot is an important relational operation that allows data in rows to be exchanged for columns although most current relational database management systems support pivottype operations to date a purely formal algebraic characterization of pivot has been lacking in this paper we present a characterization in terms of extended relational algebra operators transpose drop projection and unique optimal tuple merge this enables us to draw parallels with pivot and existing operators employed in dynamic data mapping systems ddms formally characterize invertible pivot instances and provide complexity results for pivottype operations these contributions are an important part of ongoing work on formal models for relational olap 
a novel approach for privacypreserving video sharing a novel approach for privacypreserving video sharing to support privacypreserving video sharing we have proposed a novel framework that is able to protect the video content privacy at the individual video clip level and prevent statistical inferences from video collections to protect the video content privacy at the individual video clip level we have developed an effective algorithm to automatically detect privacysensitive video objects and video events to prevent the statistical inferences from video collections we have developed a distributed framework for privacypreserving classifier training which is able to significantly reduce the costs of data transmission and reliably limit the privacy breaches by determining the optimal size of blurred test samples for classifier validation our experiments on a specific domain of patient training and counseling videos show convincing results 
determining the semantic orientation of terms through gloss classification determining the semantic orientation of terms through gloss classification sentiment classification is a recent subdiscipline of text classification which is concerned not with the topic a document is about but with the opinion it expresses it has a rich set of applications ranging from tracking users opinions about products or about political candidates as expressed in online forums to customer relationship management functional to the extraction of opinions from text is the determination of the orientation of subjective terms contained in text ie the determination of whether a term that carries opinionated content has a positive or a negative connotation in this paper we present a new method for determining the orientation of subjective terms the method is based on the quantitative analysis of the glosses of such terms ie the definitions that these terms are given in online dictionaries and on the use of the resulting term representations for semisupervised term classification the method we present outperforms all known methods when tested on the recognized standard benchmarks for this task 
using appraisal groups for sentiment analysis using appraisal groups for sentiment analysis little work to date in sentiment analysis clcikm pairwise entity resolution pairwise entity resolution information integration is one of the oldest and most important computer science problems information from diverse sources must be combined so that users can access and manipulate the information in a unified way one of the central problems in information integration is that of entity resolution er sometimes referred to as deduplication er is the process of identifying and merging incoming records judged to represent the same realworld entityfor example consider a company that has different customer databases eg one for each subsidiary and would like to integrate them identifying matching records is challenging because there are no unique identifiers across the different sources or databases a given customer may appear in different ways in each database and there is a fair amount of guesswork in determining which customers match deciding if records match is often computationally expensive eg may involve finding maximal common subsequences in two strings how to combine matching records is often also application dependent for example say different phone numbers appear in two records to be merged in some cases we may wish to keep both of them while in others we may want to pick just one as the consolidated numberanother source of complexity is that newly merged records may match with other records for instance when we combine records r and r we may obtain a record r that now matches r the original records r and r may not match with r but because r contains more information about the same realword entity that r and r represent the connection to r may now be apparent such chained matches imply that new merged records must be recursively compared to all recordsthere are many ways to perform er but in this talk i will explore only one general approach where the decision of what records represent the same realworld entity is done in a pairwise fashion furthermore we assume that the matching is done by a blackbox function which makes our approach generic and applicable to many domains thus given two records r and r the match function mr r returns true if there is enough evidence in the two records that they both refer to the same realworld entity we also assume a blackbox merge function that combines a pair of matching recordsin this talk i will discuss the advantages and disadvantages of such a generic pairwise approach to er and even though the approach is relatively simple there are still many interesting challenges for instance how can one minimize the number of invocations to the match and merge blackboxes are there any properties of the functions that can significantly reduce the number of calls if one has available multiple processors how can one distribute the computational load if records have confidences associated with them how does the problem complexity change and how can we efficiently find the confidence of the resolved records in the talk i will address these challenges and report on some preliminary work we have done at stanford this stanford work in joint with omar benjelloun tyson condie johnson heng gong jeff jonas hideki kawai tait e larson david menestrina nicolas pombourcq qi su steven whang jennifer widomfor additional information on er and our stanford serf project please visit httpwwwdbstanfordeduserf 
how i learned to stop worrying and love the imminent internet singularity how i learned to stop worrying and love the imminent internet singularity in verner vinge introduced the notion of the singularity a step function to nearly unlimited technological capability which would be realized if the acceleration of scientific progress continues to produce such things as strong ai nanotechnology and superhuman intelligence since its introduction the idea of the singularity has been met with both evangelism by ray kurzweil and apocalyptic warnings by bill joy in this talk i will introduce a more modest version of the idea which i call the internet singularity like the original the internet singularity suggests continued acceleration of progress but makes greater emphasis on our ability to improve science analytic methods and engineering on data as opposed to the physical world i make the case for the internet singularity in four stepsfirst there is a general trend of more capabilities being more available to more people these increasing capabilities span content creation community and commerce yielding more power to todays amateur than yesterdays professional as a result the boundary between producers and consumers is becoming increasingly blurred over timesecond in many parts of the internet we see power law distributions with a heavy tail one implication of heavy tail distributions is that the aggregate impact of small participants can be greater than that of the large participantsthird with the internet comes entirely new means for authoring new and derivative works aggregations mashups tagging remixing etc the greater emphasis on collaboration and sharing yields direct and indirect network effects network effects can produce entirely new utility making online activities potentially more efficient or valuable than the offline equivalentfourth on the internet advances are effectively decoupled from the physical constraints of the offline world startups costs are smaller customer collaborator and audience pools are dramatically larger and improvements happen in more of a continuous rather than discreet manner as a result the effective clock cycle of progress is potentially much faster onlineputting these four pieces together reveals a compelling pattern more people contribute to the collective pool the collective pool contains entirely new value that is derived from its data and the new value from the data increases individual and aggregate capabilities in combination these components mutually reinforce one another forming something of a virtuous cycle this is the internet singularityconceptually if we consider engineering to be the ability to create artifacts mathematical analysis to be the ability to analyze numerical properties and science to be the pursuit of knowledge then each of these activities when focused on digital objects as they exist on the internet can be amplified in a manner consistent with the internet singularitythe implications for the internet singularity are profound as they suggest nothing less than the evolution of the scientific method itself moreover these trends also imply that now may be the best possible moment in the history of the universe to be a computer scientist 
the realtime nature and value of homeland security information the realtime nature and value of homeland security information ensuring the security of our homeland depends in large measure on two quite distinct factors having the knowledge necessary to prevent predict prepare for or respond if necessary to any manner of terrorist attack or a natural or manmade disaster and collaborating or sharing knowledge with a broad range of international federal state local and tribal agencies as well as other private or public organizations the essential problem with adequately addressing these factors despite the many advancements made in the past decade is twofold first it is not so much the mass but rather the diffuse nature and complexity of the data information and knowledge required for understanding terrorism and accounting for the manifold consequences of disasters that make possession of the right knowledge difficult and second that diffuseness and complexity is magnified by the extreme diversity and wide distribution of the many potential homeland security collaborators retrospective analysis and even knowledge discovery is less useful under these conditions than prospective realtime synthesis of information for multiple users also privacy is as important as if not more important than security this suggests that database designs and techniques for information retrieval and knowledge management must take advantage of such technologies as semantic nets visualization and discrete mathematics to build knowledge systems capable for homeland security applications 
efficient processing of complex similarity queries in rdbms through query rewriting efficient processing of complex similarity queries in rdbms through query rewriting multimedia and complex data are usually queried by similarity predicates whereas there are many works dealing with algorithms to answer basic similarity predicates there are not generic algorithms able to efficiently handle similarity complex queries combining several basic similarity predicates in this work we propose a simple and effective set of algorithms that can be combined to answer complex similarity queries and a set of algebraic rules useful to rewrite similarity query expressions into an adequate format for those algorithms those rules and algorithms allow relational database management systems to turn complex queries into efficient query execution plans we present experiments that highlight interesting scenarios they show that the proposed algorithms are orders of magnitude faster than the traditional similarity algorithms moreover they are linearly scalable considering the database size 
distributed spatiotemporal similarity search distributed spatiotemporal similarity search in this paper we introduce the distributed spatiotemporal similarity search problem given a query trajectory q we want to find the trajectories that follow a motion similar to q when each of the target trajectories is segmented across a number of distributed nodes we propose two novel algorithms ubk and ublbk which combine local computations of lower and upper bounds on the matching between the distributed subsequences and q such an operation generates the desired result without pulling together all the distributed subsequences over the fundamentally expensive communication medium our solutions find applications in a wide array of domains such as cellular networks wild life monitoring and video surveillance our experimental evaluation using realistic data demonstrates that our framework is both efficient and robust to a variety of conditions 
structurebased querying of proteins using wavelets structurebased querying of proteins using wavelets the ability to retrieve molecules based on structural similarity has use in many applications from disease diagnosis and treatment to drug discovery and design in this paper we present a method to represent protein molecules that allows for the fast flexible and efficient retrieval of similar structures based on either global or local attributes we begin by computing the pairwise distance between amino acids transforming each d structure into a d distance matrix we normalize this matrix to a specific size and apply a d wavelet decomposition to generate a set of approximation coefficients which serves as our global feature vector this transformation reduces the overall dimensionality of the data while still preserving spatial features and correlations we test our method by running queries on three different protein data sets that have been used previously in the literature basing our comparisons on labels taken from the scop database we find that our method significantly outperforms existing approaches in terms of retrieval accuracy memory utilization and execution time specifically using a kd tree and running a nearestneighbor search on a dataset of proteins against itself we see an average accuracy of at the scop superfamily level and a total query time that is up to times faster than previously published techniques in addition to processing queries based on global similarity we also propose innovative extensions to effectively match proteins based solely on shared local substructures allowing for a more flexible query interface 
an approximate multiword matching algorithm for robust document retrieval an approximate multiword matching algorithm for robust document retrieval document generation from low level data and its utilization is one of the most challenging tasks in document engineering word occurrence detection is a fundamental problem in the recognized document utilization obtained by a recognizbstract document generation from low level data and its utilization is one of the most challenging tasks in document engineering word occurrence detection is a fundamental problem in the recognized document utilization obtained by a recognizer such as ocr and speech recognition given a set of words such as a dictionary this paper proposes an efficient dynamic programming dp algorithm to find the occurrences of each word in a text in this paper the string similarity is measured by a statistical similarity model that enables a definition of the similarities in the character level as well as edit operation level the proposed algorithm uses tree structures to measure similarities in order to avoid measuring similarities of the same substrings appearing in different parts of the text and words the time complexity of the proposed algorithm is ioiiwiisiiqi where iwi resp isi denote the number of nodes in the trees representing the word set resp the text and iqi donotes the number of the states of the model used for string similarity this paper shows the proposed algorithm is experimentally about six times faster than a naive dp algorithm 
movie review mining and summarization movie review mining and summarization p classabstract with the flourish of the web online review is becoming a more and more useful and important information resource for people as a result automatic review mining and summarization has become a hot research topic recently different from traditional text summarization review mining and summarization aims at extracting the features on which the reviewers express their opinions and determining whether the opinions are positive or negative in this paper we focus on a specific domain movie review a multiknowledge based approach is proposed which integrates wordnet statistical analysis and movie knowledge the experimental results show the effectiveness of the proposed approach in movie review mining and summarization 
utility scoring of product reviews utility scoring of product reviews p classabstract we identify a new task in the ongoing research in text sentiment analysis predicting utility of product reviews which is orthogonal to polarity classification and opinion extraction we build regression models by incorporating a diverse set of features and achieve highly competitive performance for utility scoring on three realworld data sets 
mining blog stories using communitybased and temporal clustering mining blog stories using communitybased and temporal clustering p classabstract in recent years weblogs or blogs for short have become an important form of online content the personal nature of blogs online interactions between bloggers and the temporal nature of blog entries differentiate blogs from other kinds of web content bloggers interact with each other by linking to each others posts thus forming online communities within these communities bloggers engage in discussions of certain issues through entries in their blogs since these discussions are often initiated in response to online or offline events a discussion typically lasts for a limited time duration we wish to extract such temporal discussions or istoriesi occurring within blogger communities based on some query keywords we propose a icontentcommunitytimei model that can leverage the content of entries their timestamps and the community structure of the blogs to automatically discover stories doing so also allows us to discover ihoti stories we demonstrate the effectiveness of our model through several case studies using realworld data collected from the blogosphere 
per we introduce a novel concept coined eigentrend to represent the temporal trend in a group of blogs with common interests and propose two new techniques for extracting eigentrends in blogs first we propose a trend analysis technique based on the singular value decomposition extracted eigentrends provide new insights into multiple trends on the same keyword second we propose another trend analysis technique based on a higherorder singular value decomposition this analyzes the blogosphere as a dynamic graph structure and extracts eigentrends that reflect the structural changes of the blogosphere over time experimental studies based on synthetic data sets and a real blog data set show that our new techniques can reveal a lot of interesting trend information and insights in the blogosphere that are not obtainable from traditional countbased methods 
on gmap on gmap as an alternative to the usual mean average precision some use is currently being made of the geometric mean average precision gmap as a measure of average search effectiveness across topics gmap is specifically used to emphasise the lower end of the average precision scale in order to shed light on poor performance of search engines this paper discusses the status of this measure and how it should be understood 
investigating the exhaustivity dimension in contentoriented xml element retrieval evaluation investigating the exhaustivity dimension in contentoriented xml element retrieval evaluation inex the evaluation initiative for contentoriented xml retrieval has since its establishment defined the relevance of an element according to two graded dimensions exhaustivity and specificity the former measures how exhaustively an xml element discusses the topic of request whereas specificity measures how focused the element is on the topic of request the reason for having two dimensions was to provide a more stable measure of relevance than if assessors were asked to rate the relevance of an element on a single scale however obtaining relevance assessments is a costly task as each document must be assessed for relevance by a human assessor in xml retrieval this problem is exacerbated as the elements of the document must also be assessed with respect to the exhaustivity and specificity dimensions a continuous discussion in inex has been whether such a sophisticated definition of relevance and in particular the exhaustivity dimension was needed this paper attempts to answer this question through extensive statistical tests to compare the conclusions about system performance that could be made under different assessment scenarios 
evaluation by comparing result sets in context evaluation by comparing result sets in context familiar evaluation methodologies for information retrieval ir are not well suited to the task of comparing systems in many real settings these systems and evaluation methods must support contextual interactive retrieval over changing heterogeneous data collections including private and confidential informationwe have implemented a comparison tool which can be inserted into the natural ir process it provides a familiar search interface presents a small number of result sets in sidebyside panels elicits searcher judgments and logs interaction events the tool permits study of real information needs as they occur uses the documents actually available at the time of the search and records judgments taking into account the instantaneous needs of the searcherwe have validated our proposed evaluation approach and explored potential biases by comparing different wholeofweb search facilities using a webbased version of the tool in four experiments one with supplied queries in the laboratory and three with real queries in the workplace subjects showed no discernable leftright bias and were able to reliably distinguish between high and lowquality result sets we found that judgments were strongly predicted by simple implicit measuresfollowing validation we undertook a case study comparing two leading wholeofweb search engines the approach is now being used in several ongoing investigations 
estimating average precision with incomplete and imperfect judgments estimating average precision with incomplete and imperfect judgments we consider the problem of evaluating retrieval systems using incomplete judgment information buckley and voorhees recently demonstrated that retrieval systems can be efficiently and effectively evaluated using incomplete judgments via the bpref measure when relevance judgments are complete the value of bpref is an approximation to the value of average precision using complete judgments however when relevance judgments are incomplete the value of bpref deviates from this value though it continues to rank systems in a manner similar to average precision evaluated with a complete judgment set in this work we propose three evaluation measures that are approximations to average precision even when the relevance judgments are incomplete and are more robust to incomplete or imperfect relevance judgments than bpref the proposed estimates of average precision are simple and accurate and we demonstrate the utility of these estimates using trec data 
window join approximation over data streams with importance semantics window join approximation over data streams with importance semantics load shedding techniques generate approximate sliding window join results when memory constraints prevent exact computation the previously proposed random load shedding method drops input tuples without consideration for the number of outputs created while the recently proposed semantic load shedding technique aims to produce the largest possible result set we consider a new model in which data stream tuples contain numerical importance values relevant to the query source and seek to maximize the importance of the approximate join result we show that both random load shedding and semantic load shedding are suboptimal in this situation while the techniques presented in this paper satisfy the objective function by considering both tuple importance and join attribute distributions we extend the existing offline semantic approximation technique to make it compatible with our objective function and show that it is less space and time efficient than our new optimal offline algorithm for small and large join memory allotments we also introduce four efficient online algorithms which are quite promising in maximizing the importance of the approximate join result without foreknowledge of input streams 
adaptive nonlinear clustering in data streams adaptive nonlinear clustering in data streams data stream clustering has emerged as a challenging and interesting problem over the past few years due to the evolving nature and onepass restriction imposed by the data stream model traditional clustering algorithms are inapplicable for stream clustering this problem becomes even more challenging when the data is highdimensional and the clusters are not linearly separable in the input space in this paper we propose a nonlinear stream clustering algorithm that adapts to the streams evolutionary changes using the kernel methods for dealing with the nonlinearity of data separation we propose a novel tier stream clustering architecture tier captures the temporal locality in the stream by partitioning it into segments using a kernelbased novelty detection approach tier exploits this segment structure to continuously project the streaming data nonlinearly onto a lowdimensional space lds before assigning them to a cluster we demonstrate the effectiveness of our approach through extensive experimental evaluation on various realworld datasets 
classification spanning correlated data streams classification spanning correlated data streams in many applications classifiers need to be built based on multiple related data streams for example stock streams and news streams are related where the classification patterns may involve features from both streams thus instead of mining on a single isolated stream we need to examine multiple related data streams in order to find such patterns and build an accurate classifier other examples of related streams include traffic reports and car accidents sensor readings of different types or at different locations etc in this paper we consider the classification problem defined over slidingwindow join of several input data streams as the data streams arrive in fast pace and the manytomany join relationship blows up the data arrival rate even more it is impractical to compute the join and then build the classifier each time the window slides forward we present an efficient algorithm to build a nave bayesian classifier in such context our method does not need to perform the join operations but is still able to build exactly the same classifier as if built on the joined result it only examines each input tuple twice independent of the number of tuples it joins in other streams therefore is able to keep pace with the fast arriving data streams in the presence of manytomany join relationships the experiments confirmed that our classification algorithm is more efficient than conventional methods while maintaining good classification accuracy 
validating associations in biological databases validating associations in biological databases erroneous data can often be found in databases and detecting it is normally a nontrivial task for example to cope with the large amount of biological sequences being produced a significant number of genes and proteins have been annotated by automated tools a protein annotation is an association between a protein and a term describing its role these tools have produced a significant number of misannotations that are now present in biological databases this paper proposes a new method for automatically scoring associations by comparing them to preexisting curated associations an association is a pair that links two entities the score can be used to filter incorrect or uncommon associationswe evaluated the method using the automated protein annotations submitted to biocreative an international evaluation of stateoftheart textmining systems in biology the method scored each of these annotations and those scored below a certain threshold were discarded the results have shown a small tradeoff in recall for a large improvement in precision for example we were able to discard and of the misannotations maintaining and of the correct annotations respectively moreover we were able to outperform each individual submission to biocreative by proper adjustment of the threshold 
finding highly correlated pairs efficiently with powerful pruning finding highly correlated pairs efficiently with powerful pruning we consider the problem of finding highly correlated pairs in a large data set that is given a threshold not too small we wish to report all the pairs of items or binary attributes whose pearson correlation coefficients are greater than the threshold correlation analysis is an important step in many statistical and knowledgediscovery tasks normally the number of highly correlated pairs is quite small compared to the total number of pairs identifying highly correlated pairs in a naive way by computing the correlation coefficients for all the pairs is wasteful with massive data sets where the total number of pairs may exceed the mainmemory capacity the computational cost of the naive method is prohibitive in their kdd paper hui xiong et al address this problem by proposing the taper algorithm the algorithm goes through the data set in two passes it uses the first pass to generate a set of candidate pairs whose correlation coefficients are then computed directly in the second pass the efficiency of the algorithm depends greatly on the selectivity pruning power of its candidategenerating stagein this work we adopt the general framework of the taper algorithm but propose a different candidategeneration method for a pair of items tapers candidategeneration method considers only the frequencies supports of individual items our method also considers the frequency support of the pair but does not explicitly count this frequency support we give a simple randomized algorithm whose falsenegative probability is negligible the space and time complexities of generating the candidate set in our algorithm are asymptotically the same as tapers we conduct experiments on synthesized and real data the results show that our algorithm produces a greatly reduced candidate set one that can be several orders of magnitude smaller than that generated by taper because of this our algorithm uses much less memory and can be faster the former is critical for dealing with massive data 
mining compressed commodity workflows from massive rfid data sets mining compressed commodity workflows from massive rfid data sets radio frequency identification rfid technology is fast becoming a prevalent tool in tracking commodities in supply chain management applications the movement of commodities through the supply chain forms a gigantic workflow that can be mined for the discovery of trends flow correlations and outlier paths that in turn can be valuable in understanding and optimizing business processesin this paper we propose a method to construct compressed probabilistic workflows that capture the movement trends and significant exceptions of the overall data sets but with a size that is substantially smaller than that of the complete rfid workflow compression is achieved based on the following observations only a relatively small minority of items deviate from the general trend only truly nonredundant deviations ie those that substantially deviate from the previously recorded ones are interesting and although rfid data is registered at the primitive level data analysis usually takes place at a higher abstraction level techniques for workflow compression based on nonredundant transition and emission probabilities are derived and an algorithm for computing approximate path probabilities is developed our experiments demonstrate the utility and feasibility of our design data structure and algorithms 
discovering and exploiting keyword and attributevalue cooccurrences to improve pp routing indices discovering and exploiting keyword and attributevalue cooccurrences to improve pp routing indices peertopeer pp search requires intelligent decisions for query routing selecting the best peers to which a given query initiated at some peer should be forwarded for retrieving additional search results these decisions are based on statistical summaries for each peer which are usually organized on a perkeyword basis and managed in a distributed directory of routing indices such architectures disregard the possible correlations among keywords together with the coarse granularity of perpeer summaries which are mandated for scalability this limitation may lead to poor search result qualitythis paper develops and evaluates two solutions to this problem skstat based on singlekey statistics only and mkstat based on additional multikey statistics for both cases hash sketch synopses are used to compactly represent a peers data items and are efficiently disseminated in the pp network to form a decentralized directory experimental studies with gnutella and web data demonstrate the viability and the tradeoffs of the approaches 
a documentcentric approach to static index pruning in text retrieval systems a documentcentric approach to static index pruning in text retrieval systems we present a static index pruning method to be used in adhoc document retrieval tasks that follows a documentcentric approach to decide whether a posting for a given term should remain in the index or not the decision is made based on the terms contribution to the documents kullbackleibler divergence from the text collections global language model our technique can be used to decrease the size of the index by over at only a minor decrease in retrieval effectiveness it thus allows us to make the index small enough to fit entirely into the main memory of a single pc even for large text collections containing millions of documents this results in great efficiency gains superior to those of earlier pruning methods and an average response time around ms on the gov document collection 
pruning strategies for mixedmode querying pruning strategies for mixedmode querying web information retrieval systems face a range of unique challenges not the least of which is the sheer scale of the data that must be handled also specific to web retrieval is that queries may be a mix of boolean and ranked features and documents may have static score components that must also be factored into the ranking process in this paper we consider a range of query semantics used in web retrieval systems and show that impactsorted indexes provide support for dynamic pruning mechanisms and in doing so allow fast documentatatime resolution of typical mixedmode queries even on relatively large volumes of data our techniques also extend to more complex query semantics including the use of phrase proximity and structural constraints 
dstring dstring classification of d objects remains an important task in many areas of data management such as engineering medicine or biology as a common preprocessing step in current approaches to classification of voxelized d objects voxel representations are transformed into a feature vector descriptionin this article we introduce an approach of transforming d objects into feature strings which represent the distribution of voxels over the voxel grid attractively this feature string extraction can be performed in linear runtime with respect to the number of voxels we define a similarity measure on these feature strings that counts common kmers in two input strings which is referred to as the spectrum kernel in the field of kernel methods we prove that on our feature strings this similarity measure can be computed in time linear to the number of different characters in these strings this linear runtime behavior makes our kernel attractive even for large datasets that occur in many application domains furthermore we explain that our similarity measure induces a metric which allows to combine it with an mtree for handling of large volumes of data classification experiments on two published benchmark datasets show that our novel approach is competitive with the best stateoftheart methods for d object classification 
effective and efficient classification on a searchengine model effective and efficient classification on a searchengine model traditional document classification frameworks which apply the learned classifier to each document in a corpus one by one are infeasible for extremely large document corpora like the web or large corporate intranets we consider the classification problem on a corpus that has been processed primarily for the purpose of searching and thus our access to documents is solely through the inverted index of a large scale search engine our main goal is to build the best short query that characterizes a document class using operators normally available within large engines we show that surprisingly good classification accuracy can be achieved on average over multiple classes by queries with as few as terms moreover we show that optimizing the efficiency of query execution by careful selection of these terms can further reduce the query costs more precisely we show that on our setup the best terms query canachieve of the accuracy of the best svm classifier terms and if we are willing to tolerate a reduction to of the best svm we can build a terms query that can be executed more than twice as fast as the best terms query 
multievidence multicriteria lazy associative document classification multievidence multicriteria lazy associative document classification we present a novel approach for classifying documents that combines different pieces of evidence eg textual features of documents links and citations transparently through a data mining technique which generates rules associating these pieces of evidence to predefined classes these rules can contain any number and mixture of the available evidence and are associated with several quality criteria which can be used in conjunction to choose the best rule to be applied at classification time our method is able to perform evidence enhancement by link forwardingbackwarding ie navigating among documents related through citation so that new pieces of linkbased evidence are derived when necessary furthermore instead of inducing a single model or rule set that is good on average for all predictions the proposed approach employs a lazy method which delays the inductive process until a document is given for classification therefore taking advantage of better qualitative evidence coming from the document we conducted a systematic evaluation of the proposed approach using documents from the acm digital library and from a brazilian web directory our approach was able to outperform in both collections all classifiers based on the best available evidence in isolation as well as stateoftheart multievidence classifiers we also evaluated our approach using the standard webkb collection where our approach showed gains of in accuracy being times faster further our approach is extremely efficient in terms of computational performance showing gains of more than one order of magnitude when compared against other multievidence classifiers 
knowing a web page by the company it keeps knowing a web page by the company it keeps web page classification is important to many tasks in information retrieval and web mining however applying traditional textual classifiers on web data often produces unsatisfying results fortunately hyperlink information provides important clues to the categorization of a web page in this paper an improved method is proposed to enhance web page classification by utilizing the class information from neighboring pages in the link graph the categories represented by four kinds of neighbors parents children siblings and spouses are combined to help with the page in question in experiments to study the effect of these factors on our algorithm we find that the method proposed is able to boost the classification accuracy of common textual classifiers from around to more than on a large dataset of pages from the open directory project and outperforms existing algorithms unlike prior techniques our approach utilizes samehost links and can improve classification accuracy even when neighboring pages are unlabeled finally while all neighbor types can contribute sibling pages are found to be the most important 
improving novelty detection for general topics using sentence level information patterns improving novelty detection for general topics using sentence level information patterns the detection of new information in a document stream is an important component of many potential applications in this work a new novelty detection approach based on the identification of sentence level information patterns is proposed first the informationpattern concept for novelty detection is presented with the emphasis on new information patterns for general topics queries that cannot be simply turned into specific questions whose answers are specific named entities nes then we elaborate a thorough analysis of sentence level information patterns on data from the trec novelty tracks including sentence lengths named entities sentence level opinion patterns this analysis provides guidelines in applying those patterns in novelty detection particularly for the general topics finally a unified patternbased approach is presented to novelty detection for both general and specific topics the new method for dealing with general topics will be the focus experimental results show that the proposed approach significantly improves the performance of novelty detection for general topics as well as the overall performance for all topics from the trec novelty tracks 
topic evolution and social interactions topic evolution and social interactions we propose a method for discovering the dependency relationships between the topics of documents shared in social networks using the latent social interactions attempting to answer the question given a seemingly new topic from where does this topic evolve in particular we seek to discover the pairwise probabilistic dependency in topics of documents which associate social actors from a latent social network where these documents are being shared by viewing the evolution of topics as a markov chain we estimate a markov transition matrix of topics by leveraging social interactions and topic semantics metastable states in a markov chain are applied to the clustering of topics applied to the citeseer dataset a collection of documents in academia we show the trends of research topics how research topics are related and which are stable we also show how certain social actors authors impact these topics and propose new ways for evaluating author impact 
a fast and robust method for web page template detection and removal a fast and robust method for web page template detection and removal the widespread use of templates on the web is considered harmful for two main reasons not only do they compromise the relevance judgment of many web ir and web mining methods such as clustering and classification but they also negatively impact the performance and resource usage of tools that process web pages in this paper we present a new method that efficiently and accurately removes templates found in collections of web pages our method works in two steps first the costly process of template detection is performed over a small set of sample pages then the derived template is removed from the remaining pages in the collection this leads to substantial performance gains when compared to previous approaches that combine template detection and removal we show through an experimental evaluation that our approach is effective for identifying terms occurring in templates obtaining fmeasure values around and that it also boosts the accuracy of web page clustering and classification methods 
outofcontext noun phrase semantic interpretation with crosslinguistic evidence outofcontext noun phrase semantic interpretation with crosslinguistic evidence the acquisition of semantic knowledge is paramount for any application that requires a deep understanding of natural language text motivated by the problem of building a noun phraselevel semantic parser and adapting it to various applications such as machine translation and multilingual question answering in this paper we present a domainindependent model for noun phrase semantic interpretation we investigate the problem based on crosslinguistic evidence from a set of four romance languages spanish italian french and romanian the focus on romance languages is well motivated it is generally the case that english noun phrases translate into constructions of the form n p n in romance languages where as we will show the p preposition varies in ways that correlate with the semantics thus based on a set of semantic interpretation categories such as partwhole agent possession we present empirical observations regarding the distribution of these semantic categories in a crosslingual corpus and their mapping to various syntactic constructions in english and romance furthermore given a training set of english noun phrases along with their translations in the four romance languages our algorithm automatically learns classification rules and applies them to unseen noun phrase instances for semantic interpretation experimental results are compared against a stateoftheart model reported in the literature 
capturing community search expertise for personalized web search using snippetindexes capturing community search expertise for personalized web search using snippetindexes we describe and evaluate an approach to capturing and reusing search expertise within a community of like minded searchers such as the employees of a company or organisation within knowledge based industries search expertise the ability to quickly and accurately locate information according to a specific information need is an important corporate asset and in our approach we attempt to capture this knowledge by mining the title and snippet texts of results that have been selected by community members in response to their queries our assumption is that the snippet text of a result must play a role in helping users to judge the initial relevance of that result and that the snippet terms of selected results must contain especially informative terms about the goals and preferences of the searchers in other words results are selected because the user recognises certain combinations of terms in their snippets which are related to their information needs our approach seeks to build a communitybased snippet index that reflects the evolving interests of a group of searchers this index is then used to rerank the results returned by some underlying search engine by boosting the ranking of key results that have been frequently selected for similar queries by community members in the past 
summarizing local context to personalize global web search summarizing local context to personalize global web search the pc desktop is a very rich repository of personal information efficiently capturing users interests in this paper we propose a new approach towards an automatic personalization of web search in which the user specific information is extracted from such local desktops thus allowing for an increased quality of user profiling while sharing less private information with the search engine more specifically we investigate the opportunities to select personalized query expansion terms for web search using three different desktop oriented approaches summarizing the entire desktop data summarizing only the desktop documents relevant to each user query and applying natural language processing techniques to extract dispersive lexical compounds from relevant desktop resources our experiments with the google api showed at least the latter two techniques to produce a very strong improvement over current web search 
a study on the effects of personalization and task information on implicit feedback performance a study on the effects of personalization and task information on implicit feedback performance while implicit relevance feedback irf algorithms exploit users interactions with information to customize support offered to users of search systems it is unclear how individual and task differences impact the effectiveness of such algorithms in this paper we describe a study on the effect on retrieval performance of using additional information about the user and their search tasks when developing irf algorithms we tested four algorithms that use document display time to estimate relevance and tailored the threshold times ie the time distinguishing relevance from nonrelevance to the task the user a combination of both or neither interaction logs gathered during a longitudinal naturalistic study of online informationseeking behavior are used as stimuli for the algorithms the findings show that tailoring display time thresholds based on task information improves irf algorithm performance but doing so based on user information worsens performance this has implications for the development of effective irf algorithms 
incorporating query difference for learning retrieval functions in world wide web search incorporating query difference for learning retrieval functions in world wide web search we discuss information retrieval methods that aim at serving a diverse stream of user queries such as those submitted to commercial search engines we propose methods that emphasize the importance of taking into consideration of query difference in learning effective retrieval functions we formulate the problem as a multitask learning problem using a risk minimization framework in particular we show how to calibrate the empirical risk to incorporate query difference in terms of introducing nuisance parameters in the statistical models and we also propose an alternating optimization method to simultaneously learn the retrieval function and the nuisance parameters we work out the details for both l and l regularization cases and provide convergence analysis for the alternating optimization method for the special case when the retrieval functions belong to a reproducing kernel hilbert space we illustrate the effectiveness of the proposed methods using modeling data extracted from a commercial search engine we also point out how the current framework can be extended in future research 
kddcs kddcs we propose an innetwork datacentric storage indcs scheme for answering adhoc queries in sensor networks previously proposed innetwork storage ins schemes suffered from storage hotspots that are formed if either the sensors locations are not uniformly distributed over the coverage area or the distribution of sensor readings is not uniform over the range of possible reading values our kd tree based datacentric storage kddcs scheme maintains the invariant that the storage of events is distributed reasonably uniformly among the sensors kddcs is composed of a set of distributed algorithms whose running time is within a polylog factor of the diameter of the network the number of messages any sensor has to send as well as the bits in those messages is polylogarithmic in the number of sensors load balancing in kddcs is based on defining and distributively solving a theoretical problem that we call the weighted split median problem in addition to analytical bounds on kddcs individual algorithms we provide experimental evidence of our schemes general efficiency as well as its ability to avoid the formation of storage hotspots of various sizes unlike all previous indcs schemes 
efficient rangeconstrained similarity search on wavelet synopses over multiple streams efficient rangeconstrained similarity search on wavelet synopses over multiple streams due to the resource limitation in the data stream environment it has been reported that answering user queries according to the wavelet synopsis of a stream is an essential ability of a data stream management system dsms in this paper motivated by the fact that a user may be interested in an arbitrary range of the data streams we investigate two important types of rangeconstrained queries in time series streaming environments the distance queries which aim at obtaining the euclidean distance between two streams and the knn queries which aim at discovering k nearest neighbors to a reference stream to achieve high efficiency in processing these two types of queries we propose procedure red standing for rangeconstrained euclidean distance and algorithm eks standing for enhanced knn search compared to the existing methods in the prior research the advantageous features of our approaches are in two folds first our approaches are capable of processing the queries directly from the wavelet synopses retained in the main memory without using idwt to reconstruct the data cells this feature allows us to save the complexity in both memory and time moreover our approaches enable the users to query the dsms within their range of interest unlike the conventional methods which only support the fullrange query processing this feature will enhance the flexibility at the client side we evaluate procedure red and algorithm eks on live and synthetic datasets empirically and show that the proposed approaches are efficient in similarity search and knn discovery within arbitrary ranges in the time series streaming environments 
a data stream language and system designed for power and extensibility a data stream language and system designed for power and extensibility by providing an integrated and optimized support for userdefined aggregates udas data stream management systems dsms can achieve superior power and generality while preserving compatibility with current sql standards this is demonstrated by the stream mill system that through is expressive stream language esl efficiently supports a wide range of applications including very advanced ones such as data stream mining streaming xml processing timeseries queries and rfid event processing esl supports physical and logical windows with optional slides and tumbles on both builtin aggregates and udas using a simple framework that applies uniformly to both aggregate functions written in an external procedural languages and those natively written in esl the constructs introduced in esl extend the power and generality of dsms and are conducive to udaspecific optimization and efficient execution as demonstrated by several experiments 
in search of meaning for time series subsequence clustering in search of meaning for time series subsequence clustering recent papers have claimed that the result of kmeans clustering for time series subsequences sts clustering is independent of the time series that created it our paper revisits this claim in particular we consider the following question given several time series sequences and a set of sts cluster centroids from one of them generated by the kmeans algorithm is it possible to reliably determine which of the sequences produced these cluster centroids while recent results suggest that the answer should be no we answer this question in the affirmativewe present cluster shape distance an alternate distance measure for time series subsequence clusters based on cluster shapes given a set of clusters its shape is the sorted list of the pairwise euclidean distances between their centroids we then present two algorithms based on this distance measure which match a set of sts cluster centroids with the time series that produced it while the first algorithm creates dqg reuse this term more smaller fingerprints for the sequences the second is more accurate in our experiments with a dataset of sequences it produced a correct match of the timefurthermore we offer an analysis that explains why our cluster shape distance provides a reliable way to match sts clusters to the original sequences whereas cluster set distance fails to do so our work establishes for the first time a strong relation between the result of kmeans sts clustering and the time series sequence that created it despite earlier predictions that this is not possible 
incremental hierarchical clustering of text documents incremental hierarchical clustering of text documents incremental hierarchical text document clustering algorithms are important in organizing documents generated from streaming online sources such as newswire and blogs however this is a relatively unexplored area in the text document clustering literature popular incremental hierarchical clustering algorithms namely cobweb and classit have not been widely used with text document data we discuss why in the current form these algorithms are not suitable for text clustering and propose an alternative formulation that includes changes to the underlying distributional assumption of the algorithm in order to conform with the data both the original classit algorithm and our proposed algorithm are evaluated using reuters newswire articles and ohsumed dataset 
efficiently clustering transactional data with weighted coverage density efficiently clustering transactional data with weighted coverage density it is widely recognized that developing efficient and fully automated algorithms for clustering large transactional datasets is a challenging problem in this paper we propose a fast memoryefficient and scalable clustering algorithm for analyzing transactional data our approach has three unique features first we use the concept of weighted coverage density as a categorical similarity measure for efficient clustering of transactional datasets the concept of weighted coverage density is intuitive and allows the weight of each item in a cluster to be changed dynamically according to the occurrences of items second we develop two transactional data clustering specific evaluation metrics based on the concept of large transactional items and the coverage density respectively third we implement the weighted coverage density clustering algorithm and the two clustering validation metrics using a fully automated transactional clustering framework called scale sampling clustering structure assessment clustering and domainspecific evaluation the scale framework is designed to combine the weighted coverage density measure for clustering over a sample dataset with selfconfiguring methods that can automatically tune the two important parameters of the clustering algorithms the candidates of the best number k of clusters and the application of two domainspecific cluster validity measures to find the best result from the set of clustering results we have conducted experimental evaluation using both synthetic and real datasets and our results show that the weighted coverage density approach powered by the scale framework can efficiently generate high quality clustering results in a fully automated manner 
ranking web objects from multiple communities ranking web objects from multiple communities vertical search is a promising direction as it leverages domainspecific knowledge and can provide more precise information for users in this paper we study the web objectranking problem one of the key issues in building a vertical search engine more specifically we focus on this problem in cases when objects lack relationships between different web communities and take highquality photo search as the test bed for this investigation we proposed two score fusion methods that can automatically integrate as many web communities web forums with rating information as possible the proposed fusion methods leverage the hidden links discovered by a duplicate photo detection algorithm and aims at minimizing score differences of duplicate photos in different forums both intermediate results and user studies show the proposed fusion methods are practical and efficient solutions to web object ranking in cases we have described though the experiments were conducted on highquality photo ranking the proposed algorithms are also applicable to other ranking problems such as movie ranking and music ranking 
voting for candidates voting for candidates in an expert search task the users need is to identify people who have relevant expertise to a topic of interest an expert search system predicts and ranks the expertise of a set of candidate persons with respect to the users query in this paper we propose a novel approach for predicting and ranking candidate expertise with respect to a query we see the problem of ranking experts as a voting problem which we model by adapting eleven data fusion techniqueswe investigate the effectiveness of the voting approach and the associated data fusion techniques across a range of document weighting models in the context of the trec enterprise track the evaluation results show that the voting paradigm is very effective without using any collection specific heuristics moreover we show that improving the quality of the underlying document representation can significantly improve the retrieval performance of the data fusion techniques on an expert search task in particular we demonstrate that applying fieldbased weighting models improves the ranking of candidates finally we demonstrate that the relative performance of the adapted data fusion techniques for the proposed approach is stable regardless of the used weighting models 
bayesian adaptive user profiling with explicit implicit feedback bayesian adaptive user profiling with explicit implicit feedback research in information retrieval is now moving into a personalized scenario where a retrieval or filtering system maintains a separate user profile for each user in this framework information delivered to the user can be automatically personalized and catered to individual users information needs however a practical concern for such a personalized system is the cold start problem any user new to the system must endure poor initial performance until sufficient feedback from that user is providedto solve this problem we use both explicit and implicit feedback to build a users profile and use bayesian hierarchical methods to borrow information from existing users we analyze the usefulness of implicit feedback and the adaptive performance of the model on two data sets gathered from user studies where users interaction with a document or implicit feedback were recorded along with explicit feedback our results are twofold first we demonstrate that the bayesian modeling approach effectively trades off between shared and userspecific information alleviating poor initial performance for each user second we find that implicit feedback has very limited unstable predictive value by itself and only marginal value when combined with explicit feedback 
salsa salsa skyline queries compute the set of paretooptimal tuples in a relation ie those tuples that are not dominated by any other tuple in the same relation although several algorithms have been proposed for efficiently evaluating skyline queries they either require to extend the relational server with specialized access methods which is not always feasible or have to perform the dominance tests on all the tuples in order to determine the result in this paper we introduce salsa sort and limit skyline algorithm which exploits the sorting machinery of a relational engine to order tuples so that only a subset of them needs to be examined for computing the skyline result this makes salsa particularly attractive when skyline queries are executed on top of systems that do not understand skyline semantics or when the skyline logic runs on clients with limited power andor bandwidth 
constrained subspace skyline computation constrained subspace skyline computation in this paper we introduce the problem of constrained subspace skyline queries this class of queries can be thought of as a generalization of subspace skyline queries using range constraints although both constrained skyline queries and subspace skyline queries have been addressed previously the implications of constrained subspace skyline queries has not been examined so far constrained skyline queries are usually more expensive than regular skylines in case of constrained subspace skyline queries additional performance degradation is caused through the projection in order to support constrained skylines for arbitrary subspaces we present approaches exploiting multiple lowdimensional indexes instead of relying on a single highdimensional index effective pruning strategies are applied to discard points from dominated regions an important ingredient of our approach is the workloadadaptive strategy for determining the number of indexes and the assignment of dimensions to the indexes extensive performance evaluation shows the superiority of our proposed technique compared to its most related competitors 
processing relaxed skylines in pdms using distributed data summaries processing relaxed skylines in pdms using distributed data summaries peer data management systems pdms are a natural extension of heterogeneous database systems one of the main tasks in such systems is efficient query processing insisting on complete answers however leads to asking almost every peer in the network relaxing these completeness requirements by applying approximate query answering techniques can significantly reduce costs since most users are not interested in the exact answers to their queries rankaware query operators like topk or skyline play an important role in query processing in this paper we present the novel concept of relaxed skylines that combines the advantages of both rankaware query operators and approximate query processing techniques furthermore we propose a strategy for processing relaxed skylines in distributed environments that allows for giving guarantees for the completeness of the result using distributed data summaries as routing indexes 
on the structural properties of massive telecom call graphs on the structural properties of massive telecom call graphs with ever growing competition in telecommunications markets operators have to increasingly rely on business intelligence to offer the right incentives to their customers toward this end existing approaches have almost solely focussed on the individual behaviour of customers call graphs that is graphs induced by people calling each other can allow telecom operators to better understand the interaction behaviour of their customers and potentially provide major insights for designing effective incentivesin this paper we use the call detail records of a mobile operator from four geographically disparate regions to construct call graphs and analyse their structural properties our findings provide business insights and help devise strategies for mobile telecom operators another goal of this paper is to identify the shape of such graphs in order to do so we extend the wellknown reachability analysis approach with some of our own techniques to reveal the shape of such massive graphs based on our analysis we introduce the treasurehunt model to describe the shape of mobile call graphs the proposed techniques are general enough for analysing any large graph finally how well the proposed model captures the shape of other mobile call graphs needs to be the subject of future studies 
heuristic containment check of partial treepattern queries in the presence of index graphs heuristic containment check of partial treepattern queries in the presence of index graphs the wide adoption of xml has increased the interest of the database community on treestructured data management techniques querying capabilities are provided through treepattern queries the need for querying treestructured data sources when their structure is not fully known and the need to integrate multiple data sources with different tree structures have driven recently the suggestion of query languages that relax the complete specification of a tree pattern in this paper we use a query language which allows partial treepattern queries ptpqs the structure in a ptpq can be flexibly specified fully partially or not at all to evaluate a ptpq we exploit index graphs which generate an equivalent set of complete treepattern queriesin order to process ptpqs we need to efficiently solve the ptpq satisfiability and containment problems these problems become more complex in the context of ptpqs because the partial specification of the structure allows new nontrivial structural expressions to be derived from those explicitly specified in a ptpq we address the problem of ptpq satisfiability and containment in the absence and in the presence of index graphs and we provide necessary and sufficient conditions for each case to cope with the high complexity of ptpq containment in the presence of index graphswe study a family of heuristic approaches for ptpq containment based on structural information extracted from the index graph in advance and onthefly we implement our approaches and we report on their extensive experimental evaluation and comparison 
trips and tides trips and tides recent research in data mining has progressed from mining frequent itemsets to more general and structured patterns like trees and graphs in this paper we address the problem of frequent subtree mining that has proven to be viable in a wide range of applications such as bioinformatics xml processing computational linguistics and web usage mining we propose novel algorithms to mine frequent subtrees from a database of rooted trees we evaluate the use of two popular sequential encodings of trees to systematically generate and evaluate the candidate patterns the proposed approach is very generic and can be used to mine embedded or induced subtrees that can be labeled unlabeled ordered unordered or edgelabeled our algorithms are highly cacheconscious in nature because of the compact and simple arraybased data structures we use typically l and l hit rates above are observed experimental evaluation showed that our algorithms can achieve up to several orders of magnitude speedup on real datasets when compared to stateoftheart tree mining algorithms 
automatic computation of semantic proximity using taxonomic knowledge automatic computation of semantic proximity using taxonomic knowledge taxonomic measures of semantic proximity allow us to compute the relatedness of two concepts these metrics are versatile instruments required for diverse applications eg the semantic web linguistics and also text mining however most approaches are only geared towards handcrafted taxonomic dictionaries such as wordnet which only feature a limited fraction of realworld concepts more specific concepts and particularly instances of concepts ie names of artists locations brand names etc are not coveredthe contributions of this paper are two fold first we introduce a framework based on google and the open directory project odp enabling us to derive the semantic proximity between arbitrary concepts and instances second we introduce a new taxonomydriven proximity metric tailored for our framework studies with human subjects corroborate our hypothesis that our new metric outperforms benchmark semantic proximity metrics and comes close to human judgement 
exploiting asymmetry in hierarchical topic extraction exploiting asymmetry in hierarchical topic extraction topic or feature extraction is often used as an important step in document classification and text mining topics are succinct representation of content in a document collection and hence are very effective when used as content identifiers in peertopeer systems and other large scale distributed content management systems effective topic extraction is dependent on the accuracy of term clustering that often has to deal with problems like synonymy and polysemy retrieval techniques based on spectral analysis like latent semantic indexing lsi are often used to effectively solve these problems most of the spectral retrieval schemes produce term similarity measures that are symmetric and often not an accurate characterization of term relationships another drawback of lsi is its running time that is polynomial in the dimensions of the m x n matrix a this can get prohibitively large for some ir applications in this paper we present efficient algorithms using the technique of localitysensitive hashing lsh to extract topics from a document collection based on the asymmetric relationships between terms in a collection the relationship is characterized by the term cooccurrences and other higherorder similarity measures our lsh based scheme can be viewed as a simple alternative to lsi we show the efficacy of our algorithms via experiments on a set of large documents an interesting feature of our algorithms is that it produces a natural hierarchical decomposition of the topic space instead of a flat clustering 
cpcv cpcv domain specific ontologies are heavily used in many applications for instance these form the bases on which similaritydissimilarity between keywords are extracted for various knowledge discovery and retrieval tasks existing similarity computation schemes can be categorized as a structure or b informationbased approaches structure based approaches compute dissimilarity between keywords using a weighted count of edges between two keywords informationbase approaches on the other hand leverage available corpora to extract additional information such as keyword frequency to achieve better performance in similarity computation than structurebased approaches unfortunately in many application domains such as applications that rely on uniquekeys in a relational database frequency information required by informationbased approaches does not exist in this paper we note that there is a third way of computing similarity if each node in a given hierarchy can be represented as a vector of related concepts these vectors could be compared to compute similarities this requires mapping conceptnodes in a given hierarchy onto a concept space in this paper we propose a concept propagation cp scheme which relies on the semantical relationships between concepts implied by the structure of the hierarchy to annotate each conceptnode with a conceptvector cv we refer to this approach as cpcv comparison of keyword similarity results shows that cpcv provides significantly better upto results than existing structurebased schemes also even if cpcv does not assume the availability of an appropriate corpus to extract keyword frequency information our approach matches and slightly improves on the performance of informationbased approaches 
secure search in enterprise webs secure search in enterprise webs document level security dls enforcing permissions prevailing at the time of search is specified as a mandatory requirement in many enterprise search applications unfortunately depending upon implementation details and values of key parameters dls may come at a high price in increased query processing time leading to an unacceptably slow search experience in this paper we present a model and a method for carrying out secure search in the presence of dls within enterprise webs we report on two alternative commercial dls search implementations using a document experimental dls environment we graph the dependence of query processing time on result set size and visibility density for different classes of user scaled up to collections of tens of thousands of documents our results suggest that query times will be unacceptable if exact counts of matching documents are required and also for users who can view only a small proportion of documents we show that the time to conduct access checks is dramatically increased if requests must be sent offserver even on a local network and discuss methods for reducing the cost of security checks we conclude that enterprises can effectively reduce dls overheads by organizing documents in such a way that most access checking can be at collection rather than document level by forgoing accurate match counts by using caching batching or hierarchical methods to cut costs of dls checking and if applicable by using a single portal both to access and search documents 
vector and matrix operations programmed with udfs in a relational dbms vector and matrix operations programmed with udfs in a relational dbms in general a relational dbms provides limited capabilities to perform multidimensional statistical analysis which requires manipulating vectors and matrices in this work we study how to extend a dbms with basic vector and matrix operators by programming userdefined functions udfs we carefully analyze udf features and limitations to implement vector and matrix operations commonly used in statistics machine learning and data mining paying attention to dbms operating system and computer architecture constraints udfs represent a c programming interface that allows the definition of scalar and aggregate functions that can be used in sql udfs have several advantages and limitations a udf allows fast evaluation of arithmetic expressions memory manipulation using multidimensional arrays and exploiting all c language control statements nevertheless a udf cannot perform disk io the amount of heap and stack memory that can be allocated is small and the udf code must consider specific architecture characteristics of the dbms we experimentally compare udfs and sql with respect to performance ease of use flexibility and scalability we profile udfs based on call overhead memory management and interleaved disk access we show udfs are faster than standard sql aggregations and as fast as sql arithmetic expressions 
polestar polestar in this paper we describe polestar policy explanation using stories and arguments an integrated suite of knowledge management and collaboration tools for intelligence analystspolestar provides builtin support for analyst workflow including collection of textual facts from source documents structured argumentation and automatic citation in analytic product documents underlying polestar is a scalable dependency repository which provides traceability from product documents to source snippets the repositorys notification engine allows polestar to alert analysts when dependent sources are discredited and aid them in repairing affected arguments the paper then discusses recent extensions to polestar to support collaborative analysis through communityofinterest finding portfolio sharing and peer review of arguments we conclude with a preview of future research and summary of polestars primary benefits from the point of view of its deployed users 
taskbased process knowhow reuse and proactive information delivery in tasknavigator taskbased process knowhow reuse and proactive information delivery in tasknavigator knowledge management approaches for weaklystructured adhoc knowledge work processes need to be lightweight ie they cannot rely on high upfront modeling efforts this paper presents tasknavigator a novel prototype to support weaklystructured processes by integrating a standard task list application with a stateoftheart document classification system the resulting system allows for a taskoriented view on office workers personal knowledge spaces in order to realize a proactive and contextsensitive information support during daily knowledgeintensive tasks moreover tasknavigator supports process knowhow reuse by proactively suggesting similar tasks or relevant process models based on textual similarities finally we report on a feasibility test and a case study that have been conducted in order to evaluate the system in the context of daily research task management and software requirements analysis 
efficient model selection for regularized linear discriminant analysis efficient model selection for regularized linear discriminant analysis classical linear discriminant analysis lda is not applicable for small sample size problems due to the singularity of the scatter matrices involved regularized lda rlda provides a simple strategy to overcome the singularity problem by applying a regularization term which is commonly estimated via crossvalidation from a set of candidates however crossvalidation may be computationally prohibitive when the candidate set is large an efficient algorithm for rlda is presented that computes the optimal transformation of rlda for a large set of parameter candidates with approximately the same cost as running rlda a small number of times thus it facilitates efficient model selection for rldaan intrinsic relationship between rlda and uncorrelated lda ulda which was recently proposed for dimension reduction and classification is presented more specifically rlda is shown to approach ulda when the regularization value tends to zero that is rlda without any regularization is equivalent to ulda it can be further shown that ulda maps all data points from the same class to a common point under a mild condition which has been shown to hold for many highdimensional datasets this leads to the overfitting problem in ulda which has been observed in several applications thetheoretical analysis presented provides further justification for the use of regularization in rlda extensive experiments confirm the claimed theoretical estimate of efficiency experiments also show that for a properly chosen regularization parameter rlda performs favorably in classification in comparison with ulda as well as other existing ldabased algorithms and support vector machines svm 
conceptbased document readability in domain specific information retrieval conceptbased document readability in domain specific information retrieval domain specific information retrieval has become in demand not only domain experts but also average nonexpert users are interested in searching domain specific eg medical and health information from online resources however a typical problem to average users is that the search results are always a mixture of documents with different levels of readability nonexpert users may want to see documents with higher readability on the top of the list consequently the search results need to be reranked in a descending order of readability it is often not practical for domain experts to manually label the readability of documents for large databases computational models of readability needs to be investigated however traditional readability formulas are designed for general purpose text and insufficient to deal with technical materials for domain specific information retrieval more advanced algorithms such as textual coherence model are computationally expensive for reranking a large number of retrieved documents in this paper we propose an effective and computationally tractable conceptbased model of text readability in addition to textual genres of a document our model also takes into account domain specific knowledge ie how the domainspecific concepts contained in the document affect the documents readability three major readability formulas are proposed and applied to health and medical information retrieval experimental results show that our proposed readability formulas lead to remarkable improvements in terms of correlation with users readability ratings over four traditional readability measures 
a probabilistic relevance propagation model for hypertext retrieval a probabilistic relevance propagation model for hypertext retrieval a major challenge in developing models for hypertext retrieval is to effectively combine content information with the link structure available in hypertext collections although several linkbased ranking methods have been developed to improve retrieval results none of them can fully exploit the discrimination power of contents as well as fully exploit all useful link structures in this paper we propose a general relevance propagation framework for combining content and link information the framework gives a probabilistic score to each document defined based on a probabilistic surfing model two main characteristics of our framework are our probabilistic view on the relevance propagation model and propagation through multiple sets of neighbors we compare eight different models derived from the probabilistic relevance propagation framework on two standard trec web test collections our results show that all the eight relevance propagation models can outperform the baseline content only ranking method for a wide range of parameter values indicating that the relevance propagation framework provides a general effective and robust way of exploiting link information our experiments also show that using multiple neighbor sets outperforms using just one type of neighbors significantly and taking a probabilistic view of propagation provides guidance on setting propagation parameters 
term context models for information retrieval term context models for information retrieval at their heart most if not all information retrieval models utilize some form of term frequencythe notion is that the more often a query term occurs in a document the more likely it is that document meets an information need we examine an alternative we propose a model which assesses the presence of a term in a document not by looking at the actual occurrence of that term but by a set of nonindependent supporting terms ie context this yields a weighting for terms in documents which is different from and complementary to tfbased methods and is beneficial for retrieval 
ranking robustness ranking robustness in this paper we introduce the notion of ranking robustness which refers to a property of a ranked list of documents that indicates how stable the ranking is in the presence of uncertainty in the ranked documents we propose a statistical measure called the robustness score to quantify this notion we demonstrate that the robustness score significantly and consistently correlates with query performance in a variety of trec test collections including the gov collection we compare the robustness score with the clarity score method which is the stateoftheart technique for query performance prediction our experimental results show that the robustness score performs better than or at least as good as the clarity score we find that the clarity score is barely correlated with query performance on the gov collection while the correlation between the robustness score and query performance remains significant we also notice that a combination of the two usually results in more prediction power 
query result ranking over ecommerce web databases query result ranking over ecommerce web databases to deal with the problem of too many results returned from an ecommerce web database in response to a user query this paper proposes a novel approach to rank the query results based on the user query we speculate how much the user cares about each attribute and assign a corresponding weight to it then for each tuple in the query result each attribute value is assigned a score according to its desirableness to the user these attribute value scores are combined according to the attribute weights to get a final ranking score for each tuple tuples with the top ranking scores are presented to the user first our ranking method is domain independent and requires no user feedback experimental results demonstrate that this ranking method can effectively capture a users preferences 
optimisation methods for ranking functions with multiple parameters optimisation methods for ranking functions with multiple parameters optimising the parameters of ranking functions with respect to standard ir rankdependent cost functions has eluded satisfactory analytical treatment we build on recent advances in alternative differentiable pairwise cost functions and show that these techniques can be successfully applied to tuning the parameters of an existing family of ir scoring functions bm in the sense that we cannot do better using sensible search heuristics that directly optimize the rankbased cost function ndcg we also demonstrate how the size of training set affects the number of parameters we can hope to tune this way 
estimating corpus size via queries estimating corpus size via queries we consider the problem of estimating the size of a collection of documents using only a standard query interface our main idea is to construct an unbiased and lowvariance estimator that can closely approximate the size of any set of documents defined by certain conditions including that each document in the set must match at least one query from a uniformly sampleable query pool of known size fixed in advanceusing this basic estimator we propose two approaches to estimating corpus size the first approach requires a uniform random sample of documents from the corpus the second approach avoids this notoriously difficult sample generation problem and instead uses two fairly uncorrelated sets of terms as query pools the accuracy of the second approach depends on the degree of correlation among the two sets of termsexperiments on a large trec collection and on three major search engines demonstrates the effectiveness of our algorithms 
concept frequency distribution in biomedical text summarization concept frequency distribution in biomedical text summarization text summarization is a data reduction process the use of text summarization enables users to reduce the amount of text that must be read while still assimilating the core information the data reduction offered by text summarization is particularly useful in the biomedical domain where physicians must continuously find clinical trial study information to incorporate into their patient treatment efforts such efforts are often hampered by the highvolume of publications our contribution is twofold to propose the frequency of domain concepts as a method to identify important sentences within a fulltext and propose a novel frequency distribution model and algorithm for identifying important sentences based on term or concept frequency distribution an evaluation of several existing summarization systems using biomedical texts is presented in order to determine a performance baseline for domain concept comparison a recent highperforming frequencybased algorithm using terms is adapted to use concepts and evaluated using both terms and concepts it is shown that the use of concepts performs closely with the use of terms for sentence selection our proposed frequency distribution model and algorithm outperforms a stateoftheart approach 
describing differences between databases describing differences between databases we study the novel problem of efficiently computing the update distance for a pair of relational databases in analogy to the edit distance of strings we define the update distance of two databases as the minimal number of setoriented insert delete and modification operations necessary to transform one database into the other we show how this distance can be computed by traversing a search space of database instances connected by update operations this insight leads to a family of algorithms that compute the update distance or approximations of it in our experiments we observed that a simple heuristic performs surprisingly well in most considered casesour motivation for studying distance measures for databases stems from the field of scientific databases there replicas of a single database are often maintained at different sites which typically leads to accidental or planned divergence of their content to recreate a consistent view these differences must be resolved such an effort requires an understanding of the process that produced them we found that minimal update sequences of setoriented update operations are a proper and concise representation of systematic errors thus giving valuable clues to domain experts responsible for conflict resolution 
a system for queryspecific document summarization a system for queryspecific document summarization there has been a great amount of work on queryindependent summarization of documents however due to the success of web search engines queryspecific document summarization query result snippets has become an important problem which has received little attention we present a method to create queryspecific summaries by identifying the most queryrelevant fragments and combining them using the semantic associations within the document in particular we first add structure to the documents in the preprocessing stage and convert them to document graphs then the best summaries are computed by calculating the top spanning trees on the document graphs we present and experimentally evaluate efficient algorithms that support computing summaries in interactive time furthermore the quality of our summarization method is compared to current approaches using a user survey 
annotation propagation revisited for key preserving views annotation propagation revisited for key preserving views this paper revisits the analysis of annotation propagation from source databases to views defined in terms of conjunctive spj queries given a source database d an spj query q the view qd and a tuple v in the view the view resp source sideeffect problem is to find a minimal set d of tuples such that the deletion of d from d results in the deletion of v from qd while minimizing the side effects on the view resp the source a third problem referred to as the annotation placement problem is to find a single base tuple d such that annotation in a field of d propagates to v while minimizing the propagation to other fields in the view qd these are important for data provenance and the management of view updates however important these problems are unfortunately nphard for most subclasses of spj views to make the annotation propagation analysis feasible in practice we propose a key preserving condition on spj views which requires that the projection fields of an spj view q retain a key of each base relation involved in q while this condition is less restrictive than other proposals it often simplifies the annotation propagation analysis indeed for keypreserving spj views the annotation placement problem coincides with the view sideeffect problem and the view and source sideeffect problems become tractable in addition we generalize the setting of by allowing v to be a group of tuples to be deleted and investigate the insertion of tuples to the view we show that group updates make the analysis harder these problems become nphard for several subclasses of spj views we also show that for spj views the source and view sideeffect problems are nphard for singletuple insertion but are tractable for some subclasses of spj for group insertions in the presence or in the absence of the key preservation condition 
query optimization using restructured views query optimization using restructured views we study optimization of relational queries using materialized views where views may be regular or restructured in a restructured view some data from the base tables are represented as metadata that is schema information such as table and attribute names or vice versausing restructured views in query optimization opens up a new spectrum of views that were not previously available and can result in significant additional savings in queryevaluation costs these savings can be obtained due to a significantly larger set of views to choose from and may involve reduced table sizes elimination of selfjoins clustering produced by restructuring and horizontal partitioningin this paper we propose a general queryoptimization framework that treats regular and restructured views in a uniform manner and is applicable to sql selectprojectjoin queries and views with or without aggregation within the framework we provide algorithms to determine when a view regular or restructured is usable in answering a query and algorithms to rewrite a query using usable viewssemantic information such as knowledge of the key of a view can be used to further optimize a rewritten query within our general queryoptimization framework we develop techniques for determining the key of a regular or restructured view and show how this information can be used to further optimize a rewritten query it is straightforward to integrate all our algorithms and techniques into standard queryoptimization algorithms 
improving query io performance by permuting and refining block request sequences improving query io performance by permuting and refining block request sequences the io performance of query processing can be improved using two complementary approaches improve the buffer and the file system management policies of the db buffer manager and the os file system manager eg page replacement or improve the sequence of requests that are submitted to a file system manager and that lead to actual ios block request sequences this paper takes the latter approach exploiting common file system practices as found in linux we propose four techniques for permuting and refining block request sequences blocklevel io grouping filelevel io grouping io ordering and block recycling to manifest these techniques we create two new plan operations mms and shj each of which adopts some of the block request refinement techniques above we implement the new plan operations on top of postgres running on linux and show experimental results that demonstrate up to a factor of performance benefit from the use of these techniques 
performance thresholding in practical text classification performance thresholding in practical text classification in practical classification there is often a mix of learnable and unlearnable classes and only a classifier above a minimum performance threshold can be deployed this problem is exacerbated if the training set is created by active learning the bias of actively learned training sets makes it hard to determine whether a class has been learned we give evidence that there is no general and efficient method for reducing the bias and correctly identifying classes that have been learned however we characterize a number of scenarios where active learning can succeed despite these difficulties 
text classification improved through multigram models text classification improved through multigram models classification algorithms and document representation approaches are two key elements for a successful document classification system in the past much work has been conducted to find better ways to represent documents however most of the attempts rely on certain extra resources such as wordnet or they face the problem of extremely high dimension in this paper we propose a new document representation approach based on nmultigram language models this approach can automatically discover the hidden semantic sequences in the documents under each category based on nmultigram language models and ngram language models we put forward two text classification algorithms the experiments on rcv show that our proposed algorithm based on nmultigram models alone can achieve the similar or even better classification performance compared with the classifier based on ngram models but the model size of our algorithm is much smaller than that of the latter another proposed algorithm based on the combination of nmultigram models and ngram models improves the microf and macrof values from to and to respectively all these observations support the validity of our proposed document representation approach 
coupling feature selection and machine learning methods for navigational query identification coupling feature selection and machine learning methods for navigational query identification it is important yet hard to identify navigational queries in web search due to a lack of sufficient information in web queries which are typically very short in this paper we study several machine learning methods including naive bayes model maximum entropy model support vector machine svm and stochastic gradient boosting tree sgbt for navigational query identification in web search to boost the performance of these machine techniques we exploit several feature selection methods and propose coupling feature selection with classification approaches to achieve the best performance different from most prior work that uses a small number of features in this paper we study the problem of identifying navigational queries with thousands of available features extracted from major commercial search engine results web search user click data query log and the whole webs relational content a multilevel feature extraction system is constructedour results on real search data show that among all the features we tested user click distribution features are the most important set of features for identifying navigational queries in order to achieve good performance machine learning approaches have to be coupled with good feature selection methods we find that gradient boosting tree coupled with linear svm feature selection is most effective with carefully coupled feature selection and classification approaches navigational queries can be accurately identified with f score which is error rate reduction compared to the best uncoupled system and error rate reduction compared to a well tuned system without feature selection 
document reranking using cluster validation and label propagation document reranking using cluster validation and label propagation this paper proposes a novel document reranking approach in information retrieval which is done by a label propagationbased semisupervised learning algorithm to utilize the intrinsic structure underlying in the large document data since no labeled relevant or irrelevant documents are generally available in ir our approach tries to extract some pseudo labeled documents from the ranking list of the initial retrieval for pseudo relevant documents we determine a cluster of documents from the top ones via cluster validationbased kmeans clustering for pseudo irrelevant ones we pick a set of documents from the bottom ones then the ranking of the documents can be conducted via label propagation evaluation on benchmark corpora shows that the approach can achieve significant improvement over standard baselines and performs better than other related approaches 
tracking dragonhunters with language models tracking dragonhunters with language models we are interested in the problem of understanding the connections between human activities and the content of textual information generated in regard to those activities firstly we define and motivate this problem as an important part in making sense of various life events secondly we introduce the domain of massive online collaborative environments specifically online virtual worlds where people meet exchange messages and perform actions as a rich data source for such an analysis finally we outline three experimental tasks and show how statistical language modeling and text clustering techniques may allow us to explore those connections successfully 
designing semanticspreserving cluster representatives for scientific input conditions designing semanticspreserving cluster representatives for scientific input conditions in scientific domains knowledge is often discovered from experiments by grouping or clustering them based on the similarity of their output the causes of similarity are analyzed based on the input conditions characterizing a given type of output ie a given cluster this analysis helps in applications such as decision support in industry cluster representatives form ataglance depictions for such applications randomly selecting a set of conditions in a cluster as its representative is not sufficient since distinct combinations of inputs could lead to the same cluster in this paper an approach called descond is proposed to design semanticspreserving cluster representatives for scientific input conditions we define a notion of distance for conditions to capture semantics based on the types of their attributes and their relative importance using this distance methods of building candidate cluster representatives with different levels of detail are proposed candidates are compared using the descond encoding proposed in this paper that assesses their complexity and information loss given user interests the candidate with the lowest encoding for each cluster is returned as its designed representative descond is evaluated with real data from materials science evaluation with domain expert interviews and formal user surveys shows that designed representatives consistently outperform randomly selected ones and different candidates suit different users 
cacheoblivious nestedloop joins cacheoblivious nestedloop joins we propose to adapt the newly emerged cacheoblivious model to relational query processing our goal is to automatically achieve an overall performance comparable to that of finetuned algorithms on a multilevel memory hierarchy this automaticity is because cacheoblivious algorithms assume no knowledge about any specific parameter values such as the capacity and block size of each level of the hierarchy as a first step we propose recursive partitioning to implement cacheoblivious nestedloop joins nljs without indexes and recursive clustering and buffering to implement cacheoblivious nljs with indexes our theoretical results and empirical evaluation on three different architectures show that our cacheoblivious nljs match the performance of their manually optimized cacheconscious counterparts 
a combination of trietrees and inverted files for the indexing of setvalued attributes a combination of trietrees and inverted files for the indexing of setvalued attributes setvalued attributes frequently occur in contexts like marketbasked analysis and stock market trends late research literature has mainly focused on set containment joins and data mining without considering simple queries on set valued attributes in this paper we address superset subset and equality queries and we propose a novel indexing scheme for answering them on setvalued attributes the proposed index superimposes a trietree on top of an inverted file that indexes a relation with setvalued data we show that we can efficiently answer the aforementioned queries by indexing only a subset of the most frequent of the items that occur in the indexed relation finally we show through extensive experiments that our approach outperforms the state of the art mechanisms and scales gracefully as database size grows 
efficient join processing over uncertain data efficient join processing over uncertain data in many applications data values are inherently uncertain this includes movingobjects sensors and biological databases there has been recent interest in the development of database management systems that can handle uncertain data some proposals for such systems include attribute values that are uncertain in particular an attribute value can be modeled as a range of possible values associated with a probability density function previous efforts for this type of data have only addressed simple queries such as range and nearestneighbor queries queries that join multiple relations have not been addressed in earlier work despite the significance of joins in databases in this paper we address join queries over uncertain data we propose a semantics for the join operation define probabilistic operators over uncertain data and propose join algorithms that provide efficient execution of probabilistic joins the paper focuses on an important class of joins termed probabilistic threshold joins that avoid some of the semantic complexities of dealing with uncertain data for this class of joins we develop three sets of optimization techniques itemlevel pagelevel and indexlevel pruning these techniques facilitate pruning with little space and time overhead and are easily adapted to most join algorithms we verify the performance of these techniques experimentally 
an integer programming approach for frequent itemset hiding an integer programming approach for frequent itemset hiding the rapid growth of transactional data brought soon enough into attention the need of its further exploitation in this paper we investigate the problem of securing sensitive knowledge from being exposed in patterns extracted during association rule mining instead of hiding the produced rules directly we decide to hide the sensitive frequent itemsets that may lead to the production of these rules as a first step we introduce the notion of distance between two databases and a measure for quantifying it by trying to minimize the distance between the original database and its sanitized version that can safely be released we propose a novel exact algorithm for association rule hiding and evaluate it on real world datasets demonstrating its effectiveness towards solving the problem 
privacy preserving sequential pattern mining in distributed databases privacy preserving sequential pattern mining in distributed databases research in the areas of privacy preserving techniques in databases and subsequently in privacy enhancement technologies have witnessed an explosive growthspurt in recent years this escalation has been fueled by the growing mistrust of individuals towards organizations collecting and disbursing their personally identifiable information pii digital repositories have become increasingly susceptible to intentional or unintentional abuse resulting in organizations to be liable under the privacy legislations that are being adopted by governments the world over these privacy concerns have necessitated new advancements in the field of distributed data mining wherein collaborating parties may be legally bound not to reveal the private information of their customers in this paper we present a new algorithm pripsep privacy preserving sequential patterns for the mining of sequential patterns from distributed databases while preserving privacy a salient feature of pripsep is that due to its flexibility it is more pertinent to mining operations for real world applications in terms of efficiency and functionality under some reasonable assumptions we prove that our architecture and protocol employed by our algorithm for multiparty computation is secure 
a dictionary for approximate string search and longest prefix search a dictionary for approximate string search and longest prefix search in this paper we propose a dictionary data structure for string search with errors where the query string may didiffer from the expected matching string by a few edits this data structure can also be used to find the database string with the longest common prefix with few errors specifically with a database of n random strings each of length of om we show how to perform string search on a query string that differs from its closest match by k edits using a data structure of linear size and query time equal to log n log n klog a m over m this means that if k m over log a m log n then the query time is this is of significant in practice as there are several applications where k is small relative to m our approach converts strings into bit vectors so that similar strings can map to similar bit vectors with small hamming distance a simple reduction can be used to obtain similar results for approximate longest prefix search 
a comparative study on classifying the functions of web page blocks a comparative study on classifying the functions of web page blocks in this paper we study the problem of learning block classification models to estimate block functions we distinguish general models which are learned across multiple sites and sitespecific models which are learned within individual sites we further consider several factors that affect the learning process and model effectiveness these factors include the layout features the content features the classifiers and the term selection methods we have empirically evaluated the performance of the models when the factors are varied our main results are that layout features do better than content features for learning both general and sitespecific models 
a neighborhoodbased approach for clustering of linked document collections a neighborhoodbased approach for clustering of linked document collections this paper addresses the problem of automatically structuring linked document collections by using clustering in contrast to traditional clustering we study the clustering problem in the light of available link structure information for the data set eg hyperlinks among web documents or coauthorship among bibliographic data entries our approach is based on iterative relaxation of cluster assignments and can be built on top of any clustering algorithm this technique results in higher cluster purity better overall accuracy and make selforganization more robust 
a structureoriented relevance feedback method for xml retrieval a structureoriented relevance feedback method for xml retrieval relevance feedback rf is a technique allowing to enrich an initial query according to the user feedback the goal is to express more precisily the users needs some open issues appear when considering semistructured documents like xml documents most of the rf approaches proposed in xml retrieval are simple adaptations of traditional rf to the new granularity of information they enrich queries by adding terms extracted from relevant elements instead of terms extracted from whole documents in this paper we show how structural constraints can also be used in rf we propose a new approach that is able to extend the initial query by adding one or more generative structures this approach is applied to unstructured queries experiments are carried out on inex collection and results show the interest of our method 
adapting association patterns for text categorization adapting association patterns for text categorization the use of association patterns for text categorization has attracted great interest and a variety of useful methods have been developed however the key characteristics of patternbased text categorization remain unclear indeed there are still no concrete answers for the following two questions first what kind of association patterns are the best candidate for patternbased text categorization second what is the most desirable way to use patterns for text categorization in this paper we focus on answering the above two questions specifically we show that hyperclique patterns are more desirable than frequent patterns for text categorization along this line we develop an algorithm for text categorization using hyperclique patterns the experimental results show that our method provides better performance than stateoftheart methods in terms of both computational performance and classification accuracy 
amnesic online synopses for moving objects amnesic online synopses for moving objects we present a hierarchical tree structure for online maintenance of timedecaying synopses over streaming data we exemplify such an amnesic behavior over streams of locations taken from numerous moving objects in order to obtain reliable trajectory approximations as well as affordable estimates regarding distinct count spatiotemporal queries 
an efficient onephase holistic twig join algorithm for xml data an efficient onephase holistic twig join algorithm for xml data in view of the inefficiency of the traditional twophase twigstack algorithm we propose a singlephase holistic twig pattern matching method based on the twigstack algorithm by applying a novel stack structure 
approximate reverse knearest neighbor queries in general metric spaces approximate reverse knearest neighbor queries in general metric spaces in this paper we propose an approach for efficient approximative rknn search in arbitrary metric spaces where the value of k is specified at query time our method uses an approximation of the nearestneighbordistances in order to prune the search space in several experiments our solution scales significantly better than existing nonapproximative approaches while producing an approximation of the true query result with a high recall 
bestk queries on database systems bestk queries on database systems note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
boosting relevance model performance with query term dependence boosting relevance model performance with query term dependence note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
collaborative filtering in dynamic usage environments collaborative filtering in dynamic usage environments note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
automatically constructing collections of online database directories automatically constructing collections of online database directories note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
combining feature selectors for text classification combining feature selectors for text classification we introduce several methods of combining feature selectors for text classification results from a large investigation of these combinations are summarized easily constructed combinations of feature selectors are shown to improve peak rprecision and f at statistically significant levels 
constructing better document and query models with markov chains constructing better document and query models with markov chains document and query expansions have been used separately in previous studies to enhance the representation of documents and queries in this paper we propose a general method that integrates both of them expansion is carried out using multistage markov chains our experiments show that this method significantly outperforms the existing approaches 
continuous keyword search on multiple text streams continuous keyword search on multiple text streams in this paper we address the issue of continuous keyword queries on multiple textual streams this line of work represents a significant departure from previous keyword search models that assumed a static database in our model the user poses a query comprised by a collection of keywords which is subsequently applied on multiple text streams these can be rss news feeds tv closed captions emails etc a result to a query is a combination of streams sufficiently correlated to each other that collectively contain all query keywords within a specified time span 
on subspace clustering with density consciousness on subspace clustering with density consciousness in this paper a problem called the density divergence problem is explored this problem is related to the phenomenon that the densities of the clusters vary in different subspace cardinalities we take the densities into consideration in subspace clustering and explore an algorithm to adaptively determine different density thresholds to discover clusters in different subspace cardinalities 
direct comparison of commercial and academic retrieval system direct comparison of commercial and academic retrieval system note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
effective and efficient similarity search in time series effective and efficient similarity search in time series we present dsa derivative time series segment approximation a novel representation model for time series designed for effective and efficient similarity search dsa substantially exploits derivative estimation segmentation and dimensionality reduction to meet at least the requirements of high sensitivity to main features trends of time series and robustness to outliers experiments show that dsa is drastically faster and still as good or better than the prominent stateoftheart similarity methods 
efficient mining of max frequent patterns in a generalized environment efficient mining of max frequent patterns in a generalized environment this poster paper summarizes our solution for mining max frequent generalized itemsets gitemsets a compact representation for frequent patterns in the generalized environment 
estimation sensitivity and generalization in parameterized retrieval models estimation sensitivity and generalization in parameterized retrieval models in this work we investigate three important aspects of parameterized retrieval models estimation sensitivity and generalization since all parameterized models even those based on heuristics have inherent uncertainty we study these issues using statistical tools 
filtering or adapting filtering or adapting noisy parallel corpora have been widely used for crosslanguage information retrieval clir however the previous studies only focus on truly parallel corpus in this paper we examine two possible approaches to exploit noisy corpora filtering out noise from the corpora or adapting the training process of translation model to the noise corpora our experiments show that the second approach is better suited to clir 
hux hux note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
improving query translation with confidence estimation for cross language information retrieval improving query translation with confidence estimation for cross language information retrieval note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
information retrieval from relational databases using semantic queries information retrieval from relational databases using semantic queries relational databases are widely used today as a mechanism for providing access to structured data they however are not suitable for typical information finding tasks of end users there is often a semantic gap between the queries users want to express and the queries that can be answered by the database in this paper we propose a system that bridges this semantic gap using domain knowledge contained in ontologies our system extends relational databases with the ability to answer semantic queries that are represented in sparql an emerging semantic web query language users express their queries in sparql based on a semantic model of the data and they get back semantically relevant results we define different categories of results that are semantically relevant to the users query and show how our system retrieves these results we evaluate the performance of our system on sample relational databases using a combination of standard and custom ontologies 
integrated rfid data modeling integrated rfid data modeling note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
integration of cluster ensemble and em based text mining for microarray gene cluster identification and annotation integration of cluster ensemble and em based text mining for microarray gene cluster identification and annotation in this paper we design and develop a unified system geminer gene expression miner to integrate cluster ensemble text clustering and multi document summarization and provide an environment for comprehensive gene expression data analysis we present a novel cluster ensemble approach to generate high quality gene cluster in our text summarization module given a gene cluster our expectation maximization em based algorithm can automatically identify subtopics and extract most probable terms for each topic then the extracted top k topical terms from each subtopic are combined to form the biological explanation of each gene cluster experimental results demonstrate that our system can obtain high quality clusters and provide informative key terms for the gene clusters 
introduction to a new farsi stemmer introduction to a new farsi stemmer in this poster a new farsi also called persian stemmer which works without dictionary is introduced evaluation results show significant improvement in performance precisionrecall of the information retrieval ir system using this stemmer 
ir principles for contentbased indexing and retrieval of functional brain images ir principles for contentbased indexing and retrieval of functional brain images in this paper we explore the concept of a library of brain images which implies not only a repository of brain images but also efficient search and retrieval mechanisms that are based on models derived from ir practice as a preliminary study we have worked with a collection of functional mri brain images assembled in the study of several distinct cognitive tasks we adapt several classical ir methods inverted indexing tfidf and latent semantic indexinglsi to contentbased brain image retrieval our results show that efficient and accurate retrieval of brain images is possible and that representations motivated by the ir perspective are somewhat more effective than are methods based on retaining the full image information 
matching directories and owl ontologies with aroma matching directories and owl ontologies with aroma this paper presents a simple and adaptable matching method dealing with web directories catalogs and owl ontologies by using a wellknown knowledge discovery in databases model such as the association rule paradigm this method has the originality to be both extensional and asymmetric it works at the terminological level by selecting conceptrelevant terms contained in documents and permits to discover equivalence and also subsumption relations holding between entities concepts and properties this method relies on the implication intensity measure a probabilistic model of deviation from independence selection of significant rules between concepts or properties is lead by two criteria permitting to assess respectively the implication quality and the generativity of the rule finally the proposed method is evaluated on two benchmarks the first contains two conceptual hierarchies populated with textual documents and the second one is composed of owl ontologies 
matching and evaluation of disjunctive predicates for data stream sharing matching and evaluation of disjunctive predicates for data stream sharing new optimization techniques eg in data stream management systems dsmss make the treatment of disjunctive predicates a necessity in this paper we introduce and compare methods for matching and evaluating disjunctive predicates 
maximizing the sustained throughput of distributed continuous queries maximizing the sustained throughput of distributed continuous queries monitoring systems today often involve continuous queries over streaming data in a distributed collaborative system the distribution of query operators over a network of processors and their processing sequence form a query configuration with inherent constraints on the throughput it can support in this paper we propose to optimize stream queries with respect to a version of throughput measure the profiled input throughput this measure is focused on matching the expected behavior of the input streams to prune the search space we used hillclimbing techniques that proved to be efficient and effective 
measuring the meaning in time series clustering of text search queries measuring the meaning in time series clustering of text search queries we use a combination of proven methods from time series analysis and machine learning to explore the relationship between temporal and semantic similarity in web query logs we discover that the combination of correlation and cycles is a good but not perfect sign of semantic relationship 
mining coherent patterns from heterogeneous microarray data mining coherent patterns from heterogeneous microarray data microarray technology is a powerful tool for geneticists to monitor interactions among tens of thousands of genes simultaneously there has been extensive research on coherent subspace clustering of gene expressions measured under consistent experimental settings however these methods assume that all experiments are run using the same batch of microarray chips with similar characteristics of noise algorithms developed under this assumption may not be applicable for analyzing data collected from heterogeneous settings where the set of genes being monitored may be different and expression levels may be not directly comparable even for the same gene in this paper we propose a model fcluster for mining subspace coherent patterns from heterogeneous gene expression data we compare our model with previously proposed models we analyze the search space of the problem and give a nave solution for it 
k nearest neighbor classification across multiple private databases k nearest neighbor classification across multiple private databases distributed privacy preserving data mining tools are critical for mining multiple databases with a minimum information disclosure we present a framework including a general model as well as multiround algorithms for mining horizontally partitioned databases using a privacy preserving k nearest neighbor knn classifier 
modeling performancedriven workload characterization of web search systems modeling performancedriven workload characterization of web search systems note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
multiquery optimization of sliding window aggregates by schedule synchronization multiquery optimization of sliding window aggregates by schedule synchronization note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
multitask text segmentation and alignment based on weighted mutual information multitask text segmentation and alignment based on weighted mutual information text segmentation is important for text analysis while text alignment is to determine shared subtopics among similar documents multitask text segmentation and alignment is the extension of singletask segmentation to utilize information of multisource documents in this paper we introduce a novel domainindependent unsupervised method for multitask segmentation and alignment based on the idea that the optimal segmentation and alignment maximizes weighted mutual information mutual information with term weights the experiment results show that our approach works well 
pepx pepx note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
on progressive sequential pattern mining on progressive sequential pattern mining when sequential patterns are generated the newly arriving patterns may not be identified as frequent sequential patterns due to the existence of old data and sequences in practice users are usually more interested in the recent data than the old ones to capture the dynamic nature of data addition and deletion we propose a general model of sequential pattern mining with a progressive database in addition we present a progressive concept to progressively discover sequential patterns in recent time period of interest 
practical private data matching deterrent to spoofing attacks practical private data matching deterrent to spoofing attacks private data matching between the data sets of two potentially distrusted parties has a wide range of applications however existing solutions have substantial weaknesses and do not meet the needs of many practical application scenarios in particular practical private data matching applications often require discouraging the matching parties from spoofing their private inputs in this paper we address this challenge by forcing the matching parties to escrow the data they use for matching to an auditorial agent and in the afterthefact period they undertake the liability to attest the genuineness of the escrowed data 
probabilistic documentcontext based relevance feedback with limited relevance judgments probabilistic documentcontext based relevance feedback with limited relevance judgments this paper presents our novel relevance feedback rf algorithm that uses the probabilistic documentcontext based retrieval model with limited relevance judgments for document reranking probabilities of the documentcontext based retrieval model are estimated from the top n documents in the initial retrieval we use documentcontext based cosine similarity measure to find similar data for better probability estimation in order to reduce the data scarcity problem and the negative weighting problem our rf algorithm is promising because its mean average precision is statistically significantly better than the baseline using trec and trec data collections 
processing information intent via weak labeling processing information intent via weak labeling note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
pseudoanchor text extraction for searching vertical objects pseudoanchor text extraction for searching vertical objects this paper examines the problem of utilizing pseudoanchor text to help ranking web objects in vertical search we adopt a machine learning based approach to extract pseudoanchor text for a vertical object from its candidate anchor blocks experiments in academic search domain indicate that our approach is able to dramatically improve search performance 
reranking search results using query logs reranking search results using query logs this work addresses two common problems in search frequently occurring with underspecified user queries the topranked results for such queries may not contain documents relevant to the users search intent and fresh and relevant pages may not get high ranks for an underspecified query due to their freshness and to the large number of pages that match the query despite the fact that a large number of users have searched for parts of their content recently we propose a novel method qrank to effectively refine the ranking of search results for any given query by constructing the query context from search query logs evaluation results show that qrank gains a considerable advantage over the current ranking system of a largescale commercial web search engine being able to improve the relevance of search results for of the queries 
query taxonomy generation for web search query taxonomy generation for web search we propose an approach that organizes the searchresult clusters into a hierarchical structure called a query taxonomy from the users perspective the proposed approach is based on an unsupervised classification method which uses the dynamic web as the training corpus with query taxonomy users can browse relevant web documents more conveniently and comprehensibly our experimental results verify the feasibility and the effectiveness of the proposed approach to query taxonomy generation in web search 
rank synopses for efficient time travel on the web graph rank synopses for efficient time travel on the web graph note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
ranking in context using vector spaces ranking in context using vector spaces note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
representing documents with named entities for story link detection sld representing documents with named entities for story link detection sld several information organization access and filtering systems can benefit from different kind of document representations than those used in traditional information retrieval ir topic detection and tracking tdt is an example of such an application in this paper we demonstrate that named entities serve as better choices of units for document representation over all words in order to test this hypothesis we study the effect of wordsbased and entitybased representations on story link detection sld a core task in tdt research the experiments on tdt corpora show that entitybased representations give significant improvements for sld we also propose a mechanism to expand the set of named entities used for document representation which enhances the performance in some cases we then take a step further and analyze the limitations of using only named entities for the document representation our studies and experiments indicate that adding additional topical terms can help in addressing such limitations 
resourceaware kernel density estimators over streaming data resourceaware kernel density estimators over streaming data a fundamental building block of many data mining and analysis approaches is density estimation as it provides a comprehensive statistical model of a data distribution for that reason its application to transient data streams is highly desirable a convenient nonparametric method for density estimation utilizes kernels however its computational complexity collides with the rigid processing requirements of data streams in this work we present a new approach to this problem that combines linear processing cost with a constant amount of allocated memory our approach also supports a dynamic memory adaptation to changing system resources 
retrieval evaluation with incomplete relevance data retrieval evaluation with incomplete relevance data note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
robust periodicity detection algorithms robust periodicity detection algorithms periodicity detection is an important preprocessing step for many time series algorithms it provides important information about the structural properties of a time series feature vectors based on periodicity can be used for clustering classification abnormality detection and human motion understanding the periodicity detection task is not difficult in case of simple and uncontaminated signal unfortunately most of the real datasets exhibit one or more of the following properties i nonstationarity ii interlaced cyclic patterns and iii data contamination which makes the period detection extremely challenging a seemingly straightforward solution is to develop individual specialized algorithms for handling each case separately however determining if a time series is nonstationary or is contaminated in itself is an extremely difficult task in this article we propose generic algorithms which can detect periods in complex noisy and incomplete datasets the algorithm leverages the frequency characterization and autocorrelation structure inherent in a time series to estimate its periodicity we extend the methods to handle nonstationary time series by tracking the candidate periods using a kalman filter we also address the interesting problem of finding multiple interlaced periodicities 
search result summarization and disambiguation via contextual dimensions search result summarization and disambiguation via contextual dimensions topic hierarchies are a popular method of summarizing the results obtained in response to a query in various search applications however topic hierarchies are rigid when they are predefined and somewhat unintuitive when they are dynamically generated by statistical techniques in this paper we propose an alternative approach to query disambiguation and result summarization by placing the results in set of contextual dimensions which can be viewed as facets for the generic search scenario we illustrate our approach by using three types of contextual dimensions namely concepts features and specializations we use nlp techniques and a data mining algorithm to select distinct contexts 
semiautomatic annotation and mpeg authoring of dance videos semiautomatic annotation and mpeg authoring of dance videos this paper presents a system danvideo that is implemented using jse and jmf to annotate manually the macro and micro features of the dance videos by the dance experts as mpeg has reached a matured state for the description of the multimedia structure and semantics through the descriptors and description schemes danvideo generates a mpeg instance that conforms to the mpeg schema semiautomatically and effortlessly from the dance annotations 
the queryvector document model the queryvector document model note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
the visual funding navigator the visual funding navigator this paper presents an interactive visualization toolkit for navigating and analyzing the national science foundation nsf funding information our design builds upon an improved d treemap layout and the stacked graph to contribute customized techniques for visually navigating and interacting with the hierarchical data of nsf programs and proposals furthermore an incremental layout method is adopted to handle information on a large scale the improved treemap visualization will help to visually analyze the static funding related data and the stacked graph is utilized to analyze the timeseries data through these visual analysis techniques research trends of nsf popular nsf programs are quickly identified 
towards interactive indexing for large chinese calligraphic character databases towards interactive indexing for large chinese calligraphic character databases in this paper based on a novel shapesimilaritybased retrieval method we propose an interactive partialdistancemap pdm based highdimensional indexing scheme to speed up the retrieval performance of the large chinese calligraphic character databases specifically we use the approximate minimal bounding hyper sphere of query character to search the pdm and utilize the users relevance feedback to refine the search process we conduct comprehensive experiments to testify the efficiency and effectiveness of the proposed method 
queryspecific clustering of search results based on documentcontext similarity scores queryspecific clustering of search results based on documentcontext similarity scores this paper presents a pilot study of queryspecific clustering that uses our novel documentcontext based similarity scores as compared with document similarity scores clustering is applied to the top retrieved documents for a given query clustering effectiveness is evaluated based on the mk score for trec trec and trec test collections encouraging results were obtained whereby documentcontext clustering produces better mk scores than document clustering with a confidence level if precision and recall are equally important 
selfcorrecting queries for xml selfcorrecting queries for xml it has been observed that queries over xml data sources are often unsatisfiable unsatisfiability may stem from several different sources eg the user may be insufficiently familiar with the labels appearing the documents or may not be intimately aware of the hierarchical structure of the documents this difficulty may be compounded by the fact that errors in query formulation lead to an empty answer and not to some sort of compilation error
evaluation of partial path queries on xml data evaluation of partial path queries on xml data xml query languages typically allow the specification of structural patterns of elements finding the occurrences of such patterns in an xml tree is the key operation in xml query processing many algorithms have been presented for this operation these algorithms focus mainly on the evaluation of pathpattern or treepattern queries in this paper we define a partial pathpattern query language and we address the problem of its efficient evaluation on xml data
effective keyword search for valuable lcas over xml documents effective keyword search for valuable lcas over xml documents in this paper we study the problem of effective keyword search over xml documents we begin by introducing the notion of valuable lowest common ancestor vlca to accurately and effectively answer keyword queries over xml documents we then propose the concept of compact vlca cvlca and compute the meaningful compact connected trees rooted as cvlcas as the answers of keyword queries to efficiently compute cvlcas we devise an effective optimization strategy for speeding up the computation and exploit the key properties of cvlca in the design of the stackbased algorithm for answering keyword queries we have conducted an extensive experimental study and the experimental results show that our proposed approach achieves both high efficiency and effectiveness when compared with existing proposals
autonomously semantifying wikipedia autonomously semantifying wikipedia bernerslees compelling vision of a semantic web is hindered by a chickenandegg problem which can be best solved by a bootstrapping method creating enough structured data to motivate the development of applications this paper argues that autonomously semantifying wikipedia is the best way to solve the problem we choose wikipedia as an initial data source because it is comprehensive not too large highquality and contains enough manuallyderived structure to bootstrap an autonomous selfsupervised process we identify several types of structures which can be automatically enhanced in wikipedia eg link structure taxonomic data infoboxes etc and we describea prototype implementation of a selfsupervised machine learning system which realizes our vision preliminary experiments demonstrate the high precision of our systems extracted data in one case equaling that of humans
randomized metric induction and evolutionary conceptual clustering for semantic knowledge bases randomized metric induction and evolutionary conceptual clustering for semantic knowledge bases we present an evolutionary clustering method which can be applied to multirelational knowledge bases storing semantic resource annotations expressed in the standard languages for the semantic web the method exploits an effective and languageindependent semidistance measure defined for the space of individual resources that is based on a finite number of dimensions corresponding to a committee of features represented by a group of concept descriptions discriminating features we show how to obtain a maximally discriminating group of features through a feature construction method based on genetic programming the algorithm represents the possible clusterings as strings of central elements medoids wrt the given metric of variable length hence the number of clusters is not needed as a parameter since the method can optimize it by means of the mutation operators and of a proper fitness function we also show how to assign each cluster with a newly constructed intensional definition in the employed concept language an experimentation with some ontologies proves the feasibility of our method and its effectiveness in terms of clustering validity indices
ontology module extraction for ontology reuse ontology module extraction for ontology reuse problems resulting from the management of shared distributed knowledge has led to ontologies being employed as a solution in order to effectively integrate information across applications this is dependent on having ways to share and reuse existing ontologies with the increased availability of ontologies on the web some of which include thousands of concepts novel and more efficient methods for reuse are being devised one possible way to achieve efficient ontology reuse is through the process of ontology module extraction a novel approach to ontology module extraction is presented that aims to achieve more efficient reuse of very large ontologies the motivation is drawn from an ontology engineering perspective this paper provides a definition of ontology modules from the reuse perspective and an approach to module extraction based on such a definition an abstract graph model for module extraction has been defined along with a module extraction algorithm the novel contribution of this paper is a module extraction algorithm that is independent of the language in which the ontology is expressed this has been implemented in modtool a tool that produces ontology modules via extraction experiments were conducted to compare modtool to other modularisation methods
semantic verification in an online fact seeking environment semantic verification in an online fact seeking environment many artificial intelligence tasks such as automated question answering reasoning or heterogeneous database integration involve verification of a semantic category eg coffee is a drink red is a color while steak is not a drink and big is not a color we present a novel algorithm to automatically validate a semantic category contrary to the methods suggested earlier our approach does not rely on any manually codified knowledge but instead capitalizes on the diversity of topics and word usage on the world wide web we have tested our approach within our online factseeking question answering environment when tested on the trec questions that expect the answer to belong to a specific semantic category our approach has improved the accuracy by up to depending on the model and metrics used
developing learning strategies for topicbased summarization developing learning strategies for topicbased summarization most uptodate wellbehaved topicbased summarization systems are built upon the extractive framework they score the sentences based on the associated features by manually assigning or experimentally tuning the weights of the features in this paper we discuss how to develop learning strategies in order to obtain the optimal feature weights automatically which can be used for assigning a sound score to a sentence characterized with a set of features the two fundamental issues are about training data and learning models to save the costly manual annotation time and effort we construct the training data by labeling the sentence with a true score calculated according to human summaries the support vector regression svr model is then used to learn how to relate the true score of the sentence to its features once the relations have been mathematically modeled svr is able to predict the estimated score for any given sentence the evaluations by rouge criterion on duc and duc document sets demonstrate the competitiveness and the adaptability of the proposed approaches
lightweight webbased fact repositories for textual question answering lightweight webbased fact repositories for textual question answering since answers to factseeking questions usually reside within small factual text nuggets often hidden within fulllength documents their relevance to a question is not necessarily correlated to the relevance of the fulllength document to the question yet previous approaches to opendomain textual question answering from large document collections quasiunanimously employ a document retrieval stage in order to apply widely different often expensive answer mining techniques to only a small subset of documents depending on the collection size or more of the documents in the collection much more in the case of the web are left out of the selected subset for any given query and thus become invisible to subsequent processing stages for actual answer mining this paper introduces a new model for answer retrieval for question answering the collection is distilled offline into large repositories of facts each fact constitutes a potential direct answer to questions seeking a particular kind of entity or relation such as questions asking about the date of particular events question answering becomes equivalent to online fact retrieval which greatly simplifies the defacto system architecture for factseeking question answering in addition to simplicity experiments on a fact repository acquired from approximately a billion web documents illustrate the impact of fact repositories in extracting accurate answers to a standard evaluation set of opendomain test questions and additional sets of domainspecific questions
just in time indexing for up to the second search just in time indexing for up to the second search ecommerce and intranet search systems require newly arriving content to be indexed and made available for search within minutes or hours of arrival applications such as file system and email search demand even faster turnaround from search systems requiring new content to become available for search almost instantaneously however incrementally updating inverted indices which are the predominant datastructure used in search engines is an expensive operation that most systems avoid performing at high rates
quickmig quickmig a common task in many database applications is the migration of legacy data from multiple sources into a new one this requires identifying semantically related elements of the source and target systems and the creation of mapping expressions to transform instances of those elements from the source format to the target format currently data migration is typically done manually a tedious and timeconsuming process which is difficult to scale to a high number of data sources in this paper we describe quickmig a new semiautomatic approach to determining semantic correspondences between schema elements for data migration applications quickmig advances the state of the art with a set of new techniques exploiting sample instances domain ontologies and reuse of existing mappings to detect not only element correspondences but also their mapping expressions quickmig further includes new mechanisms to effectively incorporate domain knowledge of users into the matching process the results from a comprehensive evaluation using realworld schemas and data indicate the high quality and practicability of the overall approach
automatic call section segmentation for contactcenter calls automatic call section segmentation for contactcenter calls this paper presents a svm support vector machine classification system which divides contactcenter call transcripts into greeting question refine research resolution closing and outoftopic sections this call section segmentation is useful to improve search and retrieval functions and to provide more detailed statistics on calls we use an offtheshelf automatic speech recognition asr system to generate call transcripts from recorded calls between customers and service representatives
learning on the border learning on the border this paper is concerned with the class imbalance problem which has been known to hinder the learning performance of classification algorithms the problem occurs when there are significantly less number of observations of the target concept various realworld classification tasks such as medical diagnosis text categorization and fraud detection suffer from this phenomenon the standard machine learning algorithms yield better prediction performance with balanced datasets in this paper we demonstrate that active learning is capable of solving the class imbalance problem by providing the learner more balanced classes we also propose an efficient way of selecting informative instances from a smaller pool of samples for active learning which does not necessitate a search through the entire dataset the proposed method yields an efficient querying system and allows active learning to be applied to very large datasets our experimental results show that with an early stopping criteria active learning achieves a fast solution with competitive prediction performance in imbalanced data classification
reconstructing ddc for interactive classification reconstructing ddc for interactive classification the automated text categorization tc has made prominent progress in recent years however seldom work is done on automatic classification with library classification systems the largest and most sophisticated classification systems people ever built such as the dewey decimal classification ddc the library classification is a very laborious and timeconsuming job that requires qualification and good training
diva diva clustering is a common technique used to extract knowledge from a dataset in unsupervised learning in contrast to classical propositional approaches that only focus on simple and flat datasets relational clustering can handle multitype interrelated data objects directly and adopt semantic information hidden in the linkage structure to improve the clustering result however exploring linkage information will greatly reduce the scalability of relational clustering moreover some characteristics of vector data space utilized to accelerate the propositional clustering procedure are no longer valid in relational data space these two disadvantages restrain the relational clustering techniques from being applied to very large datasets or in timecritical tasks such as online recommender systems in this paper we propose a new variancebased clustering algorithm to address the above difficulties our algorithm combines the advantages of divisive and agglomerative clustering paradigms to improve the quality of cluster results by adopting the idea of representative object it can be executed with linear time complexity experimental results show our algorithm achieves high accuracy efficiency and robustness in comparison with some wellknown relational clustering approaches
comparing the effectiveness of hits and salsa comparing the effectiveness of hits and salsa this paper compares the effectiveness of two wellknown querydependent linkbased ranking algorithms hyperlinkinduced topic search hits and the stochastic approach for linkstructure analysis salsa the two algorithms are evaluated on a very large web graph induced by million crawled web pages and a set of queries and results labeled by human judges we employed three different performance measures mean average precision map mean reciprocal rank mrr and normalized discounted cumulative gain ndcg we found that as an isolated feature salsa substantially outperforms hits this is quite surprising given that the two algorithms operate over the same neighborhood graph induced by the query result set we also studied the combination of salsa and hits with bmf a stateoftheart textbased scoring function that incorporates anchor text we found that the combination of salsa and bmf outperforms the combination of hits and bmf finally we broke down our query set by query specificity and found that salsa and to a lesser extent hits is most effective for general queries
computing block importance for searching on web sites computing block importance for searching on web sites in this paper we consider the problem of using the block structure of a web page to improve ranking results when searching for information on web sites given the block structure of the web pages as input we propose a method for computing the importance of each block in the form of block weights in a web collection as we show through experiments the deployment of our method may allow a significant improvement in the quality of search results we ran experiments to compare the quality of search results when using our method to the quality obtained when using no structure information when compared to a ranking method that considered pages as monolithic units our blockbased ranking method led to improvements in the quality of search results in experiments with two sites with heterogeneous structures further our method does not increase the cost of processing queries when compared to the systems using no structural information
predictive user click models based on clickthrough history predictive user click models based on clickthrough history web search engines consistently collect information about users interaction with the system they record the query they issued the url of presented and selected documents along with their ranking this information is very valuable it is a poll over millions of users on the most various topics and it has been used in many ways to mine users interests and preferences query logs have the potential to partially alleviate the search engines from thousand of searches by providing a way to predict answers for a subset of queries and users without knowing the content of a document even if the predicted result is at rank one this analysis might be of interest if there is enough confidence on a users click we might redirect the user directly to the page whose link would be clicked in this paper we present three different models for predicting user clicks ranging from most specific ones using only past user history for the query to very general ones aggregating data over all users for a given query the former model has a very high precision at low recall values while the latter can achieve high recalls we show that it is possible to combine the different models to predict with high accuracy over a high subset of query sessions of all the sessions
modeling historical and future movements of spatiotemporal objects in moving objects databases modeling historical and future movements of spatiotemporal objects in moving objects databases spatiotemporal databases deal with geometries changing over time in general geometries do not only change discretely but continuously hence we are dealing with moving objects in the past a few moving object data models and query languages have been proposed each of them supports either historical movements or future movements but not both together consequently queries that start in the past and extend into the future cannot be supported to model both historical and future movements of an object two separate concepts with different properties are required and extra attention is necessary to avoid their conflicts furthermore current definitions of moving objects are too general and vague it is unclear how a moving object is allowed to move through space and time for instance the continuity or discontinuity of motion is not specified in this paper we propose a new moving object data model called balloon model which provides integrated support for both historical and future movements of moving objects as part of the model we provide formal definitions of moving objects with respect to their past and future movements all kinds of queries including past queries future queries and queries that start in the past and end in the future are supported in our model
the tsql temporal query language the tsql temporal query language time characterizes every aspect of our life and its management when storing and querying data is very important in this paper we propose a new temporal query language called tsql supporting multiple temporal dimensions of data besides the wellknown valid and transaction times it encompasses two additional temporal dimensions namely availability and event times the availability time records when information is known and treated as true by the information system the event times record the occurrence times of both the event that starts the valid time and the event that ends it tsql is capable to deal with different temporal semantics atemporal aka nonsequenced current sequenced next with respect to every temporal dimension moreover tsql provides a novel temporal grouping clause and an orthogonal management of temporal properties when defining the selection conditions and the schema for the output relation
boolean representation based dataadaptive correlation analysis over time series streams boolean representation based dataadaptive correlation analysis over time series streams correlation analysis is a basic problem in the field of data stream mining typical approaches add sliding window to data streams to get the recent results but the window length defined by users is always fixed which is not suitable for the changing stream environment we propose a boolean representation based dataadaptive method for correlation analysis among a large number of time series streams the periodical trends of each stream series to are monitored to choose the most suitable window size and group the series with the same trends together instead of adopting complex pairwise calculation we can also quickly get the correlation pairs of series at the optimal window sizes all the processing is realized by simple boolean operations both the theory analysis and the experimental evaluations show that our method has good computation efficiency with high accuracy
discovering interesting usage patterns in text collections discovering interesting usage patterns in text collections this paper addresses the problem of making text mining results more comprehensible to humanities scholars journalists intelligence analysts and other researchers in order to support the analysis of text collections our system featurelens visualizes a text collection at several levels of granularity and enables users to explore interesting text patterns the current implementation focuses on frequent itemsets of ngrams as they capture the repetition of exact or similar expressions in the collection users can find meaningful cooccurrences of text patterns by visualizing them within and across documents in the collection this also permits users to identify the temporal evolution of usage such as increasing decreasing or sudden appearance of text patterns the interface could be used to explore other text features as well initial studies suggest that featurelens helped a literary scholar and users generate new hypotheses and interesting insights using text collections
ontology evaluation using wikipedia categories for browsing ontology evaluation using wikipedia categories for browsing ontology evaluation is a maturing discipline with methodologies and measures being developed and proposed however evaluation methods that have been proposed have not been applied to specific examples in this paper we present the stateoftheart in ontology evaluation current methodologies criteria and measures analyse appropriate evaluations that are important to our application browsing in wikipedia and apply these evaluations in the context of ontologies with varied properties specifically we seek to evaluate ontologies based on categories found in wikipedia
wikify wikify this paper introduces the use of wikipedia as a resource for automatic keyword extraction and word sense disambiguation and shows how this online encyclopedia can be used to achieve stateoftheart results on both these tasks the paper also shows how the two methods can be combined into a system able to automatically enrich a text with links to encyclopedic knowledge given an input document the system identifies the important concepts in the text and automatically links these concepts to the corresponding wikipedia pages evaluations of the system show that the automatic annotations are reliable and hardly distinguishable from manual annotations
measuring article quality in wikipedia measuring article quality in wikipedia wikipedia has grown to be the world largest and busiest free encyclopedia in which articles are collaboratively written and maintained by volunteers online despite its success as a means of knowledge sharing and collaboration the public has never stopped criticizing the quality of wikipedia articles edited by nonexperts and inexperienced contributors in this paper we investigate the problem of assessing the quality of articles in collaborative authoring of wikipedia we propose three article quality measurement models that make use of the interaction data between articles and their contributors derived from the article edit history our bltscpgtasicltscpgt model is designed based on the mutual dependency between article quality and their author authority the pltscpgteerltscpgtrltscpgteviewltscpgt model introduces the review behavior into measuring article quality finally our pltscpgtrobltscpgtrltscpgteviewltscpgt models extend pltscpgteerltscpgtrltscpgteviewltscpgt with partial reviewership of contributors as they edit various portions of the articles we conduct experiments on a set of welllabeled wikipedia articles to evaluate the effectiveness of our quality measurement models in resembling human judgement
automatic feature selection in the markov random field model for information retrieval automatic feature selection in the markov random field model for information retrieval previous applications of the markov random field model for information retrieval have used manually chosen features however it is often difficult or impossible to know a priori the best set of features to use for a given task or data set therefore there is a need to develop automatic feature selection techniques in this paper we describe a greedy procedure for automatically selecting features to use within the markov random field model for information retrieval we also propose a novel robust method for describing classes of textual information retrieval features experimental results evaluated on standard trec test collections show that our feature selection algorithm produces models that are either significantly more effective than or equally effective as models with manually selected features such as those used in the past
parameter sensitivity in the probabilistic model for adhoc retrieval parameter sensitivity in the probabilistic model for adhoc retrieval the term frequency normalisation parameter sensitivity is an important issue in the probabilistic model for information retrieval a high parameter sensitivity indicates that a slight change of the parameter value may considerably affect the retrieval performance therefore a weighting model with a high parameter sensitivity is not robust enough to provide a consistent retrieval performance across different collections and queries in this paper we suggest that the parameter sensitivity is due to the fact that the query term weights are not adequate enough to allow informative query terms to differ from noninformative ones we show that query term reweighing which is part of the relevance feedback process can be successfully used to reduce the parameter sensitivity experiments on five text retrieval conference trec collections show that the parameter sensitivity does remarkably decrease when query terms are reweighed
utilizing a geometry of context for enhanced implicit feedback utilizing a geometry of context for enhanced implicit feedback implicit feedback algorithms utilize interaction between searchers and search systems to learn more about users needs and interests than expressed in query statements alone this additional information can be used to formulate improved queries or directly improve retrieval performance in this paper we present a geometric framework that utilizes multiple sources of evidence present in this interaction context eg display time document retention to develop enhanced implicit feedback models personalized for each user and tailored for each search task we use rich interaction logs and associated metadata such as relevance judgments gathered during a longitudinal user study as relevance stimuli to compare an implicit feedback algorithm developed using the framework with alternative algorithms our findings demonstrate both the effectiveness of our proposed algorithm and the potential value of incorporating multiple sources of interaction evidence when developing implicit feedback algorithms
parallel linkage parallel linkage we study the parallelization of the record linkage problem ie to identify matching records between two collections of records a and b one of main idiosyncrasies of the linkage problem compared to database join is the fact that once two records a in a and b in b are matched and merged to c c needs to be compared to the rest of records in a and b again since it may incur new matching this refeeding stage of the linkage problem requires its solution to be iterative and complicates the problem significantly toward this problem we first discuss three plausible scenarios of inputs when both collections are clean only one is clean and both are dirty then we show that the intricate interplay between match and merge can exploit the characteristics of each scenario to achieve good parallelization our parallel algorithms achieve times faster in speedup compared to sequential ones with processors and improvement in efficiency compared to pswoosh
structurebased inference of xml similarity for fuzzy duplicate detection structurebased inference of xml similarity for fuzzy duplicate detection fuzzy duplicate detection aims at identifying multiple representations of realworld objects stored in a data source and is a task of critical practical relevance in data cleaning data mining or data integration it has a long history for relational data stored in a single table or in multiple tables with equal schema algorithms for fuzzy duplicate detection in more complex structures eg hierarchies of a data warehouse xml data or graph data have only recently emerged these algorithms use similarity measures that consider the duplicate status of their direct neighbors eg children in hierarchical data to improve duplicate detection effectiveness in this paper we propose a novel method for fuzzy duplicate detection in hierarchical and semistructured xml data unlike previous approaches it not only considers the duplicate status of children but rather the probability of descendants being duplicates probabilities are computed efficiently using a bayesian network experiments show the proposed algorithm is able to maintain high precision and recall values even when dealing with data containing a high amount of errors and missing information our proposal is also able to outperform a stateoftheart duplicate detection system on three different xml databases
a strategy for allowing meaningful and comparable scores in approximate matching a strategy for allowing meaningful and comparable scores in approximate matching the goal of approximate data matching is to assess whether two distinct data instances represent the same real world object this is usually achieved through the use of a similarity function which returns a score that defines how similar two data instances are if this score surpasses a given threshold both data instances are considered as representing the same real world object the score values returned by a similarity function depend on the algorithm that implements the function and have no meaning to the user apart from the fact that a higher similarity value means that two data instances are more similar in this paper we propose that instead of defining the threshold in terms of the scores returned by a similarity function the user specifies the precision that is expected from the matching process precision is a well known quality measure and has a clear interpretation from the users point of view our approach relies on mapping between similarity scores and precision values based on a training data set experimental results show the training may be executed against a representative data set and reused for other databases from the same domain
spam filtering for short messages spam filtering for short messages we consider the problem of contentbased spam filtering for short text messages that arise in three contexts mobile sms communication blog comments and email summary information such as might be displayed by a lowbandwidth client short messages often consist of only a few words and therefore present a challenge to traditional bagofwords based spam filters using three corpora of short messages and message fields derived from real sms blog and spam messages we evaluate featurebased and compressionmodelbased spam filters we observe that bagofwords filters can be improved substantially using different features while compressionmodel filters perform quite well asis we conclude that content filtering for short messages is surprisingly effective
hybrid results merging hybrid results merging the problem of results merging in distributed information retrieval environments has been approached by two different directions in research estimation approaches attempt to calculate the relevance of the returned documents through adhoc methodologies weighted score merging regression etc while download approaches download all the documents locally partially or completely in order to estimate first hand their relevance both have their advantages and disadvantages it is assumed that download algorithms are more effective but they are very expensive in terms of time and bandwidth estimation approaches on the other hand usually rely on document relevance scores being returned by the remote collections in order to achieve maximum performance in addition to that regression algorithms which have proved to be more effective than weighted scores merging rely on a significant number of overlap documents in order to function effectively practically requiring multiple interactions with the remote collections the new algorithm that is introduced reconciles the above two approaches combining their strengths while minimizing their weaknesses it is based on downloading a limited selected number of documents from the remote collections and estimating the relevance of the rest through regression methodologies the proposed algorithm is tested in a variety of settings and its performance is found to be better than estimation approaches while approximating that of download
justintime contextual advertising justintime contextual advertising contextual advertising is a type of web advertising which given the url of a web page aims to embed into the page typically via javascript the most relevant textual ads available for static pages that are displayed repeatedly the matching of ads can be based on prior analysis of their entire content however ads need to be matched also to new or dynamically created pages that cannot be processed ahead of time analyzing the entire body of such pages onthefly entails prohibitive communication and latency costs to solve the threehorned dilemma of either lowrelevance or highlatency or highload we propose to use text summarization techniques paired with external knowledge exogenous to the page to craft short page summaries in real time empirical evaluation proves that matching ads on the basis of such summaries does not sacrifice relevance and is competitive with matching based on the entire page content specifically we found that analyzing a carefully selected fraction of the page text sacrifices only in ad relevance furthermore our summaries are fully compatible with the standard javascript mechanisms used for ad placement they can be produced at addisplay time by simple additions to the usual script and they only add bytes to the usual request
expertise drift and query expansion in expert search expertise drift and query expansion in expert search pseudorelevance feedback or query expansion has been shown to improve retrieval performance in the adhoc retrieval task in such a scenario a few topranked documents are assumed to be relevant and these are then used to expand and refine the initial user query such that it retrieves a higher quality ranking of documents however there has been little work in applying query expansion in the expert search task in this setting query expansion is applied by assuming a few topranked candidates have relevant expertise and using these to expand the query nevertheless retrieval is not improved as expected using such an approach we show that the success of the application of query expansion is hindered by the presence of topic drift within the profiles of experts that the system considers in this work we demonstrate how topic drift occurs in the expert profiles and moreover we propose three measures to predict the amount of drift occurring in an experts profile finally we suggest and evaluate ways of enhancing query expansion in expert search using our new insights our results show that once topic drift has been anticipated query expansion can be successfully applied in a general manner in the expert search task
extending query translation to crosslanguage query expansion with markov chain models extending query translation to crosslanguage query expansion with markov chain models dictionarybased approaches to query translation have been widely used in crosslanguage information retrieval clir experiments however translation has been not only limited by the coverage of the dictionary but also affected by translation ambiguities in this paper we propose a novel method of query translation that combines other types of term relation to complement the dictionarybased translation this allows extending the literal query translation to related words which produce a beneficial effect of query expansion in clir in this paper we model query translation by markov chains mc where query translation is viewed as a process of expanding query terms to their semantically similar terms in a different language in mc terms and their relationships are modeled as a directed graph and query translation is performed as a random walk in the graph which propagates probabilities to related terms this framework allows us to incorporating different types of term relation either between two languages or within the source or target languages in addition the iterative training process of mc allows us to attribute higher probabilities to the target terms more related to the original query thus offers a solution to the translation ambiguity problem we evaluated our method on three clir benchmark collections and obtained significant improvements over traditional dictionarybased approaches
query expansion using probabilistic local feedback with application to multimedia retrieval query expansion using probabilistic local feedback with application to multimedia retrieval as one of the most effective query expansion approaches local feedback is able to automatically discover new query terms and improve retrieval accuracy for different retrieval models however the performance of local feedback is heavily dependent on the assumption that most topranked documents are relevant to the query topic although this assumption might be sensible for adhoc text retrieval it is usually violated in many other retrieval tasks such as multimedia retrieval in this paper we develop a robust local analysis approach called probabilistic local feedback plf based on a discriminative probabilistic retrieval framework the proposed model is effective for improving retrieval accuracy without assuming the most topranked documents are relevant it also provides a sound probabilistic interpretation and a convergence guarantee on the iterative result updating process although derived from variational techniques this approach only involves an iterative process of simple operations on ranking features and thus can be computed efficiently in practice our multimedia retrieval experiments on trecvid collections have demonstrated the advantage of the proposed plf approaches which can achieve noticeable gains in terms of mean average precision over various baseline methods and prfaugmented results
a fast unified optimal route query evaluation algorithm a fast unified optimal route query evaluation algorithm we investigate the problem of how to evaluate fast and efficiently classes of optimal route queries on a massive graph in a unified framework to evaluate a route query effectively a large network is partitioned into a collection of fragments and distances of some optimal routes in the network are precomputed under such a setting we find a unified algorithm that can evaluate classes of optimal route queries the classes that can be processed efficiently are called constraint preserving cp which include among others shortest path forbidden edges forbidden nodes and autonomy optimal route query classes we prove the correctness of the unified algorithm we then turn our attention to the optimization of the proposed algorithm several pruning and optimization techniques are derived that minimize the search time and io accesses we show empirically that these techniques are effective the proposed optimal route query evaluation algorithm with all these techniques incorporated is compared with a mainmemory and a diskbased bruteforce cp algorithms we show experimentally that the proposed unified algorithm outperforms the bruteforce algorithms both in term of cpu time and io cost by a wide margin
towards practically feasible answering of regular path queries in lav data integration towards practically feasible answering of regular path queries in lav data integration regular path queries rpqs are given by means of regular expressions and ask for matching patterns on labeled graphs rpqs have received great attention in the context of semistructured data which are data whose structure is irregular partially known or subject to frequent changes one of the most important problems in databases today is the integration of semistructured data from multiple sources modeled as views the wellknow paradigm of computing first a viewbased rewriting of the query and then evaluating the rewriting on the view extensions is indeed possible for rpqs however computing the rewriting is computationally hard as it can only be done in the worst case in not less than exptime in this paper we provide practical evidence that computing the rewriting is hard on the average as well on the positive side we propose automatatheoretic techniques which efficiently compute and utilize instead the complement of the rewriting notably using the latter it is possible to answer a query and this makes the viewbased answering of rpqs fairly feasible in practice
optimizing parallel itineraries for knn query processing in wireless sensor networks optimizing parallel itineraries for knn query processing in wireless sensor networks spatial queries for extracting data from wireless sensor networks are important for many applications such as environmental monitoring and military surveillance one such query is k nearest neighbor knn query that facilitates sampling of monitored sensor data in correspondence with a given query location recently itinerarybased knn query processing techniques that propagate queries and collect data along a predetermined itinerary have been developed concurrently these research works demonstrate that itinerarybased knn query processing algorithms are able to achieve better energy efficiency than other existing algorithms however how to derive itineraries based on different performance requirements remains a challenging problem in this paper we propose a new itinerarybased knn query processing technique called pciknn that derives different itineraries aiming at optimizing two performance criteria response latency and energy consumption the performance of pciknn is analyzed mathematically and evaluated through extensive experiments experimental results show that pciknn has better performance and scalability than the stateoftheart
a twostage approach to domain adaptation for statistical classifiers a twostage approach to domain adaptation for statistical classifiers in this paper we consider the problem of adapting statistical classifiers trained from some source domains where labeled examples are available to a target domain where no labeled example is available one characteristic of such a domain adaptation problem is that the examples in the source domains and the target domain are known to follow different distributions thus a regular classification method would tend to overfit the source domains we present a twostage approach to domain adaptation where at the first ltgeneralization stage we look for a set of features generalizable across domains and at the second adaptation stage we pick up useful features specific to the target domain observing that the exact objective function is hard to optimize we then propose a number of heuristics to approximately achieve the goal of generalization and adaptation our experiments on gene name recognition using a real data set show the effectiveness of our general framework and the heuristics
clustering for unsupervised relation identification clustering for unsupervised relation identification unsupervised relation identification is the task of automatically discovering interesting relations between entities in a large text corpora relations are identified by clustering the frequently cooccurring pairs of entities in such a way that pairs occurring in similar contexts end up belonging to the same clusters in this paper we compare several clustering setups some of them novel and others already tried the setups include feature extraction and selection methods and clustering algorithms in order to do the comparison we develop a clustering evaluation metric specifically adapted for the relation identification task our experiments demonstrate significant superiority of the singlelinkage hierarchical clustering with the novel threshold selection technique over the other tested clustering algorithms also the experiments indicate that for successful relation identification it is important to use rich complex features of two kinds features that test both relation slots together relation features and features that test only one slot each entity features we have found that using both kinds of features with the best of the algorithms produces very highprecision results significantly improving over the previous work
merging distributed database summaries merging distributed database summaries the database summarization system coined sltscpgtaintltscpgteltscpgttiltscpgtq provides multiresolution summaries of structured data stored into acentralized database summaries are computed online with a conceptual hierarchical clustering algorithm however most companies work in distributed legacy environments and consequently the current centralized version of sltscpgtaintltscpgteltscpgttiltscpgtq is either not feasible privacy preserving or not desirable resource limitations
semantic components enhance retrieval of domainspecific documents semantic components enhance retrieval of domainspecific documents we seek to leverage knowledge about information organization in a domain to effectively and efficiently meet targeted information needs of expert users the semantic components model represents document content in a manner that is complementary to full text and keyword indexing semantic component instances are segments of text about a particular aspect of the main topic of the document and may not correspond to structural elements in the document this paper describes the semantic components model and presents experimental evidence from a large interactive searching study showing that semantic components used to supplement full text and keyword indexing and to extend the query language enhanced the retrieval of domainspecific documents in response to realistic queries posed by real users
latent semantic fusion model for image retrieval and annotation latent semantic fusion model for image retrieval and annotation this paper studies the effect of latent semantic analysis lsa on two different tasks multimedia document retrieval mdr and automatic image annotation aia the contributions of this paper are twofold first to the best of our knowledge this work is the first study of the influence of lsa on the retrieval of a significant number of multimedia documents ie collection of tourist images second it shows how different image representations regionbased and keypointbased can be combined by lsa to improve automatic image annotation the document collections used for these experiments are the corel photo collection and imageclef collection
a knowledgebased search engine powered by wikipedia a knowledgebased search engine powered by wikipedia this paper describes koru a new search interface that offers effective domainindependent knowledgebased information retrieval koru exhibits an understanding of the topics of both queries and documents this allows it to a expand queries automatically and b help guide the user as they evolve their queries interactively its understanding is mined from the vast investment of manual effort and judgment that is wikipedia we show how this open constantly evolving encyclopedia can yield inexpensive knowledge structures that are specifically tailored to expose the topics terminology and semantics of individual document collections we conducted a detailed user study with participants and topics from the trec hard track and found that koru and its underlying knowledge base offers significant advantages over traditional keyword search it was capable of lending assistance to almost every query issued to it making their entry more efficient improving the relevance of the documents they return and narrowing the gap between expert and novice seekers
a method for online analytical processing of text data a method for online analytical processing of text data there are increasingly visible demands for structured unstructured information integration and advanced analytics however conventional database technology has not been able to present a robust and practical implementation of a truly integrated architecture for such purposes after working on several industrial applications in particular in the healthcare and life sciences area we have identified fundamental issues and technical approaches to tackle the issues in this paper we propose data representations and algebraic operations for integrating semantic information eg ontologies into olap systems which allow us to analyze a huge set of textual documents with their underlying semantic information the performance of the prototype implementation has been evaluated using real world datasets and the high scalability and flexibility of our approach have been confirmed with respect to the computation time
mapgraph mapgraph online analytical processing is a database paradigm that provides for the rich analysis of multidimensional data olap is often supported by a logical structure known as the cube however supporting efficient olap query resolution in enterprise scale environments is an issue of considerable complexity in practice the difficulty of the problem is exacerbated by the existence of dimension hierarchies that subdivide core dimensions into aggregation layers of varying granularity common hierarchysensitive query operations such as rollup and drilldown can be very costly on large cubes moreover facilities for the representation of more complex hierarchical relationships are not well supported by conventional techniques this paper presents a robust hierarchy infrastructure called mapgraph that supports the efficient and transparent manipulation of attribute hierarchies within olap environments experimental results verify that when compared to the alternatives very little additional overhead is introduced even when advanced functionality is exploited
rkhist rkhist database query engines typically rely upon query size estimators in order to evaluate the potential cost of alternate query plans in multidimensional database systems such as those typically found in large data warehousing environments these selectivity estimators often take the form of multidimensional histograms but while single dimensional histograms have proven to be quite accurate even in the presence of data skew the multidimensional variations have generally been far less reliable in this paper we present a new histogram model that is based upon an rtree space partitioning the localization of the rtree boxes is in turn controlled by a hilbert space filling curve while a series of efficient area equalization heuristics restructures the initial boxes to provide improved bucket representation experimental results demonstrate significantly improved estimation accuracy relative to state of the art alternatives as well as superior consistency across a variety of record distributions
the role of documents vs queries in extracting class attributes from text the role of documents vs queries in extracting class attributes from text challenging the implicit reliance on document collections this paper discusses the pros and cons of using query logs rather than document collections as selfcontained sources of data in textual information extraction the differences are quantified as part of a largescale study on extracting prominent attributes or quantifiable properties of classes eg top speed price and fuel consumption for carmodel from unstructured text in a headtohead qualitative comparison a lightweight extraction method produces class attributes that are more accurate on average when acquired from query logs rather than web documents
domain knowledge conceptual intermedia indexing domain knowledge conceptual intermedia indexing conceptual indexing is a way to produce only one index for many multilingual documents intermedia conceptual indexing promotes the use of common concepts between two media in order to use a single index for several media in this paper we explore such an advance indexing point of view we show the benefit of an automatic conceptual indexing for texts and its extension for text and image documents tests are conducted on the multilingual image and text medical document corpus of the clef initiative where we obtain best results on text in and and show promising results on images and best results for the combination of image and text
an experimental study of the impact of information extraction accuracy on semantic search performance an experimental study of the impact of information extraction accuracy on semantic search performance researchers have shown that various natural language processing techniques can be used in document analysis to impact search performance for the most part they examined how an analysis system with certain performance characteristics can be leveraged to improve document andor passage search results we have previously shown that semantic queries which utilize named entity and relation information extracted from the corpus can lead to significant improvement in search performance in this paper we extend our previous efforts and examine how search performance degrades in the face of imperfect named entity and relation extraction our study was carried out by developing gold standard annotated corpora and applying different error models to the gold standard annotations to simulate errors made by automatic recognizers we identify automatic recognizer characteristics that make them more amenable to our search tasks show that recognizer recall has more significant impact on semantic search performance than its precision and demonstrate that significant improvement in both map and exact precision scores can be achieved by adopting automatic named entity and relation recognizers with near stateoftheart performance
predicting individual priorities of shared activities using support vector machines predicting individual priorities of shared activities using support vector machines activitycentric collaboration environments help knowledge workers to manage the context of their shared work activities by providing a representation for an activity and its resources activity management systems provide more structure and organization than email to execute the shared activity but as the number of shared activities increases it becomes more and more difficult for users to focus on important activities that need their attention this paper describes a personalized activity prioritization approach implemented on top of the lotus connections activities management system our prototype implementation allows each user to view activities ordered by herhis predicted priorities the predictions are made using a ranking support vector machine model trained with the users past interactions with the activities system we describe the prioritization interface and the results of an offline experiment based on data from users over months our results show that our feature set derived from shared activity structures can significantly increase prediction accuracy compared to a recency baseline
web search personalization with ontological user profiles web search personalization with ontological user profiles every user has a distinct background and a specific goal when searching for information on the web the goal of web search personalization is to tailor search results to a particular user based on that users interests and preferences effective personalization of information access involves two important challenges accurately identifying the user context and organizing the information in such a way that matches the particular context we present an approach to personalized search that involves building models of user context as ontological profiles by assigning implicitly derived interest scores to existing concepts in a domain ontology a spreading activation algorithm is used to maintain the interest scores based on the users ongoing behavior our experiments show that reranking the search results based on the interest scores and the semantic evidence in an ontological user profile is effective in presenting the most relevant results to the user
designing clusteringbased web crawling policies for search engine crawlers designing clusteringbased web crawling policies for search engine crawlers the world wide web is growing and changing at an astonishing rate web information systems such as search engines have to keep up with the growth and change of the web due to resource constraints search engines usually have difficulties keeping the local database completely synchronized with the web in this paper we study how tomake good use of the limited system resource and detect as many changes as possible towards this goal a crawler for the web search engine should be able to predict the change behavior of the webpages we propose applying clusteringbased sampling approach specifically we first group all the local webpages into different clusters such that each cluster contains webpages with similar change pattern we then sample webpages from each cluster to estimate the change frequency of all the webpages in that cluster finally we let the crawler revisit the cluster containing webpages with higher change frequency with a higher probability to evaluate the performance of an incremental crawler for a web search engine we measure both the freshness and the quality of the query results provided by the search engine we run extensive experiments on a real web data set of about distinct urls distributed among websites the results demonstrate that our clustering algorithm effectively clusters the pages with similar change patterns and our solution significantly outperforms the existing methods in that it can detect more changed webpages and improve the quality of the user experience for those who query the search engine
mining web multiresolution communitybased popularity for information retrieval mining web multiresolution communitybased popularity for information retrieval the pagerank algorithm is used in web information retrieval to calculate a single list of popularity scores for each page in the web these popularity scores are used to rank query results when presented to the user by using the structure of the entire web to calculate one score per document we are calculating a general popularity score not particular to any community therefore the pagerank scores are more suited to general queries in this paper we introduce a more general form of pagerank using web multiresolution communitybased popularity scores where each document obtains a popularity score dependent on a given web community when a query is related to a specific community we choose the associated set of popularity scores and order the query results accordingly using webcommunity based popularity scores we achieved an increase in precision over pagerank
learning querybiased web page summarization learning querybiased web page summarization querybiased web page summarization is the summarization of a web page reflecting the relevance of it to a specific query it plays an important role in search results representation of web search engines in this paper we propose a learningbased querybiased web page summarization method the summarization problem is solved within the typical sentence selection framework different from existing web page summarization methods that use page content or link context alone both of them are considered as the sources of sentences in this work most of existing learningbased summarization methods treat summarization as a sentence classification problem and train a classifier to discriminate between extracted sentences and nonextracted sentences of all training documents the basic assumption of these methods is that sentences from different documents are comparable with respect to the class information in contrast to the classification scheme a ranking scheme is introduced to rank extracted sentences higher than nonextracted sentences of each training document the underlying assumption that sentences within a document are comparable is weaker and more reasonable than the assumption of classificationbased scheme extensive results using intrinsic evaluation metrics gauge many aspects of the proposed method
efficient search ranking in social networks efficient search ranking in social networks in social networks such as orkut wwworkutcom a large portion of the user queries refer to names of other people indeed more than of the queries in orkut are about names of other users with an average of terms per query further the users usually search for people with whom they maintain relationships in the network these relationships can be modelled as edges in a friendship graph a graph in which the nodes represent the users in this context search ranking can be modelled as a function that depends on the distances among users in the graph more specifically of shortest paths in the friendship graph however application of this idea to ranking is not straightforward because the large size of modern social networks dozens of millions of users prevents efficient computation of shortest paths at query time we overcome this by designing a ranking formula that strikes a balance between producing good results and reducing query processing time using data from the orkut social network which includes over million users we show that our ranking augmented by this new signal produces high quality results while maintaining query processing time small
dex dex link and graph analysis tools are important devices to boost the richness of information retrieval systems internet and the existing social networking portals are just a couple of situations where the use of these tools would be beneficial and enriching for the users and the analysts however the need for integrating different data sources and even more important the need for high performance generic tools is at odds with the continuously growing size and number of data repositories
shine shine heterogeneous entities or objects are very common and are usually interrelated with each other in many scenarios for example typical web search activities involve multiple types of interrelated entities such as end users web pages and search queries in this paper we define and study a novel problem ltulgtsltulgtearch ltulgthltulgteterogeneous ltulgtinltulgtterrelated ltulgteltulgtntities shine given a shinequery which can be any types of entities the task of shine is to retrieve multiple types of related entities to answer this query this is in contrast to the traditional searchwhich only deals with a single type of entities eg web pages the advantages of shine include it is feasible for end users to specify their information need along different dimensions by accepting queries with different types answering a query by multiple types of entities provides informative context for users to better understand the search results and facilitate their information exploration multiple relations among heterogeneous entities can be utilized to improve the ranking of any particular type of entities to attain the goal of shine we propose to represent all entities in a unified space through utilizing their interaction relationships two approaches mlsa and evsm are discussed and compared in this paper the experiments on data sets ie a literature data set a search engine log data set and a recommendation data set show the effectiveness and flexibility of our proposed methods
reasoning about vague topological information reasoning about vague topological information topological information plays a fundamental role in the human perception of spatial configurations and is thereby one of the most prominent geographical features in natural language as vagueness abounds in geography flexible formalisms with the ability to capture vague topological information are often needed in practice while such formalisms have already been introduced by various authors complete reasoning procedures are usually not discussed in this paper we show how many interesting reasoning tasks such as consistency checking and entailment checking can be supported in a generalization of the wellknown rcc calculus in particular we present decision procedures based on linear programming solving all reasoning tasks of interest we furthermore show how deciding the consistency of vague topological information can be reduced to the consistency problem of the original rcc
nugget discovery in visual exploration environments by query consolidation nugget discovery in visual exploration environments by query consolidation queries issued by casual users or specialists exploring a dataset often point us to important subsets of the data be it clusters outliers or other meaningful features capturing and caching such queries henceforth called nuggets has many potential benefits including the optimization of the system performance and the search experience of users unfortunately current visual exploration systems have not yet tapped into this potential resource of identifying and sharing important queries in this paper we introduce a query consolidation strategy aimed at solving the general problem of isolating important queries from the potentially huge amount of queries submitted our solution clusters redundant queries caused by explorationstyle query specification which is prevalent in data exploration systems to measure the similarity between queries we designed an effective distance metric that incorporates both the query specification and the actual query result to overcome its high complexity when comparing queries with large result sets we designed an approximation method which is efficient while still providing excellent accuracy a user study conducted on multivariate data sets comparing our proposed technique to others in the literature confirms that the proposed distance metric indeed matches well with users intuition as proof of feasibility we integrated our proposed query consolidation solution into the nugget management system nms framework which is based on a visual exploration system xmdvtool a second user study indicates that both the efficiency and accuracy of users visual exploration are enhanced when supported by nms
finding dense and isolated submarkets in a sponsored search spending graph finding dense and isolated submarkets in a sponsored search spending graph methods for improving sponsored search revenue are often tested or deployed within a small submarket of the larger marketplace for many applications the ideal submarket contains a small number of nodes a large amount of spending within the submarket and a small amount of spending leaving the submarket we introduce an efficient algorithm for finding submarkets that are optimal for a userspecified tradeoff between these three quantities we apply our algorithm to find submarkets that are both dense and isolated in a large spending graph from yahoo sponsored search
a comparison of statistical significance tests for information retrieval evaluation a comparison of statistical significance tests for information retrieval evaluation information retrieval ir researchers commonly use three tests of statistical significance the students paired ttest the wilcoxon signed rank test and the sign test other researchers have previously proposed using both the bootstrap and fishers randomization permutation test as nonparametric significance tests for ir but these tests have seen little use for each of these five tests we took the adhoc retrieval runs submitted to trecs and and for each pair of runs we measured the statistical significance of the difference in their mean average precision we discovered that there is little practical difference between the randomization bootstrap and t tests both the wilcoxon and sign test have a poor ability to detect significance and have the potential to lead to false detections of significance the wilcoxon and sign tests are simplified variants of the randomization test and their use should be discontinued for measuring the significance of a difference between means
inferring document relevance from incomplete information inferring document relevance from incomplete information recent work has shown that average precision can be accurately estimated from a small random sample of judged documents unfortunately such random pools cannot be used to evaluate retrieval measures in any standard way in this work we show that given such estimates of average precision one can accurately infer the relevances of the remaining unjudged documents thus obtaining a fully judged pool that can be used in standard ways for system evaluation of all kinds using trec data we demonstrate that our inferred judged pools are well correlated with assessor judgments and we further demonstrate that our inferred pools can be used to accurately infer precision recall curves and all commonly used measures of retrieval performance
hypothesis testing with incomplete relevance judgments hypothesis testing with incomplete relevance judgments information retrieval experimentation generally proceeds in a cycle of development evaluation and hypothesis testing ideally the evaluation and testing phases should be short and easy so as to maximize the amount of time spent in development there has been recent work on reducing the amount of assessor effort needed to evaluate retrieval systems but it has not for the most part investigated the effects of these methods on tests of significance in this work we explore in detail the effects of reduced sets of judgments on the sign test we demonstrate both analytically and empirically the relationship between the power of the test the number of topics evaluated and the number of judgments available using these relationships we can determine the number of topics and judgments needed for the leastcost but highestconfidence significance evaluation specifically testing pairwise significance over topics with fewer than judgments for each is as good as testing significance over topics with an average of judgments for each less effort producing no additional errors
external perfect hashing for very large key sets external perfect hashing for very large key sets we present a simple and efficient external perfect hashing scheme referred to as eph algorithm for very large static key sets we use a number of techniques from the literature to obtain a novel scheme that is theoretically wellunderstood and at the same time achieves an orderofmagnitude increase in the size of the problem to be solved compared to previous practical methods we demonstrate the scalability of our algorithm by constructing minimum perfect hash functions for a set of billion urls from the world wide web of average length characters in approximately minutes using a commodity pc our scheme produces minimal perfect hash functions using approximately bits per key for perfect hash functions in the range n the space usage drops to approximately bits per key the main contribution is the first algorithm that has experimentally proven practicality for sets in the order of billions of keys and has time and space usage carefully analyzed without unrealistic assumptions
optimal proactive caching in peertopeer network optimal proactive caching in peertopeer network as a promising new technology with the unique properties like high efficiency scalability and fault tolerance peertopeer pp technology is used as the underlying network to build new internetscale applications however one of the well known issues in such an application for example www is that the distribution of data popularities is heavily tailed with a zipflike distribution with consideration of the skewed popularity we adopt a proactive caching approach to handle the challenge and focus on two key problems where ie the placement strategy where to place the replicas and how ie the degree problem how many replicas are assigned to one specific content for the where problem we propose a novel approach which can be generally applied to structured pp networks next we solve two optimization objectives related to the how problem maxperf and mincost our solution is called ltbgtpopcacheltbgt and we discover two interesting properties the number of replicas assigned to each content is proportional to its popularity the derived optimal solutions are related to the entropy of popularity to our knowledge none of the previous works has mentioned such results finally we apply the results of popcache to propose a pp base web caching called as webpopcache by means of web cache trace driven simulation our extensive evaluation results demonstrate the advantages of popcache and webpopcache
efficient evaluation of highselective xml twig patterns with parent child edges in treeunaware rdbms efficient evaluation of highselective xml twig patterns with parent child edges in treeunaware rdbms recent study showed that native twig join algorithms and treeaware relational framework significantly outperform treeunaware approaches in evaluating structural relationships in xml twig queries in this paper we present an efficient strategy to evaluate highselective twig queries containing only parentchild relationships in a treeunaware relational environment our scheme is built on top of our sltscpgtucxentltscpgt system we show that by exploiting the encoding scheme of sltscpgtucxentltscpgt we can devise efficient strategy for evaluating such twig queries extensive performance studies on various data sets and queries show that our approach performs better than a representative treeunaware approach gltscpgtloballtscpgtoltscpgtrderltscpgt and a stateoftheart native twig join algorithm tjfltscpgtastltscpgt on all benchmark queries with the highest observed gain factors being and respectively additionally our approach reduces significantly the performance gap between treeaware and treeunaware approaches and even outperforms a treeaware approachmltscpgtonetltscpgtdbxqltscpgtueryltscpgt for certain highselective twig queries we also report our insights to the plan choices a relational optimizer made during twig query evaluation by visually characterizing its behavior over the relational selectivity space
weaklysupervised discovery of named entities using web search queries weaklysupervised discovery of named entities using web search queries a seedbased framework for textual information extraction allows for weakly supervised extraction of named entities from anonymized web search queries the extraction is guided by a small set of seed named entities without any need for handcrafted extraction patterns or domainspecific knowledge allowing for the acquisition of named entities pertaining to various classes of interest to web search users inherently noisy search queries are shown to be a highly valuable albeit little explored resource for webbased named entity discovery
leveraging context in usercentric entity detection systems leveraging context in usercentric entity detection systems a usercentric entity detection system is one in which the primary consumer of the detected entities is a person who can perform actions on the detected entities eg perform a search view a map shop etc we contrast this with machinecentric detection systems where the primary consumer of the detected entities is a machine machinecentric detection systems typically focus on the quantity of detected entities measured by precision and recall metrics with the goal of correctly identifying every single entity in a document
type nanotheories type nanotheories we present in this paper type nanotheories tn a framework for representing the knowledge necessary for performing similarity comparisons between pairs of terms of the same type tn itself uses another methodology namely support outcomes which is also introduced many ir and nlp applications use redundancy as a factor to increase confidence and tnbased comparisons can determine redundancy better than simple string comparisons results include a showing of a increase in confidenceweighted score for an endtoend qa system and an up to improvement over baseline in an answerkey equivalencing experiment
recognition and classification of noun phrases in queries for effective retrieval recognition and classification of noun phrases in queries for effective retrieval it has been shown that using phrases properly in the document retrieval leads to higher retrieval effectiveness in this paper we define four types of noun phrases and present an algorithm for recognizing these phrases in queries the strengths of several existing tools are combined for phrase recognition our algorithm is tested using a set of web queries from a query log and a set of trec queries experimental results show that our algorithm yields high phrase recognition accuracy we also use a baseline noun phrase recognition algorithm to recognize phrases from the trec queries a document retrieval experiment is conducted using the trec queries without any phrases with the phrases recognized from a baseline noun phrase recognition algorithm and with the phrases recognized from our algorithm respectively the retrieval effectiveness of is better than that of which is better than that of this demonstrates that utilizing phrases in queries does improve the retrieval effectiveness and better noun phrase recognition yields higher retrieval performance
generalizing from relevance feedback using named entity wildcards generalizing from relevance feedback using named entity wildcards traditional adaptive filtering systems learn the users interests in a rather simple way words from relevant documents are favored in the query model while words from irrelevant documents are downweighted this biases the query model towards specific words seen in the past causing the system to favor documents containing relevant but redundant information over documents that use previously unseen words to denote new facts about the same news event this paper proposes news ways of generalizing from relevance feedback by augmenting the traditional bagofwords query model with named entity wildcards that are anchored in context the use of wildcards allows generalization beyond specific words while contextual restrictions limit the wildcardmatching to entities related to the users query we test our new approach in a nuggetlevel adaptive filtering system and evaluate it in terms of both relevance and novelty of the presented information our results indicate that higher recall is obtained when lexical terms are generalized using wildcards however such wildcards must be anchored to their context to maintain good precision how the context of a wildcard is represented and matched against a given document also plays a crucial role in the performance of the retrieval system
proximitybased document representation for named entity retrieval proximitybased document representation for named entity retrieval one aspect in which retrieving named entities is different from retrieving documents is that the items to be retrieved persons locations organizations are only indirectly described by documents throughout the collection much work has been dedicated to finding references to named entities in particular to the problems of named entity extraction and disambiguation however just as important for retrieval performance is how these snippets of text are combined to build named entity representations
regularized locality preserving indexing via spectral regression regularized locality preserving indexing via spectral regression we consider the problem of document indexing and representation recently locality preserving indexing lpi was proposed for learning a compact document subspace different from latent semantic indexing lsi which is optimal in the sense of global euclidean structure lpi is optimal in the sense of local manifold structure however lpi is not efficient in time and memory which makes it difficult to be applied to very large data set specifically the computation of lpi involves eigendecompositions of two dense matrices which is expensive in this paper we propose a new algorithm called regularized locality preserving indexing rlpi benefit from recent progresses on spectral graph analysis we cast the original lpi algorithm into a regression framework which enable us to avoid eigendecomposition of dense matrices also with the regression based framework different kinds of regularizers can be naturally incorporated into our algorithm which makes it more flexible extensive experimental results show that rlpi obtains similar or better results comparing to lpi and it is significantly faster which makes it an efficient and effective data preprocessing method for large scale text clustering classification and retrieval
efficient online index maintenance for dynamic text collections by using dynamic balancing tree efficient online index maintenance for dynamic text collections by using dynamic balancing tree previous online index maintenance strategies are mainly designed for document insertions without considering document deletions in a truly dynamic search environment however documents may be added to and removed from the collection at any point in time in this paper we examine issues of online index maintenance with support for instantaneous document deletions and insertions we present a dbt merge strategy that can dynamically adjust the sequence of subindex merge operations during index construction and offers better query processing performance than previous methods while providing an equivalent level of index maintenance performance when document insertions and deletions exist in parallel using experiments on gb of web data we demonstrate the efficiency of our method in practice showing that online index construction for dynamic text collections can be performed efficiently and almost as fast as for growing text collections
index compression is good especially for random access index compression is good especially for random access index compression techniques are known to substantially decrease the storage requirements of a text retrieval system as a sideeffect they may increase its retrieval performance by reducing disk io overhead despite this advantage developers sometimes choose to store index data in uncompressed form in order to not obstruct random access into each index terms postings list
effective topk computation in retrieving structured documents with termproximity support effective topk computation in retrieving structured documents with termproximity support modern web search engines are expected to return topk results efficiently given a query although many dynamic index pruning strategies have been proposed for efficient topk computation most of them are prone to ignore some especially important factors in ranking functions eg term proximity the distance relationship between query terms in a document the inclusion of term proximity breaks the monotonicity of ranking functions and therefore leads to additional challenges for efficient query processing this paper studies the performance of some existing topk computation approaches using termproximityenabled ranking functions our investigation demonstrates that when term proximity is incorporated into ranking functions most existing index structures and topk strategies become quite inefficient according to our analysis and experimental results we propose two index structures and their corresponding index pruning strategies structured and hybrid which performs much better on the new settings moreover the efficiency of index building and maintenance would not be affected too much with the two approaches
generating concise association rules generating concise association rules association rule mining has made many achievements in the area of knowledge discovery however the quality of the extracted association rules is a big concern one problem with the quality of the extracted association rules is the huge size of the extracted rule set as a matter of fact very often tens of thousands of association rules are extracted among which many are redundant thus useless mining nonredundant rules is a promising approach to solve this problem the minmax exact basis proposed by pasquier et al pasquier has showed exciting results by generating only nonredundant rules in this paper we first propose a relaxing definition for redundancy under which the minmax exact basis still contains redundant rules then we propose a condensed representation called reliable exact basis for exact association rules the rules in the reliable exact basis are not only nonredundant but also more succinct than the rules in minmax exact basis we prove that the redundancy eliminated by the reliable exact basis does not reduce the belief to the reliable exact basis the size of the reliable exact basis is much smaller than that of the minmax exact basis moreover we prove that all exact association rules can be deduced from the reliable exact basis therefore the reliable exact basis is a lossless representation of exact association rules experimental results show that the reliable exact basis significantly reduces the number of nonredundant rules
very efficient mining of distancebased outliers very efficient mining of distancebased outliers in this work a novel algorithm named dolphin for detecting distancebased outliers is presented
gridbased subspace clustering over data streams gridbased subspace clustering over data streams a reallife data stream usually contains many dimensions and some dimensional values of its data elements may be missing in order to effectively extract the ongoing change of a data stream with respect to all the subsets of the dimensions of the data stream a gridbased subspace clustering algorithm is proposed in this paper given an ndimensional data stream the ongoing distribution statistics of data elements in each onedimension data space is firstly monitored by a list of gridcells called a sibling list once a dense gridcell of a firstlevel sibling list becomes a dense unit gridcell new secondlevel sibling lists are created as its child nodes in order to trace any cluster in all possible twodimensional rectangular subspaces in such a way a sibling tree grows up to the nth level at most and a kdimensional subcluster can be found in the kth level of the sibling tree the proposed method is comparatively analyzed by a series of experiments to identify its various characteristics
detecting distancebased outliers in streams of data detecting distancebased outliers in streams of data in this work a method for detecting distancebased outliers in data streams is presented we deal with the sliding window model where outlier queries are performed in order to detect anomalies in the current window two algorithms are presented the first one exactly answers outlier queries but has larger space requirements the second algorithm is directly derived from the exact one has limited memory requirements and returns an approximate answer based on accurate estimations with a statistical guarantee several experiments have been accomplished confirming the effectiveness of the proposed approach and the high quality of approximate solutions
finding and linking incidents in news finding and linking incidents in news news reports are being produced and disseminated in overwhelming volume making it difficult to keep up with the newest information most previous research in automatic news organization treated news topics as a flat list ignoring the intrinsic connection among individual reports we argue that more contextual information within and across the topics will benefit users in their news understanding process
opinion retrieval from blogs opinion retrieval from blogs opinion retrieval is a document retrieval process which requires documents to be retrieved and ranked according to their opinions about a query topic a relevant document must satisfy two criteria relevant to the query topic and contains opinions about the query no matter if they are positive or negative in this paper we describe an opinion retrieval algorithm it has a traditional information retrieval ir component to find topic relevant documents from a document set an opinion classification component to find documents having opinions from the results of the ir step and a component to rank the documents based on their relevance to the query and their degrees of having opinions about the query we implemented the algorithm as a working system and tested it using trec blog track data in automatic titleonly runs our result showed to improvements in map score over the best automatic runs in this track our result is also higher than a stateofart opinion retrieval system which is tested on the same data set
evaluation of phrasal query suggestions evaluation of phrasal query suggestions this paper evaluates the uptake and efficacy of a unified approach to phrasal query suggestions in the context of a highprecision search engine the search engine performs ranked extendedboolean searches with the proximity operator ltscpgtnearltscpgt being the default operation suggestions are offered to the searcher when the length of the result list falls outside predefined bounds if the list is too long the engine suggests narrowing the query through the use of super phrases if the list is too short the engine suggests broadening the query through the use of proximal subphrases
an automatic approach to construct domainspecific web portals an automatic approach to construct domainspecific web portals we describe the architecture of an automatic domainspecific web portal construction system the system has three major components i a focused crawler that collects the domainspecific pages on the web ii an information extraction engine that extracts useful fields from these web pages and iii a query engine that allows both typical keyword based queries on the pages and advanced queries on the extracted data fields we present a prototype system that works for the course homepages domain on the web a user study with the prototype system shows that our approach produces high quality results and achieves better precision figures than the typical keyword based search
language models probability of relevance and relevance likelihood language models probability of relevance and relevance likelihood this paper proposes a measure of relevance likelihood derived specifically for language models such a measure may be used to guide a user on how far to browse through the list of retrieved items or for pseudorelevance feedback to derive this measure it is necessary to make the assumption that a user is seeking an ideal usually nonexistent document and the actual relevant documents in the collection will contain fragments of this ideal document thus in deriving this measure we propose a novel way of capturing relevance in language modelling
efficient interactive query expansion with complete search efficient interactive query expansion with complete search we present an efficient realization of the following interactive search engine feature as the user is typing the query words that are related to the last query word and that would lead to good hits are suggested as well as selected such hits the realization has three parts i building clusters of related terms ii adding this information as artificial words to the index such that iii the described feature reduces to an instance of prefix search and completion an efficient solution for the latter is provided by the completesearch engine with which we have integrated the proposed feature for building the clusters of related terms we propose a variant of latent semantic indexing that unlike standard approaches is completely transparent to the user by experiments on two large testcollections we demonstrate that the feature is provided at only a slight increase in query processing time and index size
structure and semantics for expressive text kernels structure and semantics for expressive text kernels several text categorization applications require a representation beyond the standard bagofwords paradigm kernelbased learning has approached this problem by i considering information about syntactic structure or by ii incorporating knowledge about the semantic similarity of term features we propose a generalized framework consisting of a family of kernels that jointly incorporate syntactic and semantic similarity and demonstrate the power of this approach in a series of experiments
conceptual modeling by analogy and metaphor conceptual modeling by analogy and metaphor metaphor is not merely a rhetorical device characteristic of language alone but rather a fundamental feature of the human conceptual system a metaphor is understood by finding an analogy mapping between two domains this paper argues that analogy mappings facilitate conceptual modeling by allowing the designer to reinterpret fragments of familiar conceptual models in other contexts the contributions of the paper are expressed within the tradition of the entityrelation model
communities in graphs and hypergraphs communities in graphs and hypergraphs in this paper we define a type of cohesive subgroups called communities in hypergraphs based on the edge connectivity of subhypergraphs we describe a simple algorithm for the construction of these sets and show based on examples from image segmentation and information retrieval that these groups may be useful for the analysis and accessibility of large graphs and hypergraphs
semiautomatic evaluation of retrieval systems using document similarities semiautomatic evaluation of retrieval systems using document similarities semiautomatic evaluation of retrieval systems using document similarities
improving the classification of newsgroup messages through social network analysis improving the classification of newsgroup messages through social network analysis improving the classification of newsgroup messages through social network analysis
a cddbased formal model for expert finding a cddbased formal model for expert finding searching an organizations document repositories for experts is a frequently faced problem in intranet information management this paper proposes a candidatecentered model which is referred as candidate description document cddbased retrieval model the expertise evidence about an expert candidate scattered over repositories is mined and aggregated automatically to form a profile called the candidates cdd which represents his knowledge we present the model from its foundations through its logical development and argue in favor of this model for expert finding we devise and compare the different strategies for exploring a variety of expertise evidence the experiments on trec enterprise corpora demonstrate that the cddbased model achieves significant and consistent improvement on performance through comparative studies with noncdd methods
ntjfsat ntjfsat previous researches in the filed of xml databases have been done to evaluate xml queries with andbranches however as far as we know very little work has examined the efficient processing of xml queries with notpredicates also these methods have to process all of the query nodes in the document when dealing with queries with notbranch in this paper with some modification in tjfast method we propose a new manner for answering to various notqueries this method processes nodes efficiently in a way that in the ideal state we obtain part of the answer after the process of each node and we dont have any unreasonable processing of each node
genre identification and goalfocused summarization genre identification and goalfocused summarization in this paper we present a novel technique of first performing document genre identification then utilizing the genre for producing tailored summaries based on a users information seeking needs genre oriented goalfocused summarization such as a plot or opinion summary of a movie review we create a test corpus to determine genre classification accuracy for genres and examine performance on various amounts of training data for machine learning algorithms random forests svm light and nave bayes results show that random forests outperforms svm light and nave bayes the genre tag is used to inform a downstream summarization engine we define types of summaries for genres create a ground truth corpus and analyze the results of genre oriented goalfocused summarization showing that this type of user based summarization requires different algorithms than the leading sentence baseline which is known to perform well in the case of news articles
combination of evidences in relevance feedback for xml retrieval combination of evidences in relevance feedback for xml retrieval the main objective in xml retrieval is to select the relevant elements of xml document instead of the whole document many open issues appear when considering relevance feedback rf in xml documents they are mainly related to the form of xml documents which mix content and structure information and to the new information granularity in this paper a new flexible method of relevance feedback in xml retrieval using two sources of evidence is described we propose to use the context criterion to select terms to extend the initial query and to use generative structures to express structural constraints both approaches are applied in different combined forms experiments are carried out with the inex evaluation campaign and results show the effectiveness of our approach
a correlationbased model for unsupervised feature selection a correlationbased model for unsupervised feature selection we propose a new model for feature evaluation and selection that assesses the propensity of the features to support twoset classification for each item of the data set the collection of features induce a ranking ordered list of the remaining items the evaluation criterion favors features that result in the most consistent discrimination between relevant and nonrelevant items within these ranked lists the discrimination boundaries within a single list are determined combinatorially according to the degree of correlation among the relevant sets of its members the model makes no special assumptions on the nature of the data a selection heuristic based on the model is also proposed using sequential forward generation and an experimental comparison is made with other unsupervised feature selection methods
commentsoriented blog summarization by sentence extraction commentsoriented blog summarization by sentence extraction much existing research on blogs focused on posts only ignoring their comments our user study conducted on summarizing blog posts however showed that reading comments does change ones understanding about blog posts in this research we aim to extract representative sentences from a blog post that best represent the topics discussed among its comments the proposed solution first derives representative words from comments and then selects sentences containing representative words the representativeness of words is measured using requt ie reader quotation and topic evaluated on human labeled sentences requt together with summationbased sentence selection showed promising results
ixcubes ixcubes with increasing amount of data being stored in xml format olap queries over these data become important olap queries have been well studied in the relational database systems however the evaluation of olap queries over xml data is not a trivial extension of the relational solutions especially when a schema is not available in this paper we introduce the ixcube iceberg xml cube over xml data to tackle the problem we extend olap operations to xml data we also develop efficient approaches to ixcube computation and olap query evaluation using ixcubes
i know what you did last summer i know what you did last summer we investigate the subtle cues to user identity that may be exploited in attacks on the privacy of users in web search query logs we study the application of simple classifiers to map a sequence of queries into the gender age and location of the user issuing the queries we then show how these classifiers may be carefully combined at multiple granularities to map a sequence of queries into a set of candidate users that is times smaller than random chance would allow we show that this approach remains accurate even after removing personally identifiable information such as namesnumbers or limiting the size of the query log
indexing multiversion databases indexing multiversion databases an efficient management of multiversion data with branched evolution is crucial for many applications it requires database designers aware of tradeoffs among index structures and policies this paper defines a framework and an analysis method for understanding the behavior of different indexing policies given data and query characteristics the analysis allows determining the most suitable index structure the analysis is validated by an experimental study
discovering authorities in question answer communities by using link analysis discovering authorities in question answer communities by using link analysis questionanswer portals such as naver and yahoo answers are quickly becoming rich sources of knowledge on many topics which are not well served by general web search engines unfortunately the quality of the submitted answers is uneven ranging from excellent detailed answers to snappy and insulting remarks or even advertisements for commercial content furthermore user feedback for many topics is sparse and can be insufficient to reliably identify good answers from the bad ones hence estimating the authority of users is a crucial task for this emerging domain with potential applications to answer ranking spam detection and incentive mechanism design we present an analysis of the link structure of a generalpurpose question answering community to discover authoritative users and promising experimental results over a dataset of more than million answers from a popular community qa site we also describe structural differences between question topics that correlate with the success of link analysis for authority discovery
selective user interaction selective user interaction note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
ensembling bayesian network structure learning on limited data ensembling bayesian network structure learning on limited data in recent years bagging method has been applied to learn bayesian networks bns especially on limited datasets however the bns learned using bagging method from limited datasets can be biased towards complex models we present an efficient approach to produce more accurate bns from limited datasets based on the markov condition of bn learning we proposed a novel sampling method called root nodes based sampling rns and a bns fusion method the experimental results reveal that our ensemble method can achieve more accurate results in terms of accuracy on limited datasets
cto cto inspired by how search behavior works in human society we propose cto a selforganized semantic overlay based on concept tree for pp ir infrastructure which is efficient for full text search in pure pp environment without any central control or powerful peer as hub node especially cto performs very well on searching the unpopular resources shared by a few peers in our experiment while searching for the scarce documents shared by the peers cto achieves about recall rate when the search covers less than peers in the overlay the search latency of cto is also very low which is controlled in the range about hops
highperformance distributed inverted files highperformance distributed inverted files we present a general method of parallel query processing that allows scalable performance on distributed inverted files the method allows the realization of a hybrid that combines the advantages of the document and term partitioned inverted files
a dualview approach to interactive network visualization a dualview approach to interactive network visualization visualizing network data from tree structures to arbitrarily connected graphs is a difficult problem in information visualization a large part of the problem is that in network data users not only have to visualize the attributes specific to each data item but also the links specifying how those items are connected to each other past approaches to resolving these difficulties focus on zooming clustering filtering and applying various methods of laying out nodes and edges such approaches however focus only on optimizing a network visualization in a single view limiting the amount of information that can be shown and explored in parallel moreover past approaches do not allow users to cross reference different subsets or aspects of large complex networks in this paper we propose an approach to these limitations using multiple coordinated views of a given network to illustrate our approach we implement a tool called dualnet and evaluate the tool with a case study using an email communication network we show how using multiple coordinated views improves navigation and provides insight into large networks with multiple node and link properties and types
using word similarity to eradicate junk emails using word similarity to eradicate junk emails emails are one of the most commonly used modern communication media these days however unsolicited emails obstruct this otherwise fast and convenient technology for information exchange and jeopardize the continuity of this popular communication tool waste of valuable resources and time and exposure to offensive content are only a few of the problems that arise as a result of junk emails in addition the monetary cost of processing junk emails reaches billions of dollars per year and is absorbed by public users and internet service providers even though there has been extensive work in the past dedicated to eradicate junk emails none of the existing junk email detection approaches has been highly successful in solving these problems since spammers have been able to infiltrate existing detection techniques in this paper we present a new tool junex which relies on the content similarity of emails to eradicate junk emails junex compares each incoming email to a core of emails marked as junk by each individual user to identify unwanted emails while reducing the number of legitimate emails treated as junk which is critical conducted experiments on junex verify its high accuracy
satisfaction balanced mediation satisfaction balanced mediation we consider a distributed information system that allows autonomous consumers to query autonomous providers we focus on the problem of query allocation from a new point of view by considering consumers and providers satisfaction in addition to query load we define satisfaction as a longrun notion based on the consumers and providers preferences we propose and validate a mediation process called sbmediation which is compared to capacity based query allocation the experimental results show that sbmediation significantly outperforms capacity based when confronted to autonomous participants
towards efficient search on unstructured data towards efficient search on unstructured data applications that create and consume unstructured data have grown both in scale of storage requirements and complexity of search primitives we consider two such applications exhaustive search and integration of structured and unstructured data current blockbased storage systems are either incapable or inefficient to address the challenges bought forth by the above applications we propose a storage framework to efficiently store and search unstructured and structured data while controlling storage management costs experimental results based on our prototype show that the proposed system can provide impressive performance and feature benefits
computing explanations for unlively queries in databases computing explanations for unlively queries in databases a query is unlively if it always returns an empty answer debugging a database schema requires not only determining unlively queries but also fixing them to the best of our knowledge the existing methods do not provide the designer with an explanation of why a query is not lively in this paper we propose a method for computing explanations that is independent of the particular method used to determine liveliness it provides three levels of search one explanation a maximal set of nonoverlapping explanations and all explanations the first two levels require only a linear number of calls to the underlying method we also propose a filter to reduce the number of these calls and experimentally compare our method with the best known method for finding unsatisfiable subsets of constraints
more like these more like these we present a corpusbased approach to the class expansion task for a given set of seed entities we use cooccurrence statistics taken from a text collection to define a membership function that is used to rank candidate entities for inclusion in the set we describe an evaluation framework that uses data from wikipedia the performance of our class extension method improves as the size of the text collection increases
an online interactive method for finding association rules data streams an online interactive method for finding association rules data streams in order to trace the changes of association rules over an online data stream efficiently this paper proposes two different methods of generating all association rules directly over the changing set of currently frequent itemsets while all of the currently frequent itemsets are monitored by the estdec method all the association rules of every frequent itemset in the prefix tree of the estdec method are generated for this purpose a traversal stack is introduced to efficiently enumerate all association rules these online methods can avoid the drawbacks of the conventional twostep approach in an online environment a user may be interested in finding those association rules whose antecedents or consequents are fixed to be a specific itemset since generating all the association rules may take too long to produce them timely two additional methods namely assocx and assocy are introduced finally the proposed methods are compared by a series of experiments to identify their various characteristics
probabilistic correlationbased similarity measure of unstructured records probabilistic correlationbased similarity measure of unstructured records computing the similarity between unstructured records is a fundamental function in multiple applications approximate string matching and full text retrieval techniques do not show the best performance when applied directly since the information are limited in unstructured records of short record length in this paper we propose a novel probabilistic correlationbased similarity measure rather than simply conducting the exact matching tokens of two records our similarity evaluation enriches the information of records by considering the correlations of tokens we define the probabilistic correlation between tokens as the probability that these tokens appear in the same records then we compute the weight of tokens and discover the correlations of records based on the probabilistic correlations of tokens finally we present extensive experimental results to demonstrate the effectiveness of our approach
identifying opinion leaders in the blogosphere identifying opinion leaders in the blogosphere opinion leaders are those who bring in new information ideas and opinions then disseminate them down to the masses and thus influence the opinions and decisions of others by a fashion of word of mouth opinion leaders capture the most representative opinions in the social network and consequently are important for understanding the massive and complex blogosphere in this paper we propose a novel algorithm called influencerank to identify opinion leaders in the blogosphere the influencerank algorithm ranks blogs according to not only how important they are as compared to other blogs but also how novel the information they can contribute to the network experimental results indicate that our proposed algorithm is effective in identifying influential opinion leaders
ranking with semisupervised distance metric learning and its application to housing potential estimation ranking with semisupervised distance metric learning and its application to housing potential estimation this paper proposes a semisupervised distance metric learning algorithm for the ranking problem instead of giving the computer what are the important factors that affect the final rank value we only give several most certainly ranked points which implicitly contain the knowledge of the ranking factors then the computer can automatically use the most certain points and plenty of unlabeded data to learn an informative metric for ranking this metric not only can help to regress an order in the observed data but also can be used to retrieve the data by querying new test points moreover the lowerrank distance metric can be used to visualize highdimensional data we also present an application to the housing potential estimation problem it is shown that the algorithm is efficient to help consultants to refine their consulting work
a novel scheme for domaintransfer problem in the context of sentiment analysis a novel scheme for domaintransfer problem in the context of sentiment analysis in this work we attempt to tackle domaintransfer problem by combining olddomain labeled examples with newdomain unlabeled ones the basic idea is to use olddomaintrained classifier to label some informative unlabeled examples in new domain and retrain the base classifier over these selected examples the experimental results demonstrate that proposed scheme can significantly boost the accuracy of the base sentiment classifier on new domain
sigma encoded inverted files sigma encoded inverted files compression of term frequency lists and very long documentid lists within an inverted file search engine are examined several compression schemes are compared including elias and codes golomb encoding variable byte encoding and a class of wordbased encoding schemes including simple relative and carryover it is shown that these compression methods are not well suited to compressing these kinds of lists of numbers of those tested carryover is preferred because it is both effective at compression and fast at decompression
dynamic index pruning for effective caching dynamic index pruning for effective caching ram and dynamic pruning schemes to reduce query evaluation times while only a small portion of lists are processed with dynamic pruning current systems still store the entire inverted list in cache in this paper we investigate caching only the pieces of the inverted lists that are actually used to answer a query during dynamic pruning we examine an lru cache model and two recently proposed models we also introduce a new dynamic pruning scheme for impactordered inverted lists
improve retrieval accuracy for difficult queries using negative feedback improve retrieval accuracy for difficult queries using negative feedback how to improve search accuracy for difficult topics is an underaddressed yet important research question in this paper we consider a scenario when the search results are so poor that none of the topranked documents is relevant to a users query and propose to exploit negative feedback to improve retrieval accuracy for such difficult queries specifically we propose to learn from a certain number of topranked nonrelevant documents to rerank the rest unseen documents we propose several approaches to penalizing the documents that are similar to the known nonrelevant documents in the language modeling framework to evaluate the proposed methods we adapt standard trec collections to construct a test collection containing only difficult queries experiment results show that the proposed approaches are effective for improving retrieval accuracy of difficult queries
translating topics to words for image annotation translating topics to words for image annotation one of the classic techniques for image annotation is the language translation model it views an image as a document ie a set of visual words which are obtained by vector quatitizing the image regions generated by unsupervised image segmentation annotating images are achieved by translating visual words to textual words just like translating a document in english to a document in french in this paper we also view an image as a document but we view the annotation processes as two consecutive processes ie document summarization and translation in the document summarization process an image document is firstly summarized into its own visual language which we called visual topics the translation process translates these visual topics to textual words compared to the original translation model our visual topics learned by the probabilistic latent semantic analysis plsa approach provide an intermediate abstract level of visual description we show improved annotation performance on the corel image dataset
mining redundancy in candidatebearing snippets to improve web question answering mining redundancy in candidatebearing snippets to improve web question answering conventional question answering qa techniques independently process candidatebearing snippets to select an exact answer to a question from candidate answers this paper presents two novel ways of utilizing redundancy in candidatebearing snippets to help select an exact answer to a question in our web qa system ie clusterbased language model clmm and unsupervised svm classifier usvm techniques the comparative experiments demonstrate that the proposed methods significantly outperform the language modelbased lmm and supervised svmbased ssvm techniques that do not utilize this redundancy in the candidatebearing snippets using the clmm the top score is increased from lmm to and the top improvement in the usvm over the ssvm is about moreover a crossmodel comparison shows that the performance ranking of these models is usvm gt clmlm gt lmm gt ssvm gt rm the retrievalbased model
using social annotations to improve language model for information retrieval using social annotations to improve language model for information retrieval this poster is concerned with the problem of exploring the use of social annotations for improving language models for information retrieval denoted as lmir two properties of social annotations namely keyword property and structure property are studied for this aim the keyword property improves lmir by concatenating all the annotations of a document to generate a summary of the document the structure property can boost lmir further when similarity among annotations and similarity among documents are taken into consideration simultaneously the two properties of social annotations are leveraged for the use of language modeling with a mixture model named as language annotation model denoted as lam evaluations using delicious data show that lam outperforms the traditional lmir approaches significantly
efficient lca based keyword search in xml data efficient lca based keyword search in xml data keyword search in xml documents based on the notion of lowest common ancestors lcas and modifications of it has recently gained research interest in this paper we propose an efficient algorithm called indexed stack to find answers to keyword queries based on xranks semantics to lca the complexity of the indexed stack algorithm is okdslogs where k is the number of keywords in the query d is the depth of the tree and s s is the occurrence of the least most frequent keyword in the query in comparison the best worst case complexity of the core algorithms in is okds we analytically and experimentally evaluate the indexed stack algorithm and the two core algorithms in the results show that the indexed stack algorithm outperforms in terms of both cpu and io costs other algorithms by orders of magnitude when the query contains at least one low frequency keyword along with high frequency keywords
link analysis using time series of web graphs link analysis using time series of web graphs link analysis is a key technology in contemporary web search engines most of the previous work on link analysis only used information from one snapshot of web graph since commercial search engines crawl the web periodically they will naturally obtain time series data of web graphs the historical information contained in the series of web graphs can be used to improve the performance of link analysis in this paper we argue that page importance should be a dynamic quantity and propose defining page importance as a function of both pagerank of the current web graph and accumulated historical page importance from previous web graphs specifically a novel algorithm named temporalrank is designed to compute the proposed page importance we try to use a kinetic model to interpret this page importance and show that it can be regarded as the solution to an ordinary differential equation experiments on link analysis using web graph data in five snapshots show that the proposed algorithm can outperform pagerank in many measures and can effectively filter out newly appeared link spam websites
ranking very many typed entities on wikipedia ranking very many typed entities on wikipedia we discuss the problem of ranking very many entities of different types in particular we deal with a heterogeneous set of types some being very generic and some very specific we discuss two approaches for this problem i exploiting the entity containment graph and ii using a web search engine to compute entity relevance we evaluate these approaches on the real task of ranking wikipedia entities typed with a stateoftheart namedentity tagger results show that both approaches can greatly increase the performance of methods based only on passage retrieval
a constraintbased probabilistic framework for name disambiguation a constraintbased probabilistic framework for name disambiguation this paper is concerned with the problem of name disambiguation by name disambiguation we mean distinguishing persons with the same name it is a critical problem in many knowledge management applications despite much research work has been conducted the problem is still not resolved and becomes even more serious in particular with the popularity of web previously name disambiguation was often undertaken in either a supervised or unsupervised fashion this paper first gives a constraintbased probabilistic model for semisupervised name disambiguation specifically we focus on investigating the problem in an academic researcher social network httparnetminerorg the framework combines constraints and euclidean distance learning and allows the user to refine the disambiguation results experimental results on the researcher social network show that the proposed framework significantly outperforms the baseline method using unsupervised hierarchical clustering algorithm
an efficient algorithm for approximate biased quantile computation in data streams an efficient algorithm for approximate biased quantile computation in data streams we propose an efficient algorithm for approximate biased quantile computation in large data streams our algorithm computes decomposable biased quantile summaries on fixed sized blocks and dynamically maintains the biased quantile summary for the entire stream as the exponential histogram over the blockwise quantile summaries the algorithm is computationally efficient and achieves an amortized computational cost of ologlogn and a space requirement of ologn our algorithm does not assume prior knowledge of the stream sizes or the range of data values in the streams in practice our algorithm is able to efficiently maintain summaries over large data streams with over tens of millions of observations and achieves significant performance improvement over prior algorithms
a segmentbased hidden markov model for realsetting pinyintochinese conversion a segmentbased hidden markov model for realsetting pinyintochinese conversion hidden markov model hmm is frequently used for pinyintochinese conversion but it only captures the dependency with the preceding character higher order markov models can bring higher accuracy but are computationally unaffordable to average pc settings we propose a segmentbased hidden markov model shmm which has the same magnitude of complexity as firstorder hmm but generates higher decoding accuracy shmm tells a word from a bigram connecting two words and assigns a reasonable probability to words as a whole it is more powerful than hmm to decode words containing over two characters we conduct a comprehensive pinyintochinese conversion evaluation on lancaster corpus the experiment shows the perfect sentence accuracy is improved from hmm to shmm the oneerror sentence accuracy is increased from to furthermore shmm can seamlessly integrate with pinyin typing correction acronym pinyin input userdefined words and selfadaptive learning all of which are a must for a commercial pinyintochinese conversion product in order to improve the efficiency of pinyin input
web search web search in scarcely a decade web search has gone from simply scaling traditional information retrieval to a groundswell of new opportunities that are changing marketing as we know it in this lecture we begin by reviewing the progress pointing out that web search is no longer a purely computer sceince problem we then hint at the role of other disciplines in this ongoing revolution and a number of directions for research
management of data with uncertainties management of data with uncertainties note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
learning to join everything learning to join everything text speech images video dna sequences provide information about entities that people can recognize when looking at a particular instance but those entities and their attributes and relationships are not directly accessible to queries that join across types of sources information extraction methods based on supervised machine learning recognize mentions of entities and relationships of predefined types in different kinds of sources which can then be used to answer some useful types of queries however supervised learning relies on handannotated training sets that are difficult to create and limit what types of entities and relationships can be joined for new applications these limitations have prompted research into unsupervised extraction methods that rely on correlations among sources rather than handannotated training sets while these methods are not yet as accurate as those based on supervised learning they have the potential for a new querybyexample approach to information integration in which seed sets of query answers are expanded into ranked lists of potential answers by learning occurrence patterns from the seed answers i will give examples of both types of methods from our research on biomedical information extraction leading to some ideas on a possible convergence of search and databases through machine learning
using inaccurate models in reinforcement learning using inaccurate models in reinforcement learning in the modelbased policy search approach to reinforcement learning rl policies are found using a model or simulator of the markov decision process however for highdimensional continuousstate tasks it can be extremely difficult to build an accurate model and thus often the algorithm returns a policy that works in simulation but not in reallife the other extreme modelfree rl tends to require infeasibly large numbers of reallife trials in this paper we present a hybrid algorithm that requires only an approximate model and only a small number of reallife trials the key idea is to successively ground the policy evaluations using reallife trials but to rely on the approximate model to suggest local changes our theoretical results show that this algorithm achieves nearoptimal performance in the real system even when the model is only approximate empirical results also demonstrate thatwhen given only a crude model and a small number of reallife trialsour algorithm can obtain nearoptimal performance in the real system 
algorithms for portfolio management based on the newton method algorithms for portfolio management based on the newton method we experimentally study online investment algorithms first proposed by agarwal and hazan and extended by hazan et al which achieve almost the same wealth as the best constantrebalanced portfolio determined in hindsight these algorithms are the first to combine optimal logarithmic regret bounds with efficient deterministic computability they are based on the newton method for offline optimization which unlike previous approaches exploits second order information after analyzing the algorithm using the potential function introduced by agarwal and hazan we present extensive experiments on actual financial data these experiments confirm the theoretical advantage of our algorithms which yield higher returns and run considerably faster than previous algorithms with optimal regret additionally we perform financial analysis using meanvariance calculations and the sharpe ratio 
higher order learning with graphs higher order learning with graphs recently there has been considerable interest in learning with higher order relations ie threeway or higher in the unsupervised and semisupervised settings hypergraphs and tensors have been proposed as the natural way of representing these relations and their corresponding algebra as the natural tools for operating on them in this paper we argue that hypergraphs are not a natural representation for higher order relations indeed pairwise as well as higher order relations can be handled using graphs we show that various formulations of the semisupervised and the unsupervised learning problem on hypergraphs result in the same graph theoretic problem and can be analyzed using existing tools 
ranking on graph data ranking on graph data in ranking one is given examples of order relationships among objects and the goal is to learn from these examples a realvalued ranking function that induces a ranking or ordering over the object space we consider the problem of learning such a ranking function when the data is represented as a graph in which vertices correspond to objects and edges encode similarities between objects building on recent developments in regularization theory for graphs and corresponding laplacianbased methods for classification we develop an algorithmic framework for learning ranking functions on graph data we provide generalization guarantees for our algorithms via recent results based on the notion of algorithmic stability and give experimental evidence of the potential benefits of our framework 
robust probabilistic projections robust probabilistic projections principal components and canonical correlations are at the root of many exploratory data mining techniques and provide standard preprocessing tools in machine learning lately probabilistic reformulations of these methods have been proposed roweis tipping bishop b bach jordan they are based on a gaussian density model and are therefore like their nonprobabilistic counterpart very sensitive to atypical observations in this paper we introduce robust probabilistic principal component analysis and robust probabilistic canonical correlation analysis both are based on a studentt density model the resulting probabilistic reformulations are more suitable in practice as they handle outliers in a natural way we compute maximum likelihood estimates of the parameters by means of the em algorithm 
a dcprogramming algorithm for kernel selection a dcprogramming algorithm for kernel selection we address the problem of learning a kernel for a given supervised learning task our approach consists in searching within the convex hull of a prescribed set of basic kernels for one which minimizes a convex regularization functional a unique feature of this approach compared to others in the literature is that the number of basic kernels can be infinite we only require that they are continuously parameterized for example the basic kernels could be isotropic gaussians with variance in a prescribed interval or even gaussians parameterized by multiple continuous parameters our work builds upon a formulation involving a minimax optimization problem and a recently proposed greedy algorithm for learning the kernel although this optimization problem is not convex it belongs to the larger class of dc difference of convex functions programs therefore we apply recent results from dc optimization theory to create a new algorithm for learning the kernel our experimental results on benchmark data sets show that this algorithm outperforms a previously proposed method 
relational temporal difference learning relational temporal difference learning we introduce relational temporal difference learning as an effective approach to solving multiagent markov decision problems with large state spaces our algorithm uses temporal difference reinforcement to learn a distributed value function represented over a conceptual hierarchy of relational predicates we present experiments using two domains from the general game playing repository in which we observe that our system achieves higher learning rates than nonrelational methods we also discuss related work and directions for future research 
a new approach to data driven clustering a new approach to data driven clustering we consider the problem of clustering in its most basic form where only a local metric on the data space is given no parametric statistical model is assumed and the number of clusters is learned from the data we introduce analyze and demonstrate a novel approach to clustering where data points are viewed as nodes of a graph and pairwise similarities are used to derive a transition probability matrix p for a markov random walk between them the algorithm automatically reveals structure at increasing scales by varying the number of steps taken by this random walk points are represented as rows of pt which are the tstep distributions of the walk starting at that point these distributions are then clustered using a klminimizing iterative algorithm both the number of clusters and the number of steps that best reveal it are found by optimizing spectral properties of p 
agnostic active learning agnostic active learning we state and analyze the first active learning algorithm which works in the presence of arbitrary forms of noise the algorithm a for agnostic active relies only upon the assumption that the samples are drawn iid from a fixed distribution we show that a achieves an exponential improvement ie requires only o ln epsilon samples to find an epsilonoptimal classifier over the usual sample complexity of supervised learning for several settings considered before in the realizable case these include learning threshold classifiers and learning homogeneous linear separators with respect to an input distribution which is uniform over the unit sphere 
on a theory of learning with similarity functions on a theory of learning with similarity functions kernel functions have become an extremely popular tool in machine learning with an attractive theory as well this theory views a kernel as implicitly mapping data points into a possibly very high dimensional space and describes a kernel function as being good for a given learning problem if data is separable by a large margin in that implicit space however while quite elegant this theory does not directly correspond to ones intuition of a good kernel as a good similarity function furthermore it may be difficult for a domain expert to use the theory to help design an appropriate kernel for the learning task at hand since the implicit mapping may not be easy to calculate finally the requirement of positive semidefiniteness may rule out the most natural pairwise similarity functions for the given problem domainin this work we develop an alternative more general theory of learning with similarity functions ie sufficient conditions for a similarity function to allow one to learn well that does not require reference to implicit spaces and does not require the function to be positive semidefinite or even symmetric our results also generalize the standard theory in the sense that any good kernel function under the usual definition can be shown to also be a good similarity function under our definition though with some loss in the parameters in this way we provide the first steps towards a theory of kernels that describes the effectiveness of a given kernel function in terms of natural similaritybased properties 
on bayesian bounds on bayesian bounds we show that several important bayesian bounds studied in machine learning both in the batch as well as the online setting arise by an application of a simple compression lemma in particular we derive i pacbayesian bounds in the batch setting ii bayesian logloss bounds and iii bayesian boundedloss bounds in the online setting using the compression lemma although every setting has different semantics for prior posterior and loss we show that the core bound argument is the same the paper simplifies our understanding of several important and apparently disparate results as well as brings to light a powerful tool for developing similar arguments for other methods 
convex optimization techniques for fitting sparse gaussian graphical models convex optimization techniques for fitting sparse gaussian graphical models we consider the problem of fitting a largescale covariance matrix to multivariate gaussian data in such a way that the inverse is sparse thus providing model selection beginning with a dense empirical covariance matrix we solve a maximum likelihood problem with an lnorm penalty term added to encourage sparsity in the inverse for models with tens of nodes the resulting problem can be solved using standard interiorpoint algorithms for convex optimization but these methods scale poorly with problem size we present two new algorithms aimed at solving problems with a thousand nodes the first based on nesterovs firstorder algorithm yields a rigorous complexity estimate for the problem with a much better dependence on problem size than interiorpoint methods our second algorithm uses block coordinate descent updating rowcolumns of the covariance matrix sequentially experiments with genomic data show that our method is able to uncover biologically interpretable connections among genes 
cover trees for nearest neighbor cover trees for nearest neighbor we present a tree data structure for fast nearest neighbor operations in general npoint metric spaces where the data set consists of n points the data structure requires on space regardless of the metrics structure yet maintains all performance properties of a navigating net krauthgamer lee b if the point set has a bounded expansion constant c which is a measure of the intrinsic dimensionality as defined in karger ruhl the cover tree data structure can be constructed in o cn log n time furthermore nearest neighbor queries require time only logarithmic in n in particular o c log n time our experimental results show speedups over the brute force search varying between one and several orders of magnitude on natural machine learning datasets 
graph model selection using maximum likelihood graph model selection using maximum likelihood in recent years there has been a proliferation of theoretical graph models eg preferential attachment and smallworld models motivated by realworld graphs such as the internet topology to address the natural question of which model is best for a particular data set we propose a model selection criterion for graph models since each model is in fact a probability distribution over graphs we suggest using maximum likelihood to compare graph models and select their parameters interestingly for the case of graph models computing likelihoods is a difficult algorithmic task however we design and implement mcmc algorithms for computing the maximum likelihood for four popular models a powerlaw random graph model a preferential attachment model a smallworld model and a uniform random graph model we hope that this novel use of ml will objectify comparisons between graph models 
dynamic topic models dynamic topic models a family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections the approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics variational approximations based on kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics in addition to giving quantitative predictive models of a sequential corpus dynamic topic models provide a qualitative window into the contents of a large document collection the models are demonstrated by analyzing the ocred archives of the journal science from through 
predictive search distributions predictive search distributions estimation of distribution algorithms edas are a popular approach to learn a probability distribution over the good solutions to a combinatorial optimization problem here we consider the case where there is a collection of such optimization problems with learned distributions and where each problem can be characterized by some vector of features now we can define a machine learning problem to predict the distribution of good solutions qsx for a new problem with features x where s denotes a solution this predictive distribution is then used to focus the search we demonstrate the utility of our method on a compiler optimization task where the goal is to find a sequence of code transformations to make the code run fastest results on a set of different benchmarks on two distinct architectures show that our approach consistently leads to significant improvements in performance 
learning predictive state representations using nonblind policies learning predictive state representations using nonblind policies predictive state representations psrs are powerful models of nonmarkovian decision processes that differ from traditional models eg hmms pomdps by representing state using only observable quantities because of this psrs can be learned solely using data from interaction with the process the majority of existing techniques though explicitly or implicitly require that this data be gathered using a blind policy where actions are selected independently of preceding observations this is a severe limitation for practical learning of psrs we present two methods for fixing this limitation in most of the existing psr algorithms one when the policy is known and one when it is not we then present an efficient optimization for computing good exploration policies to be used when learning a psr the exploration policies which are not blind significantly lower the amount of data needed to build an accurate model thus demonstrating the importance of nonblind policies 
efficient coregularised least squares regression efficient coregularised least squares regression in many applications unlabelled examples are inexpensive and easy to obtain semisupervised approaches try to utilise such examples to reduce the predictive error in this paper we investigate a semisupervised least squares regression algorithm based on the colearning approach similar to other semisupervised algorithms our base algorithm has cubic runtime complexity in the number of unlabelled examples to be able to handle larger sets of unlabelled examples we devise a semiparametric variant that scales linearly in the number of unlabelled examples experiments show a significant error reduction by coregularisation and a large runtime improvement for the semiparametric approximation last but not least we propose a distributed procedure that can be applied without collecting all data at a single site 
semisupervised learning for structured output variables semisupervised learning for structured output variables the problem of learning a mapping between input and structured interdependent output variables covers sequential spatial and relational learning as well as predicting recursive structures joint feature representations of the input and output variables have paved the way to leveraging discriminative learners such as svms to this class of problems we address the problem of semisupervised learning in joint input output spaces the cotraining approach is based on the principle of maximizing the consensus among multiple independent hypotheses we develop this principle into a semisupervised support vector learning algorithm for joint input output spaces and arbitrary loss functions experiments investigate the benefit of semisupervised structured models in terms of accuracy and f score 
fast nonparametric clustering with gaussian blurring meanshift fast nonparametric clustering with gaussian blurring meanshift we revisit gaussian blurring meanshift gbms a procedure that iteratively sharpens a dataset by moving each data point according to the gaussian meanshift algorithm gms we give a criterion to stop the procedure as soon as clustering structure has arisen and show that this reliably produces image segmentations as good as those of gms but much faster we prove that gbms has convergence of cubic order with gaussian clusters much faster than gmss which is of linear order and that the local principal component converges last which explains the powerful clustering and denoising properties of gbms we show a connection with spectral clustering that suggests gbms is much faster we further accelerate gbms by interleaving connectedcomponents and blurring steps achieving xx speedups without introducing an approximation error in summary our accelerated gbms is a simple fast nonparametric algorithm that achieves segmentations of stateoftheart quality 
an empirical comparison of supervised learning algorithms an empirical comparison of supervised learning algorithms a number of supervised learning methods have been introduced in the last decade unfortunately the last comprehensive empirical evaluation of supervised learning was the statlog project in the early s we present a largescale empirical comparison between ten supervised learning methods svms neural nets logistic regression naive bayes memorybased learning random forests decision trees bagged trees boosted trees and boosted stumps we also examine the effect that calibrating the models via platt scaling and isotonic regression has on their performance an important aspect of our study is the use of a variety of performance criteria to evaluate the learning methods 
robust euclidean embedding robust euclidean embedding we derive a robust euclidean embedding procedure based on semidefinite programming that may be used in place of the popular classical multidimensional scaling cmds algorithm we motivate this algorithm by arguing that cmds is not particularly robust and has several other deficiencies generalpurpose semidefinite programming solvers are too memory intensive for medium to large sized applications so we also describe a fast subgradientbased implementation of the robust algorithm additionally since cmds is often used for dimensionality reduction we provide an indepth look at reducing dimensionality with embedding procedures in particular we show that it is nphard to find optimal lowdimensional embeddings under a variety of cost functions 
hierarchical classification hierarchical classification we study hierarchical classification in the general case when an instance could belong to more than one class node in the underlying taxonomy experiments done in previous work showed that a simple hierarchy of support vectors machines svm with a topdown evaluation scheme has a surprisingly good performance on this kind of task in this paper we introduce a refined evaluation scheme which turns the hierarchical svm classifier into an approximator of the bayes optimal classifier with respect to a simple stochastic model for the labels experiments on synthetic datasets generated according to this stochastic model show that our refined algorithm outperforms the simple hierarchical svm on realworld data however the advantage brought by our approach is a bit less clear we conjecture this is due to a higher noise rate for the training labels in the low levels of the taxonomy 
a continuation method for semisupervised svms a continuation method for semisupervised svms semisupervised support vector machines svms are an appealing method for using unlabeled data in classification their objective function favors decision boundaries which do not cut clusters however their main problem is that the optimization problem is nonconvex and has many local minima which often results in suboptimal performances in this paper we propose to use a global optimization technique known as continuation to alleviate this problem compared to other algorithms minimizing the same objective function our continuation method often leads to lower test errors 
a regularization framework for multipleinstance learning a regularization framework for multipleinstance learning this paper focuses on kernel methods for multiinstance learning existing methods require the prediction of the bag to be identical to the maximum of those of its individual instances however this is too restrictive as only the sign is important in classification in this paper we provide a more complete regularization framework for mi learning by allowing the use of different loss functions between the outputs of a bag and its associated instances this is especially important as we generalize this for multiinstance regression moreover both bag and instance information can now be directly used in the optimization instead of using heuristics to solve the resultant nonlinear optimization problem we use the constrained concaveconvex procedure which has wellstudied convergence properties experiments on both classification and regression data sets show that the proposed method leads to improved performance 
trading convexity for scalability trading convexity for scalability convex learning algorithms such as support vector machines svms are often seen as highly desirable because they offer strong practical properties and are amenable to theoretical analysis however in this work we show how nonconvexity can provide scalability advantages over convexity we show how concaveconvex programming can be applied to produce i faster svms where training errors are no longer support vectors and ii much faster transductive svms 
learning algorithms for online principalagent problems and selling goods online learning algorithms for online principalagent problems and selling goods online in a principalagent problem a principal seeks to motivate an agent to take a certain action beneficial to the principal while spending as little as possible on the reward this is complicated by the fact that the principal does not know the agents utility function or type we study the online setting where at each round the principal encounters a new agent and the principal sets the rewards anew at the end of each round the principal only finds out the action that the agent took but not his type the principal must learn how to set the rewards optimally we show that this setting generalizes the setting of selling a digital good onlinewe study and experimentally compare three main approaches to this problem first we show how to apply a standard bandit algorithm to this setting second for the case where the distribution of agent types is fixed but unknown to the principal we introduce a new gradient ascent algorithm third for the case where the distribution of agents types is fixed and the principal has a prior belief distribution over a limited class of type distributions we study a bayesian approach 
dealing with nonstationary environments using context detection dealing with nonstationary environments using context detection in this paper we introduce rlcd a method for solving reinforcement learning problems in nonstationary environments the method is based on a mechanism for creating updating and selecting one among several partial models of the environment the partial models are incrementally built according to the systems capability of making predictions regarding a given sequence of observations we propose formalize and show the efficiency of this method both in a simple nonstationary environment and in a noisy scenario we show that rlcd performs better than two standard reinforcement learning algorithms and that it has advantages over methods specifically designed to cope with nonstationarity finally we present known limitations of the method and future works 
locally adaptive classification piloted by uncertainty locally adaptive classification piloted by uncertainty locally adaptive classifiers are usually superior to the use of a single global classifier however there are two major problems in designing locally adaptive classifiers first how to place the local classifiers and second how to combine them together in this paper instead of placing the classifiers based on the data distribution only we propose a responsibility mixture model that uses the uncertainty associated with the classification at each training sample using this model the local classifiers are placed near the decision boundary where they are most effective a set of local classifiers are then learned to form a global classifier by maximizing an estimate of the probability that the samples will be correctly classified with a nearest neighbor classifier experimental results on both artificial and realworld data sets demonstrate its superiority over traditional algorithms 
the relationship between precisionrecall and roc curves the relationship between precisionrecall and roc curves receiver operator characteristic roc curves are commonly used to present results for binary decision problems in machine learning however when dealing with highly skewed datasets precisionrecall pr curves give a more informative picture of an algorithms performance we show that a deep connection exists between roc space and pr space such that a curve dominates in roc space if and only if it dominates in pr space a corollary is the notion of an achievable pr curve which has properties much like the convex hull in roc space we show an efficient algorithm for computing this curve finally we also note differences in the two types of curves are significant for algorithm design for example in pr space it is incorrect to linearly interpolate between points furthermore algorithms that optimize the area under the roc curve are not guaranteed to optimize the area under the pr curve 
discriminative cluster analysis discriminative cluster analysis clustering is one of the most widely used statistical tools for data analysis among all existing clustering techniques kmeans is a very popular method because of its ease of programming and because it accomplishes a good tradeoff between achieved performance and computational complexity however kmeans is prone to local minima problems and it does not scale too well with high dimensional data sets a common approach to dealing with high dimensional data is to cluster in the space spanned by the principal components pc in this paper we show the benefits of clustering in a low dimensional discriminative space rather than in the pc space generative in particular we propose a new clustering algorithm called discriminative cluster analysis dca dca jointly performs dimensionality reduction and clustering several toy and real examples show the benefits of dca versus traditional pcakmeans clustering additionally a new matrix formulation is proposed and connections with related techniques such as spectral graph methods and linear discriminant analysis are provided 
collaborative prediction using ensembles of maximum margin matrix factorizations collaborative prediction using ensembles of maximum margin matrix factorizations fast gradientbased methods for maximum margin matrix factorization mmmf were recently shown to have great promise rennie srebro including significantly outperforming the previous stateoftheart methods on some standard collaborative prediction benchmarks including movielens in this paper we investigate ways to further improve the performance of mmmf by casting it within an ensemble approach we explore and evaluate a variety of alternative ways to define such ensembles we show that our resulting ensembles can perform significantly better than a single mmmf model along multiple evaluation metrics in fact we find that ensembles of partially trained mmmf models can sometimes even give better predictions in total training time comparable to a single mmmf model 
learning the structure of factored markov decision processes in reinforcement learning problems learning the structure of factored markov decision processes in reinforcement learning problems recent decisiontheoric planning algorithms are able to find optimal solutions in large problems using factored markov decision processes fmdps however these algorithms need a perfect knowledge of the structure of the problem in this paper we propose sdyna a general framework for addressing large reinforcement learning problems by trialanderror and with no initial knowledge of their structure sdyna integrates incremental planning algorithms based on fmdps with supervised learning techniques building structured representations of the problem we describe spiti an instantiation of sdyna that uses incremental decision tree induction to learn the structure of a problem combined with an incremental version of the structured value iteration algorithm we show that spiti can build a factored representation of a reinforcement learning problem and may improve the policy faster than tabular reinforcement learning algorithms by exploiting the generalization property of decision tree induction algorithms 
efficient learning of naive bayes classifiers under classconditional classification noise efficient learning of naive bayes classifiers under classconditional classification noise we address the problem of efficiently learning naive bayes classifiers under classconditional classification noise cccn naive bayes classifiers rely on the hypothesis that the distributions associated to each class are product distributions when data is subject to cccnoise these conditional distributions are themselves mixtures of product distributions we give analytical formulas which makes it possible to identify them from data subject to cccn then we design a learning algorithm based on these formulas able to learn naive bayes classifiers under cccn we present results on artificial datasets and datasets extracted from the uci repository database these results show that cccn can be efficiently and successfully handled 
learning user preferences for sets of objects learning user preferences for sets of objects most work on preference learning has focused on pairwise preferences or rankings over individual items in this paper we present a method for learning preferences over sets of items our learning method takes as input a collection of positive examplesthat is one or more sets that have been identified by a user as desirable kernel density estimation is used to estimate the value function for individual items and the desired set diversity is estimated from the average set diversity observed in the collection since this is a new learning problem we introduce a new evaluation methodology and evaluate the learning method on two data collections synthetic blocksworld data and a new realworld music data collection that we have gathered 
rpca rpca principal component analysis pca minimizes the sum of squared errors lnorm and is sensitive to the presence of outliers we propose a rotational invariant lnorm pca rpca rpca is similar to pca in that it has a unique global solution the solution are principal eigenvectors of a robust covariance matrix reweighted to soften the effects of outliers the solution is rotational invariant these properties are not shared by the lnorm pca a new subspace iteration algorithm is given to compute rpca efficiently experiments on several reallife datasets show rpca can effectively handle outliers we extend rnorm to kmeans clustering and show that lnorm kmeans leads to poor results while rkmeans outperforms standard kmeans 
clustering documents with an exponentialfamily approximation of the dirichlet compound multinomial distribution clustering documents with an exponentialfamily approximation of the dirichlet compound multinomial distribution the dirichlet compound multinomial dcm distribution also called the multivariate polya distribution is a model for text documents that takes into account burstiness the fact that if a word occurs once in a document it is likely to occur repeatedly we derive a new family of distributions that are approximations to dcm distributions and constitute an exponential family unlike dcm distributions we use these socalled edcm distributions to obtain insights into the properties of dcm distributions and then derive an algorithm for edcm maximumlikelihood training that is many times faster than the corresponding method for dcm distributions next we investigate expectationmaximization with edcm components and deterministic annealing as a new clustering algorithm for documents experiments show that the new algorithm is competitive with the best methods in the literature and superior from the point of view of finding models with low perplexity 
a graphical model for predicting protein molecular function a graphical model for predicting protein molecular function we present a simple statistical model of molecular function evolution to predict protein function the model description encodes general knowledge of how molecular function evolves within a phylogenetic tree based on the proteins sequence inputs are a phylogeny for a set of evolutionarily related protein sequences and any available function characterizations for those proteins posterior probabilities for each protein are used to predict the molecular function of that protein we present results from applying our model to three protein families and compare our prediction results on the extant proteins to other available protein function prediction methods for the deaminase family our method achieves where related methods blast achieves gotcha achieves and orthostrapper achieves in prediction accuracy 
qualitative reinforcement learning qualitative reinforcement learning when the transition probabilities and rewards of a markov decision process are specified exactly the problem can be solved without any interaction with the environment when no such specification is available the agents only recourse is a long and potentially dangerous exploration we present a framework which allows the expert to specify imprecise knowledge of transition probabilities in terms of stochastic dominance constraints our algorithm can be used to find optimal policies for qualitatively specified problems or when no such solution is available to decrease the required amount of exploration the algorithms behavior is demonstrated on simulations of two classic problems mountain car ascent and cart pole balancing 
online multiclass learning by interclass hypothesis sharing online multiclass learning by interclass hypothesis sharing we describe a general framework for online multiclass learning based on the notion of hypothesis sharing in our framework sets of classes are associated with hypotheses thus all classes within a given set share the same hypothesis this framework includes as special cases commonly used constructions for multiclass categorization such as allocating a unique hypothesis for each class and allocating a single common hypothesis for all classes we generalize the multiclass perceptron to our framework and derive a unifying mistake bound analysis our construction naturally extends to settings where the number of classes is not known in advance but rather is revealed along the online learning process we demonstrate the merits of our approach by comparing it to previous methods on both synthetic and natural datasets 
regression with the optimised combination technique regression with the optimised combination technique we consider the sparse grid combination technique for regression which we regard as a problem of function reconstruction in some given function space we use a regularised least squares approach discretised by sparse grids and solved using the socalled combination technique where a certain sequence of conventional grids is employed the sparse grid solution is then obtained by addition of the partial solutions with combination coefficients dependent on the involved grids this approach shows instabilities in certain situations and is not guaranteed to converge with higher discretisation levels in this article we apply the recently introduced optimised combination technique which repairs these instabilities now the combination coefficients also depend on the function to be reconstructed resulting in a nonlinear approximation method which achieves very competitive results we show that the computational complexity of the improved method still scales only linear in regard to the number of data 
a note on mixtures of experts for multiclass responses a note on mixtures of experts for multiclass responses we report that mixtures of m multinomial logistic regression can be used to approximate a class of smooth probability models for multiclass responses with bounded second derivatives of logodds the approximation rate is oms in hellinger distance or oms in kullbackleibler divergence here s dimx is the dimension of the input space or the number of predictors with the availability of training data of size n we also show that consistency in multiclass regression and classification can be achieved simultaneously for all classes when posterior based inference is performed in a bayesian framework loosely speaking such consistency refers to performance being often close to the best possible for large n consistency can be achieved either by taking m mn or by taking m to be uniformly distributed among mn according to the prior where pr mn pr na in order as n grows for some a isin 
the rate adapting poisson model for information retrieval and object recognition the rate adapting poisson model for information retrieval and object recognition probabilistic modelling of text data in the bagofwords representation has been dominated by directed graphical models such as plsi lda nmf and discrete pca recently state of the art performance on visual object recognition has also been reported using variants of these models we introduce an alternative undirected graphical model suitable for modelling count data this rate adapting poisson rap model is shown to generate superior dimensionally reduced representations for subsequent retrieval or classification models are trained using contrastive divergence while inference of latent topical representations is efficiently achieved through a simple matrix multiplication 
kernelizing the output of treebased methods kernelizing the output of treebased methods we extend treebased methods to the prediction of structured outputs using a kernelization of the algorithm that allows one to grow trees as soon as a kernel can be defined on the output space the resulting algorithm called output kernel trees ok generalizes classification and regression trees as well as treebased ensemble methods in a principled way it inherits several features of these methods such as interpretability robustness to irrelevant variables and input scalability when only the gram matrix over the outputs of the learning sample is given it learns the output kernel as a function of inputs we show that the proposed algorithm works well on an image reconstruction task and on a biological network inference problem 
nightmare at test time nightmare at test time when constructing a classifier from labeled data it is important not to assign too much weight to any single input feature in order to increase the robustness of the classifier this is particularly important in domains with nonstationary feature distributions or with input sensor failures a common approach to achieving such robustness is to introduce regularization which spreads the weight more evenly between the features however this strategy is very generic and cannot induce robustness specifically tailored to the classification task at hand in this work we introduce a new algorithm for avoiding single feature overweighting by analyzing robustness using a game theoretic formalization we develop classifiers which are optimally resilient to deletion of features in a minimax sense and show how to construct such classifiers using quadratic programming we illustrate the applicability of our methods on spam filtering and handwritten digit recognition tasks where feature deletion is indeed a realistic noise model 
a choice model with infinitely many latent features a choice model with infinitely many latent features elimination by aspects eba is a probabilistic choice model describing how humans decide between several options the options from which the choice is made are characterized by binary features and associated weights for instance when choosing which mobile phone to buy the features to consider may be long lasting battery color screen etc existing methods for inferring the parameters of the model assume prespecified features however the features that lead to the observed choices are not always known here we present a nonparametric bayesian model to infer the features of the options and the corresponding weights from choice data we use the indian buffet process ibp as a prior over the features inference using markov chain monte carlo mcmc in conjugate ibp models has been previously described the main contribution of this paper is an mcmc algorithm for the eba model that can also be used in inference for other nonconjugate ibp modelsthis may broaden the use of ibp priors considerably 
connectionist temporal classification connectionist temporal classification many realworld sequence learning tasks require the prediction of sequences of labels from noisy unsegmented input data in speech recognition for example an acoustic signal is transcribed into words or subword units recurrent neural networks rnns are powerful sequence learners that would seem well suited to such tasks however because they require presegmented training data and postprocessing to transform their outputs into label sequences their applicability has so far been limited this paper presents a novel method for training rnns to label unsegmented sequences directly thereby solving both problems an experiment on the timit speech corpus demonstrates its advantages over both a baseline hmm and a hybrid hmmrnn 
practical solutions to the problem of diagonal dominance in kernel document clustering practical solutions to the problem of diagonal dominance in kernel document clustering in supervised kernel methods it has been observed that the performance of the svm classifier is poor in cases where the diagonal entries of the gram matrix are large relative to the offdiagonal entries this problem referred to as diagonal dominance often occurs when certain kernel functions are applied to sparse highdimensional data such as text corpora in this paper we investigate the implications of diagonal dominance for unsupervised kernel methods specifically in the task of document clustering we propose a selection of strategies for addressing this issue and evaluate their effectiveness in producing more accurate and stable clusterings 
fast transpose methods for kernel learning on sparse data fast transpose methods for kernel learning on sparse data kernelbased learning algorithms such as support vector machines svms or perceptron often rely on sequential optimization where a few examples are added at each iteration updating the kernel matrix usually requires matrixvector multiplications we propose a new method based on transposition to speedup this computation on sparse data instead of dotproducts over sparse feature vectors our computation incrementally merges lists of training examples and minimizes access to the data caching and shrinking are also optimized for sparsity on very large natural language tasks tagging translation text classification with sparse feature representations a to fold speedup over libsvm is observed using the same smo algorithm theory and experiments explain what type of sparsity structure is needed for this approach to work and why its adaptation to maxent sequential optimization is inefficient 
an analysis of graph cut size for transductive learning an analysis of graph cut size for transductive learning i consider the setting of transductive learning of vertex labels in graphs in which a graph with n vertices is sampled according to some unknown distribution there is a true labeling of the vertices such that each vertex is assigned to exactly one of k classes but the labels of only some random subset of the vertices are revealed to the learner the task is then to find a labeling of the remaining unlabeled vertices that agrees as much as possible with the true labeling several existing algorithms are based on the assumption that adjacent vertices are usually labeled the same in order to better understand algorithms based on this assumption i derive datadependent bounds on the fraction of mislabeled vertices based on the number or total weight of edges between vertices differing in predicted label ie the size of the cut 
learning a kernel function for classification with small training samples learning a kernel function for classification with small training samples when given a small sample we show that classification with svm can be considerably enhanced by using a kernel function learned from the training data prior to discrimination this kernel is also shown to enhance retrieval based on data similarity specifically we describe kernelboost a boosting algorithm which computes a kernel function as a combination of weak space partitions the kernel learning method naturally incorporates domain knowledge in the form of unlabeled data ie in a semisupervised or transductive settings and also in the form of labeled samples from relevant related problems ie in a learningtolearn scenario the latter goal is accomplished by learning a single kernel function for all classes we show comparative evaluations of our method on datasets from the uci repository we demonstrate performance enhancement on two challenging tasks digit classification with kernel svm and facial image retrieval based on image similarity as measured by the learnt kernel 
looping suffix treebased inference of partially observable hidden state looping suffix treebased inference of partially observable hidden state we present a solution for inferring hidden state from sensorimotor experience when the environment takes the form of a pomdp with deterministic transition and observation functions such environments can appear to be arbitrarily complex and nondeterministic on the surface but are actually deterministic with respect to the unobserved underlying state we show that there always exists a finite historybased representation that fully captures the unobserved world state allowing for perfect prediction of action effects this representation takes the form of a looping prediction suffix tree pst we derive a sound and complete algorithm for learning a looping pst from a sufficient sample of sensorimotor experience we also give empirical illustrations of the advantages conferred by this approach and characterize the approximations to the looping pst that are made by existing algorithms such as variable length markov models utile suffix memory and causal state splitting reconstruction 
batch mode active learning and its application to medical image classification batch mode active learning and its application to medical image classification the goal of active learning is to select the most informative examples for manual labeling most of the previous studies in active learning have focused on selecting a single unlabeled example in each iteration this could be inefficient since the classification model has to be retrained for every labeled example in this paper we present a framework for batch mode active learning that applies the fisher information matrix to select a number of informative examples simultaneously the key computational challenge is how to efficiently identify the subset of unlabeled examples that can result in the largest reduction in the fisher information to resolve this challenge we propose an efficient greedy algorithm that is based on the property of submodular functions our empirical studies with five uci datasets and one realworld medical image classification show that the proposed batch mode active learning algorithm is more effective than the stateoftheart algorithms for active learning 
ranking individuals by group comparisons ranking individuals by group comparisons this paper proposes new approaches to rank individuals from their group competition results many realworld problems are of this type for example ranking players from team games is important in some sports we propose an exponential model to solve such problems to estimate individual rankings through the proposed model we introduce two convex minimization formulas with easy and efficient solution procedures experiments on real bridge records and multiclass classification demonstrate the viability of the proposed model 
hidden process models hidden process models we introduce hidden process models hpms a class of probabilistic models for multivariate time series data the design of hpms has been motivated by the challenges of modeling hidden cognitive processes in the brain given functional magnetic resonance imaging fmri data fmri data is sparse highdimensional nonmarkovian and often involves prior knowledge of the form hidden event a occurs n times within the interval ttprime hpms provide a generalization of the widely used general linear model approaches to fmri analysis and hpms can also be viewed as a subclass of dynamic bayes networks 
estimating relatedness via data compression estimating relatedness via data compression we show that it is possible to use data compression on independently obtained hypotheses from various tasks to algorithmically provide guarantees that the tasks are sufficiently related to benefit from multitask learning we give uniform bounds in terms of the empirical average error for the true average error of the n hypotheses provided by deterministic learning algorithms drawing independent samples from a set of n unknown computable task distributions over finite sets 
automatic basis function construction for approximate dynamic programming and reinforcement learning automatic basis function construction for approximate dynamic programming and reinforcement learning we address the problem of automatically constructing basis functions for linear approximation of the value function of a markov decision process mdp our work builds on results by bertsekas and castantildeon who proposed a method for automatically aggregating states to speed up value iteration we propose to use neighborhood component analysis goldberger et al a dimensionality reduction technique created for supervised learning in order to map a highdimensional state space to a lowdimensional space based on the bellman error or on the temporal difference td error we then place basis function in the lowerdimensional space these are added as new features for the linear function approximator this approach is applied to a highdimensional inventory control problem 
personalized handwriting recognition via biased regularization personalized handwriting recognition via biased regularization we present a new approach to personalized handwriting recognition the problem also known as writer adaptation consists of converting a generic userindependent recognizer into a personalized userdependent one which has an improved recognition rate for a particular user the adaptation step usually involves userspecific samples which leads to the fundamental question of how to fuse this new information with that captured by the generic recognizer we propose adapting the recognizer by minimizing a regularized risk functional a modified svm where the prior knowledge from the generic recognizer enters through a modified regularization term the result is a simple personalization framework with very good practical properties experiments on a class realworld data set show that the number of errors can be reduced by over with as few as five user samples per character 
optimal kernel selection in kernel fisher discriminant analysis optimal kernel selection in kernel fisher discriminant analysis in kernel fisher discriminant analysis kfda we carry out fisher linear discriminant analysis in a high dimensional feature space defined implicitly by a kernel the performance of kfda depends on the choice of the kernel in this paper we consider the problem of finding the optimal kernel over a given convex set of kernels we show that this optimal kernel selection problem can be reformulated as a tractable convex optimization problem which interiorpoint methods can solve globally and efficiently the kernel selection method is demonstrated with some uci machine learning benchmark examples 
pareto optimal linear classification pareto optimal linear classification we consider the problem of choosing a linear classifier that minimizes misclassification probabilities in twoclass classification which is a bicriterion problem involving a tradeoff between two objectives we assume that the classconditional distributions are gaussian this assumption makes it computationally tractable to find pareto optimal linear classifiers whose classification capabilities are inferior to no other linear ones the main purpose of this paper is to establish several robustness properties of those classifiers with respect to variations and uncertainties in the distributions we also extend the results to kernelbased classification finally we show how to carry out tradeoff analysis empirically with a finite number of given labeled data 
fast particle smoothing fast particle smoothing we propose efficient particle smoothing methods for generalized statespaces models particle smoothing is an expensive on algorithm where n is the number of particles we overcome this problem by integrating dual tree recursions and fast multipole techniques with forwardbackward smoothers a new generalized twofilter smoother and a maximum a posteriori map smoother our experiments show that these improvements can substantially increase the practicality of particle smoothing 
autonomous shaping autonomous shaping we introduce the use of learned shaping rewards in reinforcement learning tasks where an agent uses prior experience on a sequence of tasks to learn a portable predictor that estimates intermediate rewards resulting in accelerated learning in later tasks that are related but distinct such agents can be trained on a sequence of relatively easy tasks in order to develop a more informative measure of reward that can be transferred to improve performance on more difficult tasks without requiring a hand coded shaping function we use a rod positioning task to show that this significantly improves performance even after a very brief training period 
data association for topic intensity tracking data association for topic intensity tracking we present a unified model of what was traditionally viewed as two separate tasks data association and intensity tracking of multiple topics over time in the data association part the task is to assign a topic a class to each data point and the intensity tracking part models the bursts and changes in intensities of topics over time our approach to this problem combines an extension of factorial hidden markov models for topic intensity tracking with exponential order statistics for implicit data association experiments on text and email datasets show that the interplay of classification and topic intensity tracking improves the accuracy of both classification and intensity tracking even a little noise in topic assignments can mislead the traditional algorithms however our approach detects correct topic intensities even with topic noise 
learning lowrank kernel matrices learning lowrank kernel matrices kernel learning plays an important role in many machine learning tasks however algorithms for learning a kernel matrix often scale poorly with running times that are cubic in the number of data points in this paper we propose efficient algorithms for learning lowrank kernel matrices our algorithms scale linearly in the number of data points and quadratically in the rank of the kernel we introduce and employ bregman matrix divergences for rankdeficient matricesthese divergences are natural for our problem since they preserve the rank as well as positive semidefiniteness of the kernel matrix special cases of our framework yield faster algorithms for various existing kernel learning problems experimental results demonstrate the effectiveness of our algorithms in learning both lowrank and fullrank kernels 
local distance preservation in the gplvm through back constraints local distance preservation in the gplvm through back constraints the gaussian process latent variable model gplvm is a generative approach to nonlinear low dimensional embedding that provides a smooth probabilistic mapping from latent to data space it is also a nonlinear generalization of probabilistic pca ppca tipping bishop while most approaches to nonlinear dimensionality methods focus on preserving local distances in data space the gplvm focusses on exactly the opposite being a smooth mapping from latent to data space it focusses on keeping things apart in latent space that are far apart in data space in this paper we first provide an overview of dimensionality reduction techniques placing the emphasis on the kind of distance relation preserved we then show how the gplvm can be generalized through back constraints to additionally preserve local distances we give illustrative experiments on common data sets 
simpler knowledgebased support vector machines simpler knowledgebased support vector machines if appropriately used prior knowledge can significantly improve the predictive accuracy of learning algorithms or reduce the amount of training data needed in this paper we introduce a simple method to incorporate prior knowledge in support vector machines by modifying the hypothesis space rather than the optimization problem the optimization problem is amenable to solution by the constrained concave convex procedure which finds a local optimum the paper discusses different kinds of prior knowledge and demonstrates the applicability of the approach in some characteristic experiments 
using queryspecific variance estimates to combine bayesian classifiers using queryspecific variance estimates to combine bayesian classifiers many of todays best classification results are obtained by combining the responses of a set of base classifiers to produce an answer for the query this paper explores a novel query specific combination rule after learning a set of simple belief network classifiers we produce an answer to each query by combining their individual responses using weights based inversely on their respective variances around their responses these variances are based on the uncertainty of the network parameters which in turn depend on the training datasample in essence this variance quantifies the base classifiers confidence of its response to this query our experimental results show that these mixtureusingvariance belief net classifiers muvs work effectively especially when the base classifiers are learned using balanced bootstrap samples and when their results are combined using jamesstein shrinkage we also found that our variancebased combination rule performed better than both bagging and adaboost even on the set of base classifiers produced by adaboost itself finally this framework is extremely efficient as both the learning and the classification components require only straightline code 
a probabilistic model for text kernels a probabilistic model for text kernels this paper explores several kernels in the context of text classification a novel view of how documents might have been created is introduced and kernels are derived from this framework the relations between these kernels as well as to the gaussian kernel are discussed moreover the popular tfidf weighting scheme will be derived as a natural consequence finally the kernels have been evaluated on the reuters corpus volume i newswire database to assess their quality in a topic classification application 
efficient map approximation for dense energy functions efficient map approximation for dense energy functions we present an efficient method for maximizing energy functions with first and second order potentials suitable for map labeling estimation problems that arise in undirected graphical models our approach is to relax the integer constraints on the solution in two steps first we efficiently obtain the relaxed global optimum following a procedure similar to the iterative power method for finding the largest eigenvector of a matrix next we map the relaxed optimum on a simplex and show that the new energy obtained has a certain optimal bound starting from this energy we follow an efficient coordinate ascent procedure that is guaranteed to increase the energy at every step and converge to a solution that obeys the initial integral constraints we also present a sufficient condition for ascent procedures that guarantees the increase in energy at every step 
nonstationary kernel combination nonstationary kernel combination the power and popularity of kernel methods stem in part from their ability to handle diverse forms of structured inputs including vectors graphs and strings recently several methods have been proposed for combining kernels from heterogeneous data sources however all of these methods produce stationary combinations ie the relative weights of the various kernels do not vary among input examples this article proposes a method for combining multiple kernels in a nonstationary fashion the approach uses a largemargin latentvariable generative model within the maximum entropy discrimination med framework latent parameter estimation is rendered tractable by variational bounds and an iterative optimization procedure the classifier we use is a logratio of gaussian mixtures in which each component is implicitly mapped via a mercer kernel function we show that the support vector machine is a special case of this model in this approach discriminative parameter estimation is feasible via a fast sequential minimal optimization algorithm empirical results are presented on synthetic data several benchmarks and on a protein function annotation task 
regionbased value iteration for partially observable markov decision processes regionbased value iteration for partially observable markov decision processes an approximate regionbased value iteration rbvi algorithm is proposed to find the optimal policy for a partially observable markov decision process pomdp the proposed rbvi approximates the true polyhedral partition of the belief simplex with an ellipsoidal partition such that the optimal value function is linear in each of the ellipsoidal regions the position and shape of each region as well as the gradient alphavector of the optimal value function in the region are parameterized explicitly and are estimated via efficient expectation maximization em and variational bayesian em vbem based on a set of selected sample belief points the rbvi maintains a much smaller number of alphavectors than pointbased methods and yields a more parsimonious representation that approximates the true value function in the maximum likelihood ml sense the results on benchmark problems show that the proposed rbvi is comparable in performance to stateoftheart algorithms despite of the small number of alphavectors that are used 
multiclass boosting with repartitioning multiclass boosting with repartitioning a multiclass classification problem can be reduced to a collection of binary problems with the aid of a coding matrix the quality of the final solution which is an ensemble of base classifiers learned on the binary problems is affected by both the performance of the base learner and the errorcorrecting ability of the coding matrix a coding matrix with strong errorcorrecting ability may not be overall optimal if the binary problems are too hard for the base learner thus a tradeoff between errorcorrecting and base learning should be sought in this paper we propose a new multiclass boosting algorithm that modifies the coding matrix according to the learning ability of the base learner we show experimentally that our algorithm is very efficient in optimizing the multiclass margin cost and outperforms existing multiclass algorithms such as adaboostecc and onevsone the improvement is especially significant when the base learner is not very powerful 
pachinko allocation pachinko allocation latent dirichlet allocation lda and other related topic models are increasingly popular tools for summarization and manifold discovery in discrete data however lda does not capture correlations between topics in this paper we introduce the pachinko allocation model pam which captures arbitrary nested and possibly sparse correlations between topics using a directed acyclic graph dag the leaves of the dag represent individual words in the vocabulary while each interior node represents a correlation among its children which may be words or other interior nodes topics pam provides a flexible alternative to recent work by blei and lafferty which captures correlations only between pairs of topics using text data from newsgroups historic nips proceedings and other research paper corpora we show improved performance of pam in document classification likelihood of heldout data the ability to support finergrained topics and topical keyword coherence 
spectral clustering for multitype relational data spectral clustering for multitype relational data clustering on multitype relational data has attracted more and more attention in recent years due to its high impact on various important applications such as web mining ecommerce and bioinformatics however the research on general multitype relational data clustering is still limited and preliminary the contribution of the paper is threefold first we propose a general model the collective factorization on related matrices for multitype relational data clustering the model is applicable to relational data with various structures second under this model we derive a novel algorithm the spectral relational clustering to cluster multitype interrelated data objects simultaneously the algorithm iteratively embeds each type of data objects into low dimensional spaces and benefits from the interactions among the hidden structures of different types of data objects extensive experiments demonstrate the promise and effectiveness of the proposed algorithm third we show that the existing spectral clustering algorithms can be considered as the special cases of the proposed model and algorithm this demonstrates the good theoretic generality of the proposed model and algorithm 
combined central and subspace clustering for computer vision applications combined central and subspace clustering for computer vision applications central and subspace clustering methods are at the core of many segmentation problems in computer vision however both methods fail to give the correct segmentation in many practical scenarios eg when data points are close to the intersection of two subspaces or when two cluster centers in different subspaces are spatially close in this paper we address these challenges by considering the problem of clustering a set of points lying in a union of subspaces and distributed around multiple cluster centers inside each subspace we propose a generalization of kmeans and ksubspaces that clusters the data by minimizing a cost function that combines both central and subspace distances experiments on synthetic data compare our algorithm favorably against four other clustering methods we also test our algorithm on computer vision problems such as face clustering with varying illumination and video shot segmentation of dynamic scenes 
fast direct policy evaluation using multiscale analysis of markov diffusion processes fast direct policy evaluation using multiscale analysis of markov diffusion processes policy evaluation is a critical step in the approximate solution of large markov decision processes mdps typically requiring os to directly solve the bellman system of s linear equations where s is the state space size in the discrete case and the sample size in the continuous case in this paper we apply a recently introduced multiscale framework for analysis on graphs to design a faster algorithm for policy evaluation for a fixed policy pi this framework efficiently constructs a multiscale decomposition of the random walk ppi associated with the policy pi this enables efficiently computing medium and long term state distributions approximation of value functions and the direct computation of the potential operator i gammappi needed to solve bellmans equation we show that even a preliminary nonoptimized version of the solver competes with highly optimized iterative techniques requiring in many cases a complexity of os 
pruning in ordered bagging ensembles pruning in ordered bagging ensembles we present a novel ensemble pruning method based on reordering the classifiers obtained from bagging and then selecting a subset for aggregation ordering the classifiers generated in bagging makes it possible to build subensembles of increasing size by including first those classifiers that are expected to perform best when aggregated ensemble pruning is achieved by halting the aggregation process before all the classifiers generated are included into the ensemble pruned subensembles containing between and of the initial pool of classifiers besides being smaller improve the generalization performance of the full bagging ensemble in the classification problems investigated 
learning highorder mrf priors of color images learning highorder mrf priors of color images in this paper we use large neighborhood markov random fields to learn rich prior models of color images our approach extends the monochromatic fields of experts model roth black a to color images in the fields of experts model the curse of dimensionality due to very large clique sizes is circumvented by parameterizing the potential functions according to a product of experts we introduce simplifications to the original approach by roth and black which allow us to cope with the increased clique size typically xx or xx pixels of color images experimental results are presented for image denoising which evidence improvements over stateoftheart monochromatic image priors 
the uniqueness of a good optimum for kmeans the uniqueness of a good optimum for kmeans if we have found a good clustering c of a data set can we prove that c is not far from the unknown best clustering copt of these data perhaps surprisingly the answer to this question is sometimes yes when goodness is measured by the distortion of kmeans clustering this paper proves spectral bounds on the distance dc copt the bounds exist in the case when the data admits a low distortion clustering 
kernel information embeddings kernel information embeddings we describe a family of embedding algorithms that are based on nonparametric estimates of mutual information mi using parzen window estimates of the distribution in the joint input embeddingspace we derive a mibased objective function for dimensionality reduction that can be optimized directly with respect to a set of latent data representatives various types of supervision signal can be introduced within the framework by replacing plain mi with several forms of conditional mi examples of the semiunsupervised algorithms that we obtain this way are a new model for manifold alignment and a new type of embedding method that performs conditional dimensionality reduction 
generalized spectral bounds for sparse lda generalized spectral bounds for sparse lda we present a discrete spectral framework for the sparse or cardinalityconstrained solution of a generalized rayleigh quotient this nphard combinatorial optimization problem is central to supervised learning tasks such as sparse lda feature selection and relevance ranking for classification we derive a new generalized form of the inclusion principle for variational eigenvalue bounds leading to exact and optimal sparse linear discriminants using branchandbound search an efficient greedy approximate technique is also presented the generalization performance of our sparse lda algorithms is demonstrated with realworld uci ml benchmarks and compared to a leading svmbased gene selection algorithm for cancer classification 
learning to impersonate learning to impersonate consider alice and bob who have some shared secret which helps alice to identify bobimpersonators and eve who does not know their secret eve wants to impersonate bob and fool alice if eve is computationally unbounded how long does she need to observe bob before she can impersonate him what is a good strategy for eve if cryptographic oneway functions exist an efficient eve cannot impersonate even very simple bobs but if they do not exist can eve learn to impersonate any efficient bobwe formalize these questions in a new computational learning model which we believe captures a wide variety of natural learning tasks and tightly bound the number of observations eve makes in terms of the secrets entropy we then show that if oneway functions do not exist then an efficient eve can learn to impersonate any efficient bob nearly as well as an unbounded evefor the full version of this work see naor rothblum 
online decoding of markov models under latency constraints online decoding of markov models under latency constraints the viterbi algorithm is an efficient and optimal method for decoding linearchain markov models however the entire input sequence must be observed before the labels for any time step can be generated and therefore viterbi cannot be directly applied to onlineinteractivestreaming scenarios without incurring significant possibly unbounded latency a widely used approach is to break the input stream into fixedsize windows and apply viterbi to each window larger windows lead to higher accuracy but result in higher latencywe propose several alternative algorithms to the fixedsized window decoding approach these approaches compute a certainty measure on predicted labels that allows us to trade off latency for expected accuracy dynamically without having to choose a fixed window size up front not surprisingly this more principled approach gives us a substantial improvement over choosing a fixed window we show the effectiveness of the approach for the task of spotting semistructured information in large documents when compared to full viterbi the approach suffers a percent error degradation with a average latency of time steps versus the potentially infinite latency of viterbi when compared to fixed windows viterbi we achieve a x reduction in error and x reduction in latency 
learning hierarchical task networks by observation learning hierarchical task networks by observation knowledgebased planning methods offer benefits over classical techniques but they are time consuming and costly to construct there has been research on learning plan knowledge from search but this can take substantial computer time and may even fail to find solutions on complex tasks here we describe another approach that observes sequences of operators taken from expert solutions to problems and learns hierarchical task networks from them the method has similarities to previous algorithms for explanationbased learning but differs in its ability to acquire hierarchical structures and in the generality of learned conditions these increase the methods capability to transfer learned knowledge to other problems and supports the acquisition of recursive procedures after presenting the learning algorithm we report experiments that compare its abilities to other techniques on two planning domains in closing we review related work and directions for future research 
reinforcement learning for optimized trade execution reinforcement learning for optimized trade execution we present the first largescale empirical application of reinforcement learning to the important problem of optimized trade execution in modern financial markets our experiments are based on years of millisecond timescale limit order data from nasdaq and demonstrate the promise of reinforcement learning methods to market microstructure problems our learning algorithm introduces and exploits a natural lowimpact factorization of the state space 
concept boundary detection for speeding up svms concept boundary detection for speeding up svms support vector machines svms suffer from an on training cost where n denotes the number of training instances in this paper we propose an algorithm to select boundary instances as training data to substantially reduce n our proposed algorithm is motivated by the result of burges that removing nonsupport vectors from the training set does not change svm training results our algorithm eliminates instances that are likely to be nonsupport vectors in the conceptindependent preprocessing step of our algorithm we prepare nearestneighbor lists for training instances in the conceptspecific sampling step we can then effectively select useful training data for each target concept empirical studies show our algorithm to be effective in reducing n outperforming other competing downsampling algorithms without significantly compromising testing accuracy 
the support vector decomposition machine the support vector decomposition machine in machine learning problems with tens of thousands of features and only dozens or hundreds of independent training examples dimensionality reduction is essential for good learning performance in previous work many researchers have treated the learning problem in two separate phases first use an algorithm such as singular value decomposition to reduce the dimensionality of the data set and then use a classification algorithm such as naiumlve bayes or support vector machines to learn a classifier we demonstrate that it is possible to combine the two goals of dimensionality reduction and classification into a single learning objective and present a novel and efficient algorithm which optimizes this objective directly we present experimental results in fmri analysis which show that we can achieve better learning performance and lowerdimensional representations than twophase approaches can 
an analytic solution to discrete bayesian reinforcement learning an analytic solution to discrete bayesian reinforcement learning reinforcement learning rl was originally proposed as a framework to allow agents to learn in an online fashion as they interact with their environment existing rl algorithms come short of achieving this goal because the amount of exploration required is often too costly andor too time consuming for online learning as a result rl is mostly used for offline learning in simulated environments we propose a new algorithm called beetle for effective online learning that is computationally efficient while minimizing the amount of exploration we take a bayesian modelbased approach framing rl as a partially observable markov decision process our two main contributions are the analytical derivation that the optimal value function is the upper envelope of a set of multivariate polynomials and an efficient pointbased value iteration algorithm that exploits this simple parameterization 
missl missl there has been much work on applying multipleinstance mi learning to contentbased image retrieval cbir where the goal is to rank all images in a known repository using a small labeled data set most existing mi learning algorithms are nontransductive in that the images in the repository serve only as test data and are not used in the learning process we present missl multipleinstance semisupervised learning that transforms any mi problem into an input for a graphbased singleinstance semisupervised learning method that encodes the mi aspects of the problem simultaneously working at both the bag and point levels unlike most prior mi learning algorithms missl makes use of the unlabeled data 
constructing informative priors using transfer learning constructing informative priors using transfer learning many applications of supervised learning require good generalization from limited labeled data in the bayesian setting we can try to achieve this goal by using an informative prior over the parameters one that encodes useful domain knowledge focusing on logistic regression we present an algorithm for automatically constructing a multivariate gaussian prior with a full covariance matrix for a given supervised learning task this prior relaxes a commonly used but overly simplistic independence assumption and allows parameters to be dependent the algorithm uses other similar learning problems to estimate the covariance of pairs of individual parameters we then use a semidefinite program to combine these estimates and learn a good prior for the current learning task we apply our methods to binary text classification and demonstrate a to test error reduction over a commonly used prior 
cn cpcn cn cpcn we address the issue of the learnability of concept classes under three classification noise models in the probably approximately correct framework after introducing the classconditional classification noise cccn model we investigate the problem of the learnability of concept classes under this particular setting and we show that concept classes that are learnable under the wellknown uniform classification noise cn setting are also cccnlearnable which gives cn cccn we then use this result to prove the equality between the set of concept classes that are cnlearnable and the set of concept classes that are learnable in the constant partition classification noise cpcn setting or in other words we show that cn cpcn 
maximum margin planning maximum margin planning imitation learning of sequential goaldirected behavior by standard supervised techniques is often difficult we frame learning such behaviors as a maximum margin structured prediction problem over a space of policies in this approach we learn mappings from features to cost so an optimal policy in an mdp with these cost mimics the experts behavior further we demonstrate a simple provably efficient approach to structured maximum margin learning based on the subgradient method that leverages existing fast algorithms for inference although the technique is general it is particularly relevant in problems where a and dynamic programming approaches make learning policies tractable in problems beyond the limitations of a qp formulation we demonstrate our approach applied to route planning for outdoor mobile robots where the behavior a designer wishes a planner to execute is often clear while specifying cost functions that engender this behavior is a much more difficult task 
quadratic programming relaxations for metric labeling and markov random field map estimation quadratic programming relaxations for metric labeling and markov random field map estimation quadratic program relaxations are proposed as an alternative to linear program relaxations and tree reweighted belief propagation for the metric labeling or map estimation problem an additional convex relaxation of the quadratic approximation is shown to have additive approximation guarantees that apply even when the graph weights have mixed sign or do not come from a metric the approximations are extended in a manner that allows tight variational relaxations of the map problem although they generally involve nonconvex optimization experiments carried out on synthetic data show that the quadratic approximations can be more accurate and computationally efficient than the linear programming and propagation based alternatives 
categorization in multiple category systems categorization in multiple category systems we explore the situation in which documents have to be categorized into more than one category system a situation we refer to as multipleview categorization more particularly we address the case where two different categorizers have already been built based on nonnecessarily identical training sets each one labeled using one category system on the top of these categorizers considered as blackboxes we propose some algorithms able to exploit a third training set containing a few examples annotated in both category systems such a situation arises for example in large companies where incoming mails have to be routed to several departments each one relying on its own category system we focus here on exploiting possible dependencies between category systems in order to refine the categorization decisions made by categorizers trained independently on different category systems after a description of the multiple categorization problem we present several possible solutions based either on a categorization or reweighting approach and compare them on real data lastly we show how the multimedia categorization problem can be cast as a multiple categorization problem and assess our methods in this framework 
how boosting the margin can also boost classifier complexity how boosting the margin can also boost classifier complexity boosting methods are known not to usually overfit training data even as the size of the generated classifiers becomes large schapire et al attempted to explain this phenomenon in terms of the margins the classifier achieves on training examples later however breiman cast serious doubt on this explanation by introducing a boosting algorithm arcgv that can generate a higher margins distribution than adaboost and yet performs worse in this paper we take a close look at breimans compelling but puzzling results although we can reproduce his main finding we find that the poorer performance of arcgv can be explained by the increased complexity of the base classifiers it uses an explanation supported by our experiments and entirely consistent with the margins theory thus we find maximizing the margins is desirable but not necessarily at the expense of other factors especially baseclassifier complexity 
combining discriminative features to infer complex trajectories combining discriminative features to infer complex trajectories we propose a new model for the probabilistic estimation of continuous state variables from a sequence of observations such as tracking the position of an object in video this mapping is modeled as a product of dynamics experts features relating the state at adjacent timesteps and observation experts features relating the state to the image sequence individual features are flexible in that they can switch on or off at each timestep depending on their inferred relevance or on additional side information and discriminative in that they need not model the full generative likelihood of the data when trained conditionally this permits the inclusion of a broad range of rich features for example features relying on observations from multiple timesteps and allows the relevance of features to be learned from labeled sequences 
sequential update of adtrees sequential update of adtrees ingcreasingly datamining algorithms must deal with databases that continuously grow over time these algorithms must avoid repeatedly scanning their databases when database attributes are symbolic adtrees have already shown to be efficient structures to store sufficient statistics in main memory and to accelerate the mining process in batch environments here we present an efficient method to sequentially update adtrees that is suitable for incremental environments 
predictive lineargaussian models of controlled stochastic dynamical systems predictive lineargaussian models of controlled stochastic dynamical systems we introduce the controlled predictive lineargaussian model cplg a model that uses predictive state to model discretetime dynamical systems with realvalued observations and vectorvalued actions this extends the plg an uncontrolled model recently introduced by rudary et al we show that the cplg subsumes controlled linear dynamical systems lds also called kalman filter models of equal dimension but requires fewer parameters we also introduce the predictive linearquadratic gaussian problem a costminimization problem based on the cplg that we show is equivalent to linearquadratic gaussian problems lqg sometimes called lqr we present an algorithm to estimate cplg parameters from data and show that our algorithm is a consistent estimation procedure finally we present empirical results suggesting that our algorithm performs favorably compared to expectation maximization on controlled lds models 
a statistical approach to rule learning a statistical approach to rule learning we present a new statistical approach to rule learning doing so we address two of the problems inherent in traditional rule learning the computational hardness of finding rule sets with low training error and the need for capacity control to avoid overfitting the chosen representation involves weights attached to rules instead of optimizing the error rate directly we optimize for rule sets that have large margin and low variance this can be formulated as a convex optimization problem allowing for efficient computation given the representation and the optimization procedure we effectively yield weighted clauses in a cnflike representation to avoid overfitting we propose a model selection strategy that utilizes a novel concentration inequality empirical tests show that the system is competitive with existing rule learning algorithms and that its flexible learning bias can be adjusted to improve predictive accuracy considerably 
efficient inference on sequence segmentation models efficient inference on sequence segmentation models sequence segmentation is a flexible and highly accurate mechanism for modeling several applications inference on segmentation models involves dynamic programming computations that in the worst case can be cubic in the length of a sequence in contrast typical sequence labeling models require linear time we remove this limitation of segmentation models visavis sequential models by designing a succinct representation of potentials common across overlapping segments we exploit such potentials to design efficient inference algorithms that are both analytically shown to have a lower complexity and empirically found to be comparable to sequential models for typical extraction tasks 
costsensitive learning with conditional markov networks costsensitive learning with conditional markov networks there has been a recent growing interest in classification and link prediction in structured domains methods such as crfs lafferty et al and rmns taskar et al support flexible mechanisms for modeling correlations due to the link structure in addition in many structured domains there is an interesting structure in the risk or cost function associated with different misclassifications there is a rich tradition of costsensitive learning applied to unstructured iid data here we propose a general framework which can capture correlations in the link structure and handle structured cost functions we present a novel costsensitive structured classifier based on maximum entropy principles that directly determines the costsensitive classification we contrast this with an approach which employs a standard loss structured classifier followed by minimization of the expected cost of misclassification we demonstrate the utility of our proposed classifier with experiments on both synthetic and realworld data 
feature value acquisition in testing feature value acquisition in testing in medical diagnosis doctors often have to order sets of medical tests in sequence in order to make an accurate diagnosis of patient diseases while doing so they have to make a tradeoff between the cost of the tests and possible misdiagnosis in this paper we use costsensitive learning to model this process we assume that test examples new patients may contain missing values and their actual values can be acquired at cost similar to doing medical tests in order to reduce misclassification errors misdiagnosis we propose a novel sequential batch test algorithm that can acquire sets of attribute values in sequence similar to sets of medical tests ordered by doctors in sequence the goal of our algorithm is to minimize the total cost ie the tradeoff of acquiring attribute values and misclassifications we demonstrate the effectiveness of our algorithm and show that it outperforms previous methods significantly our algorithm can be readily applied in realworld diagnosis tasks a case study on the heart disease is given in the paper 
permutation invariant svms permutation invariant svms we extend support vector machines to input spaces that are sets by ensuring that the classifier is invariant to permutations of subelements within each input such permutations include reordering of scalars in an input vector reorderings of tuples in an input matrix or reorderings of general objects in hilbert spaces within a set as well this approach induces permutational invariance in the classifier which can then be directly applied to unusual setbased representations of data the permutation invariant support vector machine alternates the hungarian method for maximum weight matching within the maximum margin learning procedure we effectively estimate and apply permutations to the input data points to maximize classification margin while minimizing data radius this procedure has a strong theoretical justification via well established error probability bounds experiments are shown on character recognition d object recognition and various uci datasets 
bayesian learning of measurement and structural models bayesian learning of measurement and structural models we present a bayesian search algorithm for learning the structure of latent variable models of continuous variables we stress the importance of applying search operators designed especially for the parametric family used in our models this is performed by searching for subsets of the observed variables whose covariance matrix can be represented as a sum of a matrix of low rank and a diagonal matrix of residuals the resulting search procedure is relatively efficient since the main search operator has a branch factor that grows linearly with the number of variables the resulting models are often simpler and give a better fit than models based on generalizations of factor analysis or those derived from standard hillclimbing methods 
an intrinsic reward mechanism for efficient exploration an intrinsic reward mechanism for efficient exploration how should a reinforcement learning agent act if its sole purpose is to efficiently learn an optimal policy for later use in other words how should it explore to be able to exploit later we formulate this problem as a markov decision process by explicitly modeling the internal state of the agent and propose a principled heuristic for its solution we present experimental results in a number of domains also exploring the algorithms use for learning a policy for a skill given its reward functionan important but neglected component of skill discovery 
deterministic annealing for semisupervised kernel machines deterministic annealing for semisupervised kernel machines an intuitive approach to utilizing unlabeled data in kernelbased classification algorithms is to simply treat unknown labels as additional optimization variables for marginbased loss functions one can view this approach as attempting to learn lowdensity separators however this is a hard optimization problem to solve in typical semisupervised settings where unlabeled data is abundant the popular transductive svm algorithm is a labelswitchingretraining procedure that is known to be susceptible to local minima in this paper we present a global optimization framework for semisupervised kernel machines where an easier problem is parametrically deformed to the original hard problem and minimizers are smoothly tracked our approach is motivated from deterministic annealing techniques and involves a sequence of convex optimization problems that are exactly and efficiently solved we present empirical results on several synthetic and real world datasets that demonstrate the effectiveness of our approach 
feature subset selection bias for classification learning feature subset selection bias for classification learning feature selection is often applied to highdimensional data prior to classification learning using the same training dataset in both selection and learning can result in socalled feature subset selection bias this bias putatively can exacerbate data overfitting and negatively affect classification performance however in current practice separate datasets are seldom employed for selection and learning because dividing the training data into two datasets for feature selection and classifier learning respectively reduces the amount of data that can be used in either task this work attempts to address this dilemma we formalize selection bias for classification learning analyze its statistical properties and study factors that affect selection bias as well as how the bias impacts classification learning via various experiments this research endeavors to provide illustration and explanation why the bias may not cause negative impact in classification as much as expected in regression 
classifying eeg for braincomputer interfaces classifying eeg for braincomputer interfaces classification of multichannel eeg recordings during motor imagination has been exploited successfully for braincomputer interfaces bci in this paper we consider eeg signals as the outputs of a networked dynamical system the cortex and exploit novel features from the collective dynamics of the system for classification herein we also propose a new framework for learning optimal filters automatically from the data by employing a fisher ratio criterion experimental evaluations comparing the proposed dynamical system features with the csp and the ar features reveal their competitive performance during classification results also show the benefits of employing the spatial and the temporal filters optimized using the proposed learning approach 
an investigation of computational and informational limits in gaussian mixture clustering an investigation of computational and informational limits in gaussian mixture clustering we investigate under what conditions clustering by learning a mixture of spherical gaussians is a computationally tractable and b statistically possible we show that using principal component projection greatly aids in recovering the clustering using em present empirical evidence that even using such a projection there is still a large gap between the number of samples needed to recover the clustering using em and the number of samples needed without computational restrictions and characterize the regime in which such a gap exists 
bayesian pattern ranking for move prediction in the game of go bayesian pattern ranking for move prediction in the game of go we investigate the problem of learning to predict moves in the board game of go from game records of expert players in particular we obtain a probability distribution over legal moves for professional play in a given position this distribution has numerous applications in computer go including serving as an efficient standalone go player it would also be effective as a move selector and move sorter for game tree search and as a training tool for go players our method has two major components a a pattern extraction scheme for efficiently harvesting patterns of given size and shape from expert game records and b a bayesian learning algorithm in two variants that learns a distribution over the values of a move given a board position based on the local pattern context the system is trained on expert games and shows excellent prediction performance as indicated by its ability to perfectly predict the moves made by professional go players in of test positions 
pac modelfree reinforcement learning pac modelfree reinforcement learning for a markov decision process with finite state size s and action spaces size a per state we propose a new algorithmdelayed qlearning we prove it is pac achieving near optimal performance except for otildesa timesteps using osa space improving on the otildes a bounds of best previous algorithms this result proves efficient reinforcement learning is possible without learning a model of the mdp from experience learning takes place from a single continuous thread of experienceno resets nor parallel sampling is used beyond its smaller storage and experience requirements delayed qlearnings perexperience computation cost is much less than that of previous pac algorithms 
experienceefficient learning in associative bandit problems experienceefficient learning in associative bandit problems we formalize the associative bandit problem framework introduced by kaelbling as a learningtheory problem the learning environment is modeled as a karmed bandit where arm payoffs are conditioned on an observable input selected on each trial we show that if the payoff functions are constrained to a known hypothesis class learning can be performed efficiently with respect to the vc dimension of this class we formally reduce the problem of pac classification to the associative bandit problem producing an efficient algorithm for any hypothesis class for which efficient classification algorithms are known we demonstrate the approach empirically on a scalable concept class 
full bayesian network classifiers full bayesian network classifiers the structure of a bayesian network bn encodes variable independence learning the structure of a bn however is typically of high computational complexity in this paper we explore and represent variable independence in learning conditional probability tables cpts instead of in learning structure a full bayesian network is used as the structure and a decision tree is learned for each cpt the resulting model is called full bayesian network classifiers fbcs in learning an fbc learning the decision trees for cpts captures essentially both variable independence and contextspecific independence we present a novel efficient decision tree learning which is also effective in the context of fbc learning in our experiments the fbc learning algorithm demonstrates better performance in both classification and ranking compared with other stateoftheart learning algorithms in addition its reduced effort on structure learning makes its time complexity quite low as well 
local fisher discriminant analysis for supervised dimensionality reduction local fisher discriminant analysis for supervised dimensionality reduction dimensionality reduction is one of the important preprocessing steps in highdimensional data analysis in this paper we consider the supervised dimensionality reduction problem where samples are accompanied with class labels traditional fisher discriminant analysis is a popular and powerful method for this purpose however it tends to give undesired results if samples in some class form several separate clusters ie multimodal in this paper we propose a new dimensionality reduction method called local fisher discriminant analysis lfda which is a localized variant of fisher discriminant analysis lfda takes local structure of the data into account so the multimodal data can be embedded appropriately we also show that lfda can be extended to nonlinear dimensionality reduction scenarios by the kernel trick 
iterative relief for feature weighting iterative relief for feature weighting we propose a series of new feature weighting algorithms all stemming from a new interpretation of relief as an online algorithm that solves a convex optimization problem with a marginbased objective function the new interpretation explains the simplicity and effectiveness of relief and enables us to identify some of its weaknesses we offer an analytic solution to mitigate these problems we extend the newly proposed algorithm to handle multiclass problems by using a new multiclass margin definition to reduce computational costs an online learning algorithm is also developed convergence theorems of the proposed algorithms are presented some experiments based on the uci and microarray datasets are performed to demonstrate the effectiveness of the proposed algorithms 
multiclass reducedset support vector machines multiclass reducedset support vector machines there are wellestablished methods for reducing the number of support vectors in a trained binary support vector machine often with minimal impact on accuracy we show how reducedset methods can be applied to multiclass svms made up of several binary svms with significantly better results than reducing each binary svm independently our approach is based on burges approach that constructs each reducedset vector as the preimage of a vector in kernel space but we extend this by recomputing the svm weights and bias optimally using the original svm objective function this leads to greater accuracy for a binary reducedset svm and also allows vectors to be shared between multiple binary svms for greater multiclass accuracy with fewer reducedset vectors we also propose computing preimages using differential evolution which we have found to be more robust than gradient descent alone we show experimental results on a variety of problems and find that this new approach is consistently better than previous multiclass reducedset methods sometimes with a dramatic difference 
fast and space efficient string kernels using suffix arrays fast and space efficient string kernels using suffix arrays string kernels which compare the set of all common substrings between two given strings have recently been proposed by vishwanathan smola surprisingly these kernels can be computed in linear time and linear space using annotated suffix trees even though in theory the suffix tree based algorithm requires on space for an n length string in practice at least n bytes are required n bytes for storing the suffix tree and an additional n bytes for the annotation this large memory requirement coupled with poor locality of memory access inherent due to the use of suffix trees means that the performance of the suffix tree based algorithm deteriorates on large strings in this paper we describe a new linear time yet space efficient and scalable algorithm for computing string kernels based on suffix arrays our algorithm is a faster and easier to implement b on the average requires only n bytes of storage and c exhibits strong locality of memory access we show that our algorithm can be extended to perform linear time prediction on a test string and present experiments to validate our claims 
bayesian regression with input noise for high dimensional data bayesian regression with input noise for high dimensional data this paper examines high dimensional regression with noisecontaminated input and output data goals of such learning problems include optimal prediction with noiseless query points and optimal system identification as a first step we focus on linear regression methods since these can be easily cast into nonlinear learning problems with locally weighted learning approaches standard linear regression algorithms generate biased regression estimates if input noise is present and suffer numerically when the data contains redundancy and irrelevancy inspired by factor analysis regression we develop a variational bayesian algorithm that is robust to illconditioned data automatically detects relevant features and identifies input and output noise all in a computationally efficient way we demonstrate the effectiveness of our techniques on synthetic data and on a system identification task for a rigid body dynamics model of a robotic vision head our algorithm performs to better than previously suggested methods 
probabilistic inference for solving discrete and continuous state markov decision processes probabilistic inference for solving discrete and continuous state markov decision processes inference in markov decision processes has recently received interest as a means to infer goals of an observed action policy recognition and also as a tool to compute policies a particularly interesting aspect of the approach is that any existing inference technique in dbns now becomes available for answering behavioral questionincluding those on continuous factorial or hierarchical state representations here we present an expectation maximization algorithm for computing optimal policies unlike previous approaches we can show that this actually optimizes the discounted expected future return for arbitrary reward functions and without assuming an ad hoc finite total time the algorithm is generic in that any inference technique can be utilized in the estep we demonstrate this for exact inference on a discrete maze and gaussian belief state propagation in continuous stochastic optimal control problems 
clustering graphs by weighted substructure mining clustering graphs by weighted substructure mining graph data is getting increasingly popular in eg bioinformatics and text processing a main difficulty of graph data processing lies in the intrinsic high dimensionality of graphs namely when a graph is represented as a binary feature vector of indicators of all possible subgraphs the dimensionality gets too large for usual statistical methods we propose an efficient method for learning a binomial mixture model in this feature space combining the l regularizer and the data structure called dfs code tree the map estimate of nonzero parameters are computed efficiently by means of the em algorithm our method is applied to the clustering of rna graphs and is compared favorably with graph kernels and the spectral graph distance 
active sampling for detecting irrelevant features active sampling for detecting irrelevant features the general approach for automatically driving data collection using information from previously acquired data is called active learning traditional active learning addresses the problem of choosing the unlabeled examples for which the class labels are queried with the goal of learning a classifier in contrast we address the problem of active feature sampling for detecting useless features we propose a strategy to actively sample the values of new features on classlabeled examples with the objective of feature relevance assessment we derive an active feature sampling algorithm from an information theoretic and statistical formulation of the problem we present experimental results on synthetic uci and real world datasets to demonstrate that our active sampling algorithm can provide accurate estimates of feature relevance with lower data acquisition costs than random sampling and other previously proposed sampling algorithms 
accelerated training of conditional random fields with stochastic gradient methods accelerated training of conditional random fields with stochastic gradient methods we apply stochastic metadescent smd a stochastic gradient optimization method with gain vector adaptation to the training of conditional random fields crfs on several large data sets the resulting optimizer converges to the same quality of solution over an order of magnitude faster than limitedmemory bfgs the leading method reported to date we report results for both exact and inexact inference techniques 
topic modeling topic modeling some models of textual corpora employ text generation methods involving ngram statistics while others use latent topic variables inferred using the bagofwords assumption in which word order is ignored previously these methods have not been combined in this work i explore a hierarchical generative probabilistic model that incorporates both ngram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchical dirichlet bigram language model the model hyperparameters are inferred using a gibbs em algorithm on two data sets each of documents the new model exhibits better predictive accuracy than either a hierarchical dirichlet bigram language model or a unigram topic model additionally the inferred topics are less dominated by function words than are topics discovered using unigram statistics potentially making them more meaningful 
label propagation through linear neighborhoods label propagation through linear neighborhoods a novel semisupervised learning approach is proposed based on a linear neighborhood model which assumes that each data point can be linearly reconstructed from its neighborhood our algorithm named linear neighborhood propagation lnp can propagate the labels from the labeled points to the whole dataset using these linear neighborhoods with sufficient smoothness we also derive an easy way to extend lnp to outofsample data promising experimental results are presented for synthetic data digit and text classification tasks 
twodimensional solution path for support vector regression twodimensional solution path for support vector regression recently a very appealing approach was proposed to compute the entire solution path for support vector classification svc with very low extra computational cost this approach was later extended to a support vector regression svr model called epsilonsvr however the method requires that the error parameter epsilon be set a priori which is only possible if the desired accuracy of the approximation can be specified in advance in this paper we show that the solution path for epsilonsvr is also piecewise linear with respect to epsilon we further propose an efficient algorithm for exploring the twodimensional solution space defined by the regularization and error parameters as opposed to the algorithm for svc our proposed algorithm for epsilonsvr initializes the number of support vectors to zero and then increases it gradually as the algorithm proceeds as such a good regression function possessing the sparseness property can be obtained after only a few iterations 
totally corrective boosting algorithms that maximize the margin totally corrective boosting algorithms that maximize the margin we consider boosting algorithms that maintain a distribution over a set of examples at each iteration a weak hypothesis is received and the distribution is updated we motivate these updates as minimizing the relative entropy subject to linear constraints for example adaboost constrains the edge of the last hypothesis wrt the updated distribution to be at most gamma in some sense adaboost is corrective wrt the last hypothesis a cleaner boosting method is to be totally corrective the edges of all past hypotheses are constrained to be at most gamma where gamma is suitably adaptedusing new techniques we prove the same iteration bounds for the totally corrective algorithms as for their corrective versions moreover with adaptive gamma the algorithms provably maximizes the margin experimentally the totally corrective versions return smaller convex combinations of weak hypotheses than the corrective ones and are competitive with lpboost a totally corrective boosting algorithm with no regularization for which there is no iteration bound known 
inference with the universum inference with the universum in this paper we study a new framework introduced by vapnik and vapnik that is an alternative capacity concept to the large margin approach in the particular case of binary classification we are given a set of labeled examples and a collection of nonexamples that do not belong to either class of interest this collection called the universum allows one to encode prior knowledge by representing meaningful concepts in the same domain as the problem at hand we describe an algorithm to leverage the universum by maximizing the number of observed contradictions and show experimentally that this approach delivers accuracy improvements over using labeled data alone 
kernel predictive linear gaussian models for nonlinear stochastic dynamical systems kernel predictive linear gaussian models for nonlinear stochastic dynamical systems the recent predictive linear gaussian model or plg improves upon traditional linear dynamical system models by using a predictive representation of state which makes consistent parameter estimation possible without any loss of modeling power and while using fewer parameters in this paper we extend the plg to model stochastic nonlinear dynamical systems by using kernel methods with a gaussian kernel the model admits closed form solutions to the state update equations due to conjugacy between the dynamics and the state representation we also explore an efficient sigmapoint approximation to the state updates and show how all of the model parameters can be learned directly from data and can be learned online with the kernel recursive leastsquares algorithm we empirically compare the model and its approximation to the original plg and discuss their relative advantages 
predictive state representations with options predictive state representations with options recent work on predictive state representation psr models has focused on using predictions of the outcomes of openloop action sequences as state these predictions answer questions of the form what is the probability of seeing observation sequence o o on if the agent takes action sequence a a an from some given history we would like to ask more expressive questions in our representation of state such as if i behave according to some policy until i terminate what will be my last observation we extend the linear psr framework to answer questions like these about options temporally extended closedloop courses of action bounding the size of the linear psr needed to model questions about a certain class of options we introduce a hierarchical psr hpsr that can make predictions about both options and primitive action sequences and show empirical results from learning hpsrs in simple domains 
fast time series classification using numerosity reduction fast time series classification using numerosity reduction many algorithms have been proposed for the problem of time series classification however it is clear that onenearestneighbor with dynamic time warping dtw distance is exceptionally difficult to beat this approach has one weakness however it is computationally too demanding for many realtime applications one way to mitigate this problem is to speed up the dtw calculations nonetheless there is a limit to how much this can help in this work we propose an additional technique numerosity reduction to speed up onenearestneighbor dtw while the idea of numerosity reduction for nearestneighbor classifiers has a long history we show here that we can leverage off an original observation about the relationship between dataset size and dtw constraints to produce an extremely compact dataset with little or no loss in accuracy we test our ideas with a comprehensive set of experiments and show that it can efficiently produce extremely fast accurate classifiers 
a duality view of spectral methods for dimensionality reduction a duality view of spectral methods for dimensionality reduction we present a unified duality view of several recently emerged spectral methods for nonlinear dimensionality reduction including isomap locally linear embedding laplacian eigenmaps and maximum variance unfolding we discuss the duality theory for the maximum variance unfolding problem and show that other methods are directly related to either its primal formulation or its dual formulation or can be interpreted from the optimality conditions this duality framework reveals close connections between these seemingly quite different algorithms in particular it resolves the myth about these methods in using either the top eigenvectors of a dense matrix or the bottom eigenvectors of a sparse matrix these two eigenspaces are exactly aligned at primaldual optimality 
bayesian multipopulation haplotype inference via a hierarchical dirichlet process mixture bayesian multipopulation haplotype inference via a hierarchical dirichlet process mixture uncovering the haplotypes of single nucleotide polymorphisms and their population demography is essential for many biological and medical applications methods for haplotype inference developed thus farincluding methods based on coalescence finite and infinite mixtures and maximal parsimonyignore the underlying population structure in the genotype data as noted by pritchard different populations can share certain portion of their genetic ancestors as well as have their own genetic components through migration and diversification in this paper we address the problem of multipopulation haplotype inference we capture crosspopulation structure using a nonparametric bayesian prior known as the hierarchical dirichlet process hdp teh et al conjoining this prior with a recently developed bayesian methodology for haplotype phasing known as dphaplotyper xing et al we also develop an efficient sampling algorithm for the hdp based on a twolevel nested poacutelya urn scheme we show that our model outperforms extant algorithms on both simulated and real biological data 
discriminative unsupervised learning of structured predictors discriminative unsupervised learning of structured predictors we present a new unsupervised algorithm for training structured predictors that is discriminative convex and avoids the use of em the idea is to formulate an unsupervised version of structured learning methods such as maximum margin markov networks that can be trained via semidefinite programming the result is a discriminative training criterion for structured predictors like hidden markov models that remains unsupervised and does not create local minima to reduce training cost we reformulate the training procedure to mitigate the dependence on semidefinite programming and finally propose a heuristic procedure that avoids semidefinite programming entirely experimental results show that the convex discriminative procedure can produce better conditional models than conventional baumwelch em training 
semisupervised nonlinear dimensionality reduction semisupervised nonlinear dimensionality reduction the problem of nonlinear dimensionality reduction is considered we focus on problems where prior information is available namely semisupervised dimensionality reduction it is shown that basic nonlinear dimensionality reduction algorithms such as locally linear embedding lle isometric feature mapping isomap and local tangent space alignment ltsa can be modified by taking into account prior information on exact mapping of certain data points the sensitivity analysis of our algorithms shows that prior information will improve stability of the solution we also give some insight on what kind of prior information best improves the solution we demonstrate the usefulness of our algorithm by synthetic and real life examples 
null space versus orthogonal linear discriminant analysis null space versus orthogonal linear discriminant analysis dimensionality reduction is an important preprocessing step for many applications linear discriminant analysis lda is one of the well known methods for supervised dimensionality reduction however the classical lda formulation requires the nonsingularity of scatter matrices involved for undersampled problems where the data dimension is much larger than the sample size all scatter matrices are singular and classical lda fails many extensions including null space based lda nlda orthogonal lda olda etc have been proposed in the past to overcome this problem in this paper we present a computational and theoretical analysis of nlda and olda our main result shows that under a mild condition which holds in many applications involving highdimensional data nlda is equivalent to olda we have performed extensive experiments on various types of data and results are consistent with our theoretical analysis the presented analysis and experimental results provide further insight into several lda based algorithms 
active learning via transductive experimental design active learning via transductive experimental design this paper considers the problem of selecting the most informative experiments x to get measurements y for learning a regression model y fx we propose a novel and simple concept for active learning transductive experimental design that explores available unmeasured experiments ie unlabeled data and has a better scalability in comparison with classic experimental design methods our indepth analysis shows that the new method tends to favor experiments that are on the one side hardtopredict and on the other side representative for the rest of the experiments efficient optimization of the new design problem is achieved through alternating optimization and sequential greedy search extensive experimental results on synthetic problems and three realworld tasks including questionnaire design for preference learning active learning for text categorization and spatial sensor placement highlight the advantages of the proposed approaches 
collaborative ordinal regression collaborative ordinal regression ordinal regression has become an effective way of learning user preferences but most research focuses on single regression problems in this paper we introduce collaborative ordinal regression where multiple ordinal regression tasks are handled simultaneously rather than modeling each task individually we explore the dependency between ranking functions through a hierarchical bayesian model and assign a common gaussian process gp prior to all individual functions empirical studies show that our collaborative model outperforms the individual counterpart in preference learning applications 
blockquantized kernel matrix for fast spectral embedding blockquantized kernel matrix for fast spectral embedding eigendecomposition of kernel matrix is an indispensable procedure in many learning and vision tasks however the cubic complexity on is impractical for large problem where n is the data size in this paper we propose an efficient approach to solve the eigendecomposition of the kernel matrix w the idea is to approximate w with w that is composed of m constant blocks the eigenvectors of w which can be solved in om time is then used to recover the eigenvectors of the original kernel matrix the complexity of our method is only omn m which scales more favorably than stateoftheart low rank approximation and sampling based approaches omn m and the approximation quality can be controlled conveniently our method demonstrates encouraging scaling behaviors in experiments of image segmentation by spectral clustering and kernel principal component analysis 
statistical debugging statistical debugging we describe a statistical approach to software debugging in the presence of multiple bugs due to sparse sampling issues and complex interaction between program predicates many generic offtheshelf algorithms fail to select useful bug predictors taking inspiration from biclustering algorithms we propose an iterative collective voting scheme for the program runs and predicates we demonstrate successful debugging results on several real world programs and a large debugging benchmark suite 
efficient lazy elimination for averaged onedependence estimators efficient lazy elimination for averaged onedependence estimators seminaive bayesian classifiers seek to retain the numerous strengths of naive bayes while reducing error by relaxing the attribute independence assumption backwards sequential elimination bse is a wrapper technique for attribute elimination that has proved effective at this task we explore a new technique lazy elimination le which eliminates highly related attributevalues at classification time without the computational overheads inherent in wrapper techniques we analyze the effect of le and bse on a stateoftheart seminaive bayesian algorithm averaged onedependence estimators aode our experiments show that le significantly reduces bias and error without undue computation while bse significantly reduces bias but not error with high training time complexity in the context of aode le has a significant advantage over bse in both computational efficiency and error 
quantum clustering algorithms quantum clustering algorithms by the term quantization we refer to the process of using quantum mechanics in order to improve a classical algorithm usually by making it go faster in this paper we initiate the idea of quantizing clustering algorithms by using variations on a celebrated quantum algorithm due to grover after having introduced this novel approach to unsupervised learning we illustrate it with a quantized version of three standard algorithms divisive clustering kmedians and an algorithm for the construction of a neighbourhood graph we obtain a significant speedup compared to the classical approach
learning random walks to rank nodes in graphs learning random walks to rank nodes in graphs ranking nodes in graphs is of much recent interest edges via the graph laplacian are used to encourage local smoothness of node scores in svmlike formulations with generalization guarantees in contrast pagerank variants are based on markovian random walks for directed graphs there is no simple known correspondence between these views of scoringranking recent scalable algorithms for learning the pagerank transition probabilities do not have generalization guarantees in this paper we show some correspondence results between the laplacian and the pagerank approaches and give new generalization guarantees for the latter we enhance the pageranklearning approaches to use an additive margin we also propose a general framework for ranksensitive scorelearning and apply it to laplacian smoothing experimental results are promising
uncovering shared structures in multiclass classification uncovering shared structures in multiclass classification this paper suggests a method for multiclass learning with many classes by simultaneously learning shared characteristics common to the classes and predictors for the classes in terms of these characteristics we cast this as a convex optimization problem using tracenorm regularization and study gradientbased optimization both for the linear case and the kernelized setting
twoview feature generation model for semisupervised learning twoview feature generation model for semisupervised learning we consider a setting for discriminative semisupervised learning where unlabeled data are used with a generative model to learn effective feature representations for discriminative training within this framework we revisit the twoview feature generation model of cotraining and prove that the optimum predictor can be expressed as a linear combination of a few features constructed from unlabeled data from this analysis we derive methods that employ two views but are very different from cotraining experiments show that our approach is more robust than cotraining and em under various data generation conditions
scalable training of lregularized loglinear models scalable training of lregularized loglinear models the lbfgs limitedmemory quasinewton method is the algorithm of choice for optimizing the parameters of largescale loglinear models with l regularization but it cannot be used for an lregularized loss due to its nondifferentiability whenever some parameter is zero efficient algorithms have been proposed for this task but they are impractical when the number of parameters is very large we present an algorithm orthantwise limitedmemory quasinewton owlqn based on lbfgs that can efficiently optimize the lregularized loglikelihood of loglinear models with millions of parameters in our experiments on a parse reranking task our algorithm was several orders of magnitude faster than an alternative algorithm and substantially faster than lbfgs on the analogous lregularized problem we also present a proof that owlqn is guaranteed to converge to a globally optimal parameter vector
multiclass core vector machine multiclass core vector machine even though several techniques have been proposed in the literature for achieving multiclass classification using support vector machinesvm the scalability aspect of these approaches to handle large data sets still needs much of exploration core vector machinecvm is a technique for scaling up a two class svm to handle large data sets in this paper we propose a multiclass core vector machinemcvm here we formulate the multiclass svm problem as a quadratic programmingqp problem defining an svm with vector valued output this qp problem is then solved using the cvm technique to achieve scalability to handle large data sets experiments done with several large synthetic and real world data sets show that the proposed mcvm technique gives good generalization performance as that of svm at a much lesser computational expense further it is observed that mcvm scales well with the size of the data set
the rendezvous algorithm the rendezvous algorithm we consider the problem of multiclass classification where both labeled and unlabeled data points are given we introduce and demonstrate a new approach for estimating a distribution over the missing labels where data points are viewed as nodes of a graph and pairwise similarities are used to derive a transition probability matrix p for a markov random walk between them the algorithm associates each point with a particle which moves between points according to p labeled points are set to be absorbing states of the markov random walk and the probability of each particle to be absorbed by the different labeled points as the number of steps increases is then used to derive a distribution over the associated missing label a computationally efficient algorithm to implement this is derived and demonstrated on both real and artificial data sets including a numerical comparison with other methods
focused crawling with scalable ordinal regression solvers focused crawling with scalable ordinal regression solvers in this paper we propose a novel scalable clustering based ordinal regression formulation which is an instance of a second order cone program socp with one second order cone soc constraint the main contribution of the paper is a fast algorithm cbor which solves the proposed formulation more eficiently than general purpose solvers another main contribution of the paper is to pose the problem of focused crawling as a large scale ordinal regression problem and solve using the proposed cbor focused crawling is an efficient mechanism for discovering resources of interest on the web posing the problem of focused crawling as an ordinal regression problem avoids the need for a negative class and topic hierarchy which are the main drawbacks of the existing focused crawling methods experiments on large synthetic and benchmark datasets show the scalability of cbor experiments also show that the proposed focused crawler outperforms the stateoftheart
learning distance function by coding similarity learning distance function by coding similarity we consider the problem of learning a similarity function from a set of positive equivalence constraints ie similar point pairs we define the similarity in information theoretic terms as the gain in coding length when shifting from independent encoding of the pair to joint encoding under simple gaussian assumptions this formulation leads to a nonmahalanobis similarity function which is efficient and simple to learn this function can be viewed as a likelihood ratio test and we show that the optimal similaritypreserving projection of the data is a variant of fisher linear discriminant we also show that under some naturally occurring sampling conditions of equivalence constraints this function converges to a known mahalanobis distance rca the suggested similarity function exhibits superior performance over alternative mahalanobis distances learnt from the same data its superiority is demonstrated in the context of image retrieval and graph based clustering using a large number of data sets
structural alignment based kernels for protein structure classification structural alignment based kernels for protein structure classification structural alignments are the most widely used tools for comparing proteins with low sequence similarity the main contribution of this paper is to derive various kernels on proteins from structural alignments which do not use sequence information central to the kernels is a novel alignment algorithm which matches substructures of fixed size using spectral graph matching techniques we derive positive semidefinite kernels which capture the notion of similarity between substructures using these as base more sophisticated kernels on protein structures are proposed to empirically evaluate the kernels we used a sequence nonredundant structures from different scop superfamilies the kernels when used with svms show competitive performance with ce a state of the art structure comparison program
discriminative learning for differing training and test distributions discriminative learning for differing training and test distributions we address classification problems for which the training instances are governed by a distribution that is allowed to differ arbitrarily from the test distributionproblems also referred to as classification under covariate shift we derive a solution that is purely discriminative neither training nor test distribution are modeled explicitly we formulate the general problem of learning under covariate shift as an integrated optimization problem we derive a kernel logistic regression classifier for differing training and test distributions
solving multiclass support vector machines with larank solving multiclass support vector machines with larank optimization algorithms for large margin multiclass recognizers are often too costly to handle ambitious problems with structured outputs and exponential numbers of classes optimization algorithms that rely on the full gradient are not effective because unlike the solution the gradient is not sparse and is very large the larank algorithm sidesteps this difficulty by relying on a randomized exploration inspired by the perceptron algorithm we show that this approach is competitive with gradient based optimizers on simple multiclass problems furthermore a single larank pass over the training examples delivers test error rates that are nearly as good as those of the final solution
efficiently computing minimax expectedsize confidence regions efficiently computing minimax expectedsize confidence regions given observed data and a collection of parameterized candidate models a alpha confidence region in parameter space provides useful insight as to those models which are a good fit to the data all while keeping the probability of incorrect exclusion below alpha with complex models optimally precise procedures those with small expected size are in practice difficult to derive one solution is the minimax expectedsize mes confidence procedure the key computational problem of mes is computing a minimax equilibria to a certain zerosum game we show that this game is convex with bilinear payoffs allowing us to apply any convex game solver including linear programming exploiting the sparsity of the matrix along with using fast linear programming software allows us to compute approximate minimax expectedsize confidence regions orders of magnitude faster than previously published methods we test these approaches by estimating parameters for a cosmological model
multiple instance learning for sparse positive bags multiple instance learning for sparse positive bags we present a new approach to multiple instance learning mil that is particularly effective when the positive bags are sparse ie contain few positive instances unlike other svmbased mil methods our approach more directly enforces the desired constraint that at least one of the instances in a positive bag is positive using both artificial and realworld data we experimentally demonstrate that our approach achieves greater accuracy than stateoftheart mil methods when positive bags are sparse and performs competitively when they are not in particular our approach is the best performing method for image region classification
cluster analysis of heterogeneous rank data cluster analysis of heterogeneous rank data cluster analysis of ranking data which occurs in consumer questionnaires voting forms or other inquiries of preferences attempts to identify typical groups of rank choices empirically measured rankings are often incomplete ie different numbers of filled rank positions cause heterogeneity in the data we propose a mixture approach for clustering of heterogeneous rank data rankings of different lengths can be described and compared by means of a single probabilistic model a maximum entropy approach avoids hidden assumptions about missing rank positions parameter estimators and an efficient em algorithm for unsupervised inference are derived for the ranking mixture model experiments on both synthetic data and realworld data demonstrate significantly improved parameter estimates on heterogeneous data when the incomplete rankings are included in the inference process
feature selection in a kernel space feature selection in a kernel space we address the problem of feature selection in a kernel space to select the most discriminative and informative features for classification and data analysis this is a difficult problem because the dimension of a kernel space may be infinite in the past little work has been done on feature selection in a kernel space to solve this problem we derive a basis set in the kernel space as a first step for feature selection using the basis set we then extend the marginbased feature selection algorithms that are proven effective even when many features are dependent the selected features form a subspace of the kernel space in which different stateoftheart classification algorithms can be applied for classification we conduct extensive experiments over real and simulated data to compare our proposed method with four baseline algorithms both theoretical analysis and experimental results validate the effectiveness of our proposed method
learning to rank learning to rank the paper is concerned with learning to rank which is to construct a model or a function for ranking objects learning to rank is useful for document retrieval collaborative filtering and many other applications several methods for learning to rank have been proposed which take object pairs as instances in learning we refer to them as the pairwise approach in this paper although the pairwise approach offers advantages it ignores the fact that ranking is a prediction task on list of objects the paper postulates that learning to rank should adopt the listwise approach in which lists of objects are used as instances in learning the paper proposes a new probabilistic method for the approach specifically it introduces two probability models respectively referred to as permutation probability and top k probability to define a listwise loss function for learning neural network and gradient descent are then employed as model and algorithm in the learning method experimental results on information retrieval show that the proposed listwise approach performs better than the pairwise approach
local similarity discriminant analysis local similarity discriminant analysis we propose a local generative model for similaritybased classification the method is applicable to the case that only pairwise similarities between samples are available the classifier models the local classconditional distribution using a maximum entropy estimate and empirical moment constraints the resulting exponential class conditionaldistributions are combined with class prior probabilities and misclassification costs to form the local similarity discriminant analysis local sda classifier we compare the performance of local sda to a nonlocal version to the local nearest centroid classifier the nearest centroid classifier knn and to the recentlydeveloped potential support vector machine psvm results show that local sda is competitive with knn and the computationallydemanding psvm while offering the advantages of a generative classifier
direct convex relaxations of sparse svm direct convex relaxations of sparse svm although support vector machines svms for binary classification give rise to a decision rule that only relies on a subset of the training data points support vectors it will in general be based on all available features in the input space we propose two direct novel convex relaxations of a nonconvex sparse svm formulation that explicitly constrains the cardinality of the vector of feature weights one relaxation results in a quadraticallyconstrained quadratic program qcqp while the second is based on a semidefinite programming sdp relaxation the qcqp formulation can be interpreted as applying an adaptive softthreshold on the svm hyperplane while the sdp formulation learns a weighted innerproduct ie a kernel that results in a sparse hyperplane experimental results show an increase in sparsity while conserving the generalization performance compared to a standard as well as a linear programming svm
minimum reference set based feature selection for small sample classifications minimum reference set based feature selection for small sample classifications we address feature selection problems for classification of small samples and high dimensionality a practical example is microarraybased cancer classification problems where sample size is typically less than and number of features is several thousands or higher one of the commonly used methods in addressing this problem is recursive feature elimination rfe method which utilizes the generalization capability embedded in support vector machines and is thus suitable for small samples problems we propose a novel method using minimum reference set mrs generated by the nearest neighbor rule mrs is the set of minimum number of samples that correctly classify all the training samples it is related to structural risk minimization principle and thus leads to good generalization the proposed mrs based method is compared to rfe method with several real datasets and experimental results show that the mrs method produces better classification performance
learning to compress images and videos learning to compress images and videos we present an intuitive scheme for lossy colorimage compression use the color information from a few representative pixels to learn a model which predicts color on the rest of the pixels now storing the representative pixels and the image in grayscale suffice to recover the original image a similar scheme is also applicable for compressing videos where a single model can be used to predict color on many consecutive frames leading to better compression existing algorithms for colorization the process of adding color to a grayscale image or video sequence are tedious and require intensive humanintervention we bypass these limitations by using a graphbased inductive semisupervised learning module for colorization and a simple active learning strategy to choose the representative pixels experiments on a wide variety of images and video sequences demonstrate the efficacy of our algorithm
magnitudepreserving ranking algorithms magnitudepreserving ranking algorithms this paper studies the learning problem of ranking when one wishes not just to accurately predict pairwise ordering but also preserve the magnitude of the preferences or the difference between ratings a problem motivated by its key importance in the design of search engines movie recommendation and other similar ranking systems we describe and analyze several algorithms for this problem and give stability bounds for their generalization error extending previously known stability results to nonbipartite ranking and magnitude of preferencepreserving algorithms we also report the results of experiments comparing these algorithms on several datasets and compare these results with those obtained using an algorithm minimizing the pairwise misranking error and standard regression
full regularization path for sparse principal component analysis full regularization path for sparse principal component analysis given a sample covariance matrix we examine the problem of maximizing the variance explained by a particular linear combination of the input variables while constraining the number of nonzero coefficients in this combination this is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering we formulate a new semidefinite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all numbers of non zero coefficients with complexity on where n is the number of variables we then use the same relaxation to derive sufficient conditions for global optimality of a solution which can be tested in on we show on toy examples and biological data that our algorithm does provide globally optimal solutions in many cases
kernel selection forl semisupervised kernel machines kernel selection forl semisupervised kernel machines existing semisupervised learning methods are mostly based on either the cluster assumption or the manifold assumption in this paper we propose an integrated regularization framework for semisupervised kernel machines by incorporating both the cluster assumption and the manifold assumption moreover it supports kernel learning in the form of kernel selection the optimization problem involves joint optimization over all the labeled and unlabeled data points a convex set of basic kernels and a discrete space of unknown labels for the unlabeled data when the manifold assumption is incorporated graph laplacian kernels are used as the basic kernels for learning an optimal convex combination of graph laplacian kernels comparison with related methods on the usps data set shows very promising results
boosting for transfer learning boosting for transfer learning traditional machine learning makes a basic assumption the training and test data should be under the same distribution however in many cases this identicaldistribution assumption does not hold the assumption might be violated when a task from one new domain comes while there are only labeled data from a similar old domain labeling the new data can be costly and it would also be a waste to throw away all the old data in this paper we present a novel transfer learning framework called tradaboost which extends boostingbased learning algorithms freund schapire tradaboost allows users to utilize a small amount of newly labeled data to leverage the old data to construct a highquality classification model for the new data we show that this method can allow us to learn an accurate model using only a tiny amount of new data and a large amount of old data even when the new data are not sufficient to train a model alone we show that tradaboost allows knowledge to be effectively transferred from the old data to the new the effectiveness of our algorithm is analyzed theoretically and empirically to show that our iterative algorithm can converge well to an accurate model
intractability and clustering with constraints intractability and clustering with constraints clustering with constraints is a developing area of machine learning various papers have used constraints to enforce particular clusterings seed clustering algorithms and even learn distance functions which are then used for clustering we present intractability results for some constraint combinations and illustrate both formally and experimentally the implications of these results for using constraints with clustering
informationtheoretic metric learning informationtheoretic metric learning in this paper we present an informationtheoretic approach to learning a mahalanobis distance function we formulate the problem as that of minimizing the differential relative entropy between two multivariate gaussians under constraints on the distance function we express this problem as a particular bregman optimization problemthat of minimizing the logdet divergence subject to linear constraints our resulting algorithm has several advantages over existing methods first our method can handle a wide variety of constraints and can optionally incorporate a prior on the distance function second it is fast and scalable unlike most existing methods no eigenvalue computations or semidefinite programming are required we also present an online version and derive regret bounds for the resulting algorithm finally we evaluate our method on a recent error reporting system for software called clarify in the context of metric learning for nearest neighbor classification as well as on standard data sets
an integrated approach to feature invention and model construction for drug activity prediction an integrated approach to feature invention and model construction for drug activity prediction we present a new machine learning approach for dqsar the task of predicting binding affinities of molecules to target proteins based on d structure our approach predicts binding affinity by using regression on substructures discovered by relational learning we make two contributions to the stateoftheart first we use multipleinstance mi regression which represents a molecule as a set of d conformations to model activity second the relational learning component employs the score as you use sayu method to select substructures for their ability to improve the regression model this is the first application of sayu to multipleinstance realvalued prediction we evaluate our approach on three tasks and demonstrate that i sayu outperforms standard coverage measures when selecting features for regression ii the mi representation improves accuracy over standard single featurevector encodings and iii combining sayu with mi regression is more accurate for dqsar than either approach by itself
percentile optimization in uncertain markov decision processes with application to efficient exploration percentile optimization in uncertain markov decision processes with application to efficient exploration markov decision processes are an effective tool in modeling decisionmaking in uncertain dynamic environments since the parameters of these models are typically estimated from data learned from experience or designed by hand it is not surprising that the actual performance of a chosen strategy often significantly differs from the designers initial expectations due to unavoidable model uncertainty in this paper we present a percentile criterion that captures the tradeoff between optimistic and pessimistic points of view on mdp with parameter uncertainty we describe tractable methods that take parameter uncertainty into account in the process of decision making finally we propose a costeffective exploration strategy when it is possible to invest money time or computation efforts in actions that will reduce the uncertainty in the parameters
unsupervised prediction of citation influences unsupervised prediction of citation influences publication repositories contain an abundance of information about the evolution of scientific research areas we address the problem of creating a visualization of a research area that describes the flow of topics between papers quantifies the impact that papers have on each other and helps to identify key contributions to this end we devise a probabilistic topic model that explains the generation of documents the model incorporates the aspects of topical innovation and topical inheritance via citations we evaluate the models ability to predict the strength of influence of citations against manually rated citations
nonisometric manifold learning nonisometric manifold learning in this work we take a novel view of nonlinear manifold learning usually manifold learning is formulated in terms of finding an embedding or unrolling of a manifold into a lower dimensional space instead we treat it as the problem of learning a representation of a nonlinear possibly nonisometric manifold that allows for the manipulation of novel points central to this view of manifold learning is the concept of generalization beyond the training data drawing on concepts from supervised learning we establish a framework for studying the problems of model assessment model complexity and model selection for manifold learning we present an extension of a recent algorithm locally smooth manifold learning lsml and show it has good generalization properties lsml learns a representation of a manifold or family of related manifolds and can be used for computing geodesic distances finding the projection of a point onto a manifold recovering a manifold from points corrupted by noise generating novel points on a manifold and more
hierarchical maximum entropy density estimation hierarchical maximum entropy density estimation we study the problem of simultaneously estimating several densities where the datasets are organized into overlapping groups such as a hierarchy for this problem we propose a maximum entropy formulation which systematically incorporates the groups and allows us to share the strength of prediction across similar datasets we derive general performance guarantees and show how some previous approaches such as hierarchical shrinkage and hierarchical priors can be derived as special cases we demonstrate the proposed technique on synthetic data and in a realworld application to modeling the geographic distributions of species hierarchically grouped in a taxonomy specifically we model the geographic distributions of species in the australian wet tropics and northeast new south wales in these regions small numbers of samples per species significantly hinder effective prediction substantial benefits are obtained by combining information across taxonomic groups
carpediem carpediem in this paper we present a novel algorithm carpediem it significantly improves on the time complexity of viterbi algorithm preserving the optimality of the result this fact has consequences on machine learning systems that use viterbi algorithm during learning or classification we show how the algorithm applies to the supervised sequential learning task and in particular to the hmperceptron algorithm we illustrate carpediem in full details and provide experimental results that support the proposed approach
manifoldadaptive dimension estimation manifoldadaptive dimension estimation intuitively learning should be easier when the data points lie on a lowdimensional submanifold of the input space recently there has been a growing interest in algorithms that aim to exploit such geometrical properties of the data oftentimes these algorithms require estimating the dimension of the manifold first in this paper we propose an algorithm for dimension estimation and study its finitesample behaviour the algorithm estimates the dimension locally around the data points using nearest neighbor techniques and then combines these local estimates we show that the rate of convergence of the resulting estimate is independent of the dimension of the input space and hence the algorithm is manifoldadaptive thus when the manifold supporting the data is low dimensional the algorithm can be exponentially more efficient than its counterparts that are not exploiting this property our computer experiments confirm the obtained theoretical results
combining online and offline knowledge in uct combining online and offline knowledge in uct the uct algorithm learns a value function online using samplebased search the tdlambda algorithm can learn a value function offline for the onpolicy distribution we consider three approaches for combining offline and online value functions in the uct algorithm first the offline value function is used as a default policy during montecarlo simulation second the uct value function is combined with a rapid online estimate of action values third the offline value function is used as prior knowledge in the uct search tree we evaluate these algorithms in x go against gnugo the first algorithm performs better than uct with a random simulation policy but surprisingly worse than uct with a weaker handcrafted simulation policy the second algorithm outperforms uct altogether the third algorithm outperforms uct with handcrafted prior knowledge we combine these algorithms in mogo the worlds strongest x go program each technique significantly improves mogos playing strength
robust nonlinear dimensionality reduction using successive dimensional laplacian eigenmaps robust nonlinear dimensionality reduction using successive dimensional laplacian eigenmaps nonlinear dimensionality reduction of noisy data is a challenging problem encountered in a variety of data analysis applications recent results in the literature show that spectral decomposition as used for example by the laplacian eigenmaps algorithm provides a powerful tool for nonlinear dimensionality reduction and manifold learning in this paper we discuss a significant shortcoming of these approaches which we refer to as the repeated eigendirections problem we propose a novel approach that combines successive dimensional spectral embeddings with a data advection scheme that allows us to address this problem the proposed method does not depend on a nonlinear optimization scheme hence it is not prone to local minima experiments with artificial and real data illustrate the advantages of the proposed method over existing approaches we also demonstrate that the approach is capable of correctly learning manifolds corrupted by significant amounts of noise
gradient boosting for kernelized output spaces gradient boosting for kernelized output spaces a general framework is proposed for gradient boosting in supervised learning problems where the loss function is defined using a kernel over the output space it extends boosting in a principled way to complex output spaces images text graphs etc and can be applied to a general class of base learners working in kernelized output spaces empirical results are provided on three problems a regression problem an image completion task and a graph prediction problem in these experiments the framework is combined with treebased base learners which have interesting algorithmic properties the results show that gradient boosting significantly improves these base learners and provides competitive results with other treebased ensemble methods based on randomization
bayesian actorcritic algorithms bayesian actorcritic algorithms we present a new actorcritic learning model in which a bayesian class of nonparametric critics using gaussian process temporal difference learning is used such critics model the stateaction value function as a gaussian process allowing bayes rule to be used in computing the posterior distribution over stateaction value functions conditioned on the observed data appropriate choices of the prior covariance kernel between stateaction values and of the parametrization of the policy allow us to obtain closedform expressions for the posterior distribution of the gradient of the average discounted return with respect to the policy parameters the posterior mean which serves as our estimate of the policy gradient is used to update the policy while the posterior covariance allows us to gauge the reliability of the update
exponentiated gradient algorithms for loglinear structured prediction exponentiated gradient algorithms for loglinear structured prediction conditional loglinear models are a commonly used method for structured prediction efficient learning of parameters in these models is therefore an important problem this paper describes an exponentiated gradient eg algorithm for training such models eg is applied to the convex dual of the maximum likelihood objective this results in both sequential and parallel update algorithms where in the sequential algorithm parameters are updated in an online fashion we provide a convergence proof for both algorithms our analysis also simplifies previous results on eg for maxmargin models and leads to a tighter bound on convergence rates experiments on a largescale parsing task show that the proposed algorithm converges much faster than conjugategradient and lbfgs approaches both in terms of optimization objective and test error
best of both best of both although each iteration of the popular kmeans clustering heuristic scales well to larger problem sizes it often requires an unacceptablyhigh number of iterations to converge to a solution this paper introduces an enhancement of kmeans in which local search is used to accelerate convergence without greatly increasing the average computational cost of the iterations the local search involves a carefullycontrolled number of swap operations resembling those of the more robust kmedoids clustering heuristic we show empirically that the proposed method improves convergence results when compared to standard kmeans
recovering temporally rewiring networks recovering temporally rewiring networks a plausible representation of relational information among entities in dynamic systems such as a living cell or a social community is a stochastic network which is topologically rewiring and semantically evolving over time while there is a rich literature on modeling static or temporally invariant networks much less has been done toward modeling the dynamic processes underlying rewiring networks and on recovering such networks when they are not observable we present a class of hidden temporal exponential random graph models htergms to study the yet unexplored topic of modeling and recovering temporally rewiring networks from time series of node attributes such as activities of social actors or expression levels of genes we show that one can reliably infer the latent timespecific topologies of the evolving networks from the observation we report empirical results on both synthetic data and a drosophila lifecycle gene expression data set in comparison with a static counterpart of htergm
efficient inference with cardinalitybased clique potentials efficient inference with cardinalitybased clique potentials many collective labeling tasks require inference on graphical models where the clique potentials depend only on the number of nodes that get a particular label we design efficient inference algorithms for various families of such potentials our algorithms are exact for arbitrary cardinalitybased clique potentials on binary labels and for maxlike and majoritylike clique potentials on multiple labels moving towards more complex potentials we show that inference becomes nphard even on cliques with homogeneous potts potentials we present a approximation algorithm with runtime subquadratic in the clique size in contrast the best known previous guarantee for graphs with potts potentials is only we perform empirical comparisons on real and synthetic data and show that our proposed methods are an order of magnitude faster than the wellknown treebased reparameterization trw and graphcut algorithms
sparse probabilistic classifiers sparse probabilistic classifiers the scores returned by support vector machines are often used as a confidence measures in the classification of new examples however there is no theoretical argument sustaining this practice thus when classification uncertainty has to be assessed it is safer to resort to classifiers estimating conditional probabilities of class labels here we focus on the ambiguity in the vicinity of the boundary decision we propose an adaptation of maximum likelihood estimation instantiated on logistic regression the model outputs proper conditional probabilities into a userdefined interval and is less precise elsewhere the model is also sparse in the sense that few examples contribute to the solution the computational efficiency is thus improved compared to logistic regression furthermore preliminary experiments show improvements over standard logistic regression and performances similar to support vector machines
supervised clustering of streaming data for email batch detection supervised clustering of streaming data for email batch detection we address the problem of detecting batches of emails that have been created according to the same template this problem is motivated by the desire to filter spam more effectively by exploiting collective information about entire batches of jointly generated messages the application matches the problem setting of supervised clustering because examples of correct clusterings can be collected known decoding procedures for supervised clustering are cubic in the number of instances when decisions cannot be reconsidered once they have been made owing to the streaming nature of the data then the decoding problem can be solved in linear time we devise a sequential decoding procedure and derive the corresponding optimization problem of supervised clustering we study the impact of collective attributes of email batches on the effectiveness of recognizing spam emails
a bound on the label complexity of agnostic active learning a bound on the label complexity of agnostic active learning we study the label complexity of poolbased active learning in the agnostic pac model specifically we derive general bounds on the number of label requests made by the a algorithm proposed by balcan beygelzimer langford balcan et al this represents the first nontrivial generalpurpose upper bound on label complexity in the agnostic pac model
learning nonparametric kernel matrices from pairwise constraints learning nonparametric kernel matrices from pairwise constraints many kernel learning methods have to assume parametric forms for the target kernel functions which significantly limits the capability of kernels in fitting diverse patterns some kernel learning methods assume the target kernel matrix to be a linear combination of parametric kernel matrices this assumption again importantly limits the flexibility of the target kernel matrices the key challenge with nonparametric kernel learning arises from the difficulty in linking the nonparametric kernels to the input patterns in this paper we resolve this problem by introducing the graph laplacian of the observed data as a regularizer when optimizing the kernel matrix with respect to the pairwise constraints we formulate the problem into semidefinite programs sdp and propose an efficient algorithm to solve the sdp problem the extensive evaluation on clustering with pairwise constraints shows that the proposed nonparametric kernel learning method is more effective than other stateoftheart kernel learning techniques
parameter learning for relational bayesian networks parameter learning for relational bayesian networks we present a method for parameter learning in relational bayesian networks rbns our approach consists of compiling the rbn model into a computation graph for the likelihood function and to use this likelihood graph to perform the necessary computations for a gradient ascent likelihood optimization procedure the method can be applied to all rbn models that only contain differentiable combining rules this includes models with nondecomposable combining rules as well as models with weighted combinations or nested occurrences of combining rules experimental results on artificial random graph data explores the feasibility of the approach both for complete and incomplete data
bayesian compressive sensing and projection optimization bayesian compressive sensing and projection optimization this paper introduces a new problem for which machinelearning tools may make an impact the problem considered is termed compressive sensing in which a real signal of dimension n is measured accurately based on k ltlt n real measurements this is achieved under the assumption that the underlying signal has a sparse representation in some basis eg wavelets in this paper we demonstrate how techniques developed in machine learning specifically sparse bayesian regression and active learning may be leveraged to this new problem we also point out future research directions in compressive sensing of interest to the machinelearning community
constructing basis functions from directed graphs for value function approximation constructing basis functions from directed graphs for value function approximation basis functions derived from an undirected graph connecting nearby samples from a markov decision process mdp have proven useful for approximating value functions the success of this technique is attributed to the smoothness of the basis functions with respect to the state space geometry this paper explores the properties of bases created from directed graphs which are a more natural fit for expressing state connectivity digraphs capture the effect of nonreversible mdps whose value functions may not be smooth across adjacent states we provide an analysis using the dirichlet sum of the directed graph laplacian to show how the smoothness of the basis functions is affected by the graphs invariant distribution experiments in discrete and continuous mdps with nonreversible actions demonstrate a significant improvement in the policies learned using directed graph bases
most likely heteroscedastic gaussian process regression most likely heteroscedastic gaussian process regression this paper presents a novel gaussian process gp approach to regression with inputdependent noise rates we follow goldberg et als approach and model the noise variance using a second gp in addition to the gp governing the noisefree output value in contrast to goldberg et al however we do not use a markov chain monte carlo method to approximate the posterior noise variance but a most likely noise approach the resulting model is easy to implement and can directly be used in combination with various existing extensions of the standard gps such as sparse approximations extensive experiments on both synthetic and realworld data including a challenging perception problem in robotics show the effectiveness of most likely heteroscedastic gp regression
neighbor search with global geometry neighbor search with global geometry neighbor search is a fundamental task in machine learning especially in classification and retrieval efficient nearest neighbor search methods have been widely studied with their emphasis on data structures but most of them did not consider the underlying global geometry of a data set recent graphbased semisupervised learning methods capture the global geometry but suffer from scalability and parameter tuning problems in this paper we present a nearest neighbor search method where the underlying global geometry is incorporated and the parameter tuning is not required to this end we introduce deterministic walks as a deterministic counterpart of markov random walks leading us to use the minimax distance as a global dissimilarity measure then we develop a message passing algorithm for efficient minimax distance calculation which scales linearly in both time and space empirical study reveals the useful behavior of the method in image retrieval and semisupervised learning
a recursive method for discriminative mixture learning a recursive method for discriminative mixture learning we consider the problem of learning density mixture models for classification traditional learning of mixtures for density estimation focuses on models that correctly represent the density at all points in the sample space discriminative learning on the other hand aims at representing the density at the decision boundary we introduce a novel discriminative learning method for mixtures of generative models unlike traditional discriminative learning methods that often resort to computationally demanding gradient search optimization the proposed method is highly efficient as it reduces to generative learning of individual mixture components on weighted data hence it is particularly suited to domains with complex component models such as hidden markov models or bayesian networks in general that are usually too complex for effective gradient search we demonstrate the benefits of the proposed method in a comprehensive set of evaluations on timeseries sequence classification problems
infinite mixtures of trees infinite mixtures of trees finite mixtures of treestructured distributions have been shown to be efficient and effective in modeling multivariate distributions using dirichlet processes we extend this approach to allow countably many treestructured mixture components the resulting bayesian framework allows us to deal with the problem of selecting the number of mixture components by computing the posterior distribution over the number of components and integrating out the components by bayesian model averaging we apply the proposed framework to identify the number and the properties of predominant precipitation patterns in historical archives of climate data
local dependent components local dependent components we introduce a mixture of probabilistic canonical correlation analyzers model for analyzing local correlations or more generally mutual statistical dependencies in cooccurring data pairs the model extends the traditional canonical correlation analysis and its probabilistic interpretation in three main ways first a full bayesian treatment enables analysis of small samples large p small n a crucial problem in bioinformatics for instance and rigorous estimation of the degree of dependency and independency secondly the mixture formulation generalizes the method from global linearity to the more reasonable assumption of different kinds of dependencies for different kinds of data as a third novel extension the method decomposes the variation in the data into shared and data setspecific components
statistical predicate invention statistical predicate invention we propose statistical predicate invention as a key problem for statistical relational learning spi is the problem of discovering new concepts properties and relations in structured data and generalizes hidden variable discovery in statistical models and predicate invention in ilp we propose an initial model for spi based on secondorder markov logic in which predicates as well as arguments can be variables and the domain of discourse is not fully known in advance our approach iteratively refines clusters of symbols based on the clusters of symbols they appear in atoms with eg it clusters relations by the clusters of the objects they relate since different clusterings are better for predicting different subsets of the atoms we allow multiple crosscutting clusterings we show that this approach outperforms markov logic structure learning and the recently introduced infinite relational model on a number of relational datasets
kernelizing pls degrees of freedom and efficient model selection kernelizing pls degrees of freedom and efficient model selection kernelizing partial least squares pls an algorithm which has been particularly popular in chemometrics leads to kernel pls which has several interesting properties including a subcubic runtime for learning and an iterative construction of directions which are relevant for predicting the outputs we show that the kernelization of pls introduces interesting properties not found in ordinary pls giving novel insights into the workings of kernel pls and the connections to kernel ridge regression and conjugate gradient descent methods furthermore we show how to correctly define the degrees of freedom for kernel pls and how to efficiently compute an unbiased estimate finally we address the practical problem of model selection we demonstrate how to use the degrees of freedom estimate to perform effective model selection and discuss how to implement crossvalidation schemes efficiently
nonmyopic active learning of gaussian processes nonmyopic active learning of gaussian processes when monitoring spatial phenomena such as the ecological condition of a river deciding where to make observations is a challenging task in these settings a fundamental question is when an active learning or sequential design strategy where locations are selected based on previous measurements will perform significantly better than sensing at an a priori specified set of locations for gaussian processes gps which often accurately model spatial phenomena we present an analysis and efficient algorithms that address this question central to our analysis is a theoretical bound which quantifies the performance difference between active and a priori design strategies we consider gps with unknown kernel parameters and present a nonmyopic approach for trading off exploration ie decreasing uncertainty about the model parameters and exploitation ie nearoptimally selecting observations when the parameters are approximately known we discuss several exploration strategies and present logarithmic sample complexity bounds for the exploration phase we then extend our algorithm to handle nonstationary gps exploiting local structure in the model we also present extensive empirical evaluation on several realworld problems
on one method of nondiagonal regularization in sparse bayesian learning on one method of nondiagonal regularization in sparse bayesian learning in the paper we propose a new type of regularization procedure for training sparse bayesian methods for classification transforming hessian matrix of loglikelihood function to diagonal form with further regularization of its eigenvectors allows us to optimize evidence explicitly as a product of onedimensional integrals the process of automatic regularization coefficients determination then converges in one iteration we show how to use the proposed approach for gaussian and laplace priors both algorithms show comparable performance with the stateoftheart relevance vector machines rvm but require less time for training and produce more sparse decision rules in terms of degrees of freedom
online kernel pca with entropic matrix updates online kernel pca with entropic matrix updates a number of updates for density matrices have been developed recently that are motivated by relative entropy minimization problems the updates involve a softmin calculation based on matrix logs and matrix exponentials we show that these updates can be kernelized this is important because the bounds provable for these algorithms are logarithmic in the feature dimension provided that the norm of feature vectors is bounded by a constant the main problem we focus on is the kernelization of an online pca algorithm which belongs to this family of updates
an empirical evaluation of deep architectures on problems with many factors of variation an empirical evaluation of deep architectures on problems with many factors of variation recently several learning algorithms relying on models with deep architectures have been proposed though they have demonstrated impressive performance to date they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment for which many machine learning algorithms already report reasonable results here we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation these models are compared with wellestablished algorithms such as support vector machines and single hiddenlayer feedforward neural networks
hierarchical gaussian process latent variable models hierarchical gaussian process latent variable models the gaussian process latent variable model gplvm is a powerful approach for probabilistic modelling of high dimensional data through dimensional reduction in this paper we extend the gplvm through hierarchies a hierarchical model such as a tree allows us to express conditional independencies in the data as well as the manifold structure we first introduce gaussian process hierarchies through a simple dynamical model we then extend the approach to a more complex hierarchy which is applied to the visualisation of human motion data sets
learning a metalevel prior for feature relevance from multiple related tasks learning a metalevel prior for feature relevance from multiple related tasks in many prediction tasks selecting relevant features is essential for achieving good generalization performance most feature selection algorithms consider all features to be a priori equally likely to be relevant in this paper we use transfer learninglearning on an ensemble of related tasksto construct an informative prior on feature relevance we assume that features themselves have metafeatures that are predictive of their relevance to the prediction task and model their relevance as a function of the metafeatures using hyperparameters called metapriors we present a convex optimization algorithm for simultaneously learning the metapriors and feature weights from an ensemble of related prediction tasks which share a similar relevance structure our approach transfers the metapriors among different tasks which makes it possible to deal with settings where tasks have nonoverlapping features or the relevance of the features vary over the tasks we show that learning feature relevance improves performance on two real data sets which illustrate such settings predicting ratings in a collaborative filtering task and distinguishing arguments of a verb in a sentence
scalable modeling of real graphs using kronecker multiplication scalable modeling of real graphs using kronecker multiplication given a large real graph how can we generate a synthetic graph that matches its properties ie it has similar degree distribution similar small diameter similar spectrum etc we propose to use kronecker graphs which naturally obey all of the above properties and we present kronfit a fast and scalable algorithm for fitting the kronecker graph generation model to real networks a naive approach to fitting would take superexponential time in contrast kronfit takes linear time by exploiting the structure of kronecker product and by using sampling experiments on large real and synthetic graphs show that kronfit indeed mimics very well the patterns found in the target graphs once fitted the model parameters and the resulting synthetic graphs can be used for anonymization extrapolations and graph summarization
support cluster machine support cluster machine for largescale classification problems the training samples can be clustered beforehand as a downsampling preprocess and then only the obtained clusters are used for training motivated by such assumption we proposed a classification algorithm support cluster machine scm within the learning framework introduced by vapnik for the scm a compatible kernel is adopted such that a similarity measure can be handled not only between clusters in the training phase but also between a cluster and a vector in the testing phase we also proved that the scm is a general extension of the svm with the rbf kernel the experimental results confirm that the scm is very effective for largescale classification problems due to significantly reduced computational costs for both training and testing and comparable classification accuracies as a byproduct it provides a promising approach to dealing with privacypreserving data mining problems
a transductive framework of distance metric learning by spectral dimensionality reduction a transductive framework of distance metric learning by spectral dimensionality reduction distance metric learning and nonlinear dimensionality reduction are two interesting and active topics in recent years however the connection between them is not thoroughly studied yet in this paper a transductive framework of distance metric learning is proposed and its close connection with many nonlinear spectral dimensionality reduction methods is elaborated furthermore we prove a representer theorem for our framework linking it with function estimation in an rkhs and making it possible for generalization to unseen test samples in our framework it suffices to solve a sparse eigenvalue problem thus datasets with samples can be handled finally experiment results on synthetic data several uci databases and the mnist handwritten digit database are shown
adaptive dimension reduction using discriminant analysis and kmeans clustering adaptive dimension reduction using discriminant analysis and kmeans clustering we combine linear discriminant analysis lda and kmeans clustering into a coherent framework to adaptively select the most discriminative subspace we use kmeans clustering to generate class labels and use lda to do subspace selection the clustering process is thus integrated with the subspace selection process and the data are then simultaneously clustered while the feature subspaces are selected we show the rich structure of the general ldakm framework by examining its variants and their relationships to earlier approaches relations among pca lda kmeans are clarified extensive experimental results on realworld datasets show the effectiveness of our approach
largescale rlsc learning without agony largescale rlsc learning without agony the advances in kernelbased learning necessitate the study on solving a largescale nonsparse positive definite linear system to provide a deterministic approach recent researches focus on designing fast matrixvector multiplication techniques coupled with a conjugate gradient method instead of using the conjugate gradient method our paper proposes to use a domain decomposition approach in solving such a linear system its convergence property and speed can be understood within von neumanns alternating projection framework we will report signi ficant and consistent improvements in convergence speed over the conjugate gradient method when the approach is applied to recent machine learning problems
a novel orthogonal nmfbased belief compression for pomdps a novel orthogonal nmfbased belief compression for pomdps high dimensionality of pomdps belief state space is one major cause that makes the underlying optimal policy computation intractable belief compression refers to the methodology that projects the belief state space to a lowdimensional one to alleviate the problem in this paper we propose a novel orthogonal nonnegative matrix factorization onmf for the projection the proposed onmf not only factors the belief state space by minimizing the reconstruction error but also allows the compressed pomdp formulation to be efficiently computed due to its orthogonality in a valuedirected manner so that the value function will take same values for corresponding belief states in the original and compressed state spaces we have tested the proposed approach using a number of benchmark problems and the empirical results confirms its effectiveness in achieving substantial computational cost saving in policy computation
a permutationaugmented sampler for dp mixture models a permutationaugmented sampler for dp mixture models we introduce a new inference algorithm for dirichlet process mixture models while gibbs sampling and variational methods focus on local moves the new algorithm makes more global moves this is done by introducing a permutation of the data points as an auxiliary variable the algorithm is a blocked sampler which alternates between sampling the clustering and sampling the permutation the key to the efficiency of this approach is that it is possible to use dynamic programming to consider all exponentially many clusterings consistent with a given permutation we also show that random projections can be used to effectively sample the permutation the result is a stochastic hillclimbing algorithm that yields burnin times significantly smaller than those of collapsed gibbs sampling
quadratically gated mixture of experts for incomplete data classification quadratically gated mixture of experts for incomplete data classification we introduce quadratically gated mixture of experts qgme a statistical model for multiclass nonlinear classification the qgme is formulated in the setting of incomplete data where the data values are partially observed we show that the missing values entail joint estimation of the data manifold and the classifier which allows adaptive imputation during classifier learning the expectation maximization em algorithm is derived for joint likelihood maximization with adaptive imputation performed analytically in the estep the performance of qgme is evaluated on three benchmark data sets and the results show that the qgme yields significant improvements over competing methods
trust region newton methods for largescale logistic regression trust region newton methods for largescale logistic regression largescale logistic regression arises in many applications such as document classification and natural language processing in this paper we apply a trust region newton method to maximize the loglikelihood of the logistic regression model the proposed method uses only approximate newton steps in the beginning but achieves fast convergence in the end experiments show that it is faster than the commonly used quasi newton approach for logistic regression we also compare it with linear svm implementations
relational clustering by symmetric convex coding relational clustering by symmetric convex coding relational data appear frequently in many machine learning applications relational data consist of the pairwise relations similarities or dissimilarities between each pair of implicit objects and are usually stored in relation matrices and typically no other knowledge is available although relational clustering can be formulated as graph partitioning in some applications this formulation is not adequate for general relational data in this paper we propose a general model for relational clustering based on symmetric convex coding the model is applicable to all types of relational data and unifies the existing graph partitioning formulation under this model we derive two alternative bound optimization algorithms to solve the symmetric convex coding under two popular distance functions euclidean distance and generalized idivergence experimental evaluation and theoretical analysis show the effectiveness and great potential of the proposed model and algorithms
discriminant analysis in correlation similarity measure space discriminant analysis in correlation similarity measure space correlation is one of the most widely used similarity measures in machine learning like euclidean and mahalanobis distances however compared with proposed numerous discriminant learning algorithms in distance metric space only a very little work has been conducted on this topic using correlation similarity measure in this paper we propose a novel discriminant learning algorithm in correlation measure space correlation discriminant analysis cda in this framework based on the definitions of withinclass correlation and betweenclass correlation the optimum transformation can be sought for to maximize the difference between them which is in accordance with good classification performance empirically under different cases of the transformation different implementations of the algorithm are given extensive empirical evaluations of cda demonstrate its advantage over alternative methods
adaptive mesh compression in d computer graphics using multiscale manifold learning adaptive mesh compression in d computer graphics using multiscale manifold learning this paper investigates compression of d objects in computer graphics using manifold learning spectral compression uses the eigenvectors of the graph laplacian of an objects topology to adaptively compress d objects d compression is a challenging application domain object models can have gt vertices and reliably computing the basis functions on large graphs is numerically challenging in this paper we introduce a novel multiscale manifold learning approach to d mesh compression using diffusion wavelets a general extension of wavelets to graphs with arbitrary topology unlike the global nature of laplacian bases diffusion wavelet bases are compact and multiscale in nature we decompose large graphs using a fast graph partitioning method and combine local multiscale wavelet bases computed on each subgraph we present results showing that multiscale diffusion wavelets bases are superior to the laplacian bases for adaptive compression of large d objects
simple robust scalable semisupervised learning via expectation regularization simple robust scalable semisupervised learning via expectation regularization although semisupervised learning has been an active area of research its use in deployed applications is still relatively rare because the methods are often difficult to implement fragile in tuning or lacking in scalability this paper presents expectation regularization a semisupervised learning method for exponential family parametric models that augments the traditional conditional labellikelihood objective function with an additional term that encourages model predictions on unlabeled data to match certain expectationssuch as label priors the method is extremely easy to implement scales as well as logistic regression and can handle nonindependent features we present experiments on five different data sets showing accuracy improvements over other semisupervised methods
automatic shaping and decomposition of reward functions automatic shaping and decomposition of reward functions this paper investigates the problem of automatically learning how to restructure the reward function of a markov decision process so as to speed up reinforcement learning we begin by describing a method that learns a shaped reward function given a set of state and temporal abstractions next we consider decomposition of the pertimestep reward in multieffector problems in which the overall agent can be decomposed into multiple units that are concurrently carrying out various tasks we show by example that to find a good reward decomposition it is often necessary to first shape the rewards appropriately we then give a function approximation algorithm for solving both problems together standard reinforcement learning algorithms can be augmented with our methods and we show experimentally that in each case significantly faster learning results
asymmetric boosting asymmetric boosting a costsensitive extension of boosting denoted as asymmetric boosting is presented unlike previous proposals the new algorithm is derived from sound decisiontheoretic principles which exploit the statistical interpretation of boosting to determine a principled extension of the boosting loss similarly to adaboost the costsensitive extension minimizes this loss by gradient descent on the functional space of convex combinations of weak learners and produces large margin detectors it is shown that asymmetric boosting is fully compatible with adaboost in the sense that it becomes the latter when errors are weighted equally experimental evidence is provided to demonstrate the claims of costsensitivity and large margin the algorithm is also applied to the computer vision problem of face detection where it is shown to outperform a number of previous heuristic proposals for costsensitive boosting adacost csb csb csb asymmetricadaboost adac adac and adac
linear and nonlinear generative probabilistic class models for shape contours linear and nonlinear generative probabilistic class models for shape contours we introduce a robust probabilistic approach to modeling shape contours based on a lowdimensional nonlinear latent variable model in contrast to existing techniques that use objective functions in data space without explicit noise models we are able to extract complex shape variation from noisy data most approaches to learning shape models slide observed data points around fixed contours and hence require a correctly labeled reference shape to prevent degenerate solutions in our method unobserved curves are reparameterized to explain the fixed data points so this problem does not arise the proposed algorithms are suitable for use with arbitrary basis functions and are applicable to both open and closed shapes their effectiveness is demonstrated through illustrative examples quantitative assessment on benchmark data sets and a visualization task
bottomup learning of markov logic network structure bottomup learning of markov logic network structure markov logic networks mlns are a statistical relational model that consists of weighted firstorder clauses and generalizes firstorder logic and markov networks the current stateoftheart algorithm for learning mln structure follows a topdown paradigm where many potential candidate structures are systematically generated without considering the data and then evaluated using a statistical measure of their fit to the data even though this existing algorithm outperforms an impressive array of benchmarks its greedy search is susceptible to local maxima or plateaus we present a novel algorithm for learning mln structure that follows a more bottomup approach to address this problem our algorithm uses a propositional markov network learning method to construct template networks that guide the construction of candidate clauses our algorithm significantly improves accuracy and learning time over the existing topdown approach in three realworld domains
mixtures of hierarchical topics with pachinko allocation mixtures of hierarchical topics with pachinko allocation the fourlevel pachinko allocation model pam li mccallum represents correlations among topics using a dag structure it does not however represent a nested hierarchy of topics with some topical word distributions representing the vocabulary that is shared among several more specific topics this paper presents hierarchical paman enhancement that explicitly represents a topic hierarchy this model can be seen as combining the advantages of hldas topical hierarchy representation with pams ability to mix multiple leaves of the topic hierarchy experimental results show improvements in likelihood of heldout documents as well as mutual information between automaticallydiscovered topics and humangenerated categories such as journals
three new graphical models for statistical language modelling three new graphical models for statistical language modelling the supremacy of ngram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity we propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words we show how realvalued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the realvalued distributed representations one of our models significantly outperforms the very best ngram models
fast and effective kernels for relational learning from texts fast and effective kernels for relational learning from texts in this paper we define a family of syntactic kernels for automatic relational learning from pairs of natural language sentences we provide an efficient computation of such models by optimizing the dynamic programming algorithm of the kernel evaluation experiments with support vector machines and the above kernels show the effectiveness and efficiency of our approach on two very important natural language tasks textual entailment recognition and question answering
dimensionality reduction and generalization dimensionality reduction and generalization in this paper we investigate the regularization property of kernel principal component analysis kpca by studying its application as a preprocessing step to supervised learning problems we show that performing kpca and then ordinary least squares on the projected data a procedure known as kernel principal component regression kpcr is equivalent to spectral cutoff regularization the regularization parameter being exactly the number of principal components to keep using probabilistic estimates for integral operators we can prove error estimates for kpcr and propose a parameter choice procedure allowing to prove consistency of the algorithm
unsupervised estimation for noisychannel models unsupervised estimation for noisychannel models shannons noisychannel model which describes how a corrupted message might be reconstructed has been the corner stone for much work in statistical language and speech processing the model factors into two components a language model to characterize the original message and a channel model to describe the channels corruptive process the standard approach for estimating the parameters of the channel model is unsupervised maximumlikelihood of the observation data usually approximated using the expectationmaximization em algorithm in this paper we show that it is better to maximize the joint likelihood of the data at both ends of the noisychannel we derive a corresponding bidirectional em algorithm and show that it gives better performance than standard em on two tasks translation using a probabilistic lexicon and adaptation of a partofspeech tagger between related languages
revisiting probabilistic models for clustering with pairwise constraints revisiting probabilistic models for clustering with pairwise constraints we revisit recently proposed algorithms for probabilistic clustering with pairwise constraints between data points we evaluate and compare existing techniques in terms of robustness to misspecified constraints we show that the technique that strictly enforces the given constraints namely the chunklet model produces poor results even under a small number of misspecified constraints we further show that methods that penalize constraint violation are more robust to misspecified constraints but have undesirable local behaviors based on this evaluation we propose a new learning technique extending the chunklet model to allow soft constraints represented by an intuitive measure of confidence in the constraint
comparisons of sequence labeling algorithms and extensions comparisons of sequence labeling algorithms and extensions in this paper we survey the current stateofart models for structured learning problems including hidden markov model hmm conditional random fields crf averaged perceptron ap structured svms svmstruct max margin markov networks mn and an integration of search and learning algorithm searn with all due tuning efforts of various parameters of each model on the data sets we have applied the models to we found that svmstruct enjoys better performance compared with the others in addition we also propose a new method which we call the structured learning ensemble sle to combine these structured learning models empirical results show that our sle algorithm provides more accurate solutions compared with the best results of the individual models
multitask learning for sequential data via ihmms and the nested dirichlet process multitask learning for sequential data via ihmms and the nested dirichlet process a new hierarchical nonparametric bayesian model is proposed for the problem of multitask learning mtl with sequential data sequential data are typically modeled with a hidden markov model hmm for which one often must choose an appropriate model structure number of states before learning here we model sequential data from each task with an infinite hidden markov model ihmm avoiding the problem of model selection the mtl for ihmms is implemented by imposing a nested dirichlet process ndp prior on the base distributions of the ihmms the ndpihmm mtl method allows us to perform tasklevel clustering and datalevel clustering simultaneously with which the learning for individual ihmms is enhanced and betweentask similarities are learned learning and inference for the ndpihmm mtl are based on a gibbs sampler the effectiveness of the framework is demonstrated using synthetic data as well as real music data
regression on manifolds using kernel dimension reduction regression on manifolds using kernel dimension reduction we study the problem of discovering a manifold that best preserves information relevant to a nonlinear regression solving this problem involves extending and uniting two threads of research on the one hand the literature on sufficient dimension reduction has focused on methods for finding the best linear subspace for nonlinear regression we extend this to manifolds on the other hand the literature on manifold learning has focused on unsupervised dimensionality reduction we extend this to the supervised setting our approach to solving the problem involves combining the machinery of kernel dimension reduction with laplacian eigenmaps specifically we optimize crosscovariance operators in kernel feature spaces that are induced by the normalized graph laplacian the result is a highly flexible method in which no strong assumptions are made on the regression function or on the distribution of the covariates we illustrate our methodology on the analysis of global temperature data and image manifolds
learning stateaction basis functions for hierarchical mdps learning stateaction basis functions for hierarchical mdps this paper introduces a new approach to actionvalue function approximation by learning basis functions from a spectral decomposition of the stateaction manifold this paper extends previous work on using laplacian bases for value function approximation by using the actions of the agent as part of the representation when creating basis functions the approach results in a nonlinear learned representation particularly suited to approximating actionvalue functions without incurring the wasteful duplication of state bases in previous work we discuss two techniques to create stateaction graphs offpolicy and onpolicy we show that these graphs have a greater expressive power and have better performance over statebased laplacian basis functions in domains modeled as semimarkov decision processes smdps we present a simple graph partitioning method to scale the approach to large discrete mdps
a fast linear separability test by projection of positive points on subspaces a fast linear separability test by projection of positive points on subspaces a geometric and non parametric procedure for testing if two finite set of points are linearly separable is proposed the linear separability test is equivalent to a test that determines if a strictly positive point h gt exists in the range of a matrix a related to the points in the two finite sets the algorithm proposed in the paper iteratively checks if a strictly positive point exists in a subspace by projecting a strictly positive vector with equal coordinates p on the subspace at the end of each iteration the subspace is reduced to a lower dimensional subspace the test is completed within r le minn d steps for both linearly separable and non separable problems r is the rank of a n is the number of points and d is the dimension of the space containing the points the worst case time complexity of the algorithm is onr and space complexity of the algorithm is ond a small review of some of the prominent algorithms and their time complexities is included the worst case computational complexity of our algorithm is lower than the worst case computational complexity of simplex perceptron support vector machine and convex hull algorithms if dltn
multiarmed bandit problems with dependent arms multiarmed bandit problems with dependent arms we provide a framework to exploit dependencies among arms in multiarmed bandit problems when the dependencies are in the form of a generative model on clusters of arms we find an optimal mdpbased policy for the discounted reward case and also give an approximation of it with formal error guarantee we discuss lower bounds on regret in the undiscounted reward scenario and propose a general twolevel bandit policy for it we propose three different instantiations of our general policy and provide theoretical justifications of how the regret of the instantiated policies depend on the characteristics of the clusters finally we empirically demonstrate the efficacy of our policies on largescale realworld and synthetic data and show that they significantly outperform classical policies designed for bandits with independent arms
learning for efficient retrieval of structured data with noisy queries learning for efficient retrieval of structured data with noisy queries increasingly large collections of structured data necessitate the development of efficient noisetolerant retrieval tools in this work we consider this issue and describe an approach to learn a similarity function that is not only accurate but that also increases the effectiveness of retrieval data structures we present an algorithm that uses functional gradient boosting to maximize both retrieval accuracy and the retrieval efficiency of vantage point trees we demonstrate the effectiveness of our approach on two datasets including a moderately sized realworld dataset of folk music
analyzing feature generation for valuefunction approximation analyzing feature generation for valuefunction approximation we analyze a simple bellmanerrorbased approach to generating basis functions for valuefunction approximation we show that it generates orthogonal basis functions that provably tighten approximation error bounds we also illustrate the use of this approach in the presence of noise on some sample problems
reinforcement learning by rewardweighted regression for operational space control reinforcement learning by rewardweighted regression for operational space control many robot control problems of practical importance including operational space control can be reformulated as immediate reward reinforcement learning problems however few of the known optimization or reinforcement learning algorithms can be used in online learning control for robots as they are either prohibitively slow do not scale to interesting domains of complex robots or require trying out policies generated by random search which are infeasible for a physical system using a generalization of the embase reinforcement learning framework suggested by dayan hinton we reduce the problem of learning with immediate rewards to a rewardweighted regression problem with an adaptive integrated reward transformation for faster convergence the resulting algorithm is efficient learns smoothly without dangerous jumps in solution space and works well in applications of complex high degreeoffreedom robots
tracking value function dynamics to improve reinforcement learning with piecewise linear function approximation tracking value function dynamics to improve reinforcement learning with piecewise linear function approximation reinforcement learning algorithms can become unstable when combined with linear function approximation algorithms that minimize the meansquare bellman error are guaranteed to converge but often do so slowly or are computationally expensive in this paper we propose to improve the convergence speed of piecewise linear function approximation by tracking the dynamics of the value function with the kalman filter using a randomwalk model we cast this as a general framework in which we implement the td qlearning and maxq algorithms for different domains and report empirical results demonstrating improved learning speed over previous methods
selftaught learning selftaught learning we present a new machine learning framework called selftaught learning for using unlabeled data in supervised classification tasks we do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data thus we would like to use a large number of unlabeled images or audio samples or text documents randomly downloaded from the internet to improve performance on a given image or audio or text classification task such unlabeled data is significantly easier to obtain than in typical semisupervised or transfer learning settings making selftaught learning widely applicable to many practical learning problems we describe an approach to selftaught learning that uses sparse coding to construct higherlevel features using the unlabeled data these features form a succinct input representation and significantly improve classification performance when using an svm for classification we further show how a fisher kernel can be learned for this representation
online discovery of similarity mappings online discovery of similarity mappings we consider the problem of choosing sequentially a map which assigns elements of a set a to a few elements of a set b on each round the algorithm suffers some cost associated with the chosen assignment and the goal is to minimize the cumulative loss of these choices relative to the best map on the entire sequence even though the offline problem of finding the best map is provably hard we show that there is an equivalent online approximation algorithm randomized map prediction rmp that is efficient and performs nearly as well while drawing upon results from the online prediction with expert advice setting we show how rmp can be utilized as an online approach to several standard batch problems we apply rmp to online clustering as well as online feature selection and surprisingly rmp often outperforms the standard batch algorithms on these problems
more efficiency in multiple kernel learning more efficiency in multiple kernel learning an efficient and general multiple kernel learning mkl algorithm has been recently proposed by sonnenburg et al this approach has opened new perspectives since it makes the mkl approach tractable for largescale problems by iteratively using existing support vector machine code however it turns out that this iterative algorithm needs several iterations before converging towards a reasonable solution in this paper we address the mkl problem through an adaptive norm regularization formulation weights on each kernel matrix are included in the standard svm empirical risk minimization problem with a l constraint to encourage sparsity we propose an algorithm for solving this problem and provide an new insight on mkl algorithms based on block norm regularization by showing that the two approaches are equivalent experimental results show that the resulting algorithm converges rapidly and its efficiency compares favorably to other mkl algorithms
graph clustering with network structure indices graph clustering with network structure indices graph clustering has become ubiquitous in the study of relational data sets we examine two simple algorithms a new graphical adaptation of the kmedoids algorithm and the girvannewman method based on edge betweenness centrality we show that they can be effective at discovering the latent groups or communities that are defined by the link structure of a graph however both approaches rely on prohibitively expensive computations given the size of modern relational data sets network structure indices nsis are a proven technique for indexing network structure and efficiently finding short paths we show how incorporating nsis into these graph clustering algorithms can overcome these complexity limitations we also present promising quantitative and qualitative evaluations of the modified algorithms on synthetic and real data sets
restricted boltzmann machines for collaborative filtering restricted boltzmann machines for collaborative filtering most of the existing approaches to collaborative filtering cannot handle very large data sets in this paper we show how a class of twolayer undirected graphical models called restricted boltzmann machines rbms can be used to model tabular data such as users ratings of movies we present efficient learning and inference procedures for this class of models and demonstrate that rbms can be successfully applied to the netflix data set containing over million usermovie ratings we also show that rbms slightly outperform carefullytuned svd models when the predictions of multiple rbm models and multiple svd models are linearly combined we achieve an error rate that is well over better than the score of netflixs own system
sample compression bounds for decision trees sample compression bounds for decision trees we propose a formulation of the decision tree learning algorithm in the compression settings and derive tight generalization error bounds in particular we propose sample compression and occams razor bounds we show how such bounds unlike the vc dimension or rademacher complexities based bounds are more general and can also perform a marginsparsity tradeoff to obtain better classifers potentially these risk bounds can also guide the model selection process and replace traditional pruning strategies
pegasos pegasos we describe and analyze a simple and effective iterative algorithm for solving the optimization problem cast by support vector machines svm our method alternates between stochastic gradient descent steps and projection steps we prove that the number of iterations required to obtain a solution of accuracy epsilon is otildeepsilon in contrast previous analyses of stochastic gradient descent methods require omega epsilon iterations as in previously devised svm solvers the number of iterations also scales linearly with lambda where lambda is the regularization parameter of svm for a linear kernel the total runtime of our method is otilde dlambdaepsilon where d is a bound on the number of nonzero features in each example since the runtime does not depend directly on the size of the training set the resulting algorithm is especially suited for learning from large datasets our approach can seamlessly be adapted to employ nonlinear kernels while working solely on the primal objective function we demonstrate the efficiency and applicability of our approach by conducting experiments on large text classification problems comparing our solver to existing stateoftheart svm solvers for example it takes less than seconds for our solver to converge when solving a text classification problem from reuters corpus volume rcv with training examples
a dependence maximization view of clustering a dependence maximization view of clustering we propose a family of clustering algorithms based on the maximization of dependence between the input variables and their cluster labels as expressed by the hilbertschmidt independence criterion hsic under this framework we unify the geometric spectral and statistical dependence views of clustering and subsume many existing algorithms as special cases eg kmeans and spectral clustering distinctive to our framework is that kernels can also be applied on the labels which can endow them with particular structures we also obtain a perturbation bound on the change in kmeans clustering
supervised feature selection via dependence estimation supervised feature selection via dependence estimation we introduce a framework for filtering features that employs the hilbertschmidt independence criterion hsic as a measure of dependence between the features and the labels the key idea is that good features should maximise such dependence feature selection for various supervised learning problems including classification and regression is unified under this framework and the solutions can be approximated using a backwardelimination algorithm we demonstrate the usefulness of our method on both artificial and real world datasets
sparse eigen methods by dc programming sparse eigen methods by dc programming eigenvalue problems are rampant in machine learning and statistics and appear in the context of classification dimensionality reduction etc in this paper we consider a cardinality constrained variational formulation of generalized eigenvalue problem with sparse principal component analysis pca as a special case using lnorm approximation to the cardinality constraint previous methods have proposed both convex and nonconvex solutions to the sparse pca problem in contrast we propose a tighter approximation that is related to the negative loglikelihood of a students tdistribution the problem is then framed as a dc difference of convex functions program and is solved as a sequence of locally convex programs we show that the proposed method not only explains more variance with sparse loadings on the principal directions but also has better scalability compared to other methods we demonstrate these results on a collection of datasets of varying dimensionality two of which are highdimensional gene datasets where the goal is to find few relevant genes that explain as much variance as possible
learning to solve game trees learning to solve game trees we apply probability theory to the task of proving whether a goal can be achieved by a player in an adversarial game such problems are solved by searching the game tree we view this tree as a graphical model which yields a distribution over the boolean outcome of the search before it terminates experiments show that a bestfirst search algorithm guided by this distribution explores a similar number of nodes as proofnumber search to solve go problems knowledge is incorporated into search by using domainspecific models to provide prior distributions over the values of leaf nodes of the game tree these are surrogate for the unexplored parts of the tree the parameters of these models can be learned from previous search trees experiments on go show that the speed of problem solving can be increased by orders of magnitude by this technique but care must be taken to avoid overfitting
robust mixtures in the presence of measurement errors robust mixtures in the presence of measurement errors we develop a mixturebased approach to robust density modeling and outlier detection for experimental multivariate data that includes measurement error information our model is designed to infer atypical measurements that are not due to errors aiming to retrieve potentially interesting peculiar objects since exact inference is not possible in this model we develop a treestructured variational em solution this compares favorably against a fully factorial approximation scheme approaching the accuracy of a markovchainem while maintaining computational simplicity we demonstrate the benefits of including measurement errors in the model in terms of improved outlier detection rates in varying measurement uncertainty conditions we then use this approach for detecting peculiar quasars from an astrophysical survey given photometric measurements with errors
a kernelbased causal learning algorithm a kernelbased causal learning algorithm we describe a causal learning method which employs measuring the strength of statistical dependences in terms of the hilbertschmidt norm of kernelbased crosscovariance operators following the line of the common faithfulness assumption of constraintbased causal learning our approach assumes that a variable z is likely to be a common effect of x and y if conditioning on z increases the dependence between x and y based on this assumption we collect votes for hypothetical causal directions and orient the edges by the majority principle in most experiments with known causal structures our method provided plausible results and outperformed the conventional constraintbased pc algorithm
piecewise pseudolikelihood for efficient training of conditional random fields piecewise pseudolikelihood for efficient training of conditional random fields discriminative training of graphical models can be expensive if the variables have large cardinality even if the graphical structure is tractable in such cases pseudolikelihood is an attractive alternative because its running time is linear in the variable cardinality but on some data its accuracy can be poor piecewise training sutton mccallum can have better accuracy but does not scale as well in the variable cardinality in this paper we introduce piecewise pseudolikelihood which retains the computational efficiency of pseudolikelihood but can have much better accuracy on several benchmark nlp data sets piecewise pseudolikelihood has better accuracy than standard pseudolikelihood and in many cases nearly equivalent to maximum likelihood with five to ten times less training time than batch crf training
on the role of tracking in stationary environments on the role of tracking in stationary environments it is often thought that learning algorithms that track the best solution as opposed to converging to it are important only on nonstationary problems we present three results suggesting that this is not so first we illustrate in a simple concrete example the black and white problem that tracking can perform better than any converging algorithm on a stationary problem second we show the same point on a larger more realistic problem an application of temporal difference learning to computer go our third result suggests that tracking in stationary problems could be important for metalearning research eg learning to learn feature selection transfer we apply a metalearning algorithm for stepsize adaptation idbd sutton a to the black and white problem showing that metalearning has a dramatic longterm effect on performance whereas on an analogous converging problem metalearning has only a small secondorder effect this small result suggests a way of eventually overcoming a major obstacle to metalearning research the lack of an independent methodology for task selection
crossdomain transfer for reinforcement learning crossdomain transfer for reinforcement learning a typical goal for transfer learning algorithms is to utilize knowledge gained in a source task to learn a target task faster recently introduced transfer methods in reinforcement learning settings have shown considerable promise but they typically transfer between pairs of very similar tasks this work introduces rule transfer a transfer algorithm that first learns rules to summarize a source task policy and then leverages those rules to learn faster in a target task this paper demonstrates that rule transfer can effectively speed up learning in keepaway a benchmark rl problem in the robot soccer domain based on experience from source tasks in the gridworld domain we empirically show through the use of three distinct transfer metrics that rule transfer is effective across these domains
incremental bayesian networks for structure prediction incremental bayesian networks for structure prediction we propose a class of graphical models appropriate for structure prediction problems where the model structure is a function of the output structure incremental sigmoid belief networks isbns avoid the need to sum over the possible model structures by using directed arcs and incrementally specifying the model structure exact inference in such directed models is not tractable but we derive two efficient approximations based on mean field methods which prove effective in artificial experiments we then demonstrate their effectiveness on a benchmark natural language parsing task where they achieve stateoftheart accuracy also the model which is a closer approximation to an isbn has better parsing accuracy suggesting that isbns are an appropriate abstract model of structure prediction tasks
classifying matrices with a spectral regularization classifying matrices with a spectral regularization we propose a method for the classification of matrices we use a linear classifier with a novel regularization scheme based on the spectral lnorm of its coefficient matrix the spectral regularization not only provides a principled way of complexity control but also enables automatic determination of the rank of the coefficient matrix using the linear matrix inequality technique we formulate the inference task as a single convex optimization problem we apply our method to the motorimagery eeg classification problem the method not only improves upon conventional methods in the classification performance but also determines a subspace in the signal that concentrates discriminative information without any additional feature extraction step the method can be easily generalized to regression problems by changing the loss function connections to other methods are also discussed
approximate maximum margin algorithms with rules controlled by the number of mistakes approximate maximum margin algorithms with rules controlled by the number of mistakes we present a family of incremental perceptronlike algorithms plas with margin in which both the effective learning rate defined as the ratio of the learning rate to the length of the weight vector and the misclassification condition are entirely controlled by rules involving powers of the number of mistakes we examine the convergence of such algorithms in a finite number of steps and show that under some rather mild conditions there exists a limit of the parameters involved in which convergence leads to classification with maximum margin an experimental comparison of algorithms belonging to this family with other large margin plas and decomposition svms is also presented
simpler core vector machines with enclosing balls simpler core vector machines with enclosing balls the core vector machine cvm is a recent approach for scaling up kernel methods based on the notion of minimum enclosing ball meb though conceptually simple an efficient implementation still requires a sophisticated numerical solver in this paper we introduce the enclosing ball eb problem where the balls radius is fixed and thus does not have to be minimized we develop efficient eapproximation algorithms that are simple to implement and do not require any numerical solver for the gaussian kernel in particular a suitable choice of this fixed radius is easy to determine and the center obtained from the eapproximation of this eb problem is close to the center of the corresponding meb experimental results show that the proposed algorithm has accuracies comparable to the other largescale svm implementations but can handle very large data sets and is even faster than the cvm in general
entire regularization paths for graph data entire regularization paths for graph data graph data such as chemical compounds and xml documents are getting more common in many application domains a main difficulty of graph data processing lies in the intrinsic high dimensionality of graphs namely when a graph is represented as a binary feature vector of indicators of all possible subgraph patterns the dimensionality gets too large for usual statistical methods we propose an efficient method to select a small number of salient patterns by regularization path tracking the generation of useless patterns is minimized by progressive extension of the search space in experiments it is shown that our technique is considerably more efficient than a simpler approach based on frequent substructure mining
discriminative gaussian process latent variable model for classification discriminative gaussian process latent variable model for classification supervised learning is difficult with high dimensional input spaces and very small training sets but accurate classification may be possible if the data lie on a lowdimensional manifold gaussian process latent variable models can discover low dimensional manifolds given only a small number of examples but learn a latent space without regard for class labels existing methods for discriminative manifold learning eg lda gda do constrain the class distribution in the latent space but are generally deterministic and may not generalize well with limited training data we introduce a method for gaussian process classification using latent variable models trained with discriminative priors over the latent space which can learn a discriminative latent space from a small training set
experimental perspectives on learning from imbalanced data experimental perspectives on learning from imbalanced data we present a comprehensive suite of experimentation on the subject of learning from imbalanced data when classes are imbalanced many learning algorithms can suffer from the perspective of reduced performance can data sampling be used to improve the performance of learners built from imbalanced data is the effectiveness of sampling related to the type of learner do the results change if the objective is to optimize different performance metrics we address these and other issues in this work showing that sampling in many cases will improve classifier performance
learning from interpretations learning from interpretations the paper presents a kernel for learning from ordered hypergraphs a formalization that captures relational data as used in inductive logic programming ilp the kernel generalizes previous approaches to graph kernels in calculating similarity based on walks in the hypergraph experiments on challenging chemical datasets demonstrate that the kernel outperforms existing ilp methods and is competitive with stateoftheart graph kernels the experiments also demonstrate that the encoding of graph data can affect performance dramatically a fact that can be useful beyond kernel methods
a kernel path algorithm for support vector machines a kernel path algorithm for support vector machines the choice of the kernel function which determines the mapping between the input space and the feature space is of crucial importance to kernel methods the past few years have seen many efforts in learning either the kernel function or the kernel matrix in this paper we address this model selection issue by learning the hyperparameter of the kernel function for a support vector machine svm we trace the solution path with respect to the kernel hyperparameter without having to train the model multiple times given a kernel hyperparameter value and the optimal solution obtained for that value we find that the solutions of the neighborhood hyperparameters can be calculated exactly however the solution path does not exhibit piecewise linearity and extends nonlinearly as a result the breakpoints cannot be computed in advance we propose a method to approximate the breakpoints our method is both efficient and general in the sense that it can be applied to many kernel functions in common use
dirichlet aggregation dirichlet aggregation proportional data normalized histograms have been frequently occurring in various areas and they could be mathematically abstracted as points residing in a geometric simplex a proper distance metric on this simplex is of importance in many applications including classification and information retrieval in this paper we develop a novel framework to learn an optimal metric on the simplex major features of our approach include its flexibility to handle correlations among binsdimensions widespread applicability without being limited to ad hoc backgrounds and a real global solution in contrast to existing traditional local approaches the technical essence of our approach is to fit a parametric distribution to the observed empirical data in the simplex the distribution is parameterized by affinities between simplex vertices which is learned via maximizing likelihood of observed data then these affinities induce a metric on the simplex defined as the earth movers distance equipped with ground distances derived from simplex vertex affinities
transductive regression piloted by intermanifold relations transductive regression piloted by intermanifold relations in this paper we present a novel semisupervised regression algorithm working on multiclass data that may lie on multiple manifolds unlike conventional manifold regression algorithms that do not consider the class distinction of samples our method introduces the class information to the regression process and tries to exploit the similar configurations shared by the label distribution of multiclass data to utilize the correlations among data from different classes we develop a crossmanifold label propagation process and employ labels from different classes to enhance the regression performance the interclass relations are coded by a set of intermanifold graphs and a regularization item is introduced to impose interclass smoothness on the possible solutions in addition the algorithm is further extended with the kernel trick for predicting labels of the outofsample data even without class information experiments on both synthesized data and real world problems validate the effectiveness of the proposed framework for semisupervised regression
multifactor gaussian process models for stylecontent separation multifactor gaussian process models for stylecontent separation we introduce models for density estimation with multiple hidden continuous factors in particular we propose a generalization of multilinear models using nonlinear basis functions by marginalizing over the weights we obtain a multifactor form of the gaussian process latent variable model in this model each factor is kernelized independently allowing nonlinear mappings from any particular factor to the data we learn models for human locomotion data in which each pose is generated by factors representing the persons identity gait and the current state of motion we demonstrate our approach using timeseries prediction and by synthesizing novel animation from the model
hybrid huberized support vector machines for microarray classification hybrid huberized support vector machines for microarray classification the large number of genes and the relatively small number of samples are typical characteristics for microarray data these characteristics pose challenges for both sample classification and relevant gene selection the support vector machine svm is a widely used classification technique and previous studies have demonstrated its superior classification performance in microarray analysis however a major limitation is that the svm can not perform automatic gene selection to overcome this limitation we propose the hybrid huberized support vector machine hhsvm the hhsvm uses the huberized hinge loss function and the elasticnet penalty it has two major benefits automatic gene selection the grouping effect where highly correlated genes tend to be selectedremoved together we also develop an efficient algorithm that computes the entire regularized solution path for hhsvm we have applied our method to real microarray data and achieved promising results
on learning with dissimilarity functions on learning with dissimilarity functions we study the problem of learning a classification task in which only a dissimilarity function of the objects is accessible that is data are not represented by feature vectors but in terms of their pairwise dissimilarities we investigate the sufficient conditions for dissimilarity functions to allow building accurate classifiers our results have the advantages that they apply to unbounded dissimilarities and are invariant to orderpreserving transformations the theory immediately suggests a learning paradigm construct an ensemble of decision stumps each depends on a pair of examples then find a convex combination of them to achieve a large margin we next develop a practical algorithm called dissimilarity based boosting dboost for learning with dissimilarity functions under the theoretical guidance experimental results demonstrate that dboost compares favorably with several existing approaches on a variety of databases and under different conditions
winnowing subspaces winnowing subspaces we generalize the winnow algorithm for learning disjunctions to learning subspaces of low rank subspaces are represented by symmetric projection matrices the online algorithm maintains its uncertainty about the hidden low rank projection matrix as a symmetric positive definite matrix this matrix is updated using a version of the matrix exponentiated gradient algorithm that is based on matrix exponentials and matrix logarithms as in the case of the winnow algorithm the bounds are logarithmic in the dimension n of the problem but linear in the rank r of the hidden subspace we show that the algorithm can be adapted to handle arbitrary matrices of any dimension via a reduction
what is decreased by the maxsum arc consistency algorithm what is decreased by the maxsum arc consistency algorithm inference tasks in markov random fields mrfs are closely related to the constraint satisfaction problem csp and its soft generalizations in particular map inference in mrf is equivalent to the weighted maxsum csp a wellknown tool to tackle csps are arc consistency algorithms aka relaxation labeling a promising approach to map inference in mrfs is linear programming relaxation solved by sequential treereweighted message passing trws there is a not widely known algorithm equivalent to trws maxsum diffusion which is slower but very simple we give two theoretical results first we show that arc consistency algorithms and maxsum diffusion become the same thing if formulated in an abstractalgebraic way thus we argue that maxsum arc consistency algorithm or maxsum relaxation labeling is a more suitable name for maxsum diffusion second we give a criterion that strictly decreases during these algorithms it turns out that every class of equivalent problems contains a unique problem that is minimal wrt this criterion
multitask reinforcement learning multitask reinforcement learning we consider the problem of multitask reinforcement learning where the agent needs to solve a sequence of markov decision processes mdps chosen randomly from a fixed but unknown distribution we model the distribution over mdps using a hierarchical bayesian infinite mixture model for each novel mdp we use the previously learned distribution as an informed prior for modelbased bayesian reinforcement learning the hierarchical bayesian framework provides a strong prior that allows us to rapidly infer the characteristics of new environments based on previous environments while the use of a nonparametric model allows us to quickly adapt to environments we have not encountered before in addition the use of infinite mixtures allows for the model to automatically learn the number of underlying mdp components we evaluate our approach and show that it leads to significant speedups in convergence to an optimal policy after observing only a small number of tasks
beamforming using the relevance vector machine beamforming using the relevance vector machine beamformers are spatial filters that pass source signals in particular focused locations while suppressing interference from elsewhere the widelyused minimum variance adaptive beamformer mvab creates such filters using a sample covariance estimate however the quality of this estimate deteriorates when the sources are correlated or the number of samples n is small herein a modified beamformer is derived that replaces this problematic sample covariance with a robust maximum likelihood estimate obtained using the relevance vector machine rvm a bayesian method for learning sparse models from possibly overcomplete feature sets we prove that this substitution has the natural ability to remove the undesirable effects of correlations or limited data when n becomes large and assuming uncorrelated sources this method reduces to the exact mvab simulations using directionofarrival data support these conclusions additionally rvms can potentially enhance a variety of traditional signal processing methods that rely on robust sample covariance estimates
learning to combine distances for complex representations learning to combine distances for complex representations the knearest neighbors algorithm can be easily adapted to classify complex objects eg sets graphs as long as a proper dissimilarity function is given over an input space both the representation of the learning instances and the dissimilarity employed on that representation should be determined on the basis of domain knowledge however even in the presence of domain knowledge it can be far from obvious which complex representation should be used or which dissimilarity should be applied on the chosen representation in this paper we present a framework that allows to combine different complex representations of a given learning problem andor different dissimilarities defined on these representations we build on ideas developed previously on metric learning for vectorial data we demonstrate the utility of our method in domains in which the learning instances are represented as sets of vectors by learning how to combine different set distance measures
local learning projections local learning projections this paper presents a local learning projection llp approach for linear dimensionality reduction we first point out that the well known principal component analysis pca essentially seeks the projection that has the minimal global estimation error then we propose a dimensionality reduction algorithm that leads to the projection with the minimal local estimation error and elucidate its advantages for classification tasks we also indicate that llp keeps the local information in the sense that the projection value of each point can be well estimated based on its neighbors and their projection values experimental results are provided to validate the effectiveness of the proposed algorithm
on learning linear ranking functions for beam search on learning linear ranking functions for beam search beam search is used to maintain tractability in large search spaces at the expense of completeness and optimality we study supervised learning of linear ranking functions for controlling beam search the goal is to learn ranking functions that allow for beam search to perform nearly as well as unconstrained search while gaining computational efficiency we first study the computational complexity of the learning problem showing that even for exponentially large search spaces the general consistency problem is in np we also identify tractable and intractable subclasses of the learning problem next we analyze the convergence of recently proposed and modified online learning algorithms we first provide a counterexample to an existing convergence result and then introduce alternative notions of margin that do imply convergence finally we study convergence properties for ambiguous training data
modeling changing dependency structure in multivariate time series modeling changing dependency structure in multivariate time series we show how to apply the efficient bayesian changepoint detection techniques of fearnhead in the multivariate setting we model the joint density of vectorvalued observations using undirected gaussian graphical models whose structure we estimate we show how we can exactly compute the map segmentation as well as how to draw perfect samples from the posterior over segmentations simultaneously accounting for uncertainty about the number and location of changepoints as well as uncertainty about the covariance structure we illustrate the technique by applying it to financial data and to bee tracking data
the matrix stickbreaking process for flexible multitask learning the matrix stickbreaking process for flexible multitask learning in multitask learning our goal is to design regression or classification models for each of the tasks and appropriately share information between tasks a dirichlet process dp prior can be used to encourage task clustering however the dp prior does not allow local clustering of tasks with respect to a subset of the feature vector without making independence assumptions motivated by this problem we develop a new multitasklearning prior termed the matrix stickbreaking process msbp which encourages crosstask sharing of data however the msbp allows separate clustering and borrowing of information for the different feature components this is important when tasks are more closely related for certain features than for others bayesian inference proceeds by a gibbs sampling algorithm and the approach is illustrated using a simulated example and a multinational application
map building without localization by dimensionality reduction techniques map building without localization by dimensionality reduction techniques this paper proposes a new map building framework for mobile robot named localizationfree mapping by dimensionality reduction lfmdr in this framework the robot map building is interpreted as a problem of reconstructing the d coordinates of objects so that they maximally preserve the local proximity of the objects in the space of robots observation history not only traditional linear pca but also recent manifold learning techniques can be used for solving this problem in contrast to the slam framework lfmdr framework does not require localization procedures nor explicit measurement and motion models in the latter part of this paper we will demonstrate visibilityonly and bearingonly localizationfree mappings which are derived by applying lfmdr framework to the visibility and bearing measurements respectively
asymptotic bayesian generalization error when training and test distributions are different asymptotic bayesian generalization error when training and test distributions are different in supervised learning we commonly assume that training and test data are sampled from the same distribution however this assumption can be violated in practice and then standard machine learning techniques perform poorly this paper focuses on revealing and improving the performance of bayesian estimation when the training and test distributions are different we formally analyze the asymptotic bayesian generalization error and establish its upper bound under a very general setting our important finding is that lower order termswhich can be ignored in the absence of the distribution changeplay an important role under the distribution change we also propose a novel variant of stochastic complexity which can be used for choosing an appropriate model and hyperparameters under a particular distribution change
least squares linear discriminant analysis least squares linear discriminant analysis linear discriminant analysis lda is a wellknown method for dimensionality reduction and classification lda in the binaryclass case has been shown to be equivalent to linear regression with the class label as the output this implies that lda for binaryclass classifications can be formulated as a least squares problem previous studies have shown certain relationship between multivariate linear regression and lda for the multiclass case many of these studies show that multivariate linear regression with a specific class indicator matrix as the output can be applied as a preprocessing step for lda however directly casting lda as a least squares problem is challenging for the multiclass case in this paper a novel formulation for multivariate linear regression is proposed the equivalence relationship between the proposed least squares formulation and lda for multiclass classifications is rigorously established under a mild condition which is shown empirically to hold in many applications involving highdimensional data several lda extensions based on the equivalence relationship are discussed
discriminant kernel and regularization parameter learning via semidefinite programming discriminant kernel and regularization parameter learning via semidefinite programming regularized kernel discriminant analysis rkda performs linear discriminant analysis in the feature space via the kernel trick the performance of rkda depends on the selection of kernels in this paper we consider the problem of learning an optimal kernel over a convex set of kernels we show that the kernel learning problem can be formulated as a semidefinite program sdp in the binaryclass case we further extend the sdp formulation to the multiclass case it is based on a key result established in this paper that is the multiclass kernel learning problem can be decomposed into a set of binaryclass kernel learning problems in addition we propose an approximation scheme to reduce the computational complexity of the multiclass sdp formulation the performance of rkda also depends on the value of the regularization parameter we show that this value can be learned automatically in the framework experimental results on benchmark data sets demonstrate the efficacy of the proposed sdp formulations
robust multitask learning with tprocesses robust multitask learning with tprocesses most current multitask learning frameworks ignore the robustness issue which means that the presence of outlier tasks may greatly reduce overall system performance we introduce a robust framework for bayesian multitask learning tprocesses tp which are a generalization of gaussian processes gp for multitask learning tp allows the system to effectively distinguish good tasks from noisy or outlier tasks experiments show that tp not only improves overall system performance but can also serve as an indicator for the informativeness of different tasks
on the value of pairwise constraints in classification and consistency on the value of pairwise constraints in classification and consistency in this paper we consider the problem of classification in the presence of pairwise constraints which consist of pairs of examples as well as a binary variable indicating whether they belong to the same class or not we propose a method which can effectively utilize pairwise constraints to construct an estimator of the decision boundary and we show that the resulting estimator is signinsensitive consistent with respect to the optimal linear decision boundary we also study the asymptotic variance of the estimator and extend the method to handle both labeled and pairwise examples in a natural way several experiments on simulated datasets and real world classification datasets are conducted the results not only verify the theoretical properties of the proposed method but also demonstrate its practical value in applications
maximum margin clustering made practical maximum margin clustering made practical maximum margin clustering mmc is a recent large margin unsupervised learning approach that has often outperformed conventional clustering methods computationally it involves nonconvex optimization and has to be relaxed to different semidefinite programs sdp however sdp solvers are computationally very expensive and only small data sets can be handled by mmc so far to make mmc more practical we avoid sdp relaxations and propose in this paper an efficient approach that performs alternating optimization directly on the original nonconvex problem a key step to avoid premature convergence is on the use of svr with the laplacian loss instead of svm with the hinge loss in the inner optimization subproblem experiments on a number of synthetic and realworld data sets demonstrate that the proposed approach is often more accurate much faster and can handle much larger data sets
nonlinear independent component analysis with minimal nonlinear distortion nonlinear independent component analysis with minimal nonlinear distortion nonlinear ica may not result in nonlinear blind source separation since solutions to nonlinear ica are highly nonunique in practice the nonlinearity in the data generation procedure is usually not strong thus it is reasonable to select the solution with the mixing procedure close to linear in this paper we propose to solve nonlinear ica with the minimal nonlinear distortion principle this is achieved by incorporating a regularization term to minimize the mean square error between the mixing mapping and the bestfitting linear one as an application the proposed method helps to identify linear nongaussian and acyclic causal models when mild nonlinearity exists in the data generation procedure using this method to separate daily returns of a set of stocks we successfully identify their linear causal relations the resulting causal relations give some interesting insights into the stock market
optimal dimensionality of metric space for classification optimal dimensionality of metric space for classification in many realworld applications euclidean distance in the original space is not good due to the curse of dimensionality in this paper we propose a new method called discriminant neighborhood embedding dne to learn an appropriate metric space for classification given finite training samples we define a discriminant adjacent matrix in favor of classification task ie neighboring samples in the same class are squeezed but those in different classes are separated as far as possible the optimal dimensionality of the metric space can be estimated by spectral analysis in the proposed method which is of great significance for highdimensional patterns experiments with various datasets demonstrate the effectiveness of our method
conditional random fields for multiagent reinforcement learning conditional random fields for multiagent reinforcement learning conditional random fields crfs are graphical models for modeling the probability of labels given the observations they have traditionally been trained with using a set of observation and label pairs underlying all crfs is the assumption that conditioned on the training data the labels are independent and identically distributed iid in this paper we explore the use of crfs in a class of temporal learning algorithms namely policygradient reinforcement learning rl now the labels are no longer iid they are actions that update the environment and affect the next observation from an rl point of view crfs provide a natural way to model joint actions in a decentralized markov decision process they define how agents can communicate with each other to choose the optimal joint action our experiments include a synthetic network alignment problem a distributed sensor network and road traffic control clearly outperforming rl methods which do not model the proper joint policy
spectral feature selection for supervised and unsupervised learning spectral feature selection for supervised and unsupervised learning feature selection aims to reduce dimensionality for building comprehensible learning models with good generalization performance feature selection algorithms are largely studied separately according to the type of learning supervised or unsupervised this work exploits intrinsic properties underlying supervised and unsupervised feature selection algorithms and proposes a unified framework for feature selection based on spectral graph theory the proposed framework is able to generate families of algorithms for both supervised and unsupervised feature selection and we show that existing powerful algorithms such as relieff supervised and laplacian score unsupervised are special cases of the proposed framework to the best of our knowledge this work is the first attempt to unify supervised and unsupervised feature selection and enable their joint study under a general framework experiments demonstrated the efficacy of the novel algorithms derived from the framework
spectral clustering and transductive learning with multiple views spectral clustering and transductive learning with multiple views we consider spectral clustering and transductive inference for data with multiple views a typical example is the web which can be described by either the hyperlinks between web pages or the words occurring in web pages when each view is represented as a graph one may convexly combine the weight matrices or the discrete laplacians for each graph and then proceed with existing clustering or classification techniques such a solution might sound natural but its underlying principle is not clear unlike this kind of methodology we develop multiview spectral clustering via generalizing the normalized cut from a single view to multiple views we further build multiview transductive inference on the basis of multiview spectral clustering our framework leads to a mixture of markov chains defined on every graph the experimental evaluation on realworld web classification demonstrates promising results that validate our method
on the relation between multiinstance learning and semisupervised learning on the relation between multiinstance learning and semisupervised learning multiinstance learning and semisupervised learning are different branches of machine learning the former attempts to learn from a training set consists of labeled bags each containing many unlabeled instances the latter tries to exploit abundant unlabeled instances when learning with a small number of labeled examples in this paper we establish a bridge between these two branches by showing that multiinstance learning can be viewed as a special case of semisupervised learning based on this recognition we propose the misssvm algorithm which addresses multiinstance learning using a special semisupervised support vector machine experiments show that solving multiinstance problems from the view of semisupervised learning is feasible and the misssvm algorithm is competitive with stateoftheart multiinstance learning algorithms
dynamic hierarchical markov random fields and their application to web data extraction dynamic hierarchical markov random fields and their application to web data extraction hierarchical models have been extensively studied in various domains however existing models assume fixed model structures or incorporate structural uncertainty generatively in this paper we propose dynamic hierarchical markov random fields dhmrfs to incorporate structural uncertainty in a discriminative manner dhmrfs consist of two parts structure model and class label model both are defined as exponential family distributions conditioned on observations dhmrfs relax the independence assumption as made in directed models as exact inference is intractable a variational method is developed to learn parameters and to find the map model structure and label assignment we apply the model to a realworld web data extraction task which automatically extracts product items for sale on the web the results show promise
transductive support vector machines for structured variables transductive support vector machines for structured variables we study the problem of learning kernel machines transductively for structured output variables transductive learning can be reduced to combinatorial optimization problems over all possible labelings of the unlabeled data in order to scale transductive learning to structured variables we transform the corresponding nonconvex combinatorial constrained optimization problems into continuous unconstrained optimization problems the discrete optimization parameters are eliminated and the resulting differentiable problems can be optimized efficiently we study the effectiveness of the generalized tsvm on multiclass classification and labelsequence learning problems empirically
multiclass multiple kernel learning multiclass multiple kernel learning in many applications it is desirable to learn from several kernels multiple kernel learning mkl allows the practitioner to optimize over linear combinations of kernels by enforcing sparse coefficients it also generalizes feature selection to kernel selection we propose mkl for joint feature maps this provides a convenient and principled way for mkl with multiclass problems in addition we can exploit the joint feature map to learn kernels on output spaces we show the equivalence of several different primal formulations including different regularizers we present several optimization methods and compare a convex quadratically constrained quadratic program qcqp and two semiinfinite linear programs silps on toy data showing that the silps are faster than the qcqp we then demonstrate the utility of our method by applying the silp to three real world datasets
gaussian process product models for nonparametric nonstationarity gaussian process product models for nonparametric nonstationarity stationarity is often an unrealistic prior assumption for gaussian process regression one solution is to predefine an explicit nonstationary covariance function but such covariance functions can be difficult to specify and require detailed prior knowledge of the nonstationarity we propose the gaussian process product model gppm which models data as the pointwise product of two latent gaussian processes to nonparametrically infer nonstationary variations of amplitude this approach differs from other nonparametric approaches to covariance function inference in that it operates on the outputs rather than the inputs resulting in a significant reduction in computational cost and required data for inference we present an approximate inference scheme using expectation propagation this variational approximation yields convenient gp hyperparameter selection and compact approximate predictive distributions
sequence kernels for predicting protein essentiality sequence kernels for predicting protein essentiality the problem of identifying the minimal gene set required to sustain life is of crucial importance in understanding cellular mechanisms and designing therapeutic drugs this work describes several kernelbased solutions for predicting essential genes that outperform existing models while using less training data our first solution is based on a semimanually designed kernel derived from the pfam database which includes several pfam domains we then present novel and general domainbased sequence kernels that capture sequence similarity with respect to several domains made of large sets of protein sequences we show how to deal with the large size of the problem several thousands of domains with individual domains sometimes containing thousands of sequences by representing and efficiently computing these kernels using automata we report results of extensive experiments demonstrating that they compare favorably with the pfam kernel in predicting protein essentiality while requiring no manual tuning
hierarchical kernel stickbreaking process for multitask image analysis hierarchical kernel stickbreaking process for multitask image analysis the kernel stickbreaking process ksbp is employed to segment general imagery imposing the condition that patches small blocks of pixels that are spatially proximate are more likely to be associated with the same cluster segment the number of clusters is not set a priori and is inferred from the hierarchical bayesian model further ksbp is integrated with a shared dirichlet process prior to simultaneously model multiple images inferring their interrelationships this latter application may be useful for sorting and learning relationships between multiple images the bayesian inference algorithm is based on a hybrid of variational bayesian analysis and local sampling in addition to providing details on the model and associated inference framework example results are presented for several imageanalysis problems
graph kernels between point clouds graph kernels between point clouds point clouds are sets of points in two or three dimensions most kernel methods for learning on sets of points have not yet dealt with the specific geometrical invariances and practical constraints associated with point clouds in computer vision and graphics in this paper we present extensions of graph kernels for point clouds which allow one to use kernel methods for such objects as shapes line drawings or any threedimensional point clouds in order to design rich and numerically efficient kernels with as few free parameters as possible we use kernels between covariance matrices and their factorizations on probabilistic graphical models we derive polynomial time dynamic programming recursions and present applications to recognition of handwritten digits and chinese characters from few training examples
bolasso bolasso we consider the leastsquare linear regression problem with regularization by the lnorm a problem usually referred to as the lasso in this paper we present a detailed asymptotic analysis of model consistency of the lasso for various decays of the regularization parameter we compute asymptotic equivalents of the probability of correct model selection ie variable selection for a specific rate decay we show that the lasso selects all the variables that should enter the model with probability tending to one exponentially fast while it selects all other variables with strictly positive probability we show that this property implies that if we run the lasso for several bootstrapped replications of a given sample then intersecting the supports of the lasso bootstrap estimates leads to consistent model selection this novel variable selection algorithm referred to as the bolasso is compared favorably to other linear regression methods on synthetic data and datasets from the uci machine learning repository
learning all optimal policies with multiple criteria learning all optimal policies with multiple criteria we describe an algorithm for learning in the presence of multiple criteria our technique generalizes previous approaches in that it can learn optimal policies for all linear preference assignments over the multiple reward criteria at once the algorithm can be viewed as an extension to standard reinforcement learning for mdps where instead of repeatedly backing up maximal expected rewards we back up the set of expected rewards that are maximal for some set of linear preferences given by a weight vector w we present the algorithm along with a proof of correctness showing that our solution gives the optimal policy for any linear preference function the solution reduces to the standard value iteration algorithm for a specific weight vector w
multiple instance ranking multiple instance ranking this paper introduces a novel machine learning model called multiple instance ranking mirank that enables ranking to be performed in a multiple instance learning setting the motivation for mirank stems from the hydrogen abstraction problem in computational chemistry that of predicting the group of hydrogen atoms from which a hydrogen is abstracted removed during metabolism the model predicts the preferred hydrogen group within a molecule by ranking the groups with the ambiguity of not knowing which hydrogen atom within the preferred group is actually abstracted this paper formulates mirank in its general context and proposes an algorithm for solving mirank problems using successive linear programming the method outperforms multiple instance classification models on several real and synthetic datasets
multitask learning for hiv therapy screening multitask learning for hiv therapy screening we address the problem of learning classifiers for a large number of tasks we derive a solution that produces resampling weights which match the pool of all examples to the target distribution of any given task our work is motivated by the problem of predicting the outcome of a therapy attempt for a patient who carries an hiv virus with a set of observed genetic properties such predictions need to be made for hundreds of possible combinations of drugs some of which use similar biochemical mechanisms multitask learning enables us to make predictions even for drug combinations with few or no training examples and substantially improves the overall prediction accuracy
nonnegative matrix factorization via rankone downdate nonnegative matrix factorization via rankone downdate nonnegative matrix factorization nmf was popularized as a tool for data mining by lee and seung in nmf attempts to approximate a matrix with nonnegative entries by a product of two lowrank matrices also with nonnegative entries we propose an algorithm called rankone downdate rd for computing an nmf that is partly motivated by the singular value decomposition this algorithm computes the dominant singular values and vectors of adaptively determined submatrices of a matrix on each iteration rd extracts a rankone submatrix from the original matrix according to an objective function we establish a theoretical result that maximizing this objective function corresponds to correctly classifying articles in a nearly separable corpus we also provide computational experiments showing the success of this method in identifying features in realistic datasets the method is also much faster than other nmf routines
strategy evaluation in extensive games with importance sampling strategy evaluation in extensive games with importance sampling typically agent evaluation is done through monte carlo estimation however stochastic agent decisions and stochastic outcomes can make this approach inefficient requiring many samples for an accurate estimate we present a new technique that can be used to simultaneously evaluate many strategies while playing a single strategy in the context of an extensive game this technique is based on importance sampling but utilizes two new mechanisms for significantly reducing variance in the estimates we demonstrate its effectiveness in the domain of poker where stochasticity makes traditional evaluation problematic
actively learning levelsets of composite functions actively learning levelsets of composite functions scientists frequently have multiple types of experiments and data sets on which they can test the validity of their parameterized models and locate plausible regions for the model parameters by examining multiple data sets scientists can obtain inferences which typically are much more informative than the deductions derived from each of the data sources independently several standard data combination techniques result in target functions which are a weighted sum of the observed data sources thus computing constraints on the plausible regions of the model parameter space can be formulated as finding a level set of a target function which is the sum of observable functions we propose an active learning algorithm for this problem which selects both a sample from the parameter space and an observable function upon which to compute the next sample empirical tests on synthetic functions and on real data for an eight parameter cosmological model show that our algorithm significantly reduces the number of samples required to identify the desired levelset
sparse bayesian nonparametric regression sparse bayesian nonparametric regression one of the most common problems in machine learning and statistics consists of estimating the mean response xbeta from a vector of observations y assuming y xbeta epsilon where x is known beta is a vector of parameters of interest and epsilon a vector of stochastic errors we are particularly interested here in the case where the dimension k of beta is much higher than the dimension of y we propose some flexible bayesian models which can yield sparse estimates of beta we show that as k rarr infin these models are closely related to a class of leacutevy processes simulations demonstrate that our models outperform significantly a range of popular alternatives
an empirical evaluation of supervised learning in high dimensions an empirical evaluation of supervised learning in high dimensions in this paper we perform an empirical evaluation of supervised learning on highdimensional data we evaluate performance on three metrics accuracy auc and squared loss and study the effect of increasing dimensionality on the performance of the learning algorithms our findings are consistent with previous studies for problems of relatively low dimension but suggest that as dimensionality increases the relative performance of the learning algorithms changes to our surprise the method that performs consistently well across all dimensions is random forests followed by neural nets boosted trees and svms
fast support vector machine training and classification on graphics processors fast support vector machine training and classification on graphics processors recent developments in programmable highly parallel graphics processing units gpus have enabled high performance implementations of machine learning algorithms we describe a solver for support vector machine training running on a gpu using the sequential minimal optimization algorithm and an adaptive first and second order working set selection heuristic which achieves speedups of x over libsvm running on a traditional processor we also present a gpubased system for svm classification which achieves speedups of x over libsvm x over our own cpu based svm classifier
fast nearest neighbor retrieval for bregman divergences fast nearest neighbor retrieval for bregman divergences we present a data structure enabling efficient nearest neighbor nn retrieval for bregman divergences the family of bregman divergences includes many popular dissimilarity measures including kldivergence relative entropy mahalanobis distance and itakurasaito divergence these divergences present a challenge for efficient nn retrieval because they are not in general metrics for which most nn data structures are designed the data structure introduced in this work shares the same basic structure as the popular metric ball tree but employs convexity properties of bregman divergences in place of the triangle inequality experiments demonstrate speedups over bruteforce search of up to several orders of magnitude
nearest hyperdisk methods for highdimensional classification nearest hyperdisk methods for highdimensional classification in highdimensional classification problems it is infeasible to include enough training samples to cover the class regions densely irregularities in the resulting sparse sample distributions cause local classifiers such as nearest neighbors nn and kernel methods to have irregular decision boundaries one solution is to fill in the holes by building a convex model of the region spanned by the training samples of each class and classifying examples based on their distances to these approximate models methods of this kind based on affine and convex hulls and bounding hyperspheres have already been studied here we propose a method based on the bounding hyperdisk of each class the intersection of the affine hull and the smallest bounding hypersphere of its training samples we argue that in many cases hyperdisks are preferable to affine and convex hulls and hyperspheres they bound the classes more tightly than affine hulls or hyperspheres while avoiding much of the sample overfitting and computational complexity that is inherent in highdimensional convex hulls we show that the hyperdisk method can be kernelized to provide nonlinear classifiers based on noneuclidean distance metrics experiments on several classification problems show promising results
learning to sportscast learning to sportscast we present a novel commentator system that learns language from sportscasts of simulated soccer games the system learns to parse and generate commentaries without any engineered knowledge about the english language training is done using only ambiguous supervision in the form of textual human commentaries and simulation states of the soccer games the system simultaneously tries to establish correspondences between the commentaries and the simulation states as well as build a translation model we also present a novel algorithm iterative generation strategy learning igsl for deciding which events to comment on human evaluations of the generated commentaries indicate they are of reasonable quality compared to human commentaries
training svm with indefinite kernels training svm with indefinite kernels similarity matrices generated from many applications may not be positive semidefinite and hence cant fit into the kernel machine framework in this paper we study the problem of training support vector machines with an indefinite kernel we consider a regularized svm formulation in which the indefinite kernel matrix is treated as a noisy observation of some unknown positive semidefinite one proxy kernel and the support vectors and the proxy kernel can be computed simultaneously we propose a semiinfinite quadratically constrained linear program formulation for the optimization which can be solved iteratively to find a global optimum solution we further propose to employ an additional pruning strategy which significantly improves the efficiency of the algorithm while retaining the convergence property of the algorithm in addition we show the close relationship between the proposed formulation and multiple kernel learning experiments on a collection of benchmark data sets demonstrate the efficiency and effectiveness of the proposed algorithm
learning for control from multiple demonstrations learning for control from multiple demonstrations we consider the problem of learning to follow a desired trajectory when given a small number of demonstrations from a suboptimal expert we present an algorithm that i extracts theinitially unknowndesired trajectory from the suboptimal experts demonstrations and ii learns a local model suitable for control along the learned trajectory we apply our algorithm to the problem of autonomous helicopter flight in all cases the autonomous helicopters performance exceeds that of our expert helicopter pilots demonstrations even stronger our results significantly extend the stateoftheart in autonomous helicopter aerobatics in particular our results include the first autonomous tictocs loops and hurricane vastly superior performance on previously performed aerobatic maneuvers such as inplace flips and rolls and a complete airshow which requires autonomous transitions between these and various other maneuvers
spectral clustering with inconsistent advice spectral clustering with inconsistent advice clustering with advice often known as constrained clustering has been a recent focus of the data mining community success has been achieved incorporating advice into the kmeans and spectral clustering frameworks although the theory community has explored inconsistent advice it has not yet been incorporated into spectral clustering extending work of de bie and cristianini we set out a framework for finding minimum normalised cuts subject to inconsistent advice
a unified architecture for natural language processing a unified architecture for natural language processing we describe a single convolutional neural network architecture that given a sentence outputs a host of language processing predictions partofspeech tags chunks named entity tags semantic roles semantically similar words and the likelihood that the sentence makes sense grammatically and semantically using a language model the entire network is trained jointly on all these tasks using weightsharing an instance of multitask learning all the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semisupervised learning for the shared tasks we show how both multitask learning and semisupervised learning improve the generalization of the shared tasks resulting in stateoftheartperformance
autonomous geometric precision error estimation in lowlevel computer vision tasks autonomous geometric precision error estimation in lowlevel computer vision tasks errors in mapmaking tasks using computer vision are sparse we demonstrate this by considering the construction of digital elevation models that employ stereo matching algorithms to triangulate realworld points this sparsity coupled with a geometric theory of errors recently developed by the authors allows for autonomous agents to calculate their own precision independently of ground truth we connect these developments with recent advances in the mathematics of sparse signal reconstruction or compressed sensing the theory presented here extends the autonomy of d model reconstructions discovered in the s to their errors
stability of transductive regression algorithms stability of transductive regression algorithms this paper uses the notion of algorithmic stability to derive novel generalization bounds for several families of transductive regression algorithms both by using convexity and closedform solutions our analysis helps compare the stability of these algorithms it suggests that several existing algorithms might not be stable but prescribes a technique to make them stable it also reports the results of experiments with local transductive regression demonstrating the benefit of our stability bounds for model selection in particular for determining the radius of the local neighborhood used by the algorithm
a ratedistortion oneclass model and its applications to clustering a ratedistortion oneclass model and its applications to clustering in oneclass classification we seek a rule to find a coherent subset of instances similar to a few positive examples in a large pool of instances the problem can be formulated and analyzed naturally in a ratedistortion framework leading to an efficient algorithm that compares well with two previous oneclass methods the model can be also be extended to remove background clutter in clustering to improve cluster purity
fast gaussian process methods for point process intensity estimation fast gaussian process methods for point process intensity estimation point processes are difficult to analyze because they provide only a sparse and noisy observation of the intensity function driving the process gaussian processes offer an attractive framework within which to infer underlying intensity functions the result of this inference is a continuous function defined across time that is typically more amenable to analytical efforts however a naive implementation will become computationally infeasible in any problem of reasonable size both in memory and run time requirements we demonstrate problem specific methods for a class of renewal processes that eliminate the memory burden and reduce the solve time by orders of magnitude
selftaught clustering selftaught clustering this paper focuses on a new clustering task called selftaught clustering selftaught clustering is an instance of unsupervised transfer learning which aims at clustering a small collection of target unlabeled data with the help of a large amount of auxiliary unlabeled data the target and auxiliary data can be different in topic distribution we show that even when the target data are not sufficient to allow effective learning of a high quality feature representation it is possible to learn the useful features with the help of the auxiliary data on which the target data can be clustered effectively we propose a coclustering based selftaught clustering algorithm to tackle this problem by clustering the target and auxiliary data simultaneously to allow the feature representation from the auxiliary data to influence the target data through a common set of features under the new data representation clustering on the target data can be improved our experiments on image clustering show that our algorithm can greatly outperform several stateoftheart clustering methods when utilizing irrelevant unlabeled auxiliary data
hierarchical sampling for active learning hierarchical sampling for active learning we present an active learning scheme that exploits cluster structure in data
learning to classify with missing and corrupted features learning to classify with missing and corrupted features after a classifier is trained using a machine learning algorithm and put to use in a real world system it often faces noise which did not appear in the training data particularly some subset of features may be missing or may become corrupted we present two novel machine learning techniques that are robust to this type of classificationtime noise first we solve an approximation to the learning problem using linear programming we analyze the tightness of our approximation and prove statistical risk bounds for this approach second we define the onlinelearning variant of our problem address this variant using a modified perceptron and obtain a statistical learning algorithm using an onlinetobatch technique we conclude with a set of experiments that demonstrate the effectiveness of our algorithms
maximum likelihood rule ensembles maximum likelihood rule ensembles we propose a new rule induction algorithm for solving classification problems via probability estimation the main advantage of decision rules is their simplicity and good interpretability while the early approaches to rule induction were based on sequential covering we follow an approach in which a single decision rule is treated as a base classifier in an ensemble the ensemble is built by greedily minimizing the negative loglikelihood which results in estimating the class conditional probability distribution the introduced approach is compared with other decision rule induction algorithms such as slipper lri and rulefit
learning from incomplete data with infinite imputations learning from incomplete data with infinite imputations we address the problem of learning decision functions from training data in which some attribute values are unobserved this problem can arise for instance when training data is aggregated from multiple sources and some sources record only a subset of attributes we derive a generic joint optimization problem in which the distribution governing the missing values is a free parameter we show that the optimal solution concentrates the density mass on finitely many imputations and provide a corresponding algorithm for learning from incomplete data we report on empirical results on benchmark data and on the email spam application that motivates our work
an objectoriented representation for efficient reinforcement learning an objectoriented representation for efficient reinforcement learning rich representations in reinforcement learning have been studied for the purpose of enabling generalization and making learning feasible in large state spaces we introduce objectoriented mdps oomdps a representation based on objects and their interactions which is a natural way of modeling environments and offers important generalization opportunities we introduce a learning algorithm for deterministic oomdps and prove a polynomial bound on its sample complexity we illustrate the performance gains of our representation and algorithm in the wellknown taxi domain plus a reallife videogame
optimizing estimated loss reduction for active sampling in rank learning optimizing estimated loss reduction for active sampling in rank learning learning to rank is becoming an increasingly popular research area in machine learning the ranking problem aims to induce an ordering or preference relations among a set of instances in the input space however collecting labeled data is growing into a burden in many rank applications since labeling requires eliciting the relative ordering over the set of alternatives in this paper we propose a novel active learning framework for svmbased and boostingbased rank learning our approach suggests sampling based on maximizing the estimated loss differential over unlabeled data experimental results on two benchmark corpora show that the proposed model substantially reduces the labeling effort and achieves superior performance rapidly with as much as relative improvement over the marginbased sampling baseline
reinforcement learning with limited reinforcement reinforcement learning with limited reinforcement partially observable markov decision processes pomdps have succeeded in planning domains that require balancing actions that increase an agents knowledge and actions that increase an agents reward unfortunately most pomdps are defined with a large number of parameters which are difficult to specify only from domain knowledge in this paper we present an approximation approach that allows us to treat the pomdp model parameters as additional hidden state in a modeluncertainty pomdp coupled with modeldirected queries our planner actively learns good policies we demonstrate our approach on several pomdp problems
confidenceweighted linear classification confidenceweighted linear classification we introduce confidenceweighted linear classifiers which add parameter confidence information to linear classifiers online learners in this setting update both classifier parameters and the estimate of their confidence the particular online algorithms we study here maintain a gaussian distribution over parameter vectors and update the mean and covariance of the distribution with each instance empirical evaluation on a range of nlp tasks show that our algorithm improves over other state of the art online and batch methods learns faster in the online setting and lends itself to better classifier combination after parallel training
efficient projections onto the lball for learning in high dimensions efficient projections onto the lball for learning in high dimensions we describe efficient algorithms for projecting a vector onto the lball we present two methods for projection the first performs exact projection in on expected time where n is the dimension of the space the second works on vectors k of whose elements are perturbed outside the lball projecting in ok logn time this setting is especially useful for online learning in sparse feature spaces such as text categorization applications we demonstrate the merits and effectiveness of our algorithms in numerous batch and online learning tasks we show that variants of stochastic gradient projection methods augmented with our efficient projection procedures outperform interior point methods which are considered stateoftheart optimization techniques we also show that in online settings gradient updates with l projections outperform the exponentiated gradient algorithm while obtaining models with high degrees of sparsity
pointwise exact bootstrap distributions of cost curves pointwise exact bootstrap distributions of cost curves cost curves have recently been introduced as an alternative or complement to roc curves in order to visualize binary classifiers performance of importance to both cost and roc curves is the computation of confidence intervals along with the curves themselves so that the reliability of a classifiers performance can be assessed computing confidence intervals for the difference in performance between two classifiers allows the determination of whether one classifier performs significantly better than another a simple procedure to obtain confidence intervals for costs or the difference between two costs under various operating conditions is to perform bootstrap resampling of the test set in this paper we derive exact bootstrap distributions for these values and use these dstributions to obtain confidence intervals under various operating conditions performances of these confidence intervals are measured in terms of coverage accuracies simulations show excellent results
polyhedral classifier for target detection polyhedral classifier for target detection in this study we introduce a novel algorithm for learning a polyhedron to describe the target class the proposed approach takes advantage of the limited subclass information made available for the negative samples and jointly optimizes multiple hyperplane classifiers each of which is designed to classify positive samples from a subclass of the negative samples the flat faces of the polyhedron provides robustness whereas multiple faces contributes to the flexibility required to deal with complex datasets apart from improving the prediction accuracy of the system the proposed polyhedral classifier also provides runtime speedups as a byproduct when executed in a cascaded framework in realtime we evaluate the performance of the proposed technique on a realworld colon dataset both in terms of prediction accuracy and online execution speed
active reinforcement learning active reinforcement learning when the transition probabilities and rewards of a markov decision process mdp are known an agent can obtain the optimal policy without any interaction with the environment however exact transition probabilities are difficult for experts to specify one option left to an agent is a long and potentially costly exploration of the environment in this paper we propose another alternative given initial possibly inaccurate specification of the mdp the agent determines the sensitivity of the optimal policy to changes in transitions and rewards it then focuses its exploration on the regions of space to which the optimal policy is most sensitive we show that the proposed exploration strategy performs well on several control and planning problems
training structural svms when exact inference is intractable training structural svms when exact inference is intractable while discriminative training eg crf structural svm holds much promise for machine translation image segmentation and clustering the complex inference these applications require make exact training intractable this leads to a need for approximate training methods unfortunately knowledge about how to perform efficient and effective approximate training is limited focusing on structural svms we provide and explore algorithms for two different classes of approximate training algorithms which we call undergenerating eg greedy and overgenerating eg relaxations algorithms we provide a theoretical and empirical analysis of both types of approximate trained structural svms focusing on fully connected pairwise markov random fields we find that models trained with overgenerating methods have theoretic advantages over undergenerating methods are empirically robust relative to their undergenerating brethren and relaxed trained models favor nonfractional predictions from relaxed predictors
an hdphmm for systems with state persistence an hdphmm for systems with state persistence the hierarchical dirichlet process hidden markov model hdphmm is a flexible nonparametric model which allows state spaces of unknown size to be learned from data we demonstrate some limitations of the original hdphmm formulation teh et al and propose a sticky extension which allows more robust learning of smoothly varying dynamics using dp mixtures this formulation also allows learning of more complex multimodal emission distributions we further develop a sampling algorithm that employs a truncated approximation of the dp to jointly resample the full state sequence greatly improving mixing rates via extensive experiments with synthetic data and the nist speaker diarization database we demonstrate the advantages of our sticky extension and the utility of the hdphmm in realworld applications
optimized cutting plane algorithm for support vector machines optimized cutting plane algorithm for support vector machines we have developed a new linear support vector machine svm training algorithm called ocas its computational effort scales linearly with the sample size in an extensive empirical evaluation ocas significantly outperforms current state of the art svm solvers like svmlight svmperf and bmrm achieving speedups of over on some datasets over svmlight and over svmperf while obtaining the same precise support vector solution ocas even in the early optimization steps shows often faster convergence than the so far in this domain prevailing approximative methods sgd and pegasos effectively parallelizing ocas we were able to train on a dataset of size million examples itself about gb in size in just seconds a competing string kernel svm required seconds to train on million examples subsampled from this dataset
stopping conditions for exact computation of leaveoneout error in support vector machines stopping conditions for exact computation of leaveoneout error in support vector machines we propose a new stopping condition for a support vector machine svm solver which precisely reflects the objective of the leaveoneout error computation the stopping condition guarantees that the output on an intermediate svm solution is identical to the output of the optimal svm solution with one data point excluded from the training set a simple augmentation of a general svm training algorithm allows one to use a stopping criterion equivalent to the proposed sufficient condition a comprehensive experimental evaluation of our method shows consistent speedup of the exact loo computation by our method up to the factor of for the linear kernel the new algorithm can be seen as an example of constructive guidance of an optimization algorithm towards achieving the best attainable expected risk at optimal computational cost
reinforcement learning in the presence of rare events reinforcement learning in the presence of rare events we consider the task of reinforcement learning in an environment in which rare significant events occur independently of the actions selected by the controlling agent if these events are sampled according to their natural probability of occurring convergence of conventional reinforcement learning algorithms is likely to be slow and the learning algorithms may exhibit high variance in this work we assume that we have access to a simulator in which the rare event probabilities can be artificially altered then importance sampling can be used to learn with this simulation data we introduce algorithms for policy evaluation using both tabular and function approximation representations of the value function we prove that in both cases the reinforcement learning algorithms converge in the tabular case we also analyze the bias and variance of our approach compared to tdlearning we evaluate empirically the performance of the algorithm on random markov decision processes as well as on a large network planning task
memory bounded inference in topic models memory bounded inference in topic models what type of algorithms and statistical techniques support learning from very large datasets over long stretches of time we address this question through a memory bounded version of a variational em algorithm that approximates inference in a topic model the algorithm alternates two phases model building and model compression in order to always satisfy a given memory constraint the model building phase expands its internal representation the number of topics as more data arrives through bayesian model selection compression is achieved by merging dataitems in clumps and only caching their sufficient statistics empirically the resulting algorithm is able to handle datasets that are orders of magnitude larger than the standard batch version
localized multiple kernel learning localized multiple kernel learning recently instead of selecting a single kernel multiple kernel learning mkl has been proposed which uses a convex combination of kernels where the weight of each kernel is optimized during training however mkl assigns the same weight to a kernel over the whole input space in this paper we develop a localized multiple kernel learning lmkl algorithm using a gating model for selecting the appropriate kernel function locally the localizing gating model and the kernelbased classifier are coupled and their optimization is done in a joint manner empirical results on ten benchmark and two bioinformatics data sets validate the applicability of our approach lmkl achieves statistically similar accuracy results compared with mkl by storing fewer support vectors lmkl can also combine multiple copies of the same kernel function localized in different parts for example lmkl with multiple linear kernels gives better accuracy results than using a single linear kernel on bioinformatics data sets
noregret learning in convex games noregret learning in convex games quite a bit is known about minimizing different kinds of regret in experts problems and how these regret types relate to types of equilibria in the multiagent setting of repeated matrix games much less is known about the possible kinds of regret in online convex programming problems ocps or about equilibria in the analogous multiagent setting of repeated convex games this gap is unfortunate since convex games are much more expressive than matrix games and since many important machine learning problems can be expressed as ocps in this paper we work to close this gap we analyze a spectrum of regret types which lie between external and swap regret along with their corresponding equilibria which lie between coarse correlated and correlated equilibrium we also analyze algorithms for minimizing these regret types as examples of our framework we derive algorithms for learning correlated equilibria in polyhedral convex games and extensiveform correlated equilibria in extensiveform games the former is exponentially more efficient than previous algorithms and the latter is the first of its type
boosting with incomplete information boosting with incomplete information in realworld machine learning problems it is very common that part of the input feature vector is incomplete either not available missing or corrupted in this paper we present a boosting approach that integrates features with incomplete information and those with complete information to form a strong classifier by introducing hidden variables to model missing information we form loss functions that combine fully labeled data with partially labeled data to effectively learn normalized and unnormalized models the primal problems of the proposed optimization problems with these loss functions are provided to show their close relationship and the motivations behind them we use auxiliary functions to bound the change of the loss functions and derive explicit parameter update rules for the learning algorithms we demonstrate encouraging results on two realworld problems visual object recognition in computer vision and named entity recognition in natural language processing to show the effectiveness of the proposed boosting approach
grassmann discriminant analysis grassmann discriminant analysis in this paper we propose a discriminant learning framework for problems in which data consist of linear subspaces instead of vectors by treating subspaces as basic elements we can make learning algorithms adapt naturally to the problems with linear invariant structures we propose a unifying view on the subspacebased learning method by formulating the problems on the grassmann manifold which is the set of fixeddimensional linear subspaces of a euclidean space previous methods on the problem typically adopt an inconsistent strategy feature extraction is performed in the euclidean space while noneuclidean distances are used in our approach we treat each subspace as a point in the grassmann space and perform feature extraction and classification in the same space we show feasibility of the approach by using the grassmann kernel functions such as the projection kernel and the binetcauchy kernel experiments with real image databases show that the proposed method performs well compared with stateoftheart algorithms
modified mmimpe modified mmimpe in this paper we show how common speech recognition training criteria such as the minimum phone error criterion or the maximum mutual information criterion can be extended to incorporate a margin term different marginbased training algorithms have been proposed to refine existing training algorithms for general machine learning problems however for speech recognition some special problems have to be addressed and all approaches proposed either lack practical applicability or the inclusion of a margin term enforces significant changes to the underlying model eg the optimization algorithm the loss function or the parameterization of the model in our approach the conventional training criteria are modified to incorporate a margin term this allows us to do largemargin training in speech recognition using the same efficient algorithms for accumulation and optimization and to use the same software as for conventional discriminative training we show that the proposed criteria are equivalent to support vector machines with suitable smooth loss functions approximating the nonsmooth hinge loss function or the hard error eg phone error experimental results are given for two different tasks the rather simple digit string recognition task sietill which severely suffers from overfitting and the large vocabulary european parliament plenary sessions english task which is supposed to be dominated by the risk and the generalization does not seem to be such an issue
statistical models for partial membership statistical models for partial membership we present a principled bayesian framework for modeling partial memberships of data points to clusters unlike a standard mixture model which assumes that each data point belongs to one and only one mixture component or cluster a partial membership model allows data points to have fractional membership in multiple clusters algorithms which assign data points partial memberships to clusters can be useful for tasks such as clustering genes based on microarray data gasch eisen our bayesian partial membership model bpm uses exponential family distributions to model each cluster and a product of these distibtutions with weighted parameters to model each datapoint here the weights correspond to the degree to which the datapoint belongs to each cluster all parameters in the bpm are continuous so we can use hybrid monte carlo to perform inference and learning we discuss relationships between the bpm and latent dirichlet allocation mixed membership models exponential family pca and fuzzy clustering lastly we show some experimental results and discuss nonparametric extensions to our model
active kernel learning active kernel learning identifying the appropriate kernel functionmatrix for a given dataset is essential to all kernelbased learning techniques a number of kernel learning algorithms have been proposed to learn kernel functions or matrices from side information eg either labeled examples or pairwise constraints however most previous studies are limited to passive kernel learning in which side information is provided beforehand in this paper we present a framework of active kernel learning akl that actively identifies the most informative pairwise constraints for kernel learning the key challenge of active kernel learning is how to measure the informativeness of an example pair given its class label is unknown to this end we propose a minmax approach for active kernel learning that selects the example pair that results in a large classification margin regardless of its assigned class label we furthermore approximate the related optimization problem into a convex programming problem we evaluate the effectiveness of the proposed algorithm by comparing it to two other implementations of active kernel learning empirical study with nine datasets on semisupervised data clustering shows that the proposed algorithm is more effective than its competitors
a dual coordinate descent method for largescale linear svm a dual coordinate descent method for largescale linear svm in many applications data appear with a huge number of instances as well as features linear support vector machines svm is one of the most popular tools to deal with such largescale sparse data this paper presents a novel dual coordinate descent method for linear svm with land lloss functions the proposed method is simple and reaches an epsilonaccurate solution in ologepsilon iterations experiments indicate that our method is much faster than state of the art solvers such as pegasos tron svmperf and a recent primal coordinate descent implementation
discriminative structure and parameter learning for markov logic networks discriminative structure and parameter learning for markov logic networks markov logic networks mlns are an expressive representation for statistical relational learning that generalizes both firstorder logic and graphical models existing methods for learning the logical structure of an mln are not discriminative however many relational learning problems involve specific target predicates that must be inferred from given background information we found that existing mln methods perform very poorly on several such ilp benchmark problems and we present improved discriminative methods for learning mln clauses and weights that outperform existing mln and traditional ilp methods
causal modelling combining instantaneous and lagged effects causal modelling combining instantaneous and lagged effects causal analysis of continuousvalued variables typically uses either autoregressive models or linear gaussian bayesian networks with instantaneous effects estimation of gaussian bayesian networks poses serious identifiability problems which is why it was recently proposed to use nongaussian models here we show how to combine the nongaussian instantaneous model with autoregressive models we show that such a nongaussian model is identifiable without prior knowledge of network structure and we propose an estimation method shown to be consistent this approach also points out how neglecting instantaneous effects can lead to completely wrong estimates of the autoregressive coefficients
efficient bandit algorithms for online multiclass prediction efficient bandit algorithms for online multiclass prediction this paper introduces the banditron a variant of the perceptron rosenblatt for the multiclass bandit setting the multiclass bandit setting models a wide range of practical supervised learning applications where the learner only receives partial feedback referred to as bandit feedback in the spirit of multiarmed bandit models with respect to the true label eg in many web applications users often only provide positive click feedback which does not necessarily fully disclose a true label the banditron has the ability to learn in a multiclass classification setting with the bandit feedback which only reveals whether or not the prediction made by the algorithm was correct or not but does not necessarily reveal the true label we provide relative mistake bounds which show how the banditron enjoys favorable performance and our experiments demonstrate the practicality of the algorithm furthermore this paper pays close attention to the important special case when the data is linearly separable a problem which has been exhaustively studied in the full information setting yet is novel in the bandit setting
large scale manifold transduction large scale manifold transduction we show how the regularizer of transductive support vector machines tsvm can be trained by stochastic gradient descent for linear models and multilayer architectures the resulting methods can be trained online have vastly superior training and testing speed to existing tsvm algorithms can encode prior knowledge in the network architecture and obtain competitive error rates we then go on to propose a natural generalization of the tsvm loss function that takes into account neighborhood and manifold information directly unifying the twostage low density separation method into a single criterion and leading to stateoftheart results
nonparametric policy gradients nonparametric policy gradients policy gradient approaches are a powerful instrument for learning how to interact with the environment existing approaches have focused on propositional and continuous domains only without extensive feature engineering it is difficult if not impossible to apply them within structured domains in which eg there is a varying number of objects and relations among them in this paper we describe a nonparametric policy gradient approach called nppg that overcomes this limitation the key idea is to apply friedmanns gradient boosting policies are represented as a weighted sum of regression models grown in an stagewise optimization employing offtheshelf regression learners nppg can deal with propositional continuous and relational domains in a unified way our experimental results show that it can even improve on established results
ica and isa using schweizerwolff measure of dependence ica and isa using schweizerwolff measure of dependence we propose a new algorithm for independent component and independent subspace analysis problems this algorithm uses a contrast based on the schweizerwolff measure of pairwise dependence schweizer wolff a nonparametric measure computed on pairwise ranks of the variables our algorithm frequently outperforms state of the art ica methods in the normal setting is significantly more robust to outliers in the mixed signals and performs well even in the presence of noise our method can also be used to solve independent subspace analysis isa problems by grouping signals recovered by ica methods we provide an extensive empirical evaluation using simulated sound and image data
unsupervised rank aggregation with distancebased models unsupervised rank aggregation with distancebased models the need to meaningfully combine sets of rankings often comes up when one deals with ranked data although a number of heuristic and supervised learning approaches to rank aggregation exist they require domain knowledge or supervised ranked data both of which are expensive to acquire in order to address these limitations we propose a mathematical and algorithmic framework for learning to aggregate partial rankings without supervision we instantiate the framework for the cases of combining permutations and combining topk lists and propose a novel metric for the latter experiments in both scenarios demonstrate the effectiveness of the proposed formalism
on partial optimality in multilabel mrfs on partial optimality in multilabel mrfs we consider the problem of optimizing multilabel mrfs which is in general nphard and ubiquitous in lowlevel computer vision one approach for its solution is to formulate it as an integer linear programming and relax the integrality constraints the approach we consider in this paper is to first convert the multilabel mrf into an equivalent binarylabel mrf and then to relax it the resulting relaxation can be efficiently solved using a maximum flow algorithm its solution provides us with a partially optimal labelling of the binary variables this partial labelling is then easily transferred to the multilabel problem we study the theoretical properties of the new relaxation and compare it with the standard one specifically we compare tightness and characterize a subclass of problems where the two relaxations coincide we propose several combined algorithms based on the technique and demonstrate their performance on challenging computer vision problems
spaceindexed dynamic programming spaceindexed dynamic programming we consider the task of learning to accurately follow a trajectory in a vehicle such as a car or helicopter a number of dynamic programming algorithms such as differential dynamic programming ddp and policy search by dynamic programming psdp can efficiently compute nonstationary policies for these tasks such policies in general are wellsuited to trajectory following since they can easily generate different control actions at different times in order to follow the trajectory however a weakness of these algorithms is that their policies are timeindexed in that they apply different policies depending on the current time this is problematic since the current time may not correspond well to where we are along the trajectory and the uncertainty over states can prevent these algorithms from finding any good policies at all in this paper we propose a method for spaceindexed dynamic programming that overcomes both these difficulties we begin by showing how a dynamical system can be rewritten in terms of a spatial index variable ie how far along the trajectory we are rather than as a function of time we then use these spaceindexed dynamical systems to derive spaceindexed version of the ddp and psdp algorithms finally we show that these algorithms perform well on a variety of control tasks both in simulation and on real systems
the skew spectrum of graphs the skew spectrum of graphs the central issue in representing graphstructured data instances in learning algorithms is designing features which are invariant to permuting the numbering of the vertices we present a new system of invariant graph features which we call the skew spectrum of graphs the skew spectrum is based on mapping the adjacency matrix of any weigted directed unlabeled graph to a function on the symmetric group and computing bispectral invariants the reduced form of the skew spectrum is computable in on time and experiments show that on several benchmark datasets it can outperform state of the art graph kernels
fast estimation of firstorder clause coverage through randomization and maximum likelihood fast estimation of firstorder clause coverage through randomization and maximum likelihood in inductive logic programming thetasubsumption is a widely used coverage test unfortunately testing thetasubsumption is npcomplete which represents a crucial efficiency bottleneck for many relational learners in this paper we present a probabilistic estimator of clause coverage based on a randomized restarted search strategy under a distribution assumption our algorithm can estimate clause coverage without having to decide subsumption for all examples we implement this algorithm in program recover on generated graph data and realworld datasets we show that recover provides reasonably accurate estimates while achieving dramatic runtimes improvements compared to a stateoftheart algorithm
querylevel stability and generalization in learning to rank querylevel stability and generalization in learning to rank this paper is concerned with the generalization ability of learning to rank algorithms for information retrieval ir we point out that the key for addressing the learning problem is to look at it from the viewpoint of query we define a number of new concepts including querylevel loss querylevel risk and querylevel stability we then analyze the generalization ability of learning to rank algorithms by giving querylevel generalization bounds to them using querylevel stability as a tool such an analysis is very helpful for us to derive more advanced algorithms for ir we apply the proposed theory to the existing algorithms of ranking svm and irsvm experimental results on the two algorithms verify the correctness of the theoretical analysis
modeling interleaved hidden processes modeling interleaved hidden processes hidden markov models assume that observations in time series data stem from some hidden process that can be compactly represented as a markov chain we generalize this model by assuming that the observed data stems from multiple hidden processes whose outputs interleave to form the sequence of observations exact inference in this model is nphard however a tractable and effective inference algorithm is obtained by extending structured approximate inference methods used in factorial hidden markov models the proposed model is evaluated in an activity recognition domain where multiple activities interleave and together generate a stream of sensor observations it is shown to be more accurate than a standard hidden markov model in this domain
exploration scavenging exploration scavenging we examine the problem of evaluating a policy in the contextual bandit setting using only observations collected during the execution of another policy we show that policy evaluation can be impossible if the exploration policy chooses actions based on the side information provided at each time step we then propose and prove the correctness of a principled method for policy evaluation which works when this is not the case even when the exploration policy is deterministic as long as each action is explored sufficiently often we apply this general technique to the problem of offline evaluation of internet advertising policies although our theoretical results hold only when the exploration policy chooses ads independent of side information an assumption that is typically violated by commercial systems we show how clever uses of the theory provide nontrivial and realistic applications we also provide an empirical demonstration of the effectiveness of our techniques on real ad placement data
classification using discriminative restricted boltzmann machines classification using discriminative restricted boltzmann machines recently many applications for restricted boltzmann machines rbms have been developed for a large variety of learning problems however rbms are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feedforward neural network classifiers and are not considered as a standalone solution to classification problems in this paper we argue that rbms provide a selfcontained framework for deriving competitive nonlinear classifiers we present an evaluation of different learning algorithms for rbms which aim at introducing a discriminative component to rbm training and improve their performance as classifiers this approach is simple in that rbms are used directly to build a classifier rather than as a stepping stone finally we demonstrate how discriminative rbms can also be successfully employed in a semisupervised setting
transfer of samples in batch reinforcement learning transfer of samples in batch reinforcement learning the main objective of transfer in reinforcement learning is to reduce the complexity of learning the solution of a target task by effectively reusing the knowledge retained from solving a set of source tasks in this paper we introduce a novel algorithm that transfers samples ie tuples langs a s rrang from source to target tasks under the assumption that tasks have similar transition models and reward functions we propose a method to select samples from the source tasks that are mostly similar to the target task and then to use them as input for batch reinforcementlearning algorithms as a result the number of samples an agent needs to collect from the target task to learn its solution is reduced we empirically show that following the proposed approach the transfer of samples is effective in reducing the learning complexity even when some source tasks are significantly different from the target task
local likelihood modeling of temporal text streams local likelihood modeling of temporal text streams temporal text data is often generated by a timechanging process or distribution such a drift in the underlying distribution cannot be captured by stationary likelihood techniques we consider the application of local likelihood methods to generative and conditional modeling of temporal document sequences we examine the asymptotic bias and variance and present an experimental study using the rcv dataset containing a temporal sequence of reuters news stories
a worstcase comparison between temporal difference and residual gradient with linear function approximation a worstcase comparison between temporal difference and residual gradient with linear function approximation residual gradient rg was proposed as an alternative to td for policy evaluation when function approximation is used but there exists little formal analysis comparing them except in very limited cases this paper employs techniques from online learning of linear functions and provides a worstcase nonprobabilistic analysis to compare these two types of algorithms when linear function approximation is used no statistical assumptions are made on the sequence of observations so the analysis applies to nonmarkovian and even adversarial domains as well in particular our results suggest that rg may result in smaller temporal differences while td is more likely to yield smaller prediction errors these phenomena can be observed even in two simple markov chain examples that are nonadversarial
knows what it knows knows what it knows we introduce a learning framework that combines elements of the wellknown pac and mistakebound models the kwik knows what it knows framework was designed particularly for its utility in learning settings where active exploration can impact the training examples the learner is exposed to as is true in reinforcementlearning and activelearning problems we catalog several kwiklearnable classes and open problems
pairwise constraint propagation by semidefinite programming for semisupervised classification pairwise constraint propagation by semidefinite programming for semisupervised classification we consider the general problem of learning from both pairwise constraints and unlabeled data the pairwise constraints specify whether two objects belong to the same class or not known as the mustlink constraints and the cannotlink constraints we propose to learn a mapping that is smooth over the data graph and maps the data onto a unit hypersphere where two mustlink objects are mapped to the same point while two cannotlink objects are mapped to be orthogonal we show that such a mapping can be achieved by formulating a semidefinite programming problem which is convex and can be solved globally our approach can effectively propagate pairwise constraints to the whole data set it can be directly applied to multiclass classification and can handle data labels pairwise constraints or a mixture of them in a unified framework promising experimental results are presented for classification tasks on a variety of synthetic and real data sets
an asymptotic analysis of generative discriminative and pseudolikelihood estimators an asymptotic analysis of generative discriminative and pseudolikelihood estimators statistical and computational concerns have motivated parameter estimators based on various forms of likelihood eg joint conditional and pseudolikelihood in this paper we present a unified framework for studying these estimators which allows us to compare their relative statistical efficiencies our asymptotic analysis suggests that modeling more of the data tends to reduce variance but at the cost of being more sensitive to model misspecification we present experiments validating our analysis
structure compilation structure compilation structured models often achieve excellent performance but can be slow at test time we investigate structure compilation where we replace structure with features which are often computationally simpler but unfortunately statistically more complex we analyze this tradeoff theoretically and empirically on three natural language processing tasks we also introduce a simple method to transfer predictive power from structure to features via unlabeled data while incurring a minimal statistical penalty
manifoldboost manifoldboost we describe a manifold learning framewor that naturally accommodates supervised learning partially supervised learning and unsupervised clustering as particular cases our method chooses a function by minimizing loss subject to a manifold regularization penalty this augmented cost is minimized using a greedy stagewise functional minimization procedure as in gradientboost each stage of boosting is fast and efficient we demonstrate our approach using both radial basis function approximations and trees the performance of our method is at the state of the art on many standard semisupervised learning benchmarks and we produce results for large scale datasets
random classification noise defeats all convex potential boosters random classification noise defeats all convex potential boosters a broad class of boosting algorithms can be interpreted as performing coordinatewise gradient descent to minimize some potential function of the margins of a data set this class includes adaboost logitboost and other widely used and wellstudied boosters in this paper we show that for a broad class of convex potential functions any such boosting algorithm is highly susceptible to random classification noise we do this by showing that for any such booster and any nonzero random classification noise rate eta there is a simple data set of examples which is efficiently learnable by such a booster if there is no noise but which cannot be learned to accuracy better than if there is random classification noise at rate eta this negative result is in contrast with known branching program based boosters which do not fall into the convex potential function framework and which can provably learn to high accuracy in the presence of random classification noise
uncorrelated multilinear principal component analysis through successive variance maximization uncorrelated multilinear principal component analysis through successive variance maximization tensorial data are frequently encountered in various machine learning tasks today and dimensionality reduction is one of their most important applications this paper extends the classical principal component analysis pca to its multilinear version by proposing a novel unsupervised dimensionality reduction algorithm for tensorial data named as uncorrelated multilinear pca umpca umpca seeks a tensortovector projection that captures most of the variation in the original tensorial input while producing uncorrelated features through successive variance maximization we evaluate the umpca on a secondorder tensorial problem face recognition and the experimental results show its superiority especially in lowdimensional spaces through the comparison with three other pcabased algorithms
a reproducing kernel hilbert space framework for pairwise time series distances a reproducing kernel hilbert space framework for pairwise time series distances a good distance measure for time series needs to properly incorporate the temporal structure and should be applicable to sequences with unequal lengths in this paper we propose a distance measure as a principled solution to the two requirements unlike the conventional feature vector representation our approach represents each time series with a summarizing smooth curve in a reproducing kernel hilbert space rkhs and therefore translate the distance between time series into distances between curves moreover we propose to learn the kernel of this rkhs from a population of time series with discrete observations using gaussian processbased nonparametric mixedeffect models experiments on two vastly different realworld problems show that the proposed distance measure leads to improved classification accuracy over the conventional distance measures
online discovery of temporaldifference networks online discovery of temporaldifference networks we present an algorithm for online incremental discovery of temporaldifference td networks the key contribution is the establishment of three criteria to expand a node in td network a node is expanded when the node is wellknown independent and has a prediction error that requires further explanation since none of these criteria requires centralized calculation operations they are easily computed in a parallel and distributed manner and scalable for bigger problems compared to other discovery methods of predictive state representations through computer experiments we demonstrate the empirical effectiveness of our algorithm
nonextensive entropic kernels nonextensive entropic kernels positive definite kernels on probability measures have been recently applied in structured data classification problems some of these kernels are related to classic information theoretic quantities such as mutual information and the jensenshannon divergence meanwhile driven by recent advances in tsallis statistics nonextensive generalizations of shannons information theory have been proposed this paper bridges these two trends we introduce the jensentsallis qdifference a generalization of the jensenshannon divergence we then define a new family of nonextensive mutual information kernels which allow weights to be assigned to their arguments and which includes the boolean jensenshannon and linear kernels as particular cases we illustrate the performance of these kernels on text categorization tasks
automatic discovery and transfer of maxq hierarchies automatic discovery and transfer of maxq hierarchies we present an algorithm himat hierarchy induction via models and trajectories that discovers maxq task hierarchies by applying dynamic bayesian network models to a successful trajectory from a source reinforcement learning task himat discovers subtasks by analyzing the causal and temporal relationships among the actions in the trajectory under appropriate assumptions himat induces hierarchies that are consistent with the observed trajectory and have compact valuefunction tables employing safe state abstractions we demonstrate empirically that himat constructs compact hierarchies that are comparable to manuallyengineered hierarchies and facilitate significant speedup in learning when transferred to a target task
rank minimization via online learning rank minimization via online learning minimum rank problems arise frequently in machine learning applications and are notoriously difficult to solve due to the nonconvex nature of the rank objective in this paper we present the first online learning approach for the problem of rank minimization of matrices over polyhedral sets in particular we present two online learning algorithms for rank minimization our first algorithm is a multiplicative update method based on a generalized experts framework while our second algorithm is a novel application of the online convex programming framework zinkevich in the latter we flip the role of the decision maker by making the decision maker search over the constraint space instead of feasible points as is usually the case in online convex programming a salient feature of our online learning approach is that it allows us to give provable approximation guarantees for the rank minimization problem over polyhedral sets we demonstrate the effectiveness of our methods on synthetic examples and on the reallife application of lowrank kernel learning
an analysis of reinforcement learning with function approximation an analysis of reinforcement learning with function approximation we address the problem of computing the optimal qfunction in markov decision problems with infinite statespace we analyze the convergence properties of several variations of qlearning when combined with function approximation extending the analysis of tdlearning in tsitsiklis van roy a to stochastic control settings we identify conditions under which such approximate methods converge with probability we conclude with a brief discussion on the general applicability of our results and compare them with several related works
empirical bernstein stopping empirical bernstein stopping sampling is a popular way of scaling up machine learning algorithms to large datasets the question often is how many samples are needed adaptive stopping algorithms monitor the performance in an online fashion and they can stop early saving valuable resources we consider problems where probabilistic guarantees are desired and demonstrate how recentlyintroduced empirical bernstein bounds can be used to design stopping rules that are efficient we provide upper bounds on the sample complexity of the new rules as well as empirical results on model selection and boosting in the filtering setting
efficiently solving convex relaxations for map estimation efficiently solving convex relaxations for map estimation the problem of obtaining the maximum a posteriori map estimate of a discrete random field is of fundamental importance in many areas of computer science in this work we build on the tree reweighted message passing trw framework of kolmogorov wainwright et al trw iteratively optimizes the lagrangian dual of a linear programming relaxation for map estimation we show how the dual formulation of trw can be extended to include cycle inequalities barahona mahjoub and some recently proposed second order cone soc constraints kumar et al we propose efficient iterative algorithms for solving the resulting duals similar to the method described in kolmogorov these algorithms are guaranteed to converge we test our approach on a large set of synthetic data as well as real data our experiments show that the additional constraints ie cycle inequalities and soc constraints provide better results in cases where the trw framework fails namely map estimation for nonsubmodular energy functions
on the hardness of finding symmetries in markov decision processes on the hardness of finding symmetries in markov decision processes in this work we address the question of finding symmetries of a given mdp we show that the problem is isomorphism complete that is the problem is polynomially equivalent to verifying whether two graphs are isomorphic apart from the theoretical importance of this result it has an important practical application the reduction presented can be used together with any offtheshelf graph isomorphism solver which performs well in the average case to find symmetries of an mdp in fact we present results of using nauty the best graph isomorphism solver currently available to find symmetries of mdps
bayes optimal classification for decision trees bayes optimal classification for decision trees we present an algorithm for exact bayes optimal classification from a hypothesis space of decision trees satisfying leaf constraints our contribution is that we reduce this classification problem to the problem of finding a rulebased classifier with appropriate weights we show that these rules and weights can be computed in linear time from the output of a modified frequent itemset mining algorithm which means that we can compute the classifier in practice despite the exponential worstcase complexity in experiments we compare the bayes optimal predictions with those of the maximum a posteriori hypothesis
a decoupled approach to exemplarbased unsupervised learning a decoupled approach to exemplarbased unsupervised learning a recent trend in exemplar based unsupervised learning is to formulate the learning problem as a convex optimization problem convexity is achieved by restricting the set of possible prototypes to training exemplars in particular this has been done for clustering vector quantization and mixture model density estimation in this paper we propose a novel algorithm that is theoretically and practically superior to these convex formulations this is possible by posing the unsupervised learning problem as a single convex master problem with nonconvex subproblems we show that for the above learning tasks the subproblems are extremely wellbehaved and can be solved efficiently
costsensitive multiclass classification from probability estimates costsensitive multiclass classification from probability estimates for twoclass classification it is common to classify by setting a threshold on class probability estimates where the threshold is determined by roc curve analysis an analog for multiclass classification is learning a new class partitioning of the multiclass probability simplex to minimize empirical misclassification costs we analyze the interplay between systematic errors in the class probability estimates and cost matrices for multiclass classification we explore the effect on the class partitioning of five different transformations of the cost matrix experiments on benchmark datasets with naive bayes and quadratic discriminant analysis show the effectiveness of learning a new partition matrix compared to previously proposed methods
the projectron the projectron we present a discriminative online algorithm with a bounded memory growth which is based on the kernelbased perceptron generally the required memory of the kernelbased perceptron for storing the online hypothesis is not bounded previous work has been focused on discarding part of the instances in order to keep the memory bounded in the proposed algorithm the instances are not discarded but projected onto the space spanned by the previous online hypothesis we derive a relative mistake bound and compare our algorithm both analytically and empirically to the stateoftheart forgetron algorithm dekel et al the first variant of our algorithm called projectron outperforms the forgetron the second variant called projectron outperforms even the perceptron
learning dissimilarities by ranking learning dissimilarities by ranking we consider the problem of learning dissimilarities between points via formulations which preserve a specified ordering between points rather than the numerical values of the dissimilarities dissimilarity ranking dranking learns from instances like a is more similar to b than c is to d or the distance between e and f is larger than that between g and h three formulations of dranking problems are presented and new algorithms are presented for two of them one by semidefinite programming sdp and one by quadratic programming qp among the novel capabilities of these approaches are outofsample prediction and scalability to large problems
a distance model for rhythms a distance model for rhythms modeling longterm dependencies in time series has proved very difficult to achieve with traditional machine learning methods this problem occurs when considering music data in this paper we introduce a model for rhythms based on the distributions of distances between subsequences a specific implementation of the model when considering hamming distances over a simple rhythm representation is described the proposed model consistently outperforms a standard hidden markov model in terms of conditional prediction accuracy on two different music databases
on the chance accuracies of large collections of classifiers on the chance accuracies of large collections of classifiers we provide a theoretical analysis of the chance accuracies of large collections of classifiers we show that on problems with small numbers of examples some classifier can perform well by random chance and we derive a theorem to explicitly calculate this accuracy we use this theorem to provide a principled feature selection criterion for sparse highdimensional problems we evaluate this method on microarray and fmri datasets and show that it performs very close to the optimal accuracy obtained from an oracle we also show that on the fmri dataset this technique chooses relevant features successfully while another stateoftheart method the false discovery rate fdr completely fails at standard significance levels
an analysis of linear models linear valuefunction approximation and feature selection for reinforcement learning an analysis of linear models linear valuefunction approximation and feature selection for reinforcement learning we show that linear valuefunction approximation is equivalent to a form of linear model approximation we then derive a relationship between the modelapproximation error and the bellman error and show how this relationship can guide feature selection for model improvement andor valuefunction improvement we also show how these results give insight into the behavior of existing featureselection algorithms
learning to learn implicit queries from gaze patterns learning to learn implicit queries from gaze patterns in the absence of explicit queries an alternative is to try to infer users interests from implicit feedback signals such as clickstreams or eye tracking the interests formulated as an implicit query can then be used in further searches we formulate this task as a probabilistic model which can be interpreted as a kind of transfer or metalearning the probabilistic model is demonstrated to outperform an earlier kernelbased method in a smallscale information retrieval task
multitask compressive sensing with dirichlet process priors multitask compressive sensing with dirichlet process priors compressive sensing cs is an emerging poundeld that under appropriate conditions can signipoundcantly reduce the number of measurements required for a given signal in many applications one is interested in multiple signals that may be measured in multiple cstype measurements where here each signal corresponds to a sensing task in this paper we propose a novel multitask compressive sensing framework based on a bayesian formalism where a dirichlet process dp prior is employed yielding a principled means of simultaneously inferring the appropriate sharing mechanisms as well as cs inversion for each task a variational bayesian vb inference algorithm is employed to estimate the full posterior on the model parameters
estimating labels from label proportions estimating labels from label proportions consider the following problem given sets of unlabeled observations each set with known label proportions predict the labels of another set of observations also with known label proportions this problem appears in areas like ecommerce spam filtering and improper content detection we present consistent estimators which can reconstruct the correct labels with high probability in a uniform convergence sense experiments show that our method works well in practice
learning diverse rankings with multiarmed bandits learning diverse rankings with multiarmed bandits algorithms for learning to rank web documents usually assume a documents relevance is independent of other documents this leads to learned ranking functions that produce rankings with redundant results in contrast user studies have shown that diversity at high ranks is often preferred we present two online learning algorithms that directly learn a diverse ranking of documents based on users clicking behavior we show that these algorithms minimize abandonment or alternatively maximize the probability that a relevant document is found in the top k positions of a ranking moreover one of our algorithms asymptotically achieves optimal worstcase performance even if users interests change
semisupervised learning of compact document representations with deep networks semisupervised learning of compact document representations with deep networks finding good representations of text documents is crucial in information retrieval and classification systems today the most popular document representation is based on a vector of word counts in the document this representation neither captures dependencies between related words nor handles synonyms or polysemous words in this paper we propose an algorithm to learn text document representations based on semisupervised autoencoders that are stacked to form a deep network the model can be trained efficiently on partially labeled corpora producing very compact representations of documents while retaining as much class information and joint word statistics as possible we show that it is advantageous to exploit even a few labeled samples during training
messagepassing for graphstructured linear programs messagepassing for graphstructured linear programs a large body of past work has focused on the firstorder treebased lp relaxation for the map problem in markov random fields this paper develops a family of superlinearly convergent lp solvers based on proximal minimization schemes using bregman divergences that exploit the underlying graphical structure and so scale well to large problems all of our algorithms have a doubleloop character with the outer loop corresponding to the proximal sequence and an inner loop of cyclic bregman divergences used to compute each proximal update the inner loop updates are distributed and respect the graph structure and thus can be cast as messagepassing algorithms we establish various convergence guarantees for our algorithms illustrate their performance and also present rounding schemes with provable optimality guarantees
bayesian multiple instance learning bayesian multiple instance learning we propose a novel bayesian multiple instance learning mil algorithm this algorithm automatically identifies the relevant feature subset and utilizes inductive transfer when learning multiple conceptually related classifiers experimental results indicate that the proposed mil method is more accurate than previous mil algorithms and selects a much smaller set of useful features inductive transfer further improves the accuracy of the classifier as compared to learning each task individually
online kernel selection for bayesian reinforcement learning online kernel selection for bayesian reinforcement learning kernelbased bayesian methods for reinforcement learning rl such as gaussian process temporal difference gptd are particularly promising because they rigorously treat uncertainty in the value function and make it easy to specify prior knowledge however the choice of prior distribution significantly affects the empirical performance of the learning agent and little work has been done extending existing methods for prior model selection to the online setting this paper develops replacingkernel rl an online model selection method for gptd using sequential montecarlo methods replacingkernel rl is compared to standard gptd and tilecoding on several rl domains and is shown to yield significantly better asymptotic performance for many different kernel families furthermore the resulting kernels capture an intuitively useful notion of prior state covariance that may nevertheless be difficult to capture manually
the dynamic hierarchical dirichlet process the dynamic hierarchical dirichlet process the dynamic hierarchical dirichlet process dhdp is developed to model the timeevolving statistical properties of sequential data sets the data collected at any time point are represented via a mixture associated with an appropriate underlying model in the framework of hdp the statistical properties of data collected at consecutive time points are linked via a random parameter that controls their probabilistic similarity the sharing mechanisms of the timeevolving data are derived and a relatively simple markov chain monte carlo sampler is developed experimental results are presented to demonstrate the model
closedform supervised dimensionality reduction with generalized linear models closedform supervised dimensionality reduction with generalized linear models we propose a family of supervised dimensionality reduction sdr algorithms that combine feature extraction dimensionality reduction with learning a predictive model in a unified optimization framework using data and classappropriate generalized linear models glms and handling both classification and regression problems our approach uses simple closedform update rules and is provably convergent promising empirical results are demonstrated on a variety of highdimensional datasets
bilevel path following for cross validated solution of kernel quantile regression bilevel path following for cross validated solution of kernel quantile regression modeling of conditional quantiles requires specification of the quantile being estimated and can thus be viewed as a parameterized predictive modeling problem quantile loss is typically used and it is indeed parameterized by a quantile parameter in this paper we show how to follow the path of cross validated solutions to regularized kernel quantile regression even though the bilevel optimization problem we encounter for every quantile is nonconvex the manner in which the optimal crossvalidated solution evolves with the parameter of the loss function allows tracking of this solution we prove this property construct the resulting algorithm and demonstrate it on data this algorithm allows us to efficiently solve the whole family of bilevel problems
the grouplasso for generalized linear models the grouplasso for generalized linear models the grouplasso method for finding important explanatory factors suffers from the potential nonuniqueness of solutions and also from high computational costs we formulate conditions for the uniqueness of grouplasso solutions which lead to an easily implementable test procedure that allows us to identify all potentially active groups these results are used to derive an efficient algorithm that can deal with input dimensions in the millions and can approximate the solution path efficiently the derived methods are applied to largescale learning problems where they exhibit excellent performance and where the testing procedure helps to avoid misinterpretations of the solutions
robust matching and recognition using contextdependent kernels robust matching and recognition using contextdependent kernels the success of kernel methods including support vector machines svms strongly depends on the design of appropriate kernels while initially kernels were designed in order to handle fixedlength data their extension to unordered variablelength data became more than necessary for real pattern recognition problems such as object recognition and bioinformatics
privacypreserving reinforcement learning privacypreserving reinforcement learning we consider the problem of distributed reinforcement learning drl from private perceptions in our setting agents perceptions such as states rewards and actions are not only distributed but also should be kept private conventional drl algorithms can handle multiple agents but do not necessarily guarantee privacy preservation and may not guarantee optimality in this work we design cryptographic solutions that achieve optimal policies without requiring the agents to share their private information
on the quantitative analysis of deep belief networks on the quantitative analysis of deep belief networks deep belief networks dbns are generative models that contain many layers of hidden variables efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains the main building block of a dbn is a bipartite undirected graphical model called a restricted boltzmann machine rbm due to the presence of the partition function model selection complexity control and exact maximum likelihood learning in rbms are intractable we show that annealed importance sampling ais can be used to efficiently estimate the partition function of an rbm and we present a novel ais scheme for comparing rbms with different architectures we further show how an ais estimator along with approximate inference can be used to estimate a lower bound on the logprobability that a dbn model with multiple hidden layers assigns to the test data this is to our knowledge the first step towards obtaining quantitative results that would allow us to directly assess the performance of deep belief networks as generative models of data
bayesian probabilistic matrix factorization using markov chain monte carlo bayesian probabilistic matrix factorization using markov chain monte carlo lowrank matrix approximation methods provide one of the simplest and most effective approaches to collaborative filtering such models are usually fitted to data by finding a map estimate of the model parameters a procedure that can be performed efficiently even on very large datasets however unless the regularization parameters are tuned carefully this approach is prone to overfitting because it finds a single point estimate of the parameters in this paper we present a fully bayesian treatment of the probabilistic matrix factorization pmf model in which model capacity is controlled automatically by integrating over all model parameters and hyperparameters we show that bayesian pmf models can be efficiently trained using markov chain monte carlo methods by applying them to the netflix dataset which consists of over million movie ratings the resulting models achieve significantly higher prediction accuracy than pmf models trained using map estimation
accurate maxmargin training for structured output spaces accurate maxmargin training for structured output spaces tsochantaridis et al proposed two formulations for maximum margin training of structured spaces margin scaling and slack scaling while margin scaling has been extensively used since it requires the same kind of map inference as normal structured prediction slack scaling is believed to be more accurate and betterbehaved we present an efficient variational approximation to the slack scaling method that solves its inference bottleneck while retaining its accuracy advantage over margin scaling
fast incremental proximity search in large graphs fast incremental proximity search in large graphs in this paper we investigate two aspects of ranking problems on large graphs first we augment the deterministic pruning algorithm in sarkar and moore with sampling techniques to compute approximately correct rankings with high probability under random walk based proximity measures at query time second we prove some surprising locality properties of these proximity measures by examining the short term behavior of random walks the proposed algorithm can answer queries on the fly without caching any information about the entire graph we present empirical results on a node authorwordcitation graph from the citeseer domain on a single cpu machine where the average query processing time is around seconds we present quantifiable link prediction tasks on most of them our techniques outperform personalized pagerank a wellknown diffusion based proximity measure
inverting the viterbi algorithm inverting the viterbi algorithm probabilistic grammatical formalisms such as hidden markov models hmms and stochastic contextfree grammars scfgs have been extensively studied and widely applied in a number of fields here we introduce a new algorithmic problem on hmms and scfgs that arises naturally from protein and rna design and which has not been previously studied the problem can be viewed as an inverse to the one solved by the viterbi algorithm on hmms or by the cky algorithm on scfgs we study this problem theoretically and obtain the first algorithmic results we prove that the problem is npcomplete even for a letter emission alphabet via a reduction from sat a result that has implications for the hardness of rna secondary structure design we then develop a number of approaches for making the problem tractable in particular for hmms we develop a branchandbound algorithm which can be shown to have fixedparameter tractable worstcase running time exponential in the number of states of the hmm but linear in the length of the structure we also show how to cast the problem as a mixed integer linear program
compressed sensing and bayesian experimental design compressed sensing and bayesian experimental design we relate compressed sensing cs with bayesian experimental design and provide a novel efficient approximate method for the latter based on expectation propagation in a large comparative study about linearly measuring natural images we show that the simple standard heuristic of measuring wavelet coefficients topdown systematically outperforms cs methods using random measurements the sequential projection optimisation approach of ji carin performs even worse we also show that our own approximate bayesian method is able to learn measurement filters on full images efficiently which outperform the wavelet heuristic to our knowledge ours is the first successful attempt at learning compressed sensing for images of realistic size in contrast to common cs methods our framework is not restricted to sparse signals but can readily be applied to other notions of signal complexity or noise models we give concrete ideas how our method can be scaled up to large signal representations
multiclassification by categorical features via clustering multiclassification by categorical features via clustering we derive a generalization bound for multiclassification schemes based on grid clustering in categorical parameter product spaces grid clustering partitions the parameter space in the form of a cartesian product of partitions for each of the parameters the derived bound provides a means to evaluate clustering solutions in terms of the generalization power of a builton classifier for classification based on a single feature the bound serves to find a globally optimal classification rule comparison of the generalization power of individual features can then be used for feature ranking our experiments show that in this role the bound is much more precise than mutual information or normalized correlation indices
svm optimization svm optimization we discuss how the runtime of svm optimization should decrease as the size of the training data increases we present theoretical and empirical results demonstrating how a simple subgradient descent approach indeed displays such behavior at least for linear kernels
data spectroscopy data spectroscopy in this paper we develop a spectral framework for estimating mixture distributions specifically gaussian mixture models in physics spectroscopy is often used for the identification of substances through their spectrum treating a kernel function kx y as light and the sampled data as substance the spectrum of their interaction eigenvalues and eigenvectors of the kernel matrix k unveils certain aspects of the underlying parametric distribution p such as the parameters of a gaussian mixture our approach extends the intuitions and analyses underlying the existing spectral techniques such as spectral clustering and kernel principal components analysis kpca
a generalization of hausslers convolution kernel a generalization of hausslers convolution kernel hausslers convolution kernel provides a successful framework for engineering new positive semidefinite kernels and has been applied to a wide range of data types and applications in the framework each data object represents a finite set of finer grained components then hausslers convolution kernel takes a pair of data objects as input and returns the sum of the return values of the predetermined primitive positive semidefinite kernel calculated for all the possible pairs of the components of the input data objects on the other hand the mapping kernel that we introduce in this paper is a natural generalization of hausslers convolution kernel in that the input to the primitive kernel moves over a predetermined subset rather than the entire cross product although we have plural instances of the mapping kernel in the literature their positive semidefiniteness was investigated in casebycase manners and worse yet was sometimes incorrectly concluded in fact there exists a simple and easily checkable necessary and sufficient condition which is generic in the sense that it enables us to investigate the positive semidefiniteness of an arbitrary instance of the mapping kernel this is the first paper that presents and proves the validity of the condition in addition we introduce two important instances of the mapping kernel which we refer to as the sizeofindexstructuredistribution kernel and the editcostdistribution kernel both of them are naturally derived from well known dissimilarity measurements in the literature eg the maximum agreement tree the edit distance and are reasonably expected to improve the performance of the existing measures by evaluating their distributional features rather than their peak maximumminimum features
mstruct mstruct traditional methods for analyzing population structure such as the structure program ignore the influence of mutational effects we propose mstruct an admixture of populationspecific mixtures of inheritance models that addresses the task of structure inference and mutation estimation jointly through a hierarchical bayesian framework and a variational algorithm for inference we validated our method on synthetic data and used it to analyze the hgdpceph cell line panel of microsatellites used in rosenberg et al and the hgdp snp data used in conrad et al a comparison of the structural maps of world populations estimated by mstruct and structure is presented and we also report potentially interesting mutation patterns in world populations estimated by mstruct which is not possible by structure
expectationmaximization for sparse and nonnegative pca expectationmaximization for sparse and nonnegative pca we study the problem of finding the dominant eigenvector of the sample covariance matrix under additional constraints on the vector a cardinality constraint limits the number of nonzero elements and nonnegativity forces the elements to have equal sign this problem is known as sparse and nonnegative principal component analysis pca and has many applications including dimensionality reduction and feature selection based on expectationmaximization for probabilistic pca we present an algorithm for any combination of these constraints its complexity is at most quadratic in the number of dimensions of the data we demonstrate significant improvements in performance and computational efficiency compared to other constrained pca algorithms on large data sets from biology and computer vision finally we show the usefulness of nonnegative sparse pca for unsupervised feature selection in a gene clustering task
samplebased learning and search with permanent and transient memories samplebased learning and search with permanent and transient memories we present a reinforcement learning architecture dyna that encompasses both samplebased learning and samplebased search and that generalises across states during both learning and search we apply dyna to high performance computer go in this domain the most successful planning methods are based on samplebased search algorithms such as uct in which states are treated individually and the most successful learning methods are based on temporaldifference learning algorithms such as sarsa in which linear function approximation is used in both cases an estimate of the value function is formed but in the first case it is transient computed and then discarded after each move whereas in the second case it is more permanent slowly accumulating over many moves and games the idea of dyna is for the transient planning memory and the permanent learning memory to remain separate but for both to be based on linear function approximation and both to be updated by sarsa to apply dyna to x computer go we use a million binary features in the function approximator based on templates matching small fragments of the board using only the transient memory dyna performed at least as well as uct using both memories combined it significantly outperformed uct our program based on dyna achieved a higher rating on the computer go online server than any handcrafted or traditional search based program
an rkhs for multiview learning and manifold coregularization an rkhs for multiview learning and manifold coregularization inspired by cotraining many multiview semisupervised kernel methods implement the following idea find a function in each of multiple reproducing kernel hilbert spaces rkhss such that a the chosen functions make similar predictions on unlabeled examples and b the average prediction given by the chosen functions performs well on labeled examples in this paper we construct a single rkhs with a datadependent coregularization norm that reduces these approaches to standard supervised learning the reproducing kernel for this rkhs can be explicitly derived and plugged into any kernel method greatly extending the theoretical and algorithmic scope of coregularization in particular with this development the rademacher complexity bound for coregularization given in rosenberg bartlett follows easily from wellknown results furthermore more refined bounds given by localized rademacher complexity can also be easily applied we propose a coregularization based algorithmic alternative to manifold regularization belkin et al sindhwani et al a that leads to major empirical improvements on semisupervised tasks unlike the recently proposed transductive approach of yu et al our rkhs formulation is truly semisupervised and naturally extends to unseen test data
the asymptotics of semisupervised learning in discriminative probabilistic models the asymptotics of semisupervised learning in discriminative probabilistic models semisupervised learning aims at taking advantage of unlabeled data to improve the efficiency of supervised learning procedures for discriminative models however this is a challenging task in this contribution we introduce an original methodology for using unlabeled data through the design of a simple semisupervised objective function we prove that the corresponding semisupervised estimator is asymptotically optimal the practical consequences of this result are discussed for the case of the logistic regression model
tailoring density estimation via reproducing kernel moment matching tailoring density estimation via reproducing kernel moment matching moment matching is a popular means of parametric density estimation we extend this technique to nonparametric estimation of mixture models our approach works by embedding distributions into a reproducing kernel hilbert space and performing moment matching in that space this allows us to tailor density estimators to a function class of interest ie for which we would like to compute expectations we show our density estimation approach is useful in applications such as message compression in graphical models and image classification and retrieval
detecting statistical interactions with additive groves of trees detecting statistical interactions with additive groves of trees discovering additive structure is an important step towards understanding a complex multidimensional function because it allows the function to be expressed as the sum of lowerdimensional components when variables interact however their effects are not additive and must be modeled and interpreted simultaneously we present a new approach for the problem of interaction detection our method is based on comparing the performance of unrestricted and restricted prediction models where restricted models are prevented from modeling an interaction in question we show that an additive modelbased regression ensemble additive groves can be restricted appropriately for use with this framework and thus has the right properties for accurately detecting variable interactions
metric embedding for kernel classification rules metric embedding for kernel classification rules in this paper we consider a smoothing kernel based classification rule and propose an algorithm for optimizing the performance of the rule by learning the bandwidth of the smoothing kernel along with a datadependent distance metric the datadependent distance metric is obtained by learning a function that embeds an arbitrary metric space into a euclidean space while minimizing an upper bound on the resubstitution estimate of the error probability of the kernel classification rule by restricting this embedding function to a reproducing kernel hilbert space we reduce the problem to solving a semidefinite program and show the resulting kernel classification rule to be a variation of the knearest neighbor rule we compare the performance of the kernel rule using the learned datadependent distance metric to stateoftheart distance metric learning algorithms designed for knearest neighbor classification on some benchmark datasets the results show that the proposed rule has either better or as good classification accuracy as the other metric learning algorithms
discriminative parameter learning for bayesian networks discriminative parameter learning for bayesian networks bayesian network classifiers have been widely used for classification problems given a fixed bayesian network structure parameters learning can take two different approaches generative and discriminative learning while generative parameter learning is more efficient discriminative parameter learning is more effective in this paper we propose a simple efficient and effective discriminative parameter learning method called discriminative frequency estimate dfe which learns parameters by discriminatively computing frequencies from data empirical studies show that the dfe algorithm integrates the advantages of both generative and discriminative learning it performs as well as the stateoftheart discriminative parameter learning method elr in accuracy but is significantly more efficient
a least squares formulation for canonical correlation analysis a least squares formulation for canonical correlation analysis canonical correlation analysis cca is a wellknown technique for finding the correlations between two sets of multidimensional variables it projects both sets of variables into a lowerdimensional space in which they are maximally correlated cca is commonly applied for supervised dimensionality reduction in which one of the multidimensional variables is derived from the class label it has been shown that cca can be formulated as a least squares problem in the binaryclass case however their relationship in the more general setting remains unclear in this paper we show that under a mild condition which tends to hold for highdimensional data cca in multilabel classifications can be formulated as a least squares problem based on this equivalence relationship we propose several cca extensions including sparse cca using norm regularization experiments on multilabel data sets confirm the established equivalence relationship results also demonstrate the effectiveness of the proposed cca extensions
apprenticeship learning using linear programming apprenticeship learning using linear programming in apprenticeship learning the goal is to learn a policy in a markov decision process that is at least as good as a policy demonstrated by an expert the difficulty arises in that the mdps true reward function is assumed to be unknown we show how to frame apprenticeship learning as a linear programming problem and show that using an offtheshelf lp solver to solve this problem results in a substantial improvement in running time over existing methodsup to two orders of magnitude faster in our experiments additionally our approach produces stationary policies while all existing methods for apprenticeship learning output policies that are mixed ie randomized combinations of stationary policies the technique used is general enough to convert any mixed policy to a stationary policy
composite kernel learning composite kernel learning the support vector machine svm is an acknowledged powerful tool for building classifiers but it lacks flexibility in the sense that the kernel is chosen prior to learning multiple kernel learning mkl enables to learn the kernel from an ensemble of basis kernels whose combination is optimized in the learning process here we propose composite kernel learning to address the situation where distinct components give rise to a group structure among kernels our formulation of the learning problem encompasses several setups putting more or less emphasis on the group structure we characterize the convexity of the learning problem and provide a general wrapper algorithm for computing solutions finally we illustrate the behavior of our method on multichannel data where groups correpond to channels
the many faces of optimism the many faces of optimism the explorationexploitation dilemma has been an intriguing and unsolved problem within the framework of reinforcement learning optimism in the face of uncertainty and model building play central roles in advanced exploration methods here we integrate several concepts and obtain a fast and simple algorithm we show that the proposed algorithm finds a nearoptimal policy in polynomial time and give experimental evidence that it is robust and efficient compared to its ascendants
support vector machine as conditional valueatrisk minimization support vector machine as conditional valueatrisk minimization the nusupport vector classification nusvc algorithm was shown to work well and provide intuitive interpretations eg the parameter nu roughly specifies the fraction of support vectors although nu corresponds to a fraction it cannot take the entire range between and in its original form this problem was settled by a nonconvex extension of nusvc and the extended method was experimentally shown to generalize better than original nusvc however its good generalization performance and convergence properties of the optimization algorithm have not been studied yet in this paper we provide new theoretical insights into these issues and propose a novel nusvc algorithm that has guaranteed generalization performance and convergence properties
training restricted boltzmann machines using approximations to the likelihood gradient training restricted boltzmann machines using approximations to the likelihood gradient a new algorithm for training restricted boltzmann machines is introduced the algorithm named persistent contrastive divergence is different from the standard contrastive divergence algorithms in that it aims to draw samples from almost exactly the model distribution it is compared to some standard contrastive divergence and pseudolikelihood algorithms on the tasks of modeling and classifying various types of data the persistent contrastive divergence algorithm outperforms the other algorithms and is equally fast and simple
a semiparametric statistical approach to modelfree policy evaluation a semiparametric statistical approach to modelfree policy evaluation reinforcement learning rl methods based on leastsquares temporal difference lstd have been developed recently and have shown good practical performance however the quality of their estimation has not been well elucidated in this article we discuss lstdbased policy evaluation from the new viewpoint of semiparametric statistical inference in fact the estimator can be obtained from a particular estimating function which guarantees its convergence to the true value asymptotically without specifying a model of the environment based on these observations we analyze the asymptotic variance of an lstdbased estimator derive the optimal estimating function with the minimum asymptotic estimation variance and derive a suboptimal estimator to reduce the computational burden in obtaining the optimal estimating function
topologicallyconstrained latent variable models topologicallyconstrained latent variable models in dimensionality reduction approaches the data are typically embedded in a euclidean latent space however for some data sets this is inappropriate for example in human motion data we expect latent spaces that are cylindrical or a toroidal that are poorly captured with a euclidean space in this paper we present a range of approaches for embedding data in a noneuclidean latent space our focus is the gaussian process latent variable model in the context of human motion modeling this allows us to a learn models with interpretable latent directions enabling for example stylecontent separation and b generalise beyond the data set enabling us to learn transitions between motion styles even though such transitions are not present in the data
beam sampling for the infinite hidden markov model beam sampling for the infinite hidden markov model the infinite hidden markov model is a nonparametric extension of the widely used hidden markov model our paper introduces a new inference algorithm for the infinite hidden markov model called beam sampling beam sampling combines slice sampling which limits the number of states considered at each time step to a finite number with dynamic programming which samples whole state trajectories efficiently our algorithm typically outperforms the gibbs sampler and is more robust we present applications of ihmm inference using the beam sampler on changepoint detection and text prediction problems
extracting and composing robust features with denoising autoencoders extracting and composing robust features with denoising autoencoders previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations we introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern this approach can be used to train autoencoders and these denoising autoencoders can be stacked to initialize deep architectures the algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite
prediction with expert advice for the brier game prediction with expert advice for the brier game we show that the brier game of prediction is mixable and find the optimal learning rate and substitution function for it the resulting prediction algorithm is applied to predict results of football and tennis matches the theoretical performance guarantee turns out to be rather tight on these data sets especially in the case of the more extensive tennis data
sparse multiscale gaussian process regression sparse multiscale gaussian process regression most existing sparse gaussian process gp models seek computational advantages by basing their computations on a set of m basis functions that are the covariance function of the gp with one of its two inputs fixed we generalise this for the case of gaussian covariance function by basing our computations on m gaussian basis functions with arbitrary diagonal covariance matrices or length scales for a fixed number of basis functions and any given criteria this additional flexibility permits approximations no worse and typically better than was previously possible we perform gradient based optimisation of the marginal likelihood which costs omn time where n is the number of data points and compare the method to various other sparse gp methods although we focus on gp regression the central idea is applicable to all kernel based algorithms and we also provide some results for the support vector machine svm and kernel ridge regression krr our approach outperforms the other methods particularly for the case of very few basis functions i e a very high sparsity ratio
manifold alignment using procrustes analysis manifold alignment using procrustes analysis in this paper we introduce a novel approach to manifold alignment based on procrustes analysis our approach differs from semisupervised alignment in that it results in a mapping that is defined everywhere when used with a suitable dimensionality reduction method rather than just on the training data points we describe and evaluate our approach both theoretically and experimentally providing results showing useful knowledge transfer from one domain to another novel applications of our method including crosslingual information retrieval and transfer learning in markov decision processes are presented
dirichlet component analysis dirichlet component analysis we consider feature extraction dimensionality reduction for compositional data where the data vectors are constrained to be positive and constantsum in realworld problems the data components variables usually have complicated correlations while their total number is huge such scenario demands feature extraction that is we shall decorrelate the components and reduce their dimensionality traditional techniques such as the principle component analysis pca are not suitable for these problems due to unique statistical properties and the need to satisfy the constraints in compositional data this paper presents a novel approach to feature extraction for compositional data our method first identifies a family of dimensionality reduction projections that preserve all relevant constraints and then finds the optimal projection that maximizes the estimated dirichlet precision on projected data it reduces the compositional data to a given lower dimensionality while the components in the lowerdimensional space are decorrelated as much as possible we develop theoretical foundation of our approach and validate its effectiveness on some synthetic and realworld datasets
adaptive pposterior mixturemodel kernels for multiple instance learning adaptive pposterior mixturemodel kernels for multiple instance learning in multiple instance learning mil how the instances determine the baglabels is an essential issue both algorithmically and intrinsically in this paper we show that the mechanism of how the instances determine the baglabels is different for different application domains and does not necessarily obey the traditional assumptions of mil we therefore propose an adaptive framework for mil that adapts to different application domains by learning the domainspecific mechanisms merely from labeled bags our approach is especially attractive when we are encountered with novel application domains for which the mechanisms may be different and unknown specifically we exploit mixture models to represent the composition of each bag and an adaptable kernel function to represent the relationship between the bags we validate on synthetic mil datasets that the kernel function automatically adapts to different mechanisms of how the instances determine the baglabels we also compare our approach with stateoftheart mil techniques on realworld benchmark datasets
graph transduction via alternating minimization graph transduction via alternating minimization graph transduction methods label input data by learning a classification function that is regularized to exhibit smoothness along a graph over labeled and unlabeled samples in practice these algorithms are sensitive to the initial set of labels provided by the user for instance classification accuracy drops if the training set contains weak labels if imbalances exist across label classes or if the labeled portion of the data is not chosen at random this paper introduces a propagation algorithm that more reliably minimizes a cost function over both a function on the graph and a binary label matrix the cost function generalizes prior work in graph transduction and also introduces node normalization terms for resilience to label imbalances we demonstrate that global minimization of the function is intractable but instead provide an alternating minimization scheme that incrementally adjusts the function and the labels towards a reliable local minimum unlike prior methods the resulting propagation of labels does not prematurely commit to an erroneous labeling and obtains more consistent labels experiments are shown for synthetic and real classification tasks including digit and text recognition a substantial improvement in accuracy compared to state of the art semisupervised methods is achieved the advantage are even more dramatic when labeled instances are limited
on multiview active learning and the combination with semisupervised learning on multiview active learning and the combination with semisupervised learning multiview learning has become a hot topic during the past few years in this paper we first characterize the sample complexity of multiview active learning under the alphaexpansion assumption we get an exponential improvement in the sample complexity from usual otildeepsilon to otildelog epsilon requiring neither strong assumption on data distribution such as the data is distributed uniformly over the unit sphere in rd nor strong assumption on hypothesis class such as linear separators through the origin we also give an upper bound of the error rate when the alphaexpansion assumption does not hold then we analyze the combination of multiview active learning and semisupervised learning and get a further improvement in the sample complexity finally we study the empirical behavior of the two paradigms which verifies that the combination of multiview active learning and semisupervised learning is efficient
fast solvers and efficient implementations for distance metric learning fast solvers and efficient implementations for distance metric learning in this paper we study how to improve nearest neighbor classification by learning a mahalanobis distance metric we build on a recently proposed framework for distance metric learning known as large margin nearest neighbor lmnn classification our paper makes three contributions first we describe a highly efficient solver for the particular instance of semidefinite programming that arises in lmnn classification our solver can handle problems with billions of large margin constraints in a few hours second we show how to reduce both training and testing times using metric ball trees the speedups from ball trees are further magnified by learning low dimensional representations of the input space third we show how to learn different mahalanobis distance metrics in different parts of the input space for large data sets the use of locally adaptive distance metrics leads to even lower error rates
deep learning via semisupervised embedding deep learning via semisupervised embedding we show how nonlinear embedding algorithms popular for use with shallow semisupervised learning techniques such as kernel methods can be applied to deep multilayer architectures either as a regularizer at the output layer or on each layer of the architecture this provides a simple alternative to existing approaches to deep learning whilst yielding competitive error rates compared to those methods and existing shallow semisupervised techniques
efficiently learning linearlinear exponential family predictive representations of state efficiently learning linearlinear exponential family predictive representations of state exponential family psr efpsr models capture stochastic dynamical systems by representing state as the parameters of an exponential family distribution over a shortterm window of future observations they are appealing from a learning perspective because they are fully observed meaning expressions for maximum likelihood do not involve hidden quantities but are still expressive enough to both capture existing models and predict new models while maximumlikelihood learning algorithms for efpsrs exist they are not computationally feasible we present a new computationally efficient learning algorithm based on an approximate likelihood function the algorithm can be interpreted as attempting to induce stationary distributions of observations features and states which match their empirically observed counterparts the approximate likelihood and the idea of matching stationary distributions may apply to other models
fully distributed em for very large datasets fully distributed em for very large datasets in em and related algorithms estep computations distribute easily because data items are independent given parameters for very large data sets however even storing all of the parameters in a single node for the mstep can be impractical we present a framework that fully distributes the entire em procedure each node interacts only with parameters relevant to its data sending messages to other nodes along a junctiontree topology we demonstrate improvements over a mapreduce topology on two tasks word alignment and topic modeling
listwise approach to learning to rank listwise approach to learning to rank this paper aims to conduct a study on the listwise approach to learning to rank the listwise approach learns a ranking function by taking individual lists as instances and minimizing a loss function defined on the predicted list and the groundtruth list existing work on the approach mainly focused on the development of new algorithms methods such as rankcosine and listnet have been proposed and good performances by them have been observed unfortunately the underlying theory was not sufficiently studied so far to amend the problem this paper proposes conducting theoretical analysis of learning to rank algorithms through investigations on the properties of the loss functions including consistency soundness continuity differentiability convexity and efficiency a sufficient condition on consistency for ranking is given which seems to be the first such result obtained in related research the paper then conducts analysis on three loss functions likelihood loss cosine loss and cross entropy loss the latter two were used in rankcosine and listnet the use of the likelihood loss leads to the development of a new listwise method called listmle whose loss function offers better properties and also leads to better experimental results
democratic approximation of lexicographic preference models democratic approximation of lexicographic preference models previous algorithms for learning lexicographic preference models lpms produce a best guess lpm that is consistent with the observations our approach is more democratic we do not commit to a single lpm instead we approximate the target using the votes of a collection of consistent lpms we present two variations of this methodvariable voting and model votingand empirically show that these democratic algorithms outperform the existing methods we also introduce an intuitive yet powerful learning bias to prune some of the possible lpms we demonstrate how this learning bias can be used with variable and model voting and show that the learning bias improves the learning curve significantly especially when the number of observations is small
preconditioned temporal difference learning preconditioned temporal difference learning this paper extends many of the recent popular policy evaluation algorithms to a generalized framework that includes leastsquares temporal difference lstd learning leastsquares policy evaluation lspe and a variant of incremental lstd ilstd the basis of this extension is a preconditioning technique that solves a stochastic model equation this paper also studies three significant issues of the new framework it presents a new rule of stepsize that can be computed online provides an iterative way to apply preconditioning and reduces the complexity of related algorithms to near that of temporal difference td learning
a quasinewton approach to nonsmooth convex optimization a quasinewton approach to nonsmooth convex optimization we extend the wellknown bfgs quasinewton method and its limitedmemory variant lbfgs to the optimization of nonsmooth convex objectives this is done in a rigorous fashion by generalizing three components of bfgs to subdifferentials the local quadratic model the identification of a descent direction and the wolfe line search conditions we apply the resulting sublbfgs algorithm to lregularized risk minimization with binary hinge loss and its directionfinding component to lregularized risk minimization with logistic loss in both settings our generic algorithms perform comparable to or better than their counterparts in specialized stateoftheart solvers
predicting diverse subsets using structural svms predicting diverse subsets using structural svms in many retrieval tasks one important goal involves retrieving a diverse set of results eg documents covering a wide range of topics for a search query first of all this reduces redundancy effectively showing more information with the presented results secondly queries are often ambiguous at some level for example the query jaguar can refer to many different topics such as the car or feline a set of documents with high topic diversity ensures that fewer users abandon the query because no results are relevant to them unlike existing approaches to learning retrieval functions we present a method that explicitly trains to diversify results in particular we formulate the learning problem of predicting diverse subsets and derive a training method based on structural svms
improved nystroumlm lowrank approximation and error analysis improved nystroumlm lowrank approximation and error analysis lowrank matrix approximation is an effective tool in alleviating the memory and computational burdens of kernel methods and sampling as the mainstream of such algorithms has drawn considerable attention in both theory and practice this paper presents detailed studies on the nystroumlm sampling scheme and in particular an error analysis that directly relates the nystroumlm approximation quality with the encoding powers of the landmark points in summarizing the data the resultant error bound suggests a simple and efficient sampling scheme the kmeans clustering algorithm for nystroumlm lowrank approximation we compare it with stateoftheart approaches that range from greedy schemes to probabilistic sampling our algorithm achieves significant performance gains in a number of supervisedunsupervised learning tasks including kernel pca and least squares svm
estimating local optimums in em algorithm over gaussian mixture model estimating local optimums in em algorithm over gaussian mixture model em algorithm is a very popular iterationbased method to estimate the parameters of gaussian mixture model from a large observation set however in most cases em algorithm is not guaranteed to converge to the global optimum instead it stops at some local optimums which can be much worse than the global optimum therefore it is usually required to run multiple procedures of em algorithm with different initial configurations and return the best solution to improve the efficiency of this scheme we propose a new method which can estimate an upper bound on the logarithm likelihood of the local optimum based on the current configuration after the latest em iteration this is accomplished by first deriving some region bounding the possible locations of local optimum followed by some upper bound estimation on the maximum likelihood with this estimation we can terminate an em algorithm procedure if the estimated local optimum is definitely worse than the best solution seen so far extensive experiments show that our method can effectively and efficiently accelerate conventional multiple restart em algorithm
efficient multiclass maximum margin clustering efficient multiclass maximum margin clustering this paper presents a cutting plane algorithm for multiclass maximum margin clustering mmc the proposed algorithm constructs a nested sequence of successively tighter relaxations of the original mmc problem and each optimization problem in this sequence could be efficiently solved using the constrained concaveconvex procedure cccp experimental evaluations on several real world datasets show that our algorithm converges much faster than existing mmc methods with guaranteed accuracy and can thus handle much larger datasets efficiently
laplace maximum margin markov networks laplace maximum margin markov networks we propose laplace maxmargin markov networks lapmn and a general class of bayesian mn bmn of which the lapmn is a special case with sparse structural bias for robust structured prediction bmn generalizes extant structured prediction rules based on point estimator to a bayespredictor using a learnt distribution of rules we present a novel structured maximum entropy discrimination smed formalism for combining bayesian and maxmargin learning of markov networks for structured prediction and our approach subsumes the conventional mn as a special case an efficient learning algorithm based on variational inference and standard convexoptimization solvers for mn and a generalization bound are offered our method outperforms competing ones on both synthetic and real ocr data
selforganizing wireless sensor networks in action selforganizing wireless sensor networks in action wireless sensor networks wsn composed of large numbers of small devices that selforganize are being investigated for a wide variety of applications two key advantages of these networks over more traditional sensor networks are that they can be dynamically and quickly deployed and that they can provide finegrained sensing applications such as emergency response to natural or manmade disasters detection and tracking and fine grained sensing of the environment are key examples of applications that can benefit from these types of wsn current research for these systems is widespread however many of the proposed solutions are developed with simplifying assumptions about wireless communication and the environment even though the realities of wireless communication and environmental sensing are well known many of the solutions are evaluated only by simulation in this talk i describe a fully implemented system consisting of a suite of more than synthesized protocols the system supports a power aware surveillance tracking and classification application running on xsm motes and evaluated in a realistic largearea environment technical details and evaluations are presented i end with a discussion of opportunities and problems for data mining related to wsn 
new cachedsufficient statistics algorithms for quickly answering statistical questions new cachedsufficient statistics algorithms for quickly answering statistical questions this talk is about recent work on new ways to exploit preprocessed views of data tables for tractably solving big statistical queries well describe deployments of these new algorithms in the realms of detecting killer asteroids and unnatural disease outbreaksin recent years several groups have looked at methods for prestoring general sufficient statistics of the data in spatial data structures such as kdtrees and balltrees so that both frequentist and bayesian statistical operations become fast for large datasets in this talk we will look at two other classes of optimization required in important statistical queriesthe first involves iterating over all spatial regions big and small the second involves detection of tracks from noisy intermittent observations separated far apart in time and space we will also discuss the implications that have arisen from making these operations tractable we will focus particularly ondetecting all asteroids in the solar system larger than pittsburghs cathedral of learning data to be collected over early detection of emerging diseases based on national monitoring of healthrelated transactions 
next frontier next frontier this talk is about the next frontier in knowledge discovery and data mining 
deriving quantitative models for correlation clusters deriving quantitative models for correlation clusters correlation clustering aims at grouping the data set into correlation clusters such that the objects in the same cluster exhibit a certain density and are all associated to a common arbitrarily oriented hyperplane of arbitrary dimensionality several algorithms for this task have been proposed recently however all algorithms only compute the partitioning of the data into clusters this is only a first step in the pipeline of advanced data analysis and system modelling the second postclustering step of deriving a quantitative model for each correlation cluster has not been addressed so far in this paper we describe an original approach to handle this second step we introduce a general method that can extract quantitative information on the linear dependencies within a correlation clustering our concepts are independent of the clustering model and can thus be applied as a postprocessing step to any correlation clustering algorithm furthermore we show how these quantitative models can be used to predict the probability distribution that an object is created by these models our broad experimental evaluation demonstrates the beneficial impact of our method on several applications of significant practical importance 
learning to rank networked entities learning to rank networked entities several algorithms have been proposed to learn to rank entities modeled as feature vectors based on relevance feedback however these algorithms do not model network connections or relations between entities meanwhile pagerank and variants find the stationary distribution of a reasonable but arbitrary markov walk over a network but do not learn from relevance feedback we present a framework for ranking networked entities based on markov walks with parameterized conductance values associated with the network edges we propose two flavors of conductance learning problems in our framework in the first setting relevance feedback comparing nodepairs hints that the user has one or more hidden preferred communities with large edge conductance and the algorithm must discover these communities we present a constrained maximum entropy network flow formulation whose dual can be solved efficiently using a cuttingplane approach and a quasinewton optimizer in the second setting edges have types and relevance feedback hints that each edge type has a potentially different conductance but this is fixed across the whole network our algorithm learns the conductances using an approximate newton method 
spatial scan statistics spatial scan statistics spatial scan statistics are used to determine hotspots in spatial data and are widely used in epidemiology and biosurveillance in recent years there has been much effort invested in designing efficient algorithms for finding such high discrepancy regions with methods ranging from fast heuristics for special cases to general gridbased methods and to efficient approximation algorithms with provable guarantees on performance and qualityin this paper we make a number of contributions to the computational study of spatial scan statistics first we describe a simple exact algorithm for finding the largest discrepancy region in a domain second we propose a new approximation algorithm for a large class of discrepancy functions including the kulldorff scan statistic that improves the approximation versus run time tradeoff of prior methods third we extend our simple exact and our approximation algorithms to data sets which lie naturally on a grid or are accumulated onto a grid fourth we conduct a detailed experimental comparison of these methods with a number of known methods demonstrating that our approximation algorithm has far superior performance in practice to prior methods and exhibits a good performanceaccuracy tradeoffall extant methods including those in this paper are suitable for data sets that are modestly sized if data sets are of the order of millions of data points none of these methods scale well for such massive data settings it is natural to examine whether smallspace streaming algorithms might yield accurate answers here we provide some negative results showing that any streaming algorithms that even provide approximately optimal answers to the discrepancy maximization problem must use space linear in the input 
global distancebased segmentation of trajectories global distancebased segmentation of trajectories this work introduces distancebased criteria for segmentation of object trajectories segmentation leads to simplification of the original objects into smaller less complex primitives that are better suited for storage and retrieval purposes previous work on trajectory segmentation attacked the problem locally segmenting separately each trajectory of the database therefore they did not directly optimize the interobject separability which is necessary for mining operations such as searching clustering and classification on large databases in this paper we analyze the trajectory segmentation problem from a global perspective utilizing data aware distancebased optimization techniques which optimize pairwise distance estimates hence leading to more efficient object pruning we first derive exact solutions of the distancebased formulation due to the intractable complexity of the exact solution we present anapproximate greedy solution that exploits forward searching of locally optimal solutions since the greedy solution also imposes a prohibitive computational cost we also put forward more light weight variancebased segmentation techniques which intelligently relax the pairwise distance only in the areas that affect the least the mining operation 
group formation in large social networks group formation in large social networks the processes by which communities come together attract new members and develop over time is a central research issue in the social sciences political movements professional organizations and religious denominations all provide fundamental examples of such communities in the digital domain online groups are becoming increasingly prominent due to the growth of community and social networking sites such as myspace and livejournal however the challenge of collecting and analyzing largescale timeresolved data on social groups and communities has left most basic questions about the evolution of such groups largely unresolved what are the structural features that influence whether individuals will join communities which communities will grow rapidly and how do the overlaps among pairs of communities change over timehere we address these questions using two large sources of data friendship links and community membership on livejournal and coauthorship and conference publications in dblp both of these datasets provide explicit userdefined communities where conferences serve as proxies for communities in dblp we study how the evolution of these communities relates to properties such as the structure of the underlying social networks we find that the propensity of individuals to join communities and of communities to grow rapidly depends in subtle ways on the underlying network structure for example the tendency of an individual to join a community is influenced not just by the number of friends he or she has within the community but also crucially by how those friends are connected to one another we use decisiontree techniques to identify the most significant structural determinants of these properties we also develop a novel methodology for measuring movement of individuals between communities and show how such movements are closely aligned with changes in the topics of interest within the communities 
detecting outliers using transduction and statistical testing detecting outliers using transduction and statistical testing outlier detection can uncover malicious behavior in fields like intrusion detection and fraud analysis although there has been a significant amount of work in outlier detection most of the algorithms proposed in the literature are based on a particular definition of outliers eg densitybased and use adhoc thresholds to detect them in this paper we present a novel technique to detect outliers with respect to an existing clustering model however the test can also be successfully utilized to recognize outliers when the clustering information is not available our method is based on transductive confidence machines which have been previously proposed as a mechanism to provide individual confidence measures on classification decisions the test uses hypothesis testing to prove or disprove whether a point is fit to be in each of the clusters of the model we experimentally demonstrate that the test is highly robust and produces very few misdiagnosed points even when no clustering information is available furthermore our experiments demonstrate the robustness of our method under the circumstances of data contaminated by outliers we finally show that our technique can be successfully applied to identify outliers in a noisy data set for which no information is available eg ground truth clustering structure etc as such our proposed methodology is capable of bootstrapping from a noisy data set a clean one that can be used to identify future outliers 
robust informationtheoretic clustering robust informationtheoretic clustering how do we find a natural clustering of a real world point set which contains an unknown number of clusters with different shapes and which may be contaminated by noise most clustering algorithms were designed with certain assumptions gaussianity they often require the user to give input parameters and they are sensitive to noise in this paper we propose a robust framework for determining a natural clustering of a given data set based on the minimum description length mdl principle the proposed framework robust informationtheoretic clustering ric is orthogonal to any known clustering algorithm given a preliminary clustering ric purifies these clusters from noise and adjusts the clusterings such that it simultaneously determines the most natural amount and shape subspace of the clusters our ric method can be combined with any clustering technique ranging from kmeans and kmedoids to advanced methods such as spectral clustering in fact ric is even able to purify and improve an initial coarse clustering even if we start with very simple methods such as gridbased space partitioning moreover ric scales well with the data set size extensive experiments on synthetic and real world data sets validate the proposed ric framework 
efficient anonymitypreserving data collection efficient anonymitypreserving data collection the output of a data mining algorithm is only as good as its inputs and individuals are often unwilling to provide accurate data about sensitive topics such as medical history and personal finance individuals maybe willing to share their data but only if they are assured that it will be used in an aggregate study and that it cannot be linked back to them protocols for anonymitypreserving data collection provide this assurance in the absence of trusted parties by allowing a set of mutually distrustful respondents to anonymously contribute data to an untrusted data minerto effectively provide anonymity a data collection protocol must be collusion resistant which means that even if all dishonest respondents collude with a dishonest data miner in an attempt to learn the associations between honest respondents and their responses they will be unable to do so to achieve collusion resistance previously proposed protocols for anonymitypreserving data collection have quadratically many communication rounds in the number of respondents and employ sometimes incorrectly complicated cryptographic techniques such as zeroknowledge proofswe describe a new protocol for anonymitypreserving collusion resistant data collection our protocol has linearly many communication rounds and achieves collusion resistance without relying on zeroknowledge proofs this makes it especially suitable for data mining scenarios with a large number of respondents 
outofcore frequent pattern mining on a commodity pc outofcore frequent pattern mining on a commodity pc in this work we focus on the problem of frequent itemset mining on large outofcore data sets after presenting a characterization of existing outofcore frequent itemset mining algorithms and their drawbacks we introduce our efficient highly scalable solution presented in the context of the fpgrowth algorithm our technique involves several novel ioconscious optimizations such as approximate hashbased sorting and blocking and leverages recent architectural advancements in commodity computers such as bit processing we evaluate the proposed optimizations on truly large data setsup to gb and show they yield greater than a fold execution time improvement finally we discuss the impact of this research in the context of other pattern mining challenges such as sequence mining and graph mining 
mining rankcorrelated sets of numerical attributes mining rankcorrelated sets of numerical attributes we study the mining of interesting patterns in the presence of numerical attributes instead of the usual discretization methods we propose the use of rank based measures to score the similarity of sets of numerical attributes new support measures for numerical data are introduced based on extensions of kendalls tau and spearmans footrule and rho we show how these support measures are related furthermore we introduce a novel type of pattern combining numerical and categorical attributes we give efficient algorithms to find all frequent patterns for the proposed support measures and evaluate their performance on reallife datasets 
nemofinder nemofinder recent works in network analysis have revealed the existence of network motifs in biological networks such as the proteinprotein interaction ppi networks however existing motif mining algorithms are not sufficiently scalable to find mesoscale network motifs also there has been little or no work to systematically exploit the extracted network motifs for dissecting the vast interactomeswe describe an efficient network motif discovery algorithm nemofinder that can mine mesoscale network motifs that are repeated and unique in large ppi networks using nemofinder we successfully discovered for the first time up to size network motifs in a large wholegenome s cerevisiae yeast ppi network we also show that such network motifs can be systematically exploited for indexing the reliability of ppi data that were generated via highly erroneous highthroughput experimental methods 
estimating the global pagerank of web communities estimating the global pagerank of web communities localized search engines are smallscale systems that index a particular community on the web they offer several benefits over their largescale counterparts in that they are relatively inexpensive to build and can provide more precise and complete search capability over their relevant domains one disadvantage such systems have over largescale search engines is the lack of global pagerank values such information is needed to assess the value of pages in the localized search domain within the context of the web as a whole in this paper we present wellmotivated algorithms to estimate the global pagerank values of a local domain the algorithms are all highly scalable in that given a local domain of size n they use on resources that include computation time bandwidth and storage we test our methods across a variety of localized domains including sitespecific domains and topicspecific domains we demonstrate that by crawling as few as n or n additional pages our methods can give excellent global pagerank estimates 
orthogonal nonnegative matrix tfactorizations for clustering orthogonal nonnegative matrix tfactorizations for clustering currently most research on nonnegative matrix factorization nmffocus on factor xfgt factorization we provide a systematicanalysis of factor xfsgt nmf while it unconstrained factor nmf is equivalent to it unconstrained factor nmf itconstrained factor nmf brings new features to it constrained factor nmf we study the orthogonality constraint because it leadsto rigorous clustering interpretation we provide new rules for updating fs g and prove the convergenceof these algorithms experiments on datasets and a real world casestudy are performed to show the capability of biorthogonal factornmf on simultaneously clustering rows and columns of the input datamatrix we provide a new approach of evaluating the quality ofclustering on words using class aggregate distribution andmultipeak distribution we also provide an overview of various nmf extensions andexamine their relationships 
a general framework for accurate and fast regression by data summarization in random decision trees a general framework for accurate and fast regression by data summarization in random decision trees predicting the values of continuous variable as a function of several independent variables is one of the most important problems for data mining a very large number of regression methods both parametric and nonparametric have been proposed in the past however since the list is quite extensive and many of these models make rather explicit strong yet different assumptions about the type of applicable problems and involve a lot of parameters and options choosing the appropriate regression methodology and then specifying the parameter values is a nonetrivial sometimes frustrating task for data mining practitioners choosing the inappropriate methodology can have rather disappointing results this issue is against the general utility of data mining software for examplelinear regression methods are straightforward and wellunderstood however since the linear assumption is very strong its performance is compromised for complicated nonlinear problems kernelbased methods perform quite well if the kernel functions are selected correctly in this paper we propose a straightforward approach based on summarizing the training data using an ensemble of random decisions trees it requires very little knowledge from the user yet is applicable to every type of regression problem that we are currently aware of we have experimented on a wide range of problems including those that parametric methods performwell a large selection of benchmark datasets for nonparametric regression as well as highly nonlinear stochastic problems our results are either significantly better than or identical to many approaches that are known to perform well on these problems 
reverse testing reverse testing one of the most important assumptions made by many classification algorithms is that the training and test sets are drawn from the same distribution ie the socalled stationary distribution assumption that the future and the past data sets are identical from a probabilistic standpoint in many domains of realworld applications such as marketing solicitation fraud detection drug testing loan approval subpopulation surveys school enrollment among others this is rarely the case this is because the only labeled sample available for training is biased in different ways due to a variety of practical reasons and limitations in these circumstances traditional methods to evaluate the expected generalization error of classification algorithms such as structural risk minimization tenfold crossvalidation and leaveoneout validation usually return poor estimates of which classification algorithm when trained on biased dataset will be the most accurate for future unbiased dataset among a number of competing candidates sometimes the estimated order of the learning algorithms accuracy could be so poor that it is not even better than random guessing thereforea method to determine the most accurate learner is needed for data mining under sample selection bias for many realworld applications we present such an approach that can determine which learner will perform the best on an unbiased test set given a possibly biased training set in a fraction of the computational cost to use crossvalidation based approaches 
quantifying trends accurately despite classifier error and class imbalance quantifying trends accurately despite classifier error and class imbalance this paper promotes a new task for supervised machine learning research quantification the pursuit of learning methods for accurately estimating the class distribution of a test set with no concern for predictions on individual cases a variant for cost quantification addresses the need to total up costs according to categories predicted by imperfect classifiers these tasks cover a large and important family of applications that measure trends over timethe paper establishes a research methodology and uses it to evaluate several proposed methods that involve selecting the classification threshold in a way that would spoil the accuracy of individual classifications in empirical tests median sweep methods show outstanding ability to estimate the class distribution despite wide disparity in testing and training conditions the paper addresses shifting class priors and costs but not concept drift in general 
assessing data mining results via swap randomization assessing data mining results via swap randomization the problem of assessing the significance of data mining results on highdimensional data sets has been studied extensively in the literature for problems such as mining frequent sets and finding correlations significance testing can be done by eg chisquare tests or many other methods however the results of such tests depend only on the specific attributes and not on the dataset as a whole moreover the tests are more difficult to apply to sets of patterns or other complex results of data mining in this paper we consider a simple randomization technique that deals with this shortcoming the approach consists of producing random datasets that have the same row and column margins with the given dataset computing the results of interest on the randomized instances and comparing them against the results on the actual data this randomization technique can be used to assess the results of many different types of data mining algorithms such as frequent sets clustering and rankings to generate random datasets with given margins we use variations of a markov chain approach which is based on a simple swap operation we give theoretical results on the efficiency of different randomization methods and apply the swap randomization method to several wellknown datasets our results indicate that for some datasets the structure discovered by the data mining algorithms is a random artifact while for other datasets the discovered structure conveys meaningful information 
a new efficient probabilistic model for mining labeled ordered trees a new efficient probabilistic model for mining labeled ordered trees mining frequent patterns is a general and important issue in data mining complex and unstructured or semistructured datasets have appeared in major data mining applications including text mining web mining and bioinformatics mining patterns from these datasets is the focus of many of the current data mining approaches we focus on labeled ordered trees typical datasets of semistructured data in data mining and propose a new probabilistic model and its efficient learning scheme for mining labeled ordered trees the proposed approach significantly improves the time and space complexity of an existing probabilistic modeling for labeled ordered trees while maintaining its expressive power we evaluated the performance of the proposed model comparing it with that of the existing model using synthetic as well as real datasets from the field of glycobiology experimental results showed that the proposed model drastically reduced the computation time of the competing model keeping the predictive power and avoiding overfitting to the training data finally we assessed our results using the proposed model on real data from a variety of biological viewpoints verifying known facts in glycobiology 
learning the unified kernel machines for classification learning the unified kernel machines for classification kernel machines have been shown as the stateoftheart learning techniques for classification in this paper we propose a novel general framework of learning the unified kernel machines ukm from both labeled and unlabeled data our proposed framework integrates supervised learning semisupervised kernel learning and active learning in a unified solution in the suggested framework we particularly focus our attention on designing a new semisupervised kernel learning method ie spectral kernel learning skl which is built on the principles of kernel target alignment and unsupervised kernel design our algorithm is related to an equivalent quadratic programming problem that can be efficiently solved empirical results have shown that our method is more effective and robust to learn the semisupervised kernels than traditional approaches based on the framework we present a specific paradigm of unified kernel machines with respect to kernel logistic regresions klr ie unified kernel logistic regression uklr we evaluate our proposed uklr classification scheme in comparison with traditional solutions the promising results show that our proposed uklr paradigm is more effective than the traditional classification approaches 
frequent subgraph mining in outerplanar graphs frequent subgraph mining in outerplanar graphs in recent years there has been an increased interest in algorithms that can perform frequent pattern discovery in large databases of graph structured objects while the frequent connected subgraph mining problem for tree datasets can be solved in incremental polynomial time it becomes intractable for arbitrary graph databases existing approaches have therefore resorted to various heuristic strategies and restrictions of the search space but have not identified a practically relevant tractable graph class beyond trees in this paper we define the class of so called tenuous outerplanar graphs a strict generalization of trees develop a frequent subgraph mining algorithm for tenuous outerplanar graphs that works in incremental polynomial time and evaluate the algorithm empirically on the nci molecular graph dataset 
adaptive event detection with timevarying poisson processes adaptive event detection with timevarying poisson processes timeseries of count data are generated in many different contexts such as web access logging freeway traffic monitoring and security logs associated with buildings since this data measures the aggregated behavior of individual human beings it typically exhibits a periodicity in time on a number of scales daily weeklyetc that reflects the rhythms of the underlying human activity and makes the data appear nonhomogeneous at the same time the data is often corrupted by a number of bursty periods of unusual behavior such as building events traffic accidents and so forth the data mining problem of finding and extracting these anomalous events is made difficult by both of these elements in this paper we describe a framework for unsupervised learning in this context based on a timevarying poisson process model that can also account for anomalous events we show how the parameters of this model can be learned from count time series using statistical estimation techniques we demonstrate the utility of this model on two datasets for which we have partial ground truth in the form of known events one from freeway traffic data and another from building access data and show that the model performs significantly better than a nonprobabilistic thresholdbased technique we also describe how the model can be used to investigate different degrees of periodicity in the data including systematic dayofweek and timeofday effects and make inferences about the detected events eg popularity or level of attendance our experimental results indicate that the proposed timevarying poisson model provides a robust and accurate framework for adaptively and autonomously learning how to separate unusual bursty events from traces of normal human activity 
training linear svms in linear time training linear svms in linear time linear support vector machines svms have become one of the most prominent machine learning techniques for highdimensional sparse data commonly encountered in applications like text classification wordsense disambiguation and drug design these applications involve a large number of examples n as well as a large number of features n while each example has only s ltlt n nonzero features this paper presents a cutting plane algorithm for training linear svms that provably has training time sn for classification problems and osn log nfor ordinal regression problems the algorithm is based on an alternative but equivalent formulation of the svm optimization problem empirically the cuttingplane algorithm is several orders of magnitude faster than decomposition methods like svm light for large datasets 
mining quantitative correlated patterns using an informationtheoretic approach mining quantitative correlated patterns using an informationtheoretic approach existing research on mining quantitative databases mainly focuses on mining associations however mining associations is too expensive to be practical in many cases in this paper we study mining correlations from quantitative databases and show that it is a more effective approach than mining associations we propose a new notion of quantitative correlated patterns qcps which is founded on two formal concepts mutual information and allconfidence we first devise a normalization on mutual information and apply it to qcp mining to capture the dependency between the attributes we further adopt allconfidence as a quality measure to control at a finer granularity the dependency between the attributes with specific quantitative intervals we also propose a supervised method to combine the consecutive intervals of the quantitative attributes based on mutual information such that the interval combining is guided by the dependency between the attributes we develop an algorithm qcomine to efficiently mine qcps by utilizing normalized mutual information and allconfidence to perform a twolevel pruning our experiments verify the efficiency of qcomine and the quality of the qcps 
maximally informative kitemsets and their efficient discovery maximally informative kitemsets and their efficient discovery in this paper we present a new approach to mining binary data we treat each binary feature item as a means of distinguishing two sets of examples our interest is in selecting from the total set of items an itemset of specified size such that the database is partitioned with as uniform a distribution over the parts as possible to achieve this goal we propose the use of joint entropy as a quality measure for itemsets and refer to optimal itemsets of cardinality k as maximally informative kitemsets we claim that this approach maximises distinctive power as well as minimises redundancy within the feature set a number of algorithms is presented for computing optimal itemsets efficiently 
measuring and extracting proximity in networks measuring and extracting proximity in networks measuring distance or some other form of proximity between objects is a standard data mining tool connection subgraphs were recently proposed as a way to demonstrate proximity between nodes in networks we propose a new way of measuring and extracting proximity in networks called cycle free effective conductancecfec our proximity measure can handle more than two endpoints directed edges is statistically wellbehaved and produces an effectiveness score for the computed subgraphs we provide an efficien talgorithm also we report experimental results and show examples for three large network data sets a telecommunications calling graph the imdb actors graph and an academic coauthorship network 
hierarchical topic segmentation of websites hierarchical topic segmentation of websites in this paper we consider the problem of identifying and segmenting topically cohesive regions in the url tree of a large website each page of the website is assumed to have a topic label or a distribution on topic labels generated using a standard classifier we develop a set of cost measures characterizing the benefit accrued by introducing a segmentation of the site based on the topic labels we propose a general framework to use these measures for describing the quality of a segmentation we also provide an efficient algorithm to find the best segmentation in this framework extensive experiments on humanlabeled data confirm the soundness of our framework and suggest that a judicious choice of cost measures allows the algorithm to perform surprisingly accurate topical segmentations 
new em derived from kullbackleibler divergence new em derived from kullbackleibler divergence we introduce a new em framework in which it is possible not only to optimize the model parameters but also the number of model components a key feature of our approach is that we use nonparametric density estimation to improve parametric density estimation in the em framework while the classical em algorithm estimates model parameters empirically using the data points themselves we estimate them using nonparametric density estimatesthere exist many possible applications that require optimal adjustment of model components we present experimental results in two domains one is polygonal approximation of laser range data which is an active research topic in robot navigation the other is grouping of edge pixels to contour boundaries which still belongs to unsolved problems in computer vision 
workloadaware anonymization workloadaware anonymization protecting data privacy is an important problem in microdata distribution anonymization algorithms typically aim to protect individual privacy with minimal impact on the quality of the resulting data while the bulk of previous work has measured quality through onesizefitsall measures we argue that quality is best judged with respect to the workload for which the data will ultimately be usedthis paper provides a suite of anonymization algorithms that produce an anonymous view based on a target class of workloads consisting of one or more data mining tasks as well as selection predicates an extensive experimental evaluation indicates that this approach is often more effective than previous anonymization techniques 
very sparse random projections very sparse random projections there has been considerable interest in random projections an approximate algorithm for estimating distances between pairs of points in a highdimensional vector space let a in rn x d be our n points in d dimensions the method multiplies a by a random matrix r in rd x k reducing the d dimensions down to just k for speeding up the computation r typically consists of entries of standard normal n it is well known that random projections preserve pairwise distances in the expectation achlioptas proposed sparse random projections by replacing the n entries in r with entries in with probabilities achieving a threefold speedup in processing timewe recommend using r of entries in with probabilities d d d for achieving a significant dfold speedup with little loss in accuracy 
rule interestingness analysis using olap operations rule interestingness analysis using olap operations the problem of interestingness of discovered rules has been investigated by many researchers the issue is that data mining algorithms often generate too many rules which make it very hard for the user to find the interesting ones over the years many techniques have been proposed however few have made it to reallife applications since august we have been working on a major application for motorola the objective is to find causes of cellular phone call failures from a large amount of usage log data class association rules have been shown to be suitable for this type of diagnostic data mining application we were also able to put several existing interestingness methods to the test which revealed some major shortcomings one of the main problems is that most existing methods treat rules individually however we discovered that users seldom regard a single rule to be interesting by itself a rule is only interesting in the context of some other rules furthermore in many cases each individual rule may not be interesting but a group of them together can represent an important piece of knowledge this led us to discover a deficiency of the current rule mining paradigm using nonzero minimum support and nonzero minimum confidence eliminates a large amount of context information which makes rule analysis difficult this paper proposes a novel approach to deal with all of these issues which casts rule analysis as olap operations and general impression mining this approach enables the user to explore the knowledge space to find useful knowledge easily and systematically it also provides a natural framework for visualization as an evidence of its effectiveness our system called opportunity map based on these ideas has been deployed and it is in daily use in motorola for finding actionable knowledge from its engineering and other types of data sets 
fast mining of high dimensional expressive contrast patterns using zerosuppressed binary decision diagrams fast mining of high dimensional expressive contrast patterns using zerosuppressed binary decision diagrams patterns of contrast are a very important way of comparing multidimensional datasets such patterns are able to capture regions of high difference between two classes of data and are useful for human experts and the construction of classifiers however mining such patterns is particularly challenging when the number of dimensions is large this paper describes a new technique for mining several varieties of contrast pattern based on the use of zerosuppressed binary decision diagrams zbdds a powerful data structure for manipulating sparse data we study the mining of both simple contrast patterns such as emerging patterns and more novel and complex contrasts which we call disjunctive emerging patterns a performance study demonstrates our zbdd technique is highly scalable substantially improves on state of the art mining for emerging patterns and can be effective for discovering complex contrasts from datasets with thousands of attributes 
unsupervised learning on kpartite graphs unsupervised learning on kpartite graphs various data mining applications involve data objects of multiple types that are related to each other which can be naturally formulated as a kpartite graph however the research on mining the hidden structures from a kpartite graph is still limited and preliminary in this paper we propose a general model the relation summary network to find the hidden structures the local cluster structures and the global community structures from a kpartite graph the model provides a principal framework for unsupervised learning on kpartite graphs of various structures under this model we derive a novel algorithm to identify the hidden structures of a kpartite graph by constructing a relation summary network to approximate the original kpartite graph under a broad range of distortion measures experiments on both synthetic and real datasets demonstrate the promise and effectiveness of the proposed model and algorithm we also establish the connections between existing clustering approaches and the proposed model to provide a unified view to the clustering approaches 
tensorcur decompositions for tensorbased data tensorcur decompositions for tensorbased data motivated by numerous applications in which the data may be modeled by a variable subscripted by three or more indices we develop a tensorbased extension of the matrix cur decomposition the tensorcur decomposition is most relevant as a data analysis tool when the data consist of one mode that is qualitatively different than the others in this case the tensorcur decomposition approximately expresses the original data tensor in terms of a basis consisting of underlying subtensors that are actual data elements and thus that have natural interpretation in terms ofthe processes generating the data in order to demonstrate the general applicability of this tensor decomposition we apply it to problems in two diverse domains of data analysis hyperspectral medical image analysis and consumer recommendation system analysis in the hyperspectral data application the tensorcur decomposition is used to compress the data and we show that classification quality is not substantially reduced even after substantial data compression in the recommendation system application the tensorcur decomposition is used to reconstruct missing entries in a userproductproduct preference tensor and we show that high quality recommendations can be made on the basis of a small number of basis users and a small number of productproduct comparisons from a new user 
generating semantic annotations for frequent patterns with context analysis generating semantic annotations for frequent patterns with context analysis as a fundamental data mining task frequent pattern mining has widespread applications in many different domains research in frequent pattern mining has so far mostly focused on developing efficient algorithms to discover various kinds of frequent patterns but little attention has been paid to the important nextstep interpreting the discovered frequent patterns although some recent work has studied the compression and summarization of frequent patterns the proposed techniques can only annotate a frequent pattern with nonsemantical information eg support which provides only limited help for a user to understand the patternsin this paper we propose the novel problem of generating semantic annotations for frequent patterns the goal is to annotate a frequent pattern with indepth concise and structured information that can better indicate the hidden meanings of the pattern we propose a general approach to generate such anannotation for a frequent pattern by constructing its context model selecting informative context indicators and extracting representative transactions and semantically similar patterns this general approach has potentially many applications such as generating a dictionarylike description for a pattern finding synonym patterns discovering semantic relations and summarizing semantic classes of a set of frequent patterns experiments on different datasets show that our approach is effective in generating semantic pattern annotations 
aggregating time partitions aggregating time partitions partitions of sequential data exist either per se or as a result of sequence segmentation algorithms it is often the case that the same timeline is partitioned in many different ways for example different segmentation algorithms produce different partitions of the same underlying data points in such cases we are interested in producing an aggregate partition ie a segmentation that agrees as much as possible with the input segmentations each partition is defined as a set of continuous nonoverlapping segments of the timeline we show that this problem can be solved optimally in polynomial time using dynamic programming we also propose faster greedy heuristics that work well in practice we experiment with our algorithms and we demonstrate their utility in clustering the behavior of mobilephone users and combining the results of different segmentation algorithms on genomic sequences 
using structure indices for efficient approximation of network properties using structure indices for efficient approximation of network properties statistics on networks have become vital to the study of relational data drawn from areas such as bibliometrics fraud detection bioinformatics and the internet calculating many of the most important measures such as betweenness centrality closeness centrality and graph diameterrequires identifying short paths in these networks however finding these short paths can be intractable for even moderatesize networks we introduce the concept of a network structure index nsi a composition of a set of annotations on every node in the network and a function that uses the annotations to estimate graph distance between pairs of nodes we present several varieties of nsis examine their time and space complexity and analyze their performance on synthetic and real data sets we show that creating an nsi for a given network enables extremely efficient and accurate estimation of a wide variety of network statistics on that network 
learning sparse metrics via linear programming learning sparse metrics via linear programming calculation of object similarity for example through a distance function is a common part of data mining and machine learning algorithms this calculation is crucial for efficiency since distances are usually evaluated a large number of times the classical example being querybyexample find objects that are similar to a given query object moreover the performance of these algorithms depends critically on choosing a good distance function however it is often the case that the correct distance is unknown or chosen by hand and its calculation is computationally expensive eg such as for large dimensional objects in this paper we propose a method for constructing relativedistance preserving lowdimensional mapping sparse mappings this method allows learning unknown distance functions or approximating known functions with the additional property of reducing distance computation time we present an algorithm that given examples of proximity comparisons among triples of objects object i is more like object j than object k learns a distance function in as few dimensions as possible that preserves these distance relationships the formulation is based on solving a linear programming optimization problem that finds an optimal mapping for the given dataset and distance relationships unlike other popular embedding algorithms this method can easily generalize to new points does not have local minima and explicitly models computational efficiency by finding a mapping that is sparse ie one that depends on a small subset of features or dimensions experimental evaluation shows that the proposed formulation compares favorably with a stateofthe art method in several publicly available datasets 
beyond streams and graphs beyond streams and graphs how do we find patterns in authorkeyword associations evolving over time or in data cubes with productbranchcustomer sales information matrix decompositions like principal component analysis pca and variants are invaluable tools for mining dimensionality reduction feature selection rule identification in numerous settings like streaming data text graphs social networks and many more however they have only two orders like author and keyword in the above examplewe propose to envision such higher order data as tensorsand tap the vast literature on the topic however these methods do not necessarily scale up let alone operate on semiinfinite streams thus we introduce the dynamic tensor analysis dta method and its variants dta provides a compact summary for highorder and highdimensional data and it also reveals the hidden correlations algorithmically we designed dta very carefully so that it is a scalable b space efficient it does not need to store the past and c fully automatic with no need for user defined parameters moreover we propose sta a streaming tensor analysis method which provides a fast streaming approximation to dtawe implemented all our methods and applied them in two real settings namely anomaly detection and multiway latent semantic indexing we used two real large datasets one on network flow data gb over month and one from dblp mb over years our experiments show that our methods are fast accurate and that they find interesting patterns and outliers on the real datasets 
acclimatizing taxonomic semantics for hierarchical content classification from semantics to datadriven taxonomy acclimatizing taxonomic semantics for hierarchical content classification from semantics to datadriven taxonomy hierarchical models have been shown to be effective in content classification however we observe through empirical study that the performance of a hierarchical model varies with given taxonomies even a semantically sound taxonomy has potential to change its structure for better classification by scrutinizing typical cases we elucidate why a given semanticsbased hierarchy does not work well in content classification and how it could be improved for accurate hierarchical classification with these understandings we propose effective localized solutions that modify the given taxonomy for accurate hierarchical classification we conduct extensive experiments on both toy and realworld data sets report improved performance and interesting findings and provide further analysis of algorithmic issues such as time complexity robustness and sensitivity to the number of features 
mining distancebased outliers from large databases in any metric space mining distancebased outliers from large databases in any metric space let r be a set of objects an object o r is an outlier if there exist less than k objects in r whose distances to o are at most r the values of k r and the distance metric are provided by a user at the run time the objective is to return all outliers with the smallest io costthis paper considers a generic version of the problem where no information is available for outlier computation except for objects mutual distances we prove an upper bound for the memory consumption which permits the discovery of all outliers by scanning the dataset times the upper bound turns out to be extremely low in practice eg less than of r since the actual memory capacity of a realistic dbms is typically larger we develop a novel algorithm which integrates our theoretical findings with carefullydesigned heuristics that leverage the additional memory to improve io efficiency our technique reports all outliers by scanning the dataset at most twice in some cases even once and significantly outperforms the existing solutions by a factor up to an order of magnitude 
centerpiece subgraphs centerpiece subgraphs given q nodes in a social network say authorship network how can we find the nodeauthor that is the centerpiece and has direct or indirect connections to all or most of them for example this node could be the common advisor or someone who started the research area that the q nodes belong to isomorphic scenarios appear in law enforcement find the mastermind criminal connected to all current suspects gene regulatory networks find the protein that participates in pathways with all or most of the given q proteins viral marketing and many moreconnection subgraphs is an important first step handling the case of q query nodes then the connection subgraph algorithm finds the b intermediate nodes that provide a good connection between the two original query nodeshere we generalize the challenge in multiple dimensions first we allow more than two query nodes second we allow a whole family of queries ranging from or to and with softand inbetween finally we design and compare a fast approximation and study the qualityspeed tradeoffwe also present experiments on the dblp dataset the experiments confirm that our proposed method naturally deals with multisource queries and that the resulting subgraphs agree with our intuition wallclock timing results on the dblp dataset show that our proposed approximation achieve good accuracy for about speedup 
anonymizing sequential releases anonymizing sequential releases an organization makes a new release as new information become available releases a tailored view for each data request releases sensitive information and identifying information separately the availability of related releases sharpens the identification of individuals by a global quasiidentifier consisting of attributes from related releases since it is not an option to anonymize previously released data the current release must be anonymized to ensure that a global quasiidentifier is not effective for identification in this paper we study the sequential anonymization problem under this assumption a key question is how to anonymize the current release so that it cannot be linked to previous releases yet remains useful for its own release purpose we introduce the lossy join a negative property in relational database design as a way to hide the join relationship among releases and propose a scalable and practical solution 
topics over time topics over time this paper presents an ldastyle topic model that captures not only the lowdimensional structure of data but also how the structure changes over time unlike other recent work that relies on markov assumptions or discretization of time here each topic is associated with a continuous distribution over timestamps and for each generated document the mixture distribution over topics is influenced by both word cooccurrences and the documents timestamp thus the meaning of a particular topic can be relied upon as constant but the topics occurrence and correlations change significantly over time we present results on nine months of personal email years of nips research papers and over years of presidential stateoftheunion addresses showing improved topics better timestamp prediction and interpretable trends 
discovering significant rules discovering significant rules in many applications association rules will only be interesting if they represent nontrivial correlations between all constituent items numerous techniques have been developed that seek to avoid false discoveries however while all provide useful solutions to aspects of this problem none provides a generic solution that is both flexible enough to accommodate varying definitions of true and false discoveries and powerful enough to provide strict control over the risk of false discoveries this paper presents generic techniques that allow definitions of true and false discoveries to be specified in terms of arbitrary statistical hypothesis tests and which provide strict control over the experiment wise risk of false discoveries 
extracting redundancyaware topk patterns extracting redundancyaware topk patterns observed in many applications there is a potential need of extracting a small set of frequent patterns having not only high significance but also low redundancy the significance is usually defined by the context of applications previous studies have been concentrating on how to compute topk significant patterns or how to remove redundancy among patterns separately there is limited work on finding those topk patterns which demonstrate highsignificance and lowredundancy simultaneouslyin this paper we study the problem of extracting redundancyaware topk patterns from a large collection of frequent patterns we first examine the evaluation functions for measuring the combined significance of a pattern set and propose the mms maximal marginal significance as the problem formulation the problem is known as nphard we further present a greedy algorithm which approximates the optimal solution with performance bound olog k with conditions on redundancy where k is the number of reported patterns the direct usage of redundancyaware topk patterns is illustrated through two real applications disk block prefetch and document theme extraction our method can also be applied to processing redundancyaware topk queries in traditional database 
regularized discriminant analysis for high dimensional low sample size data regularized discriminant analysis for high dimensional low sample size data linear and quadratic discriminant analysis have been used widely in many areas of data mining machine learning and bioinformatics friedman proposed a compromise between linear and quadratic discriminant analysis called regularized discriminant analysis rda which has been shown to be more flexible in dealing with various class distributions rda applies the regularization techniques by employing two regularization parameters which are chosen to jointly maximize the classification performance the optimal pair of parameters is commonly estimated via crossvalidation from a set of candidate pairs it is computationally prohibitive for high dimensional data especially when the candidate set is large which limits the applications of rda to low dimensional datain this paper a novel algorithm for rda is presented for high dimensional data it can estimate the optimal regularization parameters from a large set of parameter candidates efficiently experiments on a variety of datasets confirm the claimed theoretical estimate of the efficiency and also show that for a properly chosen pair of regularization parameters rda performs favorably in classification in comparison with other existing classification methods 
supervised probabilistic principal component analysis supervised probabilistic principal component analysis principal component analysis pca has been extensively applied in data mining pattern recognition and information retrieval for unsupervised dimensionality reduction when labels of data are available eg in a classification or regression task pca is however not able to use this information the problem is more interesting if only part of the input data are labeled ie in a semisupervised setting in this paper we propose a supervised pca model called sppca and a semisupervised pca model called sppca both of which are extensions of a probabilistic pca model the proposed models are able to incorporate the label information into the projection phase and can naturally handle multiple outputs ie in multitask learning problems we derive an efficient em learning algorithm for both models and also provide theoretical justifications of the model behaviors sppca and sppca are compared with other supervised projection methods on various learning tasks and show not only promising performance but also good scalability 
extracting keysubstringgroup features for text classification extracting keysubstringgroup features for text classification in many text classification applications it is appealing to take every document as a string of characters rather than a bag of words previous research studies in this area mostly focused on different variants of generative markov chain models although discriminative machine learning methods like support vector machine svm have been quite successful in text classification with word features it is neither effective nor efficient to apply them straightforwardly taking all substrings in the corpus as features in this paper we propose to partition all substrings into statistical equivalence groups and then pick those groups which are important in the statistical sense as features named keysubstringgroup features for text classification in particular we propose a suffix tree based algorithm that can extract such features in linear time with respect to the total number of characters in the corpus our experiments on english chinese and greek datasets show that svm with keysubstringgroup features can achieve outstanding performance for various text classification tasks 
event detection from evolution of clickthrough data event detection from evolution of clickthrough data previous efforts on event detection from the web have focused primarily on web content and structure data ignoring the rich collection of web log data in this paper we propose the first approach to detect events from the clickthrough data which is the log data of web search engines the intuition behind event detection from clickthrough data is that such data is often eventdriven and each event can be represented as a set ofquerypage pairs that are not only semantically similar but also have similar evolution pattern over time given the clickthrough data in our proposed approach we first segment it into a sequence of bipartite graphs based on theuserdefined time granularity next the sequence of bipartite graphs is represented as a vectorbased graph which records the semantic and evolutionary relationships between queries and pages after that the vectorbased graph is transformed into its dual graph where each node is a querypage pair that will be used to represent real world events then the problem of event detection is equivalent to the problem of clustering the dual graph of the vectorbased graph the clustering process is based on a twophase graph cut algorithm in the first phase querypage pairs are clustered based on thesemanticbased similarity such that each cluster in the result corresponds to a specific topic in the second phase querypage pairs related to the same topic are further clustered based on the evolution patternbased similarity such that each cluster is expected to represent a specific event under the specific topic experiments with real clickthrough data collected from a commercial web search engine show that the proposed approach produces high quality results 
simultaneous record detection and attribute labeling in web data extraction simultaneous record detection and attribute labeling in web data extraction recent work has shown the feasibility and promise of templateindependent web data extraction however existing approaches use decoupled strategies attempting to do data record detection and attribute labeling in two separate phases in this paper we show that separately extracting data records and attributes is highly ineffective and propose a probabilistic model to perform these two tasks simultaneously in our approach record detection can benefit from the availability of semantics required in attribute labeling and at the same time the accuracy of attribute labeling can be improved when data records are labeled in a collective manner the proposed model is called hierarchical conditional random fields it can efficiently integrate all useful features by learning their importance and it can also incorporate hierarchical interactions which are very important for web data extraction we empirically compare the proposed model with existing decoupled approaches for product information extraction and the results show significant improvements in both record detection and attribute labeling 
outlier detection by active learning outlier detection by active learning most existing approaches to outlier detection are based on density estimation methods there are two notable issues with these methods one is the lack of explanation for outlier flagging decisions and the other is the relatively high computational requirement in this paper we present a novel approach to outlier detection based on classification in an attempt to address both of these issues our approach isbased on two key ideas first we present a simple reduction of outlier detection to classification via a procedure that involves applying classification to a labeled data set containing artificially generated examples that play the role of potential outliers once the task has been reduced to classification we then invoke a selective sampling mechanism based on active learning to the reduced classification problem we empirically evaluate the proposed approach using a number of data sets and find that our method is superior to other methods based on the same reduction to classification but using standard classification methods we also show that it is competitive to the stateoftheart outlier detection methods in the literature based on density estimation while significantly improving the computational complexity and explanatory power 
on privacy preservation against adversarial data mining on privacy preservation against adversarial data mining privacy preserving data processing has become an important topic recently because of advances in hardware technology which have lead to widespread proliferation of demographic and sensitive data a rudimentary way to preserve privacy is to simply hide the information in some of the sensitive fields picked by a user however such a method is far from satisfactory in its ability to prevent adversarial data mining real data records are not randomly distributed as a result some fields in the records may be correlated with one another if the correlation is sufficiently high it may be possible for an adversary to predict some of the sensitive fields using other fieldsin this paper we study the problem of privacy preservation against adversarial data mining which is to hide a minimal set of entries so that the privacy of the sensitive fields are satisfactorily preserved in other words even by data mining an adversary still cannot accurately recover the hidden data entries we model the problem concisely and develop an efficient heuristic algorithm which can find good solutions in practice an extensive performance study is conducted on both synthetic and real data sets to examine the effectiveness of our approach 
cccs cccs in this paper we propose cccs a new algorithm for classification based on association rule mining the key innovation in cccs is the use of a new measure the complement class support ccs whose application results in rules which are guaranteed to be positively correlated furthermore the antimonotonic property that ccs possesses has very different semantics visavis the traditional support measure in particular good rules have a low ccs value this makes ccs an ideal measure to use in conjunction with a topdown algorithm finally the nature of ccs allows the pruning of rules without the setting of any threshold parameter to the best of our knowledge this is the first thresholdfree algorithm in association rule mining for classification 
a framework for analysis of dynamic social networks a framework for analysis of dynamic social networks finding patterns of social interaction within a population has wideranging applications including disease modeling cultural and information transmission and behavioral ecology social interactions are often modeled with networks a key characteristic of social interactions is their continual change however most past analyses of social networks are essentially static in that all information about the time that social interactions take place is discarded in this paper we propose a new mathematical and computational framework that enables analysis of dynamic social networks and that explicitly makes use of information about when social interactions occur 
querytime entity resolution querytime entity resolution the goal of entity resolution is to reconcile database references corresponding to the same realworld entities given the abundance of publicly available databases where entities are not resolved we motivate the problem of quickly processing queries that require resolved entities from such unclean databases we propose a twostage collective resolution strategy for processing queries we then show how it can be performed onthefly by adaptively extracting and resolving those database references that are the most helpful for resolving the query we validate our approach on two large realworld publication databases where we show the usefulness of collective resolution and at the same time demonstrate the need for adaptive strategies for query processing we then show how the same queries can be answered in real time using our adaptive approach while preserving the gains of collective resolution 
model compression model compression often the best performing supervised learning models are ensembles of hundreds or thousands of baselevel classifiers unfortunately the space required to store this many classifiers and the time required to execute them at runtime prohibits their use in applications where test sets are large eg google where storage space is at a premium eg pdas and where computational power is limited eg hearing aids we present a method for compressing large complex ensembles into smaller faster models usually without significant loss in performance 
classification features for attack detection in collaborative recommender systems classification features for attack detection in collaborative recommender systems collaborative recommender systems are highly vulnerable to attack attackers can use automated means to inject a large number of biased profiles into such a system resulting in recommendations that favor or disfavor given items since collaborative recommender systems must be open to user input it is difficult to design a system that cannot be so attacked researchers studying robust recommendation have therefore begun to identify types of attacks and study mechanisms for recognizing and defeating them in this paper we propose and study different attributes derived from user profiles for their utility in attack detection we show that a machine learning classification approach that includes attributes derived from attack models is more successful than more generalized detection algorithms previously studied 
singlepass online learning singlepass online learning to learn concepts over massive data streams it is essential to design inference and learning methods that operate in real time with limited memory online learning methods such as perceptron or winnow are naturally suited to stream processing however in practice multiple passes over the same training data are required to achieve accuracy comparable to stateoftheart batch learners in the current work we address the problem of training an online learner with a single passover the data we evaluate several existing methods and also propose a new modification of margin balanced winnow which has performance comparable to linear svm we also explore the effect of averaging aka voting on online learning finally we describe how the new modified margin balanced winnow algorithm can be naturally adapted to perform feature selection this scheme performs comparably to widelyused batch feature selection methods like information gain or chisquare with the advantage of being able to select features onthefly taken together these techniques allow singlepass online learning to be competitive with batch techniques and still maintain the advantages of online learning 
evolutionary clustering evolutionary clustering we consider the problem of clustering data over time an evolutionary clustering should simultaneously optimize two potentially conflicting criteria first the clustering at any point in time should remain faithful to the current data as much as possible and second the clustering should not shift dramatically from one timestep to the next we present a generic framework for this problem and discuss evolutionary versions of two widelyused clustering algorithms within this framework kmeans and agglomerative hierarchical clustering we extensively evaluate these algorithms on real data sets and show that our algorithms can simultaneously attain both high accuracy in capturing todays data and high fidelity in reflecting yesterdays clustering 
algorithms for discovering bucket orders from data algorithms for discovering bucket orders from data ordering and ranking items of different types are important tasks in various applications such as query processing and scientific data mining a total order for the items can be misleading since there are groups of items that have practically equal rankswe consider bucket orders ie total orders with ties they can be used to capture the essential order information without overfitting the data they form a useful concept class between total orders and arbitrary partial orders we address the question of finding a bucket order for a set of items given pairwise precedence information between the items we also discuss methods for computing the pairwise precedence datawe describe simple and efficient algorithms for finding good bucket orders several of the algorithms have a provable approximation guarantee and they scale well to large datasets we provide experimental results on artificial and a real data that show the usefulness of bucket orders and demonstrate the accuracy and efficiency of the algorithms 
mining relational data through correlationbased multiple view validation mining relational data through correlationbased multiple view validation commercial relational databases currently store vast amounts of realworld data the data within these relational repositories are represented by multiple relations which are interconnected by means of foreign key joins the mining of such interrelated data poses a major challenge to the data mining community unfortunately traditional data mining algorithms usually only explore one relation the socalled target relation thus excluding crucial knowledge embedded in the related socalled background relations in this paper we propose a novel approach for classifying relational such domains this strategy employs multiple views to capture crucial information not only from the target relation but also from related relations this information is integrated into the relational mining process the framework presented here firstly explore the relational domain to partition its features space into multiple subsets subsequently these subsets are used to construct multiple uncorrelated views based on a novel correlationbased view validation method against the target concept finally the knowledge possessed by multiple views are incorporated into a metalearning mechanism to augment one another based on this framework a wide range of conventional data mining methods can be applied to mine relational databases our experiments on benchmark realworld data sets show that the proposed method achieves promising results both in terms of overall accuracy obtained and run time when compared with two other relational data mining approaches 
recommendation method for extending subscription periods recommendation method for extending subscription periods online stores providing subscription services need to extend user subscription periods as long as possible to increase their profits conventional recommendation methods recommend items that best coincide with users interests to maximize the purchase probability which does not necessarily contribute to extend subscription periods we present a novel recommendation method for subscription services that maximizes the probability of the subscription period being extended our method finds frequent purchase patterns in the long subscription period users and recommends items for a new user to simulate the found patterns using survival analysis techniques we efficiently extract information from the log data for finding the patterns furthermore we infer users interests from purchase histories based on maximum entropy models and use the interests to improve the recommendations since a longer subscription period is the result of greater user satisfaction our method benefits users as well as online stores we evaluate our method using the real log data of an online cartoon distribution service for cellphone in japan 
dynamic realtime forecasting of online auctions via functional models dynamic realtime forecasting of online auctions via functional models we propose a dynamic model for forecasting price in online auctions one of the key features of our model is that it operates during the liveauction which makes it different from previous approaches that only consider static models our model is also different with respect to how information about price is incorporated while one part of the model is based on the more traditional notion of an auctions pricelevel another part incorporates its dynamics in the form of a prices velocity and acceleration in that sense it incorporates key features of a dynamic environment such as an online auction the use of novel functional data methodology allows us to measure and subsequently include dynamic price characteristics we illustrate our model on a diverse set of ebay auctions across many different book categories we find significantly higher prediction accuracy compared to standard approaches 
polynomial association rules with applications to logistic regression polynomial association rules with applications to logistic regression a new class of associations polynomial itemsets and polynomial association rules is presented which allows for discovering nonlinear relationships between numeric attributes without discretization for binary attributes proposed associations reduce to classic itemsets and association rules many standard association rule mining algorithms can be adapted to finding polynomial itemsets and association rules we applied polynomial associations to add nonlinear terms to logistic regression models significant performance improvement was achieved over stepwise methods traditionally used in statistics with comparable accuracy 
cfistream cfistream mining frequent closed itemsets provides complete and condensed information for nonredundant association rules generation extensive studies have been done on mining frequent closed itemsets but they are mainly intended for traditional transaction databases and thus do not take data stream characteristics into consideration in this paper we propose a novel approach for mining closed frequent itemsets over data streams it computes and maintains closed itemsets online and incrementally and can output the current closed frequent itemsets in real time based on users specified thresholds experimental results show that our proposed method is both time and space efficient has good scalability as the number of transactions processed increases and adapts very rapidly to the change in data streams 
reducing the human overhead in text categorization reducing the human overhead in text categorization many applications in text processing require significant human effort for either labeling large document collections when learning statistical models or extrapolating rules from them when using knowledge engineering in this work we describe away to reduce this effort while retaining the methods accuracy by constructing a hybrid classifier that utilizes human reasoning over automatically discovered text patterns to complement machine learning using a standard sentimentclassification dataset and real customer feedback data we demonstrate that the resulting technique results in significant reduction of the human effort required to obtain a given classification accuracy moreover the hybrid text classifier also results in a significant boost in accuracy over machinelearning based classifiers when a comparable amount of labeled data is used 
algorithms for storytelling algorithms for storytelling we formulate a new data mining problem called it storytelling as a generalization of redescription mining in traditional redescription mining we are given a set of objects and a collection of subsets defined over these objects the goal is to view the set system as a vocabulary and identify two expressions in this vocabulary that induce the same set of objects storytelling on the other hand aims to explicitly relate object sets that are disjoint and hence maximally dissimilar by finding a chain of approximate redescriptions between the sets this problem finds applications in bioinformatics for instance where the biologist is trying to relate a set of genes expressed in one experiment to another set implicated in a different pathway we outline an efficient storytelling implementation that embeds the cart wheels redescription mining algorithm in an a search procedure using the former to supply next move operators on search branches to the latter this approach is practical and effective for mining large datasets and at the same time exploits the structure of partitions imposed by the given vocabulary three application case studies are presented a study of word overlaps in large english dictionaries exploring connections between genesets in a bioinformatics dataset and relating publications in the pubmed index of abstracts 
structure and evolution of online social networks structure and evolution of online social networks in this paper we consider the evolution of structure within large online social networks we present a series of measurements of two such networks together comprising in excess of five million people and ten million friendship links annotated with metadata capturing the time of every event in the life of the network our measurements expose a surprising segmentation of these networks into three regions singletons who do not participate in the network isolated communities which overwhelmingly display star structure and a giant component anchored by a wellconnected core region which persists even in the absence of starswe present a simple model of network growth which captures these aspects of component structure the model follows our experimental results characterizing users as either passive members of the network inviters who encourage offline friends and acquaintances to migrate online and linkers who fully participate in the social evolution of the network 
cryptographically private support vector machines cryptographically private support vector machines we propose private protocols implementing the kernel adatron and kernel perceptron learning algorithms give private classification protocols and private polynomial kernel computation protocols the new protocols return their outputs either the kernel value the classifier or the classifications in encrypted form so that they can be decrypted only by a common agreement by the protocol participants we show how to use the encrypted classifications to privately estimate many properties of the data and the classifier the new svm classifiers are the first to be proven private according to the standard cryptographic definitions 
bias and controversy bias and controversy in this paper we investigate how deviation in evaluation activities may reveal bias on the part of reviewers and controversy on the part of evaluated objects we focus on a datacentric approach where the evaluation data is assumed to represent the ground truth the standard statistical approaches take evaluation and deviation at face value we argue that attention should be paid to the subjectivity of evaluation judging the evaluation score not just on what is being said deviation but also on who says it reviewer as well as on whom it is said about object furthermore we observe that bias and controversy are mutually dependent as there is more bias if there is higher deviation on a less controversial object to address this mutual dependency we propose a reinforcement model to identify bias and controversy we test our model on reallife data to verify its applicability 
sampling from large graphs sampling from large graphs given a huge real graph how can we derive a representative sample there are many known algorithms to compute interesting measures shortest paths centrality betweenness etc but several of them become impractical for large graphs thus graph sampling is essentialthe natural questions to ask are a which sampling method to use b how small can the sample size be and c how to scale up the measurements of the sample eg the diameter to get estimates for the large graph the deeper underlying question is subtle how do we measure successwe answer the above questions and test our answers by thorough experiments on several diverse datasets spanning thousands nodes and edges we consider several sampling methods propose novel methods to check the goodness of sampling and develop a set of scaling laws that describe relations between the properties of the original and the samplein addition to the theoretical contributions the practical conclusions from our work are sampling strategies based on edge selection do not perform well simple uniform random node selection performs surprisingly well overall best performing methods are the ones based on randomwalks and forest fire they match very accurately both static as well as evolutionary graph patterns with sample sizes down to about of the original graph 
clustering pairwise dissimilarity data into partially ordered sets clustering pairwise dissimilarity data into partially ordered sets ontologies represent data relationships as hierarchies of possibly overlapping classes ontologies are closely related to clustering hierarchies and in this article we explore this relationship in depth in particular we examine the space of ontologies that can be generated by pairwise dissimilarity matrices we demonstrate that classical clustering algorithms which take dissimilarity matrices as inputs do not incorporate all available information in fact only special types of dissimilarity matrices can be exactly preserved by previous clustering methods we model ontologies as a partially ordered set poset over the subset relation in this paper we propose a new clustering algorithm that generates a partially ordered set of clusters from a dissimilarity matrix 
visual data mining using principled projection algorithms and information visualization techniques visual data mining using principled projection algorithms and information visualization techniques we introduce a flexible visual data mining framework which combines advanced projection algorithms from the machine learning domain and visual techniques developed in the information visualization domain the advantage of such an interface is that the user is directly involved in the data mining process we integrate principled projection algorithms such as generative topographic mapping gtm and hierarchical gtm hgtm with powerful visual techniques such as magnification factors directional curvatures parallel coordinates and billboarding to provide a visual data mining framework results on a reallife chemoinformatics dataset using gtm are promising and have been analytically compared with the results from the traditional projection methods it is also shown that the hgtm algorithm provides additional value for large datasets the computational complexity of these algorithms is discussed to demonstrate their suitability for the visual data mining framework 
a mixture model for contextual text mining a mixture model for contextual text mining contextual text mining is concerned with extracting topical themes from a text collection with context information eg time and location and comparinganalyzing the variations of themes over different contexts since the topics covered in a document are usually related to the context of the document analyzing topical themes within context can potentially reveal many interesting theme patterns in this paper we generalize some of these models proposed in the previous work and we propose a new general probabilistic model for contextual text mining that can cover several existing models as special cases specifically we extend the probabilistic latent semantic analysis plsa model by introducing context variables to model the context of a document the proposed mixture model called contextual probabilistic latent semantic analysis cplsa model can be applied to many interesting mining tasks such as temporal text mining spatiotemporal text mining authortopic analysis and crosscollection comparative analysis empirical experiments show that the proposed mixture model can discover themes and their contextual variations effectively 
a new multiview regression approach with an application to customer wallet estimation a new multiview regression approach with an application to customer wallet estimation motivated by the problem of customer wallet estimation we propose a new setting for multiview regression where we learn a completely unobserved target in our case customer wallet by modeling it as a central link in a directed graphical model connecting multiple sets of observed variables the resulting conditional independence allows us to reduce the maximum discriminative likelihood estimation problem to a convex optimization problem for exponential linear models we show that under certain modeling assumptions in particular when there exist two conditionally independent views and the noise is gaussian this problem can be reduced to a single least squares regression thus for this specific but widely applicable setting the unsupervised multiview problem can be solved via a simple supervised learning approach this reduction also allows us to test the statistical independence assumptions underlying the graphical model and perform variable selection we demonstrate the effectiveness of our approach on our motivating problem of customer wallet estimation and on simulation data 
efficient multidimensional data representations based on multiple correspondence analysis efficient multidimensional data representations based on multiple correspondence analysis in the on line analytical processing olap context exploration of huge and sparse data cubes is a tedious task which does not always lead to efficient results in this paper we couple olap with the multiple correspondence analysis mca in order to enhance visual representations of data cubes and thus facilitate their interpretations and analysis we also provide a quality criterion to measure the relevance of obtained representations the criterion is based on a geometric neighborhood concept and a similarity metric between cells of a data cube experimental results on real data proved the interest and the efficiency of our approach 
algorithms for time series knowledge mining algorithms for time series knowledge mining temporal patterns composed of symbolic intervals are commonly formulated with allens interval relations originating in temporal reasoning this representation has severe disadvantages for knowledge discovery the time series knowledge representation tskr is a new hierarchical language for interval patterns expressing the temporal concepts of coincidence and partial order we present effective and efficient mining algorithms for such patterns based on itemset techniques a novel form of search space pruning effectively reduces the size of the mining result to ease interpretation and speed up the algorithms on a real data set a concise set of tskr patterns can explain the underlying temporal phenomena whereas the patterns found with allens relations are far more numerous yet only explain fragments of the data 
clustering based large margin classification clustering based large margin classification this paper presents a novel second order cone programming socp formulation for large scale binary classification tasks assuming that the class conditional densities are mixture distributions where each component of the mixture has a spherical covariance the second order statistics of the components can be estimated efficiently using clustering algorithms like birch for each cluster the second order moments are used to derive a second order cone constraint via a chebyshevcantelli inequality this constraint ensures that any data point in the cluster is classified correctly with a high probability this leads to a large margin socp formulation whose size depends on the number of clusters rather than the number of training data points hence the proposed formulation scales well for large datasets when compared to the stateoftheart classifiers support vector machines svms experiments on real world and synthetic datasets show that the proposed algorithm outperforms svm solvers in terms of training time and achieves similar accuracies 
statistical entitytopic models statistical entitytopic models the primary purpose of news articles is to convey information about who what when and where but learning and summarizing these relationships for collections of thousands to millions of articles is difficult while statistical topic models have been highly successful at topically summarizing huge collections of text documents they do not explicitly address the textual interactions between whowhere ie named entities persons organizations locations and what ie the topics we present new graphical models that directly learn the relationship between topics discussed in news articles and entities mentioned in each article we show how these entitytopic models through a better understanding of the entitytopic relationships are better at making predictions about entities 
mining for misconfigured machines in grid systems mining for misconfigured machines in grid systems grid systems are proving increasingly useful for managing the batch computing jobs of organizations one wellknown example is intel whose internally developed netbatch system manages tens of thousands of machines the size heterogeneity and complexity of grid systems make them very difficult however to configure this often results in misconfigured machines which may adversely affect the entire systemwe investigate a distributed data mining approach for detection of misconfigured machines our grid monitoring system gms nonintrusively collects data from all sources log files system services etc available throughout the grid system it converts raw data to semantically meaningful data and stores this data on the machine it was obtained from limiting incurred overhead and allowing scalability afterwards when analysis is requested a distributed outliers detection algorithm is employed to identify misconfigured machines the algorithm itself is implemented as a recursive workflow of grid jobs it is especially suited to grid systems in which the machines might be unavailable most of the time and often fail altogether 
automatic mining of fruit fly embryo images automatic mining of fruit fly embryo images we present femine an automatic system for imagebased gene expression analysis we perform experiments on the largest publicly available collection of drosophila ish in situ hybridization images showing that our femine system achieves excellent performance in classification clustering and contentbased image retrieval the major innovation of femine is the use of automatically discovered latent spatial themes of gene expressions lges in the wholeembryo context as opposed to patterns in nearly disjoint portions of an embryo proposed in previous methods 
nave filterbots for robust coldstart recommendations nave filterbots for robust coldstart recommendations the goal of a recommender system is to suggest items of interest to a user based on historical behavior of a community of users given detailed enough history itembased collaborative filtering cf often performs as well or better than almost any other recommendation method however in coldstart situations where a user an item or the entire system is new simple nonpersonalized recommendations often fare better we improve the scalability and performance of a previous approach to handling coldstart situations that uses filterbots or surrogate users that rate items based only on user or item attributes we show that introducing a very small number of simple filterbots helps make cf algorithms more robust in particular adding just seven global filterbots improves both userbased and itembased cf in coldstart user coldstart item and coldstart system settings performance is better when data is scarce performance is no worse when data is plentiful and algorithm efficiency is negligibly affected we systematically compare a nonpersonalized baseline userbased cf itembased cf and our botaugmented user and itembased cf algorithms using three data sets yahoo movies movielens and eachmovie with the normalized mae metric in three types of coldstart situations the advantage of our nave filterbot approach is most pronounced for the yahoo data the sparsest of the three data sets 
monic monic there is much recent work on detecting and tracking change in clusters often based on the study of the spatiotemporal properties of a cluster for the many applications where cluster change is relevant among them customer relationship management fraud detection and marketing it is also necessary to provide insights about the nature of cluster change is a cluster corresponding to a group of customers simply disappearing or are its members migrating to other clusters is a new emerging cluster reflecting a new target group of customers or does it rather consist of existing customers whose preferences shift to answer such questions we propose the framework monic for modeling and tracking of cluster transitions our cluster transition model encompasses changes that involve more than one cluster thus allowing for insights on cluster change in the whole clustering our transition tracking mechanism is not based on the topological properties of clusters which are only available for some types of clustering but on the contents of the underlying data stream we present our first results on monitoring cluster transitions over the acm digital library 
combining linguistic and statistical analysis to extract relations from web documents combining linguistic and statistical analysis to extract relations from web documents the world wide web provides a nearly endless source of knowledge which is mostly given in natural language a first step towards exploiting this data automatically could be to extract pairs of a given semantic relation from text documents for example all pairs of a person and her birthdate one strategy for this task is to find text patterns that express the semantic relation to generalize these patterns and to apply them to a corpus to find new pairs in this paper we show that this approach profits significantly when deep linguistic structures are used instead of surface text patterns we demonstrate how linguistic structures can be represented for machine learning and we provide a theoretical analysis of the pattern matching approach we show the benefits of our approach by extensive experiments with our prototype system leila 
mining longterm search history to improve search accuracy mining longterm search history to improve search accuracy longterm search history contains rich information about a users search preferences which can be used as search context to improve retrieval performance in this paper we study statistical language modeling based methods to mine contextual information from longterm search history and exploit it for a more accurate estimate of the query language model experiments on real web search data show that the algorithms are effective in improving search accuracy for both fresh and recurring queries the best performance is achieved when using clickthrough data of past searches that are related to the current query 
efficient kernel feature extraction for massive data sets efficient kernel feature extraction for massive data sets maximum margin discriminant analysis mmda was proposed that uses the margin idea for feature extraction it often outperforms traditional methods like kernel principal component analysis kpca and kernel fisher discriminant analysis kfd however as in other kernel methods its time complexity is cubic in the number of training points m and is thus computationally inefficient on massive data sets in this paper we propose an approximation algorithm for obtaining the mmda features by extending the core vector machines the resultant time complexity is only linear in m while its space complexity is independent of m extensive comparisons with the original mmda kpca and kfd on a number of large data sets show that the proposed feature extractor can improve classification accuracy and is also faster than these kernelbased methods by more than an order of magnitude 
summarizing itemset patterns using probabilistic models summarizing itemset patterns using probabilistic models in this paper we propose a novel probabilistic approach to summarize frequent itemset patterns such techniques are useful for summarization postprocessing and enduser interpretation particularly for problems where the resulting set of patterns are huge in our approach items in the dataset are modeled as random variables we then construct a markov random fields mrf on these variables based on frequent itemsets and their occurrence statistics the summarization proceeds in a levelwise iterative fashion occurrence statistics of itemsets at the lowest level are used to construct an initial mrf statistics of itemsets at the next level can then be inferred from the model we use those patterns whose occurrence can not be accurately inferred from the model to augment the model in an iterative manner repeating the procedure until all frequent itemsets can be modeled the resulting mrf model affords a concise and useful representation of the original collection of itemsets extensive empirical study on real datasets show that the new approach can effectively summarize a large number of itemsets and typically significantly outperforms extant approaches 
suppressing model overfitting in mining conceptdrifting data streams suppressing model overfitting in mining conceptdrifting data streams mining data streams of changing class distributions is important for realtime business decision support the stream classifier must evolve to reflect the current class distribution this poses a serious challenge on the one hand relying on historical data may increase the chances of learning obsolete models on the other hand learning only from the latest data may lead to biased classifiers as the latest data is often an unrepresentative sample of the current class distribution the problem is particularly acute in classifying rare events when for example instances of the rare class do not even show up in the most recent training data in this paper we use a stochastic model to describe the concept shifting patterns and formulate this problem as an optimization one from the historical and the current training data that we have observed find the mostlikely current distribution and learn a classifier based on the mostlikely distribution we derive an analytic solution and approximate this solution with an efficient algorithm which calibrates the influence of historical data carefully to create an accurate classifier we evaluate our algorithm with both synthetic and realworld datasets our results show that our algorithm produces accurate and efficient classification 
a largescale analysis of query logs for assessing personalization opportunities a largescale analysis of query logs for assessing personalization opportunities query logs the patterns of activity left by millions of users contain a wealth of information that can be mined to aid personalization we perform a largescale study of yahoo search engine logs tracking million browsercookies over a period of months we define metrics to address questions such as how much history is available how do users topical interests vary as reflected by their queries and what can we learn from user clicks we find that there is significantly more expected history for the user of a randomly picked query than for a randomly picked user we show that users exhibit consistent topical interests that vary between users we also see that user clicks indicate a variety of special interests our findings shed light on user activity and can inform future personalization efforts 
semisupervised time series classification semisupervised time series classification the problem of time series classification has attracted great interest in the last decade however current research assumes the existence of large amounts of labeled training data in reality such data may be very difficult or expensive to obtain for example it may require the time and expertise of cardiologists space launch technicians or other domain specialists as in many other domains there are often copious amounts of unlabeled data available for example the physiobank archive contains gigabytes of ecg data in this work we propose a semisupervised technique for building time series classifiers while such algorithms are well known in text domains we will show that special considerations must be made to make them both efficient and effective for the time series domain we evaluate our work with a comprehensive set of experiments on diverse data sources including electrocardiograms handwritten documents and video datasets the experimental results demonstrate that our approach requires only a handful of labeled examples to construct accurate classifiers 
 kanonymity kanonymity privacy preservation is an important issue in the release of data for mining purposes the kanonymity model has been introduced for protecting individual identification recent studies show that a more sophisticated model is necessary to protect the association of individuals to sensitive information in this paper we propose an kanonymity model to protect both identifications and relationships to sensitive information in data we discuss the properties of kanonymity model we prove that the optimal kanonymity problem is nphard we first presentan optimal globalrecoding method for the kanonymity problem next we propose a localrecoding algorithm which is more scalable and result in less data distortion the effectiveness and efficiency are shown by experiments we also describe how the model can be extended to more general case 
incremental approximate matrix factorization for speeding up support vector machines incremental approximate matrix factorization for speeding up support vector machines traditional decompositionbased solutions to support vector machines svms suffer from the widelyknown scalability problem for example given a onemillion training set it takes about six days for svmlight to run on a pentium sever with gbyte memory in this paper we propose an incremental algorithm which performs approximate matrixfactorization operations to speed up svms two approximate factorization schemes kronecker and incomplete cholesky are utilized in the primaldual interiorpoint method ipm to directly solve the quadratic optimization problem in svms we found out that a coarse approximate algorithm enjoys good speedup performance but may suffer from poor training accuracy conversely a finegrained approximate algorithm enjoys good training quality but may suffer from long training time we subsequently propose an incremental training algorithm which uses the approximate ipm solution of a coarse factorization to initialize the ipm of a finegrained factorization extensive empirical studies show that our proposed incremental algorithm with approximate factorizations substantially speeds up svm training while maintaining high training accuracy in addition we show that our proposed algorithm is highly parallelizable on an intel dualcoreprocessor 
outlier detection by sampling with accuracy guarantees outlier detection by sampling with accuracy guarantees an effective approach to detecting anomalous points in a data setis distancebased outlier detection this paper describes a simplesampling algorithm to effciently detect distancebased outliers indomains where each and every distance computation is veryexpensive unlike any existing algorithms the sampling algorithmrequires a xed number of distance computations and can return goodresults with accuracy guarantees the most computationallyexpensive aspect of estimating the accuracy of the result issorting all of the distances computed by the sampling algorithmthe experimental study on two expensive domains as well as tenadditional reallife datasets demonstrates both the effciency andeffectiveness of the sampling algorithm in comparison with thestateoftheart algorithm and there liability of the accuracyguarantees 
discovering interesting patterns through users interactive feedback discovering interesting patterns through users interactive feedback in this paper we study the problem of discovering interesting patterns through users interactive feedback we assume a set of candidate patterns ie frequent patterns has already been mined our goal is to help a particular user effectively discover interesting patterns according to his specific interest without requiring a user to explicitly construct a prior knowledge to measure the interestingness of patterns we learn the users prior knowledge from his interactive feedback we propose two models to represent a users prior the log linear model and biased belief model the former is designed for itemset patterns whereas the latter is also applicable to sequential and structural patterns to learn these models we present a twostage approach progressive shrinking and clustering to select sample patterns for feedback the experimental results on real and synthetic data sets demonstrate the effectiveness of our approach 
kmeans clustering versus validation measures kmeans clustering versus validation measures kmeans is a widely used partitional clustering method while there are considerable research efforts to characterize the key features of kmeans clustering further investigation is needed to reveal whether and how the data distributions can have the impact on the performance of kmeans clustering indeed in this paper we revisit the kmeans clustering problem by answering three questions first how the true cluster sizes can make impact on the performance of kmeans clustering second is the entropy an algorithmindependent validation measure for kmeans clustering finally what is the distribution of the clustering results by kmeans to that end we first illustrate that kmeans tends to generate the clusters with the relatively uniform distribution on the cluster sizes in addition we show that the entropy measure an external clustering validation measure has the favorite on the clustering algorithms which tend to reduce high variation on the cluster sizes finally our experimental results indicate that kmeans tends to produce the clusters in which the variation of the cluster sizes as measured by the coefficient of variationcv is in a specific range approximately from to 
utilitybased anonymization using local recoding utilitybased anonymization using local recoding privacy becomes a more and more serious concern in applications involving microdata recently efficient anonymization has attracted much research work most of the previous methods use global recoding which maps the domains of the quasiidentifier attributes to generalized or changed values however global recoding may not always achieve effective anonymization in terms of discernability and query answering accuracy using the anonymized data moreover anonymized data is often for analysis as well accepted in many analytical applications different attributes in a data set may have different utility in the analysis the utility of attributes has not been considered in the previous methodsin this paper we study the problem of utilitybased anonymization first we propose a simple framework to specify utility of attributes the framework covers both numeric and categorical data second we develop two simple yet efficient heuristic local recoding methods for utilitybased anonymization our extensive performance study using both real data sets and synthetic data sets shows that our methods outperform the stateoftheart multidimensional global recoding methods in both discernability and query answering accuracy furthermore our utilitybased method can boost the quality of analysis using the anonymized data 
integration of semanticbased bipartite graph representation and mutual refinement strategy for biomedical literature clustering integration of semanticbased bipartite graph representation and mutual refinement strategy for biomedical literature clustering we introduce a novel document clustering approach that overcomes those problems by combining a semanticbased bipartite graph representation and a mutual refinement strategy the primary contributions of this paper are the following first we introduce a new representation of documents using a bipartite graph between documents and cooccurrence concepts in the documents second we show how to enhance clustering quality by applying the mutual refinement strategy to the initial clustering results third through the experiments on medline documents we show that our integrated method significantly enhances cluster quality and clustering reliability compared to existing clustering methods our approach improves on the average cluster quality and clustering reliability in terms of misclassification index over bisecting kmeans with the best parameters 
coherent closed quasiclique discovery from large dense graph databases coherent closed quasiclique discovery from large dense graph databases frequent coherent subgraphs can provide valuable knowledge about the underlying internal structure of a graph database and mining frequently occurring coherent subgraphs from large dense graph databases has been witnessed several applications and received considerable attention in the graph mining community recently in this paper we study how to efficiently mine the complete set of coherent closed quasicliques from large dense graph databases which is an especially challenging task due to the downwardclosure property no longer holds by fully exploring some properties of quasicliques we propose several novel optimization techniques which can prune the unpromising and redundant subsearch spaces effectively meanwhile we devise an efficient closure checking scheme to facilitate the discovery of only closed quasicliques we also develop a coherent closed quasiclique mining algorithm ltbgtcocainltbgt thorough performance study shows that cocain is very efficient and scalable for large dense graph databases 
mining progressive confident rules mining progressive confident rules many real world objects have states that change over time by tracking the state sequences of these objects we can study their behavior and take preventive measures before they reach some undesirable states in this paper we propose a new kind of pattern called progressive confident rules to describe sequences of states with an increasing confidence that lead to a particular end state we give a formal definition of progressive confident rules and their concise set we devise pruning strategies to reduce the enormous search space experiment result shows that the proposed algorithm is efficient and scalable we also demonstrate the application of progressive confident rules in classification 
attack detection in time series for recommender systems attack detection in time series for recommender systems recent research has identified significant vulnerabilities in recommender systems shilling attacks in which attackers introduce biased ratings in order to influence future recommendations have been shown to be effective against collaborative filtering algorithms we postulate that the distribution of item ratings in time can reveal the presence of a wide range of shilling attacks given reasonable assumptions about their duration to construct a time series of ratings for an item we use a window size of k to group consecutive ratings for the item into disjoint windows and compute the sample average and sample entropy in each window we derive a theoretically optimal window size to best detect an attack event if the number of attack profiles is known for practical applications where this number is unknown we propose a heuristic algorithm that adaptively changes the window size our experimental results demonstrate that monitoring rating distributions in time series is an effective approach for detecting shilling attacks 
identifying bridging rules between conceptual clusters identifying bridging rules between conceptual clusters a bridging rule in this paper has its antecedent and action from different conceptual clusters we first design two algorithms for mining bridging rules between clusters in a database and then propose two nonlinear metrics for measuring the interestingness of bridging rules bridging rules can be distinct from association rules or frequent itemsets this is because bridging rules can be generated by infrequent itemsets that are pruned in association rule mining and bridging rules are measured by the importance that includes the distance between two conceptual clusters whereas frequent itemsets are measured by only the support 
linear prediction models with graph regularization for webpage categorization linear prediction models with graph regularization for webpage categorization we present a risk minimization formulation for learning from both text and graph structures which is motivated by the problem of collective inference for hypertext document categorization the method is based on graph regularization formulated as a wellformed convex optimization problem we present numerical algorithms for our formulation and show that such combination of local text features and link information can lead to improved predictive accuracy 
blosom blosom we introduce a novel framework called blosom for mining frequent boolean expressions over binaryvalued datasets we organize the space of boolean expressions into four categories pure conjunctions pure disjunctions conjunction of disjunctions and disjunction of conjunctions we focus on mining the simplest expressions the minimal generators for each class we also propose a closure operator for each class that yields closed boolean expressions blosom efficiently mines frequent boolean expressions by utilizing a number of methodical pruning techniques experiments showcase the behavior of blosom and an application study on a real dataset is also given 
introducing perpetual analytics introducing perpetual analytics common strategies to liberate an organizations information assets for situational awareness frequently rely on infrastructure components such as data integration enterprise search federation data warehousing and so on and while these traditional platforms enable analysts to get better and faster answers to their queries the next big advance will change this paradigm users cannot be expected to formulate and ask every smart question every day and to escape this impractical and unscalable model the new paradigm will involve technologies where the data finds the data and relevance finds the user perpetual analytics describes a class of application whereby enterprise context is assembled in realtime on data streams as fast as operational systems record observations context construction is a data finds the data activity which enables events of interest to be streamed to subscribers in this talk i will talk at some depth about the dynamics of such systems including scalability and sustainability 
capital ones statistical problems capital ones statistical problems capital one is a highly quantitatively driven diversified financial services firm as such we make broad and deep use of the entire repertory of highly quantitative techniques this talk will present our top ten statistical problems indeed one of them has as a sub point the data mining dimension but it will likely be useful for data miners to see how their research needs to complement and fit into the entire range of hard statistical issues 
information extraction data mining and joint inference information extraction data mining and joint inference although information extraction and data mining appear together in many applications their interface in most current systems would better be described as serial juxtaposition than as tight integration information extraction populates slots in a database by identifying relevant subsequences of text but is usually not aware of the emerging patterns and regularities in the database data mining methods begin from a populated database and are often unaware of where the data came from or its inherent uncertainties the result is that the accuracy of both suffers and accurate mining of complex text sources has been beyond reachin this talk i will describe work in probabilistic models that perform joint inference across multiple components of an information processing pipeline in order to avoid the brittle accumulation of errors after briefly introducing conditional random fields i will describe recent work in information extraction leveraging factorial state representations entity resolution and transfer learning as well as scalable methods of inference and learning ill close with some recent work on probabilistic models for social network analysis and a demonstration of rexainfo a new research paper search engine 
data mining challenges in the automotive domain data mining challenges in the automotive domain automotive companies such as ford motor company have no shortage of large databases with abundant opportunities for cost reduction and revenue enhancement the data mining group at ford has worked in the areas of quality customer satisfaction and warranty analytics for close to ten years in this time we have developed a number of methods for building systems to help the business one area of particular success has been in warranty analysis while traditional hazard analysis has been applied at ford for a number of years we have used techniques from other industries eg retail as well as text mining to view warranty analytics in a new way however our success has been tempered by serious challenges particularly in the areas of data understanding computing meaningful aggregations and implementation case studies from the automobile industry warranty quality forecasting etc as well as from other industries will be used 
computer aided detection via asymmetric cascade of sparse hyperplane classifiers computer aided detection via asymmetric cascade of sparse hyperplane classifiers this paper describes a novel classification method for computer aided detection cad that identifies structures of interest from medical images cad problems are challenging largely due to the following three characteristics typical cad training data sets are large and extremely unbalanced between positive and negative classes when searching for descriptive features researchers often deploy a large set of experimental features which consequently introduces irrelevant and redundant features finally a cad system has to satisfy stringent realtime requirementsthis work is distinguished by three key contributions the first is a cascade classification approach which is able to tackle all the above difficulties in a unified framework by employing an asymmetric cascade of sparse classifiers each trained to achieve high detection sensitivity and satisfactory false positive rates the second is the incorporation of feature computational costs in a linear program formulation that allows the feature selection process to take into account different evaluation costs of various features the third is a boosting algorithm derived from column generation optimization to effectively solve the proposed cascade linear programswe apply the proposed approach to the problem of detecting lung nodules from helical multislice ct images our approach demonstrates superior performance in comparison against support vector machines linear discriminant analysis and cascade adaboost especially the resulting detection system is significantly sped up with our approach 
onboard classifiers for science event detection on a remote sensing spacecraft onboard classifiers for science event detection on a remote sensing spacecraft typically data collected by a spacecraft is downlinked to earth and preprocessed before any analysis is performed we have developed classifiers that can be used onboard a spacecraft to identify high priority data for downlink to earth providing a method for maximizing the use of a potentially bandwidth limited downlink channel onboard analysis can also enable rapid reaction to dynamic events such as flooding volcanic eruptions or sea ice breakupfour classifiers were developed to identify cryosphere events using hyperspectral images these classifiers include a manually constructed classifier a support vector machine svm a decision tree and a classifier derived by searching over combinations of thresholded band ratios each of the classifiers was designed to run in the computationally constrained operating environment of the spacecraft a set of scenes was handlabeled to provide training and testing data performance results on the test data indicate that the svm and manual classifiers outperformed the decision tree and bandratio classifiers with the svm yielding slightly better classifications than the manual classifierthe manual and svm classifiers have been uploaded to the eo spacecraft and have been running onboard the spacecraft for over a year results of the onboard analysis are used by the autonomous sciencecraft experiment ase of nasas new millennium program onboard eo to automatically target the spacecraft to collect followon imagery the software demonstrates the potential for future deep space missions to use onboard decision making to capture shortlived science events 
pragmatic text mining pragmatic text mining we discuss our experiences in analyzing customersupport issues from the unstructured freetext fields of technicalsupport call logs the identification of frequent issues and their accurate quantification is essential in order to track aggregate costs broken down by issue type to appropriately target engineering resources and to provide the best diagnosis support and documentation for most common issues we present a new set of techniques for doing this efficiently on an industrial scale without requiring manual coding of calls in the call center our approach involves a new text clustering method to identify common and emerging issues a method to rapidly train large numbers of categorizers in a practical interactive manner and a method to accurately quantify categories even in the face of inaccurate classifications and training sets that necessarily cannot match the class distribution of each new months data we present our methodology and a tool we developed and deployed that uses these methods for tracking ongoing support issues and discovering emerging issues at hp 
mining for proposal reviewers mining for proposal reviewers in this paper we discuss a prototype application deployed at the us national science foundation for assisting program directors in identifying reviewers for proposals the application helps program directors sort proposals into panels and find reviewers for proposals to accomplish these tasks it extracts information from the full text of proposals both to learn about the topics of proposals and the expertise of reviewers we discuss a variety of alternatives that were explored the solution that was implemented and the experience in using the solution within the workflow of nsf 
gplag gplag along with the blossom of open source projects comes the convenience for software plagiarism a company if less selfdisciplined may be tempted to plagiarize some open source projects for its own products although current plagiarism detection tools appear sufficient for academic use they are nevertheless short for fighting against serious plagiarists for example disguises like statement reordering and code insertion can effectively confuse these tools in this paper we develop a new plagiarism detection tool called gplag which detects plagiarism by mining program dependence graphs pdgs a pdg is a graphic representation of the data and control dependencies within a procedure because pdgs are nearly invariant during plagiarism gplag is more effective than stateoftheart tools for plagiarism detection in order to make gplag scalable to large programs a statistical lossy filter is proposed to prune the plagiarism search space experiment study shows that gplag is both effective and efficient it detects plagiarism that easily slips over existing tools and it usually takes a few seconds to find simulated plagiarism in programs having thousands of lines of code 
understandable models of music collections based on exhaustive feature generation with temporal statistics understandable models of music collections based on exhaustive feature generation with temporal statistics data mining in large collections of polyphonic music has recently received increasing interest by companies along with the advent of commercial online distribution of music important applications include the categorization of songs into genres and the recommendation of songs according to musical similarity and the customers musical preferences modeling genre or timbre of polyphonic music is at the core of these tasks and has been recognized as a difficult problem many audio features have been proposed but they do not provide easily understandable descriptions of music they do not explain why a genre was chosen or in which way one song is similar to another we present an approach that combines large scale feature generation with meta learning techniques to obtain meaningful features for musical similarity we perform exhaustive feature generation based on temporal statistics and train regression models to summarize a subset of these features into a single descriptor of a particular notion of music using several such models we produce a concise semantic description of each song genre classification models based on these semantic features are shown to be better understandable and almost as accurate as traditional methods 
opportunity map opportunity map in this paper we report a deployed data mining application system for motorola originally its intended use was for identifying causes of cellular phone failures but it has been found to be useful for many other engineering data sets as well for this report the case study is a dataset containing cellular phone call records this data set is like any dataset used in classification applications ie with a set of attributes which can be continuous or discrete and a discrete class attribute in our application the classes are normally ended calls calls which failed to setup and calls which failed while in progress however the task is not to predict any failure but to identify possible causes that resulted in failures then engineering efforts may focus on improvements that can be made to the phones in the course of the project various classification techniques eg decision trees nave bayesian classification and svm were tried however the results were unsatisfactory after several demonstrations and interaction with domain experts we finally designed and implemented an effective approach to perform the task the final system is based on class association rules general impressions and visualization the system has been deployed and is in regular use at motorola in this paper we first describe our experiences with some existing classification systems and discuss why they are not suitable for the task we then present our techniques as an illustration we show several visualization screens in the case study which reveal some important knowledge due to confidentiality we will not give specifics but only present a general discussion about the results 
identifying best bet web search results by mining past user behavior identifying best bet web search results by mining past user behavior the top web search result is crucial for user satisfaction with the web search experience we argue that the importance of the relevance at the top position necessitates special handling of the top web search result for some queries we propose an effective approach of leveraging millions of past user interactions with a web search engine to automatically detect best bet top results preferred by majority of users interestingly this problem can be more effectively addressed with classification than using stateoftheart general ranking methods furthermore we show that our general machine learning approach achieves precision comparable to a heavily tuned domainspecific algorithm with significantly higher coverage our experiments over millions of user interactions for thousands of queries demonstrate the effectiveness and robustness of our techniques 
mining citizen science data to predict orevalence of wild bird species mining citizen science data to predict orevalence of wild bird species the cornell laboratory of ornithologys mission is to interpret and conserve the earths biological diversity through research education and citizen science focused on birds over the years the lab has accumulated one of the largest and longestrunning collections of environmental data sets in existence the data sets are not only large but also have many attributes contain many missing values and potentially are very noisy the ecologists are interested in identifying which features have the strongest effect on the distribution and abundance of bird species as well as describing the forms of these relationships we show how data mining can be successfully applied enabling the ecologists to discover unanticipated relationships we compare a variety of methods for measuring attribute importance with respect to the probability of a bird being observed at a feeder and present initial results for the impact of important attributes on bird prevalence 
a componentbased framework for knowledge discovery in bioinformatics a componentbased framework for knowledge discovery in bioinformatics motivation in the field of bioinformatics there is an emerging need to integrate all knowledge discovery steps into a standardized modular framework indeed componentbased development can significantly enhance reusability and productivity for short timeline projects with a small team we present interactive knowledge discovery and data mining ikdd an application framework written in java that was specifically designed for these purposesresults ikdd consists of a componentbased architecture and a webbased tool for preclinical research and prototype development the platform provides an intuitive and consistent interface to create and maintain components eg data structures algorithms and utilities to load save and visualize data and pipelines the richfeatured tool supplies database connectivity workflow processing and rapid prototype building the architecture was carefully designed using an objectoriented approach that respects crucial goals usability openness robustness and functionality especially in the abstraction and description of the components which distinguishes it from other packages ikdd is wellsuited to serve as a public repository of components to run scientific experiments with a highlevel of reproducibility and also to rapidly build prototypes this paper describes the general architecture and demonstrates through examples the ease by which a complex scenario implementation can be facilitated with ikdd 
discovering significant opsm subspace clusters in massive gene expression data discovering significant opsm subspace clusters in massive gene expression data orderpreserving submatrixes opsms have been accepted as a biologically meaningful subspace cluster model capturing the general tendency of gene expressions across a subset of conditions in an opsm the expression levels of all genes induce the same linear ordering of the conditions opsm mining is reducible to a special case of the sequential pattern mining problem in which a pattern and its supporting sequences uniquely specify an opsm cluster those small twig clusters specified by long patterns with naturally low support incur explosive computational costs and would be completely pruned off by most existing methods for massive datasets containing thousands of conditions and hundreds of thousands of genes which are common in todays gene expression analysis however it is in particular interest of biologists to reveal such small groups of genes that are tightly coregulated under many conditions and some pathways or processes might require only two genes to act in concert in this paper we introduce the kiwi mining framework for massive datasets that exploits two parameters k and w to provide a biased testing on a bounded number of candidates substantially reducing the search space and problem scale targeting on highly promising seeds that lead to significant clusters and twig clusters extensive biological and computational evaluations on real datasets demonstrate that kiwi can effectively mine biologically meaningful opsm subspace clusters with good efficiency and scalability 
maximum profit mining and its application in software development maximum profit mining and its application in software development while most software defects ie bugs are corrected and tested as part of the lengthy software development cycle enterprise software vendors often have to release software products before all reported defects are corrected due to deadlines and limited resources a small number of these defects will be escalated by customers and they must be resolved immediately by the software vendors at a very high cost in this paper we develop an escalation prediction ep system that mines historic defect report data and predict the escalation risk of the defects for maximum net profit more specifically we first describe a simple and general framework to convert the maximum net profit problem to costsensitive learning we then apply and compare several wellknown costsensitive learning approaches for ep our experiments suggest that the costsensitive decision tree is the best method for producing the highest positive net profit and comprehensible results the ep system has been deployed successfully in the product group of an enterprise software vendor 
yale yale kdd is a complex and demanding task while a large number of methods has been established for numerous problems many challenges remain to be solved new tasks emerge requiring the development of new methods or processing schemes like in software development the development of such solutions demands for careful analysis specification implementation and testing rapid prototyping is an approach which allows crucial design decisions as early as possible a rapid prototyping system should support maximal reuse and innovative combinations of existing methods as well as simple and quick integration of new onesthis paper describes yale a free opensource environment forkdd and machine learning yale provides a rich variety of methods whichallows rapid prototyping for new applications and makes costlyreimplementations unnecessary additionally yale offers extensive functionality for process evaluation and optimization which is a crucial property for any kdd rapid prototyping tool following the paradigm of visual programming eases the design of processing schemes while the graphical user interface supports interactive design the underlying xml representation enables automated applications after the prototyping phaseafter a discussion of the key concepts of yale we illustrate the advantages of rapid prototyping for kdd on case studies ranging from data preprocessing to result visualization these case studies cover tasks like feature engineering text mining data stream mining and tracking drifting concepts ensemble methods and distributed data mining this variety of applications is also reflected in a broad user base we counted more than downloads during the last twelve months 
camouflaged fraud detection in domains with complex relationships camouflaged fraud detection in domains with complex relationships we describe a data mining system to detect frauds that are camouflaged to look like normal activities in domains with high number of known relationships examples include accounting fraud detection for rating and investment insider attacks on corporate networks and health care insurance fraud our goal is to help analysts who are overwhelmed with information about companies or online system access logs or insurance claims to focus their attentions on features that cause damage in the future we focused on accounting fraud where the task is to detect the subset of companies that were potentially committing accounting fraud within the total population of public companies that file quarterly and annual filings with the securities and exchange commission sec using a representation of changes b a mix of decision tree learning locally weighted logistic regression kmeans clustering and constant regression in a two phase pipe line we developed models that rank companies based on the probability of forecasting future damaging performance the learned models were tested extensively over four years with public data available from sec filings and private data available from rating companies and investment firms cross validation experiments and analyst based validation of private experiments were found to show that the approach performed as well as or better than domain experts and discovered new relationships that domain experts did not use on a regular basis finally the detections preceded public knowledge of such problems by six to eighteen months 
beyond classification and ranking beyond classification and ranking classification has been commonly used in many data mining projects in the financial service industry for instance to predict collectability of accounts receivable a binary class label is created based on whether a payment is received within a certain period however optimization of the classifier does not necessarily lead to maximization of return on investment roi since maximization of the true positive rate is often different from maximization of the collectable amount which determines the roi under a fixed budget constraint the typical cost sensitive learning does not solve this problem either since it involves an unknown opportunity cost due to the budget constraint learning the ranks of collectable amount would ultimately solve the problem but it tries to tackle an unnecessarily difficult problem and often results in poorer results for our specific target we propose a new algorithm that uses gradient descent to directly optimize the related monetary measure under the budget constraint and thus maximizes the roi by comparison with several classification regression and ranking algorithms we demonstrate the new algorithms substantial improvement of the financial impact on our clients in the financial service industry 
is there a grand challenge or xprize for data mining is there a grand challenge or xprize for data mining this panel will discuss possible exciting and motivating grand challenge problems for data mining focusing on bioinformatics multimedia mining link mining text mining and web mining 
calculating latent demand in the long tail calculating latent demand in the long tail an analytical framework for using powerlaw theory to estimate market size for niche products and consumer groups
from mining the web to inventing the new sciences underlying the internet from mining the web to inventing the new sciences underlying the internet this is an abstract of the invited keynote presentation to be presented at kdd
challenges in mining social network data challenges in mining social network data the profileration of rich social media online communities and collectively produced knowledge resources has accelerated the convergence of technological and social networks producing environments that reflect both the architecture of the underlying information systems and the social structure on their members in studying the consequences of these developments we are faced with the opportunity to analyze social network data at unprecedented levels of scale and temporal resolution this has led to a growing body of research at the intersection of the computing and social sciences
efficient and effective explanation of change in hierarchical summaries efficient and effective explanation of change in hierarchical summaries dimension attributes in data warehouses are typically hierarchical eg geographic locations in sales data urls in web traffic logs olap tools are used to summarize the measure attributes eg total sales along a dimension hierarchy and to characterize changes eg trends and anomalies in a hierarchical summary over time when thenumber of changes identified is large eg total sales in many stores differed from their expected values a parsimonious explanation of the most significant changes is desirable in this paper we propose a natural model of parsimonious explanation as a composition of node weights along the roottoleaf paths in a dimension hierarchy which permits changes to be aggregated with maximal generalization along the dimension hierarchy we formalize this model of explaining changes in hierarchical summaries and investigate the problem of identifying optimally parsimonious explanations on arbitrary rooted one dimensional tree hierarchies we show that such explanations can be computed efficiently in time essentially proportional to the number of leaves and the depth of the hierarchy further our method can produce parsimonious explanations from the output of any statistical model that provides predictions and confidence intervals making it widely applicable our experiments use real data sets to demonstrate the utility and robustness of our proposed model for explaining significant changes as well as its superior parsimony compared to alternatives
estimating rates of rare events at multiple resolutions estimating rates of rare events at multiple resolutions we consider the problem of estimating occurrence rates of rare eventsfor extremely sparse data using preexisting hierarchies to perform inference at multiple resolutions in particular we focus on the problem of estimating click rates for webpage advertisement pairs called impressions where both the pages and the ads are classified into hierarchies that capture broad contextual information at different levels of granularity typically the click rates are low and the coverage of the hierarchies is sparse to overcome these difficulties we devise a sampling method whereby we analyze aspecially chosen sample of pages in the training set and then estimate click rates using a twostage model the first stage imputes the number of webpage ad pairs at all resolutions of the hierarchy to adjust for the sampling bias the second stage estimates clickrates at all resolutions after incorporating correlations among sibling nodes through a treestructured markov model both models are scalable and suited to large scale data mining applications on a realworld dataset consisting of billion impressions we demonstrate that even with negative nonclicked events in the training set our method can effectively discriminate extremely rare events in terms of their click propensity
predictive discrete latent factor models for large scale dyadic data predictive discrete latent factor models for large scale dyadic data we propose a novel statistical method to predict large scale dyadic response variables in the presence of covariate information our approach simultaneously incorporates the effect of covariates and estimates local structure that is induced by interactions among the dyads through a discrete latent factor model the discovered latent factors provide a redictive model that is both accurate and interpretable we illustrate our method by working in a framework of generalized linear models which include commonly used regression techniques like linear regression logistic regression and poisson regression as special cases we also provide scalable generalized embased algorithms for model fitting using both hard and soft cluster assignments we demonstrate the generality and efficacy of our approach through large scale simulation studies and analysis of datasets obtained from certain realworld movie recommendation and internet advertising applications
on string classification in data streams on string classification in data streams string data has recently become important because of its use in a number of applications such as computational and molecular biology protein analysis and market basket data in many cases these strings contain a wide variety of substructures which may have physical significance for that application for example such substructures could represent important fragments of a dna string or an interesting portion of a fraudulent transaction in such a case it is desirable to determine the identity location and extent of that substructure in the data this is a much more difficult generalization of the classification problem since the latter problem labels entire strings rather than deal with the more complex task of determining string fragments with a particular kind of behavior the problem becomes even more complicated when different kinds of substrings show complicated nesting patterns therefore we define a somewhat different problem which we refer to as the generalized classification problem we propose a scalable approach based on hidden markov models for this problem we show how to implement the generalized string classification procedure for very large data bases and data streams we present experimental results over a number of large data sets and data streams
xproj xproj xml has become a popular method of data representation both on the web and in databases in recent years one of the reasons for the popularity of xml has been its ability to encode structural information about data records however this structural characteristic of data sets also makes it a challenging problem for a variety of data mining problems one such problem is that of clustering in which the structural aspects of the data result in a high implicit dimensionality of the data representation as a result it becomes more difficult to cluster the data in a meaningful way in this paper we propose an effective clustering algorithm for xml data which uses substructures of the documents in order to gain insights about the important underlying structures we propose new ways of using multiple substructuralinformation in xml documents to evaluate the quality of intermediate cluster solutions and guide the algorithms to a final solution which reflects the true structural behavior in individual partitions we test the algorithm on a variety of real and synthetic data sets
show me the money show me the money the increasing pervasiveness of the internet has dramatically changed the way that consumers shop for goods consumergenerated product reviews have become a valuable source of information for customers who read the reviews and decide whether to buy the product based on the information provided in this paper we use techniques that decompose the reviews into segments that evaluate the individual characteristics of a product eg image quality and battery life for a digital camera then as a major contribution of this paper we adapt methods from the econometrics literature specifically the hedonic regression concept to estimate a the weight that customers place on each individual product feature b the implicit evaluation score that customers assign to each feature and c how these evaluations affect the revenue for a given product towards this goal we develop a novel hybrid technique combining text mining and econometrics that models consumer product reviews as elements in a tensor product of feature and evaluation spaces we then impute the quantitative impact of consumer reviews on product demand as a linear functional from this tensor product space we demonstrate how to use a lowdimension approximation of this functional to significantly reduce the number of model parameters while still providing good experimental results we evaluate our technique using a data set from amazoncom consisting of sales data and the related consumer reviews posted over a month period for products our experimental evaluation shows that we can extract actionable business intelligence from the data and better understand the customer preferences and actions we also show that the textual portion of the reviews can improve product sales prediction compared to a baseline technique that simply relies on numeric data
temporal causal modeling with graphical granger methods temporal causal modeling with graphical granger methods the need for mining causality beyond mere statistical correlations for real world problems has been recognized widely many of these applications naturally involve temporal data which raises the challenge of how best to leverage the temporal information for causal modeling recently graphical modeling with the concept of granger causality based on the intuition that a cause helps predict its effects in the future has gained attention in many domains involving time series data analysis with the surge of interest in model selection methodologies for regression such as the lasso as practical alternatives to solving structural learning of graphical models the question arises whether and how to combine these two notions into a practically viable approach for temporal causal modeling in this paper we examine a host of related algorithms that loosely speaking fall under the category of graphical granger methods and characterize their relative performance from multiple viewpoints our experiments show for instance that the lasso algorithm exhibits consistent gain over the canonical pairwise graphical granger method we also characterize conditions under which these variants of graphical granger methods perform well in comparison to other benchmark methods finally we apply these methods to a real world data set involving key performance indicators of corporations and present some concrete results
extracting semantic relations from query logs extracting semantic relations from query logs in this paper we study a large query log of more than twenty million queries with the goal of extracting the semantic relations that are implicitly captured in the actions of users submitting queries and clicking answers previous query log analyses were mostly done with just the queries and not the actions that followed after them we first propose a novel way to represent queries in a vector space based on a graph derived from the queryclick bipartite graph we then analyze the graph produced by our query log showing that it is less sparse than previous results suggested and that almost all the measures of these graphs follow power laws shedding some light on the searching user behavior as well as on the distribution of topics that people want in the web the representation we introduce allows to infer interesting semantic relationships between queries second we provide an experimental analysis on the quality of these relations showing that most of them are relevant finally we sketch an application that detects multitopical urls
realtime ranking with concept drift using expert advice realtime ranking with concept drift using expert advice in many practical applications one is interested in generating a ranked list of items using information mined from continuous streams of data for example in the context of computer networks one might want to generate lists of nodes ranked according to their susceptibility to attack in addition realworld data streams often exhibit concept drift making the learning task even more challenging we present an online learning approach to ranking with concept drift using weighted majority techniques by continuously modeling different snapshots of the data and tuning our measure of belief in these models over time we capture changes in the underlying concept and adapt our predictions accordingly we measure the performance of our algorithm on real electricity data as well as asynthetic data stream and demonstrate that our approach to ranking from stream data outperforms previously known batchlearning methods and other online methods that do not account for concept drift
modeling relationships at multiple scales to improve accuracy of large recommender systems modeling relationships at multiple scales to improve accuracy of large recommender systems the collaborative filtering approach to recommender systems predicts user preferences for products or services by learning past useritem relationships in this work we propose novel algorithms for predicting user ratings of items by integrating complementary models that focus on patterns at different scales at a local scale we use a neighborhoodbased technique that infers ratings from observed ratings by similar users or of similar items unlike previous local approaches our method is based on a formal model that accounts for interactions within the neighborhood leading to improved estimation quality at a higher regional scale we use svdlike matrix factorization for recovering the major structural patterns in the useritem rating matrix unlike previous approaches that require imputations in order to fill in the unknown matrix entries our new iterative algorithm avoids imputation because the models involve estimation of millions or even billions of parameters shrinkage of estimated values to account for sampling variability proves crucial to prevent overfitting both the local and the regional approaches and in particular their combination through a unifying model compare favorably with other approaches and deliver substantially better results than the commercial netflix cinematch recommender system on a large publicly available data set
contentbased document routing and index partitioning for scalable similaritybased searches in a large corpus contentbased document routing and index partitioning for scalable similaritybased searches in a large corpus we present a document routing and index partitioning scheme for scalable similaritybased search of documents in a large corpus we consider the case when similaritybased search is performed by finding documents that have features in common with the query document while it is possible to store all the features of all the documents in one index this suffers from obvious scalability problems our approach is to partition the feature index into multiple smaller partitions that can be hosted on separate servers enabling scalable and parallel search execution when a document is ingested into the repository a small number of partitions are chosen to store the features of the document to perform similaritybased search also only a small number of partitions are queried our approach is stateless and incremental the decision as to which partitions the features of the document should be routed to for storing at ingestion time and for similarity based search at query time is solely based on the features of the document
support feature machine for classification of abnormal brain activity support feature machine for classification of abnormal brain activity in this study a novel multidimensional time series classification technique namely support feature machine sfm is proposed sfm is inspired by the optimization model of support vector machine and the nearest neighbor rule to incorporate both spatial and temporal of the multidimensional time series data this paper also describes an application of sfm for detecting abnormal brain activity epilepsy is a case in point in this study in epilepsy studies electroencephalograms eegs acquired in multidimensional time series format have been traditionally used as a goldstandard tool for capturing the electrical changes in the brain from multidimensional eeg time series data sfm was used to identify seizure precursors and detect seizure susceptibility preseizure periods the empirical results showed that sfm achieved over correct classification of perseizure eeg on average in patients using fold cross validation the proposed optimization model of sfm is very compact and scalable and can be implemented as an online algorithm the outcome of this study suggests that it is possible to construct a computerized algorithm used to detect seizure precursors and warn of impending seizures through eeg classification
nonlinear adaptive distance metric learning for clustering nonlinear adaptive distance metric learning for clustering a good distance metric is crucial for many data mining tasks to learn a metric in the unsupervised setting most metric learning algorithms project observed data to a lowdimensional manifold where geometric relationships such as pairwise distances are preserved it can be extended to the nonlinear case by applying the kernel trick which embeds the data into a feature space by specifying the kernel function that computes the dot products between data points in the feature space in this paper we propose a novel unsupervised nonlinear adaptive metric learning algorithm called naml which performs clustering and distance metric learning simultaneously naml firstmaps the data to a highdimensional space through a kernel function then applies a linear projection to find a lowdimensional manifold where the separability of the data is maximized and finally performs clustering in the lowdimensional space the performance of naml depends on the selection of the kernel function and the projection we show that the joint kernel learning dimensionality reduction and clustering can be formulated as a trace maximization problem which can be solved via an iterative procedure in the em framework experimental results demonstrated the efficacy of the proposed algorithm
densitybased clustering for realtime stream data densitybased clustering for realtime stream data existing datastream clustering algorithms such as clustream arebased on kmeans these clustering algorithms are incompetent tofind clusters of arbitrary shapes and cannot handle outliers further they require the knowledge of k and userspecified time window to address these issues this paper proposes dstream a framework for clustering stream data using adensitybased approach the algorithm uses an online component which maps each input data record into a grid and an offline component which computes the grid density and clusters the grids based on the density the algorithm adopts a density decaying technique to capture the dynamic changes of a data stream exploiting the intricate relationships between the decay factor data density and cluster structure our algorithm can efficiently and effectively generate and adjust the clusters in real time further a theoretically sound technique is developed to detect and remove sporadic grids mapped to by outliers in order to dramatically improve the space and time efficiency of the system the technique makes highspeed data stream clustering feasible without degrading the clustering quality the experimental results show that our algorithm has superior quality and efficiency can find clusters of arbitrary shapes and can accurately recognize the evolving behaviors of realtime data streams
crosslanguage information retrieval using parafac crosslanguage information retrieval using parafac a standard approach to crosslanguage information retrieval clir uses latent semantic analysis lsa in conjunction with a multilingual parallel aligned corpus this approach has been shown to be successful in identifying similar documents across languages or more precisely retrieving the most similar document in one language to a query in another language however the approach has severe drawbacks when applied to a related task that of clustering documents languageindependently so that documents about similar topics end up closest to one another in the semantic space regardless of their language the problem is that documents are generally more similar to other documents in the same language than they are to documents in a different language but on the same topic as a result when using multilingual lsa documents will in practice cluster by language not by topic
evolutionary spectral clustering by incorporating temporal smoothness evolutionary spectral clustering by incorporating temporal smoothness evolutionary clustering is an emerging research area essential to important applications such as clustering dynamic web and blog contents and clustering data streams in evolutionary clustering a good clustering result should fit the current data well while simultaneously not deviate too dramatically from the recent history to fulfill this dual purpose a measure of temporal smoothness is integrated in the overall measure of clustering quality in this paper we propose two frameworks that incorporate temporal smoothness in evolutionary spectral clustering for both frameworks we start with intuitions gained from the wellknown kmeans clustering problem and then propose and solve corresponding cost functions for the evolutionary spectral clustering problems our solutions to the evolutionary spectral clustering problems provide more stable and consistent clustering results that are less sensitive to shortterm noises while at the same time are adaptive to longterm cluster drifts furthermore we demonstrate that our methods provide the optimal solutions to the relaxed versions of the corresponding evolutionary kmeans clustering problems performance experiments over a number of real and synthetic data sets illustrate our evolutionary spectral clustering methods provide more robust clustering results that are not sensitive to noise and can adapt to data drifts
structural and temporal analysis of the blogosphere through community factorization structural and temporal analysis of the blogosphere through community factorization the blogosphere has unique structural and temporal properties since blogs are typically used as communication media among human individuals in this paper we propose a novel technique that captures the structure and temporal dynamics of blog communities in our framework a community is a set of blogs that communicate with each other triggered by some events such as a news article the community is represented by its structure and temporal dynamics a community graph indicates how often one blog communicates with another and a community intensity indicates the activity level of the community that varies over time our method community factorization extracts such communities from the blogosphere where the communication among blogs is observed as a set of subgraphs ie threads of discussion this community extraction is formulated as a factorization problem in the framework of constrained optimization in which the objective is to best explain the observed interactions in the blogosphere over time we further provide a scalable algorithm for computing solutions to the constrained optimization problems extensive experimental studies on both synthetic and real blog data demonstrate that our technique is able to discover meaningful communities that are not detectable by traditional methods
discovering the hidden structure of house prices with a nonparametric latent manifold model discovering the hidden structure of house prices with a nonparametric latent manifold model in many regression problems the variable to be predicted depends not only on a samplespecific feature vector but also on an unknown latent manifold that must satisfy known constraints an example is house prices which depend on the characteristics of the house and on the desirability of the neighborhood which is not directly measurable the proposed method comprises two trainable components the first one is a parametric model that predicts the intrinsic price of the house from its description the second one is a smooth nonparametric model of the latent desirability manifold the predicted price of a house is the product of its intrinsic price and desirability the two components are trained simultaneously using a deterministic form of the em algorithm the model was trained on a large dataset of houses from los angeles county it produces better predictions than pure parametric and nonparametric models it also produces useful estimates of the desirability surface at each location
stochastic processes and temporal data mining stochastic processes and temporal data mining this article tries to give an answer to a fundamental question intemporal data mining under what conditions a temporal rule extracted from uptodate temporal data keeps its confidencesupport for future data a possible solution is given by using on the one hand a temporal logic formalism which allows the definition of the main notions event temporal rule support confidence in a formal way and on the other hand the stochastic limit theory under this probabilistic temporal framework the equivalence between the existence of the support of a temporal rule and the law of large numbers is systematically analyzed
exploiting underrepresented query aspects for automatic query expansion exploiting underrepresented query aspects for automatic query expansion users attempt to express their search goals through web search queries when a search goal has multiple components or aspects documents that represent all the aspects are likely to be more relevant than those that only represent some aspects current web search engines often produce result sets whose top ranking documents represent only a subset of the query aspects by expanding the query using the right keywords the search engine can find documents that represent more query aspects and performance improves this paper describes abraq an approach for automatically finding the right keywords to expand the query abraq identifies the aspects in the query identifies which aspects are underrepresented in the result set of the original query and finally for any particularly underrepresented aspect identifies keywords that would enhance that aspects representation and automatically expands the query using the best one the paper presents experiments that show abraq significantly increases the precision of hard queries whereas traditional automatic query expansion techniques have not improved precision abraq also compared favourably against a range of interactive query expansion techniques that require user involvement including clustering weblog analysis relevance feedback and pseudo relevance feedback
canonicalization of database records using adaptive similarity measures canonicalization of database records using adaptive similarity measures it is becoming increasingly common to construct databases from information automatically culled from many heterogeneous sources for example a research publication database can be constructed by automatically extracting titles authors and conference information from online papers a common difficulty in consolidating data from multiple sources is that records are referenced in a variety of ways eg abbreviations aliases and misspellings therefore it can be difficult to construct a single standard representation to present to the user we refer to the task of constructing this representation as canonicalization despite its importance there is little existing work on canonicalization
coclustering based classification for outofdomain documents coclustering based classification for outofdomain documents in many real world applications labeled data are in short supply it often happens that obtaining labeled data in a new domain is expensive and time consuming while there may be plenty of labeled data from a related but different domain traditional machine learning is not able to cope well with learning across different domains in this paper we address this problem for a textmining task where the labeled data are under one distribution in one domain known as indomain data while the unlabeled data are under a related but different domain known as outofdomain data our general goal is to learn from the indomain and apply the learned knowledge to outofdomain we propose a coclustering based classification cocc algorithm to tackle this problem coclustering is used as a bridge to propagate the class structure and knowledge from the indomain to the outofdomain we present theoretical and empirical analysis to show that our algorithm is able to produce high quality classification results even when the distributions between the two data are different the experimental results show that our algorithm greatly improves the classification performance over the traditional learning algorithms
detecting anomalous records in categorical datasets detecting anomalous records in categorical datasets we consider the problem of detecting anomalies in high aritycategorical datasets in most applications anomalies are defined as datapoints that are abnormal quite often we have access to data which consists mostly of normal records a long with a small percentage of unlabelled anomalous records we are interested in the problem of unsupervised anomaly detection where we use the unlabelled data for training and detect records that do not follow the definition of normality
feature selection methods for text classification feature selection methods for text classification we consider feature selection for text classification both theoretically and empirically our main result is an unsupervised feature selection strategy for which we give worstcase theoretical guarantees on the generalization power of the resultant classification function f with respect to the classification function f obtained when keeping all the features to the best of our knowledge this is the first feature selection method with such guarantees in addition the analysis leads to insights as to when and why this feature selection strategy will perform well in practice we then use the techtc newsgroups and reutersrcv data sets to evaluate empirically the performance of this and two simpler but related feature selection strategies against two commonlyused strategies our empirical evaluation shows that the strategy with provable performance guarantees performs well in comparison with other commonlyused feature selection strategies in addition it performs better on certain datasets under very aggressive feature selection
efficient incremental constrained clustering efficient incremental constrained clustering clustering with constraints is an emerging area of data mining research however most work assumes that the constraints are given as one large batch in this paper we explore the situation where the constraints are incrementally given in this way the user after seeing a clustering can provide positive and negative feedback via constraints to critique a clustering solution we consider the problem of efficiently updating a clustering to satisfy the new and old constraints rather than reclustering the entire data set we show that the problem of incremental clustering under constraints is nphard in general but identify several sufficient conditions which lead to efficiently solvable versions these translate into a set of rules on the types of constraints thatcan be added and constraint set properties that must be maintained we demonstrate that this approach is more efficient than reclustering the entire data set and has several other advantages
a framework for simultaneous coclustering and learning from complex data a framework for simultaneous coclustering and learning from complex data for difficult classification or regression problems practitioners often segment the data into relatively homogenous groups and then build a model for each group this twostep procedure usually results in simpler more interpretable and actionable models without any lossin accuracy we consider problems such as predicting customer behavior across products where the independent variables can be naturally partitioned into two groups a pivoting operation can now result in the dependent variable showing up as entries in a customer by product data matrix we present a modelbased coclustering metaalgorithm that interleaves clustering and construction of prediction models to iteratively improve both cluster assignment and fit of the models this algorithm provably converges to a local minimum of a suitable cost function the framework not only generalizes coclustering and collaborative filtering to modelbasedcoclustering but can also be viewed as simultaneous cosegmentation and classification or regression which is better than independently clustering the data first and then building models moreover it applies to a wide range of bimodal or multimodal data and can be easily specialized to address classification and regression problems we demonstrate the effectiveness of our approach on both these problems through experimentation on real and synthetic data
a learning framework using greens function and kernel regularization with application to recommender system a learning framework using greens function and kernel regularization with application to recommender system greens function for the laplace operator represents the propagation of influence of point sources and is the foundation for solving many physics problems on a graph of pairwise similarities the greens function is the inverse of the combinatorial laplacian we resolve the zeromode difficulty by showing its physical origin as the consequence of the von neumann boundary condition we propose to use greens function to propagate label information for both semisupervised and unsupervised learning we also derive this learning framework from the kernel regularization using reproducing kernel hilbert space theory at strong regularization limit greens function provides a welldefined distance metric on a generic weighted graph either as the effective distance on the network of electric resistors or the average commute time in random walks we show that for unsupervised learning this approach is identical to ratio cut and normalized cut spectral clustering algorithms experiments on newsgroups and six uci datasets illustrate the effectiveness of this approach finally we propose a novel itembased recommender system using greens function and show its effectiveness
development of neuroelectromagnetic ontologiesnemo development of neuroelectromagnetic ontologiesnemo eventrelated potentials erp are brain electrophysiological patterns created by averaging electroencephalographic eeg data timelocking to events of interest eg stimulus or response onset in this paper we propose a generic framework for mining anddeveloping domain ontologies and apply it to mine brainwave erp ontologies the concepts and relationships in erp ontologies can be mined according to the following steps pattern decomposition extraction of summary metrics for concept candidates hierarchical clustering of patterns for classes and class taxonomies and clusteringbased classification and association rules mining for relationships axioms of concepts we have applied this process to several densearray channel erp datasets results suggest good correspondence between mined concepts and rules on the one hand and patterns and rules that were independently formulated by domain experts on the other data mining results also suggest ways in which expertdefined rules might be refined to improve ontologyrepresentation and classification results the next goal of our erp ontology mining framework is to address some longstanding challenges in conducting largescale comparison and integration of results across erp paradigms and laboratories in a more general context this work illustrates the promise of an interdisciplinary research program which combines data mining neuroinformatics andontology engineering to address realworld problems
semisupervised classification with hybrid generativediscriminative methods semisupervised classification with hybrid generativediscriminative methods we compare two recently proposed frameworks for combining generative and discriminative probabilistic classifiers and apply them to semisupervised classification in both cases we explore the tradeoff between maximizing a discriminative likelihood of labeled data and a generative likelihood of labeled and unlabeled data while prominent semisupervised learning methods assume low density regions between classes or are subject to generative modeling assumptions we conjecture that hybrid generativediscriminative methods allow semisupervised learning in the presence of strongly overlapping classes and reduce the risk of modeling structure in the unlabeled data that is irrelevant for the specific classification task of interest we apply both hybrid approaches within naively structured markov random field models and provide a thorough empirical comparison with two wellknown semisupervised learning methods on six text classification tasks a semisupervised hybrid generativediscriminative method provides the best accuracy in of the experiments and the multiconditional learning hybrid approach achieves the highest overall mean accuracy across all tasks
finding tribes finding tribes we present a family of algorithms to uncover tribesgroups of individuals who share unusual sequences of affiliations while much work inferring community structure describes largescale trends we instead search for small groups of tightly linked individuals who behave anomalously with respect to those trends we apply the algorithms to a large temporal and relational data set consisting of millions of employment records from the national association of securities dealers the resulting tribes contain individuals at higher risk for fraud are homogenous with respect to risk scores and are geographically mobile all at significant levels compared to random or to other sets of individuals who share affiliations
timedependent event hierarchy construction timedependent event hierarchy construction in this paper an algorithm called time driven documentspartition tdd is proposed to construct an event hierarchy in a text corpus based on a given query specifically assume that a query contains only one feature election election is directly related to the events such as us midterm elections campaign us presidential election campaign and taiwan presidential election campaign where these events may further be divided into several smaller events eg the us midterm elections campaign can be broken down into events such as campaign for vote election results and the resignation of donald h rumsfeld as such an event hierarchy is resulted our proposed algorithm tdd tackles the problem by three major steps identify the features that are related to the query according to both the timestamps and the contents of the documents the features identified are regarded as bursty features extract the documents that are highly related to the bursty features based on time partition the extracted documents to form events and organize them in a hierarchicalstructure to the best of our knowledge there is little works targeting for constructing a featurebased event hierarchy for a text corpus practically event hierarchies can assist us to efficiently locate our target information in a text corpus easily again assume that election is used for a query without an event hierarchy it is very difficult to identify what are the major events related to it when do these events happened as well as the features and the news articles that are related to each of these events we have archived twoyear news articles to evaluate the feasibility of tdd the encouraging results indicated that tdd is practically sound and highly effective
the minimum consistent subset cover problem and its applications in data mining the minimum consistent subset cover problem and its applications in data mining in this paper we introduce and study the minimum consistent subset cover mcsc problem given a finite ground set x and a constraint t find the minimum number of consistent subsets that cover x where a subset of x is consistent if it satisfies t the mcsc problem generalizes the traditional set covering problem and has minimum clique partition a dual problem of graph coloring as an instance many practical data mining problems in the areas of rule learning clustering and frequent pattern mining can be formulated as mcsc instances in particular we discuss the minimum rule set problem that minimizes model complexity of decision rules as well as some converse kclustering problems that minimize the number of clusters satisfying certain distance constraints we also show how the mcsc problem can find applications in frequent pattern summarization for any of these mcsc formulations our proposed novel graphbased generic algorithm cag can be directly applicable cag starts by constructing a maximal optimal partial solution then performs an exampledriven specifictogeneral search on a dynamically maintained bipartite assignment graph to simultaneously learn a set of consistent subsets with small cardinality covering the ground set our experiments on benchmark datasets show that cag achieves good results compared to existing popular heuristics
constraintdriven clustering constraintdriven clustering clustering methods can be either datadriven or needdriven datadriven methods intend to discover the true structure of the underlying data while needdriven methods aims at organizing the true structure to meet certain application requirements thus needdriven eg constrained clustering is able to find more useful and actionable clusters in applications such as energy aware sensor networks privacy preservation and market segmentation however the existing methods of constrained clustering require users to provide the number of clusters which is often unknown in advance but has a crucial impact on the clustering result in this paper we argue that a more natural way to generate actionable clusters is to let the applicationspecific constraints decide the number of clusters for this purpose we introduce a novel cluster model constraintdriven clustering cdc which finds an a priori unspecified number of compact clusters that satisfy all userprovided constraints two general types of constraints are considered ie minimum significance constraints and minimum variance constraints as well as combinations of these two types we prove the nphardness of the cdc problem with different constraints we propose a novel dynamic data structure the cdtree which organizes data points in leaf nodes such that each leaf node approximately satisfies the cdc constraints and minimizes the objective function based on cdtrees we develop an efficient algorithm to solve the new clustering problem our experimental evaluation on synthetic and real datasets demonstrates the quality of the generated clusters and the scalability of the algorithm
trajectory pattern mining trajectory pattern mining the increasing pervasiveness of locationacquisition technologies gps gsm networks etc is leading to the collection of large spatiotemporal datasets and to the opportunity of discovering usable knowledge about movement behaviour which fosters novel applications and services in this paper we move towards this direction and develop an extension of the sequential pattern mining paradigm that analyzes the trajectories of moving objects we introduce trajectory patterns as concise descriptions of frequent behaviours in terms of both space ie the regions of space visited during movements and time ie the duration of movements in this setting we provide a general formal statement of the novel mining problem and then study several different instantiations of different complexity the various approaches are then empirically evaluated over real data and synthetic benchmarks comparing their strengths and weaknesses
enhanced max margin learning on multimodal data mining in a multimedia database enhanced max margin learning on multimodal data mining in a multimedia database the problem of multimodal data mining in a multimedia database can be addressed as a structured prediction problem where we learn the mapping from an input to the structured and interdependent output variables in this paper built upon the existing literature on the max margin based learning we develop a new max margin learning approach called enhanced max margin learning emml framework in addition we apply emml framework to developing an effective and efficient solution to the multimodal data mining problem in a multimedia database the main contributions include we have developed a new max margin learning approach the enhanced max margin learning framework that is much more efficient in learning with a much faster convergence rate which is verified in empirical evaluations we have applied this emml approach to developing an effective and efficient solution to the multimodal data mining problem that is highly scalable in the sense that the query response time is independent of the database scale allowing facilitating a multimodal data mining querying to a very large scale multimedia databaseand excelling many existing multimodal data mining methods in the literature that do not scale up at all this advantage is also supported through the complexity analysis as well as empirical evaluations against a stateoftheart multimodal data mining method from the literature while emml is a general framework for the evaluation purpose we apply it to the berkeley drosophila embryo image database and report the performance comparison with a stateoftheart multimodal data mining method
finding lowentropy sets and trees from binary data finding lowentropy sets and trees from binary data the discovery of subsets with special properties from binary data hasbeen one of the key themes in pattern discovery pattern classes suchas frequent itemsets stress the cooccurrence of the value in the data while this choice makes sense in the context of sparse binary data it disregards potentially interesting subsets of attributes that have some other type of dependency structure
dynamic hybrid clustering of bioinformatics by incorporating text mining and citation analysis dynamic hybrid clustering of bioinformatics by incorporating text mining and citation analysis to unravel the concept structure and dynamics of the bioinformatics field we analyze a set of publications from the web of science and medline databases publication years for delineating this complex interdisciplinary field a novel bibliometric retrieval strategy is used given that the performance of unsupervised clustering and classification of scientific publications is significantly improved by deeply merging textual contents with the structure of the citation graph we proceed with a hybrid clustering method based on fishers inverse chisquare the optimal number of clusters is determined by a compound semiautomatic strategy comprising a combination of distancebased and stabilitybased methods we also investigate the relationship between number of latent semantic indexing factors number of clusters and clustering performance the hits and pagerank algorithms are used to determine representative publications in each cluster next we develop a methodology for dynamic hybrid clustering of evolving bibliographic data sets the same clustering methodology is applied to consecutive periods defined by time windows on the set and in a subsequent phase chains are formed by matching and tracking clusters through time term networks for the eleven resulting cluster chains present the cognitive structure of the field finally we provide a view on how much attention the bioinformatics community has devoted to the different subfields through time
detecting research topics via the correlation between graphs and texts detecting research topics via the correlation between graphs and texts in this paper we address the problem of detecting topics in largescale linked document collections recently topic detection has become a very active area of research due to its utility for information navigation trend analysis and highlevel description of data we present a unique approach that uses the correlation between the distribution of a term that represents a topic and the link distribution in the citation graph where the nodes are limited to the documents containing the term this tight coupling between term and graph analysis is distinguished from other approaches such as those that focus on language models we develop a topic score measure for each term using the likelihood ratio of binary hypotheses based on a probabilistic description of graph connectivity our approach is based on the intuition that if a term is relevant to a topic the documents containing the term have denser connectivity than a random selection of documents we extend our algorithm to detect a topic represented by a set of terms using the intuition that if the cooccurrence of terms represents a new topic the citation pattern should exhibit the synergistic effect we test our algorithm on two electronic research literature collectionsarxiv and citeseerour evaluation shows that the approach is effective and reveals some novel aspects of topic detection
exploiting duality in summarization with deterministic guarantees exploiting duality in summarization with deterministic guarantees summarization is an important task in data mining a major challenge over the past years has been the efficient construction of fixedspace synopses that provide a deterministic quality guarantee often expressed in terms of a maximumerror metric histograms and several hierarchical techniques have been proposed for this problem however their time andor space complexities remain impractically high and depend not only on the data set size n but also on the space budget b these handicaps stem from a requirement to tabulate all allocations of synopsis space to different regions of the data in this paper we develop an alternative methodology that dispels these deficiencies thanks to a fruitful application of the solution to the dual problem given a maximum allowed error determine the minimumspace synopsis that achieves it compared to the stateoftheart our histogram construction algorithm reduces time complexity by at least a blogn over log factor and our hierarchical synopsis algorithm reduces the complexity by at least a factor of logb over log logn in time and blog b over log n in space where is the optimal error these complexity advantages offer both a spaceefficiency and a scalability that previous approaches lacked we verify the benefits of our approach in practice by experimentation
correlation search in graph databases correlation search in graph databases correlation mining has gained great success in many application domains for its ability to capture the underlying dependency between objects however the research of correlation mining from graph databases is still lacking despite the fact that graph data especially in various scientific domains proliferate in recent years in this paper we propose a new problem of correlation mining from graph databases called correlated graph search cgs cgs adopts pearsons correlation coefficient as a correlation measure to take into consideration the occurrence distributions of graphs however the problem poses significant challenges since every subgraph of a graph in the database is a candidate but the number of subgraphs is exponential we derive two necessary conditions which set bounds on the occurrence probability of a candidate in the database with this result we design an efficient algorithm that operates on a much smaller projected database and thus we are able to obtain a significantly smaller set of candidates to further improve the efficiency we develop three heuristic rules and apply them on the candidate set to further reduce the search space our extensive experiments demonstrate the effectiveness of our method on candidate reduction the results also justify the efficiency of our algorithm in mining correlations from large real and synthetic datasets
raising the baseline for highprecision text classifiers raising the baseline for highprecision text classifiers many important application areas of text classifiers demand high precision andit is common to compare prospective solutions to the performance of naive bayes this baseline is usually easy to improve upon but in this work we demonstrate that appropriate document representation can make out performing this classifier much more challenging most importantly we provide a link between naive bayes and the logarithmic opinion pooling of the mixtureofexperts framework which dictates a particular type of document length normalization motivated by documentspecific feature selection we propose monotonic constraints on document term weighting which is shown as an effective method of finetuning document representation the discussion is supported by experiments using three large email corpora corresponding to the problem of spam detection where high precision is of particular importance
a fast algorithm for finding frequent episodes in event streams a fast algorithm for finding frequent episodes in event streams frequent episode discovery is a popular framework for mining data available as a long sequence of events an episode is essentially a short ordered sequence of event types and the frequency of an episode is some suitable measure of how often the episode occurs in the data sequence recentlywe proposed a new frequency measure for episodes based on the notion of nonoverlapped occurrences of episodes in the event sequence and showed that such a definition in addition to yielding computationally efficient algorithms has some important theoretical properties in connecting frequent episode discovery with hmm learning this paper presents some new algorithms for frequent episode discovery under this nonoverlapped occurrencesbased frequency definition the algorithms presented here are better by a factor of n where n denotes the size of episodes being discovered in terms of both time and space complexities when compared to existing methods for frequent episode discovery we show through some simulation experiments that our algorithms are very efficient the new algorithms presented here have arguably the least possible orders of spaceand time complexities for the task of frequent episode discovery
costeffective outbreak detection in networks costeffective outbreak detection in networks given a water distribution network where should we place sensors toquickly detect contaminants or which blogs should we read to avoid missing important stories
mining statistically important equivalence classes and deltadiscriminative emerging patterns mining statistically important equivalence classes and deltadiscriminative emerging patterns the supportconfidence framework is the most common measure used in itemset mining algorithms for its antimonotonicity that effectively simplifies the search lattice this computational convenience brings both quality and statistical flaws to the results as observed by many previous studies in this paper we introduce a novel algorithm that produces itemsets with ranked statistical merits under sophisticated test statistics such as chisquare risk ratio odds ratio etc our algorithm is based on the concept of equivalence classes an equivalence class is a set of frequent itemsets that always occur together in the same set of transactions therefore itemsets within an equivalence class all share the same level of statistical significance regardless of the variety of test statistics as an equivalence class can be uniquely determined and concisely represented by a closed pattern and a set of generators we just mine closed patterns and generators taking a simultaneous depthfirst search scheme this parallel approach has not been exploited by any prior work we evaluate our algorithm on two aspects in general we compare to lcm and fpclose which are the best algorithms tailored for mining only closed patterns in particular we compare to epminer which is the most recent algorithm for mining a type of relative risk patterns known as minimal emerging patterns experimental results show that our algorithm is faster than all of them sometimes even multiple orders of magnitude faster these statistically ranked patterns and the efficiency have a high potential for reallife applications especially in biomedical and financial fields where classical test statistics are of dominant interest
very sparse stable random projections for dimension reduction in l norm very sparse stable random projections for dimension reduction in l norm the method of stable random projections is a useful tool for efficiently computing the l a rnxd if we multiply a with a projection matrix r r dxk k dwhose entries are iid samples of an stable distributionthen the projected matrix b ax r r nxkx containsenough information to approximately recover the l properties in a
boostcluster boostcluster data clustering is an important task in many disciplines a large number of studies have attempted to improve clustering by using the side information that is often encoded as pairwise constraints however these studies focus on designing special clustering algorithms that can effectively exploit the pairwise constraints we present a boosting framework for data clusteringtermed as boostcluster that is able to iteratively improve the accuracy of any given clustering algorithm by exploiting the pairwise constraints the key challenge in designing a boosting framework for data clustering is how to influence an arbitrary clustering algorithm with the side information since clustering algorithms by definition are unsupervised the proposed framework addresses this problem by dynamically generating new data representations at each iteration that are on the one hand adapted to the clustering results at previous iterations by the given algorithm and on the other hand consistent with the given side information our empirical study shows that the proposed boosting framework is effective in improving the performance of a number of popular clustering algorithms kmeans partitional singlelink spectral clustering and its performance is comparable to the stateoftheart algorithms for data clustering with side information
efficient mining of iterative patterns for software specification discovery efficient mining of iterative patterns for software specification discovery studies have shown that program comprehension takes up to of software development costs such high costs are caused by the lackof documented specification and further aggravated by the phenomenon of software evolution there is a need for automated tools to extract specifications to aid program comprehension in this paper a novel technique to efficiently mine common software temporal patterns from traces is proposed these patterns shed light on program behaviors and are termed iterative patterns they capture unique characteristic of software traces typically not found in arbitrary sequences specifically due to loops interesting iterative patterns can occur multiple times within a trace furthermore an occurrence of an iterative pattern in a trace can extend across a sequence of indefinite length since a program behavior can be manifested in numerous ways analyzing a single trace will not be sufficient iterative pattern mining extends sequential pattern and episode minings to discover frequent iterative patterns which occur repetitively both within a program trace and across multiple traces in this paper we present cliper closed iterative pattern miner to efficiently mine a closed set of iterative patterns a performance study on several simulated and real datasets shows the efficiency of our mining algorithm and effectiveness of our pruning strategy our case study on jboss application server confirms the usefulness of mined patterns in discovering interesting software behavioral specification
a probabilistic framework for relational clustering a probabilistic framework for relational clustering relational clustering has attracted more and more attention due to its phenomenal impact in various important applications which involve multitype interrelated data objects such as web mining search marketing bioinformatics citation analysis and epidemiology in this paper we propose a probabilistic model for relational clustering which also provides a principal framework to unify various important clustering tasks including traditional attributesbased clustering semisupervised clustering coclustering and graph clustering the proposed model seeks to identify cluster structures for each type of data objects and interaction patterns between different types of objects under this model we propose parametric hard and soft relational clustering algorithms under a large number of exponential family distributions the algorithms are applicable to relational data of various structures and at the same time unifies a number of statoftheart clustering algorithms coclustering algorithms the kpartite graph clustering bregman kmeans and semisupervised clustering based on hidden markov random fields
nestedness and segmented nestedness nestedness and segmented nestedness consider each row of a dataset as the subset of the columns for which the row has an then a dataset is nested if for all pairs of rows one row is either a superset or subset of the other the concept of nestedness has its origins in ecology where approximate versions of it has been used to model the species distribution in different locations we argue that nestedness and its extensions are interesting properties of datasets and that they can be applied also to domains other than ecology
automatic labeling of multinomial topic models automatic labeling of multinomial topic models multinomial distributions over words are frequently used to model topics in text collections a common major challenge in applying all such topic models to any text mining problem is to label a multinomial topic model accurately so that a user can interpret the discovered topic so far such labels have been generated manually in a subjective way in this paper we propose probabilistic approaches to automatically labeling multinomial topic models in an objective way we cast this labeling problem as an optimization problem involving minimizing kullbackleibler divergence between word distributions and maximizing mutual information between a label and a topic model experiments with user study have been done on two text data sets with different genresthe results show that the proposed labeling methods are quite effective to generate labels that are meaningful and useful for interpreting the discovered topic models our methods are general and can be applied to labeling topics learned through all kinds of topic models such as plsa lda and their variations
expertise modeling for matching papers with reviewers expertise modeling for matching papers with reviewers an essential part of an expertfinding task such as matching reviewers to submitted papers is the ability to model the expertise of a person based on documents we evaluate several measures of the association between an author in an existing collection of research papers and a previously unseen document we compare two language model based approaches with a novel topic model authorpersonatopic apt in this model each author can write under one or more personas which are represented as independent distributions over hidden topics examples of previous papers written by prospective reviewers are gathered from the rexa database which extracts and disambiguates author mentions from documents gathered from the web we evaluate the models using a reviewer matching task based on human relevance judgments determining how well the expertise of proposed reviewers matches a submission we find that the apt topic model outperforms the other models
joint cluster analysis of attribute and relationship data withoutapriori specification of the number of clusters joint cluster analysis of attribute and relationship data withoutapriori specification of the number of clusters in many applications attribute and relationship data areavailable carrying complementary information about real world entities in such cases a joint analysis of both types of data can yield more accurate results than classical clustering algorithms that either use only attribute data or only relationship graph data the connected kcenter ckc has been proposed as the first joint cluster analysis model to discover k clusters which are cohesive on both attribute and relationship data however it is wellknown that prior knowledge on the number of clusters is often unavailable in applications such as community dentification and hotspot analysis in this paper we introduce and formalize the problem of discovering an apriori unspecified number of clusters in the context of joint cluster analysis of attribute and relationship data called connected x clusters cxc problem true clusters are assumed to be compact and distinctive from their neighboring clusters in terms of attribute data and internally connected in terms of relationship data different from classical attributebased clustering methods the neighborhood of clusters is not defined in terms of attribute data but in terms of relationship data to efficiently solve the cxc problem we present jointclust an algorithm which adopts a dynamic twophase approach in the first phase we find so called cluster atoms we provide a probability analysis for thisphase which gives us a probabilistic guarantee that each true cluster is represented by at least one of the initial cluster atoms in the second phase these cluster atoms are merged in a bottomup manner resulting in a dendrogram the final clustering is determined by our objective function our experimental evaluation on several real datasets demonstrates that jointclust indeed discovers meaningful and accurate clusterings without requiring the user to specify the number of clusters
multiscale topic tomography multiscale topic tomography modeling the evolution of topics with time is of great value in automatic summarization and analysis of large document collections in this work we propose a new probabilistic graphical model to address this issue the new model which we call the multiscale topic tomography model mttm employs nonhomogeneous poisson processes to model generation of wordcounts the evolution of topics is modeled through a multiscale analysis using haar wavelets one of the new features of the model is its modeling the evolution of topics at various timescales of resolution allowing the user to zoom in and out of the timescales our experiments on science data using the new model uncovers some interesting patterns in topics the new model is also comparable to lda in predicting unseen data as demonstrated by our perplexity experiments
mining optimal decision trees from itemset lattices mining optimal decision trees from itemset lattices we present dl an exact algorithm for finding a decision tree that optimizes a ranking function under size depth accuracy and leaf constraints because the discovery of optimal trees has high theoretical complexity until now few efforts have been made to compute such trees for realworld datasets an exact algorithm is of both scientific and practical interest from a scientific point of view it can be used as a gold standard to evaluate the performance of heuristic constraintbased decision tree learners and to gain new insight in traditional decision tree learners from the application point of view it can be used to discover trees that cannot be found by heuristic decision tree learners the key idea behind our algorithm is that there is a relation between constraints on decision trees and constraints on itemsets we show that optimal decision trees can be extracted from lattices of itemsets in linear time we give several strategies to efficiently build these lattices experiments show that under the same constraints dl obtains better results than c which confirms that exhaustive search does not always imply overfitting the results also show that dl is a useful and interesting tool to learn decision trees under constraints
association analysisbased transformations for protein interaction networks association analysisbased transformations for protein interaction networks protein interaction networks are one of the most promising types of biological data for the discovery of functional modules and the prediction of individual protein functions however it is known that these networks are both incomplete and inaccurate ie they have spurious edges and lackbiologically valid edges one way to handle this problem is by transforming the original interaction graph into new graphs that remove spurious edges add biologically valid ones and assign reliability scores to the edges constituting the final network we investigate currently existing methods as well as propose a robust association analysisbased method for this task this method is based on the concept of hconfidence which is a measure that can be used to extract groups of objects having high similarity with each other experimental evaluation on several protein interaction data sets show that hypercliquebased transformations enhance the performance of standard function prediction algorithms significantly and thus have merit
applying collaborative filtering techniques to movie search for better ranking and browsing applying collaborative filtering techniques to movie search for better ranking and browsing we propose a new ranking method which combines recommender systems with information search tools for better search and browsing our method uses a collaborative filtering algorithm to generate personal item authorities for each user and combines them with item proximities for better ranking to demonstrate our approach we build a prototype movie search and browsing engine called mad movies actors and directors degrees of separation we conduct offline and online tests of our ranking algorithm for offline testing we use yahoo search queries that resulted in a click on a yahoo movies or internet movie database imdb movie url our online test involved yahoo employees providing subjective assessments of results quality in both tests our ranking methods show significantly better recall and quality than imdb search and yahoo movies current search
tracking multiple topics for finding interesting articles tracking multiple topics for finding interesting articles we introduce multiple topic tracking mtt for iscore to better recommend news articles for users with multiple interests and to address changes in user interests over time as an extension of the basic rocchio algorithm traditional topic detection and tracking and singlepass clustering mtt maintains multiple interest profiles to identify interesting articles for a specific user given userfeedback focusing on only interesting topics enables iscore to discard useless profiles to address changes in user interests and to achieve a balance between resource consumption and classification accuracy also by relating a topics interestingness to an articles interestingness iscore is able to achieve higher quality results than traditional methods such as the rocchio algorithm we identify several operating parameters that work well for mtt using the same parameters we show that mtt alone yields high quality results for recommending interesting articles from several corpora the inclusion of mtt improves iscores performance by in recommending news articles from the yahoo news rss feeds and the trec adaptive filter article collection and through a small user study we show that iscore can still perform well when only provided with little user feedback
active exploration for learning rankings from clickthrough data active exploration for learning rankings from clickthrough data we address the task of learning rankings of documents from search enginelogs of user behavior previous work on this problem has relied onpassively collected clickthrough data in contrast we show that anactive exploration strategy can provide data that leads to much fasterlearning specifically we develop a bayesian approach for selectingrankings to present users so that interactions result in more informativetraining data our results using the trec web corpus as well assynthetic data demonstrate that a directed exploration strategy quicklyleads to users being presented improved rankings in an online learningsetting we find that active exploration substantially outperformspassive observation and random exploration
hierarchical mixture models hierarchical mixture models mixture models form one of the most widely used classes of generative models for describing structured and clustered data in this paper we develop a new approach for the analysis of hierarchical mixture models more specifically using a text clustering problem as a motivation we describe a natural generative process that creates a hierarchical mixture model for the data in this process an adversary starts with an arbitrary base distribution and then builds a topic hierarchy via some evolutionary process where he controls the parameters of the process we prove that under our assumptions given a subset of topics that represent generalizations of one another such as baseball sports base for any document which was produced via some topic in this hierarchy we can efficiently determine the most specialized topic in this subset it still belongs to the quality of the classification is independent of the total number of topics in the hierarchy and our algorithm does not need to know the total number of topics in advance our approach also yields an algorithm for clustering and unsupervised topical tree reconstruction we validate our model by showing that properties predicted by our theoretical results carry over to real data we then apply our clustering algorithm to two different datasets i newsgroups and ii a snapshot of abstracts of arxiv categories abstracts in both cases our algorithm performs extremely well
knowledge discovery of multipletopic document using parametric mixture model with dirichlet prior knowledge discovery of multipletopic document using parametric mixture model with dirichlet prior documents such as those seen on wikipedia and folksonomy have tended to be assigned with multiple topics as a metadatatherefore it is more and more important to analyze a relationship between a document and topics assigned to the document in this paper we proposed a novel probabilistic generative model of documents with multiple topics as a metadata by focusing on modeling the generation process of a document with multiple topics we can extract specific properties of documents with multiple topicsproposed model is an expansion of an existing probabilistic generative model parametric mixture model pmm pmm models documents with multiple topics by mixing model parameters of each single topic since however pmm assigns the same mixture ratio to each single topic pmm cannot take into account the bias of each topic within a document to deal with this problem we propose a model that considers dirichlet distribution as a prior distribution of the mixture ratiowe adopt variational bayes method to infer the bias of each topic within a document we evaluate the proposed model and pmm using medline corpusthe results of fmeasure precision and recall show that the proposed model is more effective than pmm on multipletopic classification moreover we indicate the potential of the proposed model that extracts topics and documentspecific keywords using information about the assigned topics
using hierarchical clustering for learning theontologies used in recommendation systems using hierarchical clustering for learning theontologies used in recommendation systems ontologies are being successfully used to overcome semanticheterogeneity and are becoming fundamental elements of the semanticweb recently it has also been shown that ontologies can be used tobuild more accurate and more personalized recommendation systems byinferencing missing users preferences however these systemsassume the existence of ontologies without considering theirconstruction with product catalogs changing continuously newtechniques are required in order to build these ontologies in realtime and autonomously from any expert interventionthis paper focuses on this problem and show that it is possible tolearn ontologies autonomously by using clustering algorithms results on the movielens and jester data sets show that recommendersystem with learnt ontologies significantly outperform the classical recommendation approach
practical learning from onesided feedback practical learning from onesided feedback in many data mining applications online labeling feedback is only available for examples which were predicted to belong to the positive class such applications includespam filtering in the case where users never checkemails marked spam document retrieval where users cannotgive relevance feedback on unretrieved documentsand online advertising where user behavior cannot beobserved for unshown advertisements onesided feedback can cripple the performance of classical mistakedriven online learners such as perceptron previous work under the apple tasting framework showed how to transform standard online learners into successful learners from one sided feedback however we find in practice that this transformation may request more labels than necessary to achieve strong performance in this paperwe employ two active learning methods which reduce the number of labels requested in practice one method is the use of label efficient active learning the other methodsomewhat surprisingly is the use of marginbased learners without modification which we show combines implicit active learning and a greedy strategy to managing the exploration exploitation tradeoff experimental results show that these methods can be significantly more effective in practice than those using the apple tasting transformation even on minority class problems
information genealogy information genealogy we now have incrementallygrown databases of text documents ranging back for over a decade in areas ranging from personal email to newsarticles and conference proceedings while accessing individual documents is easy methods for overviewing and understanding these collections as a whole are lacking in number and in scope in this paper we address one such global analysis task namely the problem of automatically uncovering how ideas spread through the collection over time we refer to this problem as information genealogy in contrast to bibliometric methods that are limited to collections with explicit citation structure we investigate contentbased methods requiring only the text and timestamps of the documents in particular we propose a languagemodeling approach and a likelihood ratio test to detect influence between documents in a statistically wellfounded way furthermore we show how this method can be used to infer citation graphs and to identify the most influential documents in the collection experiments on the nips conference proceedings and the physics arxiv show that our method is more effective than methods based on document similarity
a conceptbased model for enhancing text categorization a conceptbased model for enhancing text categorization most of text categorization techniques are based on word andor phrase analysis of the text statistical analysis of a term frequency captures the importance of the term within a document only however two terms can have the same frequency in their documents but one term contributes moreto the meaning of its sentences than the other term thus the underlying model should indicate terms that capture these mantics of text in this case the model can capture terms that present the concepts of the sentence which leads todiscover the topic of the document a new conceptbased model that analyzes terms on the sentence and document levels rather than the traditional analysis of document only is introduced the conceptbased model can effectively discriminate between nonimportant terms with respect to sentence semantics and terms which hold the concepts that represent the sentence meaning the proposed model consists of conceptbased statistical analyzer conceptual ontological graph representationand concept extractor the term which contributes to the sentence semantics is assigned two different weights by the conceptbased statistical analyzer and the conceptual ontological graph representation these two weights are combined into a new weight the concepts that have maximum combined weights are selected by the concept extractor a set of experiments using the proposed conceptbasedmodel on different datasets in text categorization is conducted the experiments demonstrate the comparison between traditional weighting and the conceptbased weighting obtained by the combined approach of the conceptbased statistical analyzer and the conceptual ontological graph the evaluation of results is relied on two quality measures the macroaveraged f and the error rate these quality measures are improved when the newly developedconceptbased model is used to enhance the quality of thetext categorization
partial example acquisition in costsensitive learning partial example acquisition in costsensitive learning it is often expensive to acquire data in realworld data mining applications most previous data mining and machine learning research however assumes that a fixed set of training examples is given in this paper we propose an online costsensitive framework that allows a learner to dynamically acquire examples as it learns and to decide the ideal number of examples needed to minimize the total cost we also propose a new strategy for partial example acquisition pas in which the learner can acquire examples with a subset of attribute values to reduce the data acquisition cost experiments on uci datasets show that the new pas strategy is an effective method in reducing the total cost for data acquisition
a spectral clustering approach to optimally combining numericalvectors with a modular network a spectral clustering approach to optimally combining numericalvectors with a modular network we address the issue of clustering numerical vectors with a network the problem setting is basically equivalent to constrained clustering by wagstaff and cardie and semisupervised clustering by basu et al but our focus is more on the optimal combination of two heterogeneous data sources an application of this setting is web pages which can be numerically vectorized by their contents eg term frequencies and which are hyperlinked to each other showing a network another typical application is genes whose behavior can be numerically measured and a gene network can be given from another data sourcewe first define a new graph clustering measure which we call normalized network modularity by balancing the cluster size of the original modularity we then propose a new clustering method which integrates the cost of clustering numerical vectors with the cost of maximizing the normalized network modularity into a spectral relaxation problem our learning algorithm is based on spectral clustering which makes our issue an eigenvalue problem and uses kmeans for final cluster assignments a significant advantage of our method is that we can optimize the weight parameter for balancing the two costs from the given data by choosing the minimum total cost we evaluated the performance of our proposed method using a variety of datasets including synthetic data as well as realworld data from molecular biology experimental results showed that our method is effective enough to have good results for clustering by numerical vectors and a network
making generative classifiers robust to selection bias making generative classifiers robust to selection bias this paper presents approaches to semisupervised learning when the labeled training data and test data are differently distributed specifically the samples selected for labeling are a biased subset of some general distribution and the test set consists of samples drawn from either that general distribution or the distribution of the unlabeled samples an example of the former appears in loan application approval where samples with repaydefault labels exist only for approved applicants and the goal is to model the repaydefault behavior of all applicants an example of the latter appears in spam filtering in which the labeled samples can be outdated due to the cost of labeling email by hand but an unlabeled set of uptodate emails exists and the goal is to build a filter to sort new incoming emailmost approaches to overcoming such bias in the literature rely on the assumption that samples are selected for labeling depending only on the features not the labels a case in which provably correct methods exist the missing labels are said to be missing at random mar in real applications however the selection bias can be more severe when the mar conditional independence assumption is not satisfied and missing labels are said to be missing not at random mnar and no learning method is provably always correctwe present a generative classifier the shifted mixture model smm with separate representations of the distributions of the labeled samples and the unlabeled samples the smm makes no conditional independence assumptions and can model distributions of semilabeled data sets with arbitrary bias in the labeling we present a learning method based on the expectation maximization em algorithm that while not always able to overcome arbitrary labeling bias learns smms with higher testset accuracy in realworld data sets with mnar bias than existing learning methods that are proven to overcome mar bias
statistical change detection for multidimensional data statistical change detection for multidimensional data this paper deals with detecting change of distribution in multidimensional data sets for a given baseline data set and a set of newly observed data points we define a statistical test called the density test for deciding if the observed data points are sampled from the underlying distribution that produced the baseline data set we define a test statistic that is strictly distributionfree under the null hypothesis our experimental results show that the density test has substantially more power than the two existing methods for multidimensional change detection
use of ranked cross document evidence trails for hypothesis generation use of ranked cross document evidence trails for hypothesis generation this paper focuses on detecting how concepts are linked across multiple textdocuments by generating an evidence trail explaining the connection a traditional search involving for example two or more person names willattempt to find documents mentioning both of these individuals this researchfocuses on a different interpretation of such a query what is the best evidencetrail across documents that explains a connection between these individuals for example allmay be good golfers a generalization ofthis task involves query terms representing general concepts eg indictmentforeign policy such queries reflect a special case oftext mining previous attempts to solve this problem have focused on graphapproaches involving hyperlinked documents and link analysis tools exploiting named entities a new robust framework is presented based on i generating concept chain graphs a hybrid content representation ii performing graph matching to select candidate subgraphs and iii subsequently using graphical models to validate hypotheses using ranked evidence trails we adapt the duc data set for crossdocument summarization to evaluate evidence trails generated by this approach
graphscope graphscope how can we find communities in dynamic networks of socialinteractions such as who calls whom who emails whom or who sells to whom how can we spot discontinuity timepoints in such streams of graphs in an online anytime fashion we propose graphscope that addresses both problems using information theoretic principles contrary to the majority of earlier methods it needs no userdefined parameters moreover it is designed to operate on large graphs in a streaming fashion we demonstrate the efficiency and effectiveness of our graphscope on real datasets from several diverse domains in all cases it produces meaningful timeevolving patterns that agree with human intuition
weighting versus pruning in rule validation for detecting network and host anomalies weighting versus pruning in rule validation for detecting network and host anomalies for intrusion detection the lerad algorithm learns a succinct set of comprehensible rules for detecting anomalies which could be novel attacks lerad validates the learned rules on a separate heldout validation set and removes rules that cause false alarms however removing rules with possible high coverage can lead to missed detections we propose to retain these rules and associate weights to them we present three weighting schemes and our empirical results indicate that for lerad rule weighting can detect more attacks than pruning with minimal computational overhead
enhancing semisupervised clustering enhancing semisupervised clustering semisupervised clustering employs limited supervision in the form of labeled instances or pairwise instance constraints to aid unsupervised clustering and often significantly improves the clustering performance despite the vast amount of expert knowledge spent on this problem most existing work is not designed for handling highdimensional sparse data this paper thus fills this crucial void by developing a semisupervised clustering method based on spherical kmeans via feature projection screen specifically we formulate the problem of constraintguided feature projection which can be nicely integrated with semisupervised clustering algorithms and has the ability to effectively reduce data dimension indeed our experimental results on several realworld data sets show that the screen method can effectively deal with highdimensional data and provides an appealing clustering performance
a framework for community identification in dynamic social networks a framework for community identification in dynamic social networks we propose frameworks and algorithms for identifying communities in social networks that change over time communities are intuitively characterized as unusually densely knit subsets of a social network this notion becomes more problematic if the social interactions change over time aggregating social networks over time can radically misrepresent the existing and changing community structure instead we propose an optimizationbased approach for modeling dynamic community structure we prove that finding the most explanatory community structure is nphard and apxhard and propose algorithms based on dynamic programming exhaustive search maximum matching and greedy heuristics we demonstrate empirically that the heuristics trace developments of community structure accurately for several synthetic and realworld examples
a scalable modular convex solver for regularized risk minimization a scalable modular convex solver for regularized risk minimization a wide variety of machine learning problems can be described as minimizing a regularized risk functional with different algorithms using different notions of risk and different regularizers examples include linear support vector machines svms logistic regression conditional random fields crfs and lasso amongst others this paper describes the theory and implementation of a highly scalable and modular convex solver which solves all these estimation problems it can be parallelized on a cluster of workstations allows for datalocality and can deal with regularizers such as l and l penalties at present our solver implements different estimation problems can be easily extended scales to millions of observations and is up to times faster than specialized solvers for many applications the open source code is freely available as part of the elefant toolbox
fast besteffort pattern matching in large attributed graphs fast besteffort pattern matching in large attributed graphs we focus on large graphs where nodes have attributes such as a social network where the nodes are labelled with each persons job title in such a setting we want to find subgraphs that match a user query pattern for example a star query would be find a ceo who has strong interactions with a manager a lawyerand an accountant or another structure as close to that as possible similarly a loop query could help spot a money laundering ring
fast directionaware proximity for graph mining fast directionaware proximity for graph mining in this paper we study asymmetric proximity measures on directed graphs which quantify the relationships between two nodes or two groups of nodes the measures are useful in several graph mining tasks including clustering link prediction and connection subgraph discovery our proximity measure is based on the conceptof escape probability this way we strive to summarize the multiple facets of nodesproximity while avoiding some of the pitfalls to which alternative proximity measures are susceptible a unique feature of the measures is accounting for the underlying directional information we put a special emphasis on computational efficiency and develop fast solutions that are applicable in several settings our experimental study shows the usefulness of our proposed directionaware proximity method for several applications and that our algorithms achieve a significant speedup up to x over straight forward implementations
scalable lookahead linear regression trees scalable lookahead linear regression trees most decision tree algorithms base their splitting decisions on a piecewise constant model often these splitting algorithms are extrapolated to trees with nonconstant models at the leaf nodes the motivation behind lookahead linear regression trees llrt is that out of all the methods proposed to date there has been no scalable approach to exhaustively evaluate all possible models in the leaf nodes in order to obtain an optimal split using several optimizations llrt is able to generate and evaluate thousands of linear regression models per second this allows for a nearexhaustive evaluation of all possible splits in a node based on the quality of fit of linear regression models in the resulting branches we decompose the calculation of the residual sum of squares in such a way that a large part of it is precomputed the resulting method is highly scalable we observe it to obtain high predictive accuracy for problems with strong mutual dependencies between attributes we report on experiments with two simulated and seven real data sets
characterising the difference characterising the difference characterising the differences between two databases is an often occurring problem in data mining detection of change over time is a prime example comparing databases from two branches is another one the key problem is to discover the patterns that describe the difference emerging patterns provide only a partial answer to this question
privacypreservation for gradient descent methods privacypreservation for gradient descent methods gradient descent is a widely used paradigm for solving many optimization problems stochastic gradient descent performs a series of iterations to minimize a target function in order to reach a local minimum in machine learning or data mining this function corresponds to a decision model that is to be discovered the gradient descent paradigm underlies many commonly used techniques in data mining and machine learning such as neural networks bayesian networks genetic algorithms and simulated annealing to the best of our knowledge there has not been any work that extends the notion of privacy preservation or secure multiparty computation to gradientdescentbased techniques in this paper we propose a preliminary approach to enable privacy preservation in gradient descent methods in general and demonstrate its feasibility in specific gradient descent methods
mining correlated bursty topic patterns from coordinated text streams mining correlated bursty topic patterns from coordinated text streams previous work on text mining has almost exclusively focused on a single stream however we often have available multiple text streams indexed by the same set of time points called coordinated text streams which offer new opportunities for text mining for example when a major event happens all the news articles published by different agencies in different languages tend to cover the same event for a certain period exhibiting a correlated bursty topic pattern in all the news article streams in general mining correlated bursty topic patterns from coordinated text streams can reveal interesting latent associations or events behind these streams in this paper we define and study this novel text mining problem we propose a general probabilistic algorithm which can effectively discover correlated bursty patterns and their bursty periods across text streams even if the streams have completely different vocabularies eg english vs chinese evaluation of the proposed method on a news data set and a literature data set shows that it can effectively discover quite meaningful topic patterns from both data sets the patterns discovered from the news data set accurately reveal the major common events covered in the two streams of news articles in english and chinese respectively while the patterns discovered from two database publication streams match well with the major research paradigm shifts in database research since the proposed method is general and does not require the streams to share vocabulary it can be applied to any coordinated text streams to discover correlated topic patterns that burst in multiple streams in the same period
generalized component analysis for text with heterogeneous attributes generalized component analysis for text with heterogeneous attributes we present a class of richly structured undirected hidden variable models suitable for simultaneously modeling text along with other attributes encoded in different modalities our model generalizes techniques such as principal component analysis to heterogeneous data types in contrast to other approaches this framework allows modalities such as words authors and timestamps to be captured in their natural probabilistic encodings a latent space representation for a previously unseen document can be obtained through a fast matrix multiplication using our method we demonstrate the effectiveness of our framework on the task of author prediction from years of the nips conference proceedings and for a recipient prediction task using a month academic email archive of a researcher our approach should be more broadly applicable to many realworld applications where one wishes to efficiently make predictions for a large number of potential outputs using dimensionality reduction in a well defined probabilistic framework
mining favorable facets mining favorable facets the importance of dominance and skyline analysis has been well recognized in multicriteria decision making applications most previous studies assume a fixed order on the attributes in practice different customers may have different preferences on nominal attributes in this paper we identify an interesting data mining problem finding favorable facets which has not been studied before given a set of points in a multidimensional space for a specific target point p we want to discover with respect to which combinations of orders eg customer preferences on the nominal attributes p is not dominated by any other points such combinations are called the favorable facets of p
local decomposition for rare class analysis local decomposition for rare class analysis given its importance the problem of predicting rare classes in largescale multilabeled data sets has attracted great attentions in the literature however the rareclass problem remains a critical challenge because there is no natural way developed for handling imbalanced class distributions this paper thus fills this crucial void by developing a method for classification using local clustering cog specifically for a data set with an imbalanced class distribution we perform clustering within each large class and produce subclasses with relatively balanced sizes then we apply traditional supervised learning algorithms such as support vector machines svms for classification indeed our experimental results on various realworld data sets show that our method produces significantly higher prediction accuracies on rare classes than stateoftheart methods furthermore we show that cog can also improve the performance of traditional supervised learning algorithms on data sets with balanced class distributions
scan scan network clustering or graph partitioning is an important task for the discovery of underlying structures in networks many algorithms find clusters by maximizing the number of intracluster edges while such algorithms find useful and interesting structures they tend to fail to identify and isolate two kinds of vertices that play special roles vertices that bridge clusters hubs and vertices that are marginally connected to clusters outliers identifying hubs is useful for applications such as viral marketing and epidemiology since hubs are responsible for spreading ideas or disease in contrast outliers have little or no influence and may be isolated as noise in the data in this paper we proposed a novel algorithm called scan structural clustering algorithm for networks which detects clusters hubs and outliers in networks it clusters vertices based on a structural similarity measure the algorithm is fast and efficient visiting each vertex only once an empirical evaluation of the method using both synthetic and real datasets demonstrates superior performance over other methods such as the modularitybased algorithms
modelshared subspace boosting for multilabel classification modelshared subspace boosting for multilabel classification typical approaches to the multilabel classification problem require learning an independent classifier for every label from all the examples and features this can become a computational bottleneck for sizeable datasets with a large label space in this paper we propose an efficient and effective multilabel learning algorithm called modelshared subspace boosting mssboost as an attempt to reduce the information redundancy in the learning process this algorithm automatically finds shares and combines a number of base models across multiple labels where each model is learned from random feature subspace and boots trap data samples the decision functions for each label are jointly estimated and thus a small number of shared subspace models can support the entire label space our experimental results on both synthetic data and real multimedia collections have demonstrated that the proposed algorithm can achieve better classification performance than the nonensemble baselineclassifiers with a significant speedup in the learning and prediction processes it can also use a smaller number of base models to achieve the same classification performance as its nonmodelshared counterpart
detecting time series motifs under uniform scaling detecting time series motifs under uniform scaling time series motifs are approximately repeated patterns foundwithin the data such motifs have utility for many data mining algorithms including rulediscoverynoveltydetection summarization and clustering since the formalization of the problem and the introduction of efficient linear time algorithms motif discovery has been successfully applied tomany domains including medicine motion capture robotics and meteorology
learning the kernel matrix in discriminant analysis via quadratically constrained quadratic programming learning the kernel matrix in discriminant analysis via quadratically constrained quadratic programming the kernel function plays a central role in kernel methods in this paper we consider the automated learning of the kernel matrix over a convex combination of prespecified kernel matrices in regularized kernel discriminant analysis rkda which performs lineardiscriminant analysis in the feature space via the kernel trick previous studies have shown that this kernel learning problem can be formulated as a semidefinite program sdp which is however computationally expensive even with the recent advances in interior point methods based on the equivalence relationship between rkda and least square problems in the binaryclass case we propose a quadratically constrained quadratic programming qcqp formulation for the kernel learning problem which can be solved more efficiently than sdp while most existing work on kernel learning deal with binaryclass problems only we show that our qcqp formulation can be extended naturally to the multiclass case experimental results on both binaryclass and multiclass benchmarkdata sets show the efficacy of the proposed qcqp formulations
from frequent itemsets to semantically meaningful visual patterns from frequent itemsets to semantically meaningful visual patterns data mining techniques that are successful in transaction and text data may not be simply applied to image data that contain highdimensional features and have spatial structures it is not a trivial task to discover meaningful visual patterns in image databases because the content variations and spatial dependency in the visual data greatly challenge most existing methods this paper presents a novel approach to coping with these difficulties for mining meaningful visual patterns specifically the novelty of this work lies in the following new contributions a principled solution to the discovery of meaningful itemsets based on frequent itemset mining a selfsupervised clustering scheme of the highdimensional visual features by feeding back discovered patterns to tune the similarity measure through metric learning and a pattern summarization method that deals with the measurement noises brought by the image data the experimental results in the real images show that our method can discover semantically meaningful patterns efficiently and effectively
information distance from a question to an answer information distance from a question to an answer we provide three key missing pieces of a general theory of information distance we take bold steps in formulating a revised theory to avoid some pitfalls in practical applications the new theory is then used to construct a question answering system extensive experiments are conducted to justify the new theory
mining templates from search result records of search engines mining templates from search result records of search engines metasearch engine comparisonshopping and deep web crawling applications need to extract search result records enwrapped in result pages returned from search engines in response to user queries the search result records from a given search engine are usually formatted based on a template precisely identifying this template can greatly help extract and annotate the data units within each record correctly in this paper we propose a graph model to represent record template and develop a domain independent statistical method to automatically mine the record template for any search engine using sample search result records our approach can identify both template tags html tags and template texts nontag texts and it also explicitly addresses the mismatches between the tag structures and the data structures of search result records our experimental results indicate that this approach is very effective
joint optimization of wrapper generation and template detection joint optimization of wrapper generation and template detection many websites have large collections of pages generated dynamically from an underlying structured source like a database the data of a category are typically encoded into similar pages by a common script or template in recent years some valueadded services such as comparison shopping and vertical search in a specific domain have motivated the research of extraction technologies with high accuracy almost all previous works assume that input pages of a wrapper induction system conform to a common template and they can be easily identified in terms of a common schema of url however we observed that it is hard to distinguish different templates using dynamic urls today moreover since extraction accuracy heavily depends on how consistent input pages are we argue that it is risky to determine whether pages share a common template solely based on urls instead we propose a new approach that utilizes similarity between pages to detect templates our approach separates pages with notable inner differences and then generates wrappers respectively experimental results show that our proposed approach is feasible and effective for improving extraction accuracy
webpage understanding webpage understanding recent work has shown the effectiveness of leveraging layout and tagtree structure for segmenting webpages and labeling html elements however how to effectively segment and label the text contents inside html elements is still an open problem since many text contents on a webpage are often text fragments and not strictly grammatical traditional natural language processing techniques that typically expect grammatical sentences are no longer directly applicable in this paper we examine how to use layout and tagtree structure in a principled way to help understand text contents on webpages we propose to segment and label the page structure and the text content of a webpage in a joint discriminative probabilistic model in this model semantic labels of page structure can be leveraged to help text content understanding and semantic labels ofthe text phrases can be used in page structure understanding tasks such as data record detection thus integration of both page structure and text content understanding leads to an integrated solution of webpage understanding experimental results on research homepage extraction show the feasibility and promise of our approach
an eventbased framework for characterizing the evolutionary behavior of interaction graphs an eventbased framework for characterizing the evolutionary behavior of interaction graphs interaction graphs are ubiquitous in many fields such as bioinformatics sociology and physical sciences there have been many studies in the literature targeted at studying and mining these graphs however almost all of them have studied these graphs from a static point of view the study of the evolution of these graphs over time can provide tremendous insight on the behavior of entities communities and the flow of information among them in this work we present an eventbased characterization of critical behavioral patterns for temporally varying interaction graphs we use nonoverlapping snapshots of interaction graphs and develop a framework for capturing and identifying interesting events from them we use these events to characterize complex behavioral patterns of individuals and communities over time we demonstrate the application of behavioral patterns for the purposes of modeling evolution link prediction and influence maximization finally we present a diffusion model for evolving networks based on our framework
onboard analysis of uncalibrated data for a spacecraft at mars onboard analysis of uncalibrated data for a spacecraft at mars analyzing data onboard a spacecraft as it is collected enables several advanced spacecraft capabilities such as prioritizing observations to make the best use of limited bandwidth and reacting to dynamic events as they happen in this paper we describe how we addressed the unique challenges associated with onboard mining of data as it is collected uncalibrated data noisy observations and severe limitations on computational and memory resources the goal of this effort which falls into the emerging application area of spacecraftbased data mining was to study three specific science phenomena on mars following previous work that used a linear support vector machine svm onboard the earth observing eospacecraft we developed three data mining techniques for use onboard the mars odyssey spacecraft these methods range from simple thresholding to stateoftheart reducedset svm technology we tested these algorithms on archived data in a flight software testbed we also describe a significant serendipitous science discovery of this data mining effort the confirmation of a water ice annulus around the north polar cap of mars we conclude with a discussion on lessons learned in developing algorithms for use onboard a spacecraft
relational data preprocessing techniques for improved securities fraud detection relational data preprocessing techniques for improved securities fraud detection commercial datasets are often large relational and dynamic they contain many records of people places things events and their interactions over time such datasets are rarely structured appropriately for knowledge discovery and they often contain variables whose meanings change across different subsets of the data we describe how these challenges were addressed in a collaborative analysis project undertaken by the university of massachusetts amherst and the national association of securities dealersnasd we describe several methods for data preprocessing that we applied to transform a large dynamic and relational dataset describing nearly the entirety of the us securities industry and we show how these methods made the dataset suitable for learning statistical relational models to better utilize social structure we first applied known consolidation and link formation techniques to associate individuals with branch office locations in addition we developed an innovative technique to infer professional associations by exploiting dynamic employment histories finally we applied normalization techniques to create a suitable class label that adjusts for spatial temporal and other heterogeneity within the data we show how these preprocessing techniques combine to provide the necessary foundation for learning highperforming statistical models of fraudulent activity
cleaning disguised missing data cleaning disguised missing data in some applications such as filling in a customer information form on the web some missing values may not be explicitly represented as such but instead appear as potentially valid data values such missing values are known as disguised missing data which may impair the quality of data analysis severely such as causing significant biases and misleading results in hypothesis tests correlation analysis and regressions the very limited previous studies on cleaning disguised missing data use outlier mining and distribution anomaly detection they highly rely on domain background knowledge in specific applications and may not work well for the cases where the disguise values are inliers
practical guide to controlled experiments on the web practical guide to controlled experiments on the web the web provides an unprecedented opportunity to evaluate ideas quickly using controlled experiments also called randomized experiments single factor or factorial designs ab tests and their generalizations split tests controltreatment tests and parallel flights controlled experiments embody the best scientific design for establishing a causal relationship between changes and their influence on userobservable behavior we provide a practical guide to conducting online experiments where endusers can help guide the development of features our experience indicates that significant learning and returnoninvestment roi are seen when development teams listen to their customers not to the highest paid persons opinion hippo we provide several examples of controlled experiments with surprising results we review the important ingredients of running controlled experiments and discuss their limitations both technical and organizational we focus on several areas that are critical to experimentation including statistical power sample size and techniques for variance reduction we describe common architectures for experimentation systems and analyze their advantages and disadvantages we evaluate randomization and hashing techniques which we show are not as simple in practice as is often assumed controlled experiments typically generate large amounts of data which can be analyzed using data mining techniques to gain deeper understanding of the factors influencing the outcome of interest leading to new hypotheses and creating a virtuous cycle of improvements organizations that embrace controlled experiments with clear evaluation criteria can evolve their systems with automated optimizations and realtime analyses based on our extensive practical experience with multiple systems and organizations we share key lessons that will help practitioners in running trustworthy controlled experiments
distributed classification in peertopeer networks distributed classification in peertopeer networks this work studies the problem of distributed classification in peertopeerpp networks while there has been a significant amount of work in distributed classification most of existing algorithms are not designed for pp networks indeed as serverless and routerless systems pp networks impose several challenges for distributed classification it is not practical to have global synchronization in largescale pp networks there are frequent topology changes caused by frequent failure and recovery of peers and there are frequent onthefly data updates on each peer
highquantile modeling for customer wallet estimation and other applications highquantile modeling for customer wallet estimation and other applications in this paper we discuss the important practical problem of customer wallet estimation ie estimation of potential spending by customersrather than their expected spending for this purpose we utilize quantile modeling whose goal is to estimate a quantile of the discriminative conditional distribution of the response rather than the mean which is the implicit goal of most standard regression approaches we argue that a notion of wallet can be captured through high quantile modeling eg estimating the th percentile and describe a wallet estimation implementation within ibms market alignment program map we also discuss the wide range of domains where highquantile modeling can be practically important estimating opportunities in sales and marketing domains defining surprising patterns for outlier and fraud detection and more we survey some existing approaches for quantile modeling and propose adaptations of nearestneighbor and regressiontree approaches to quantile modeling we demonstrate the various models performance in high quantile estimation in several domains including our motivating problem of estimating the realistic it wallets of ibm customers
mining complex power networks for blackout prevention mining complex power networks for blackout prevention following the recent devastating blackouts in north america uk and italy blackout prevention has attracted significant attention though it is known as a notoriously difficult task to prevent the blackout it is essential to accurately predict the instable status of power network components in the largescale power network however existing analysis tools fail to perform accurate and intime prediction of component instability because of the sophisticated structure of realworld power networks and the huge amount of system variables to be analyzed to prevent the blackout we need an accurate and efficient method that a can discover interesting features and patterns relevant to the blackout from the highly complex structure and ten thousands of system variables of a power network and b can give accurate and fast prediction of system instability whenever required so that the network operator can take necessary actions in time in this paper we report our tool developed for power network instability prediction the proposed method consists of two major stages in the first stagea novel type of patterns namely local correlation network pattern lcnp is mined from the structure and system variables of the power network correlation rules which are useful for the network operator to locate potentially instable components can be further generated from the lcnp in the second stage a kernel based network classification method is developed to predict the system instability by testing on a real world power network the new england system we demonstrate that the proposed tool is effective in predicting system instability and thus highly useful for blackout prevention
corroborate and learn facts from the web corroborate and learn facts from the web the web contains lots of interesting factual information about entities such as celebrities movies or products this paper describes a robust bootstrapping approach to corroborate facts and learn more facts simultaneously this approach starts with retrieving relevant pages from a crawl repository for each entity in the seed set in each learning cycle known facts of an entity are corroborated first in a relevant page to find fact mentions when fact mentions are found they are taken as examples for learning new facts from the page via html pattern discovery extracted new facts are added to the known fact set for the next learning cycle the bootstrapping process continues until no new facts can be learned this approach is languageindependent it demonstrated good performance in experiment on country facts results of a large scale experiment will also be shown with initial facts imported from wikipedia
extracting relevant named entities for automated expense reimbursement extracting relevant named entities for automated expense reimbursement expense reimbursement is a timeconsuming and laborintensive process across organizations in this paper we present a prototype expense reimbursement system that dramatically reduces the elapsed time and costs involved by eliminating paper from the process life cycle our complete solution involves an electronic submission infrastructure that provides multi channel image capture secure transport and centralized storage of paper documents an unconstrained data mining approach to extracting relevant named entities from unstructured document images automation of auditing procedures that enables automatic expense validation with minimum human interaction
a framework for classification and segmentation of massive audio data streams a framework for classification and segmentation of massive audio data streams in recent years the proliferation of voip data has created a number of applications in which it is desirable to perform quick online classification and recognition of massive voice streams typically such applications are encountered in real time intelligence and surveillance in many cases the data streams can be in compressed format and the rate of data processing can often run at the rate of gigabits per second all known techniques for speaker voice analysis require the use of an offline training phase in which the system is trained with known segments of speech the stateoftheart method for textindependent speaker recognition is known as gaussian mixture modeling gmm and it requires an iterative expectation maximization procedure for training which cannot be implemented in real time in this paper we discuss the details of such an online voice recognition system for this purpose we use our microclustering algorithms to design concise signatures of the target speakers one of the surprising and insightful observations from our experiences with such a system is that while it was originally designed only for efficiency we later discovered that it was also more accurate than the widely used gaussian mixture model gmm this was because of the conciseness of the microcluster model which made it less prone to over training this is evidence of the fact that it is often possible to get the best of both worlds and do better than complex models both from an efficiency and accuracy perspective
detecting changes in large data sets of payment card data detecting changes in large data sets of payment card data an important problem in data mining is detecting changes in large datasets although there are a variety of change detection algorithms that have been developed in practice it can be a problem to scale these algorithms to large data sets due to the heterogeneity of the data in this paper we describe a case study involving payment card data in which we built and monitored a separate change detection model for each cell in a multidimensional data cube we describe a system that has been in operation for the past two years that builds and monitors over separate baseline models and the process that isused for generating and investigating alerts using these baselines
domainconstrained semisupervised mining of tracking models in sensor networks domainconstrained semisupervised mining of tracking models in sensor networks accurate localization of mobile objects is a major research problem in sensor networks and an important data mining application specifically the localization problem is to determine the location of a client device accurately given the radio signal strength values received at the client device from multiple beacon sensors or access points conventional data mining and machine learning methods can be applied to solve this problem however all of them require large amounts of labeled training data which can be quite expensive in this paper we propose a probabilistic semi supervised learning approach to reduce the calibration effort and increase the tracking accuracy our method is based on semisupervised conditional random fields which can enhance the learned model from a small set of training data with abundant unlabeled data effectively to make our method more efficient we exploit a generalized em algorithm coupled with domain constraints we validate our method through extensive experiments in a real sensor network using crossbow mica sensors the results demonstrate the advantages of methods compared to other stateoftheart objecttracking algorithms
event summarization for system management event summarization for system management in system management applications an overwhelming amount of data are generated and collected in the form of temporal events while mining temporal event data to discover interesting and frequent patterns has obtained rapidly increasing research efforts users of the applications are overwhelmed by the mining results the extracted patterns are generally of large volume and hard to interpret they may be of no emphasis intricate and meaningless to nonexperts even to domain experts while traditional research efforts focus on finding interesting patterns in this paper we take a novel approach called event summarization towards the understanding of the seemingly chaotic temporal data event summarization aims at providing a concise interpretation of the seemingly chaotic data so that domain experts may take actions upon the summarized models event summarization decomposes the temporal information into many independent subsets and finds well fitted models to describe each subset
lungcad lungcad we present lungcad a computer aided diagnosis cad system that employs a classification algorithm for detecting solid pulmonary nodules from ct thorax studies we briefly describe some of the machine learning techniques developed to overcome the real world challenges in this medical domain the most significant hurdle in transitioning from a machine learning research prototype that performs well on an inhouse dataset into a clinically deployable system is the requirement that the cad system be tested in a clinical trial we describe the clinical trial in which lungcad was tested a large scale multireader multicase mrmc retrospective observational study to evaluate the effect of cad in clinical practice for detecting solid pulmonary nodules from ct thorax studies the clinical trial demonstrates that every radiologist that participated in the trial had a significantly greater accuracy with lungcad both for detecting nodules and identifying potentially actionable nodules this along with other findings from the trial has resulted in fda approval for lungcad in late 
machine learning for stock selection machine learning for stock selection in this paper we propose a new method called prototype ranking pr designed for the stock selection problem pr takes into account the huge size of realworld stock data and applies a modified competitive learning technique to predict the ranks of stocks the primary target of pr is to select the top performing stocks among many ordinary stocks pr is designed to perform the learning and testing in a noisy stocks sample set where the top performing stocks are usually the minority the performance of pr is evaluated by a trading simulation of the real stock data each week the stocks with the highest predicted ranks are chosen to construct a portfolio in the period of prs portfolio earns a much higher average return as well as a higher riskadjusted return than coopers method which shows that the pr method leads to a clear profit improvement
imds imds the proliferation of malware has presented a serious threat to the security of computer systems traditional signaturebased antivirus systems fail to detect polymorphic and new previously unseen malicious executables in this paper resting on the analysis of windows api execution sequences called by pe files we develop the intelligent malware detection system imds using objectiveoriented association ooa mining based classification imds is an integrated system consisting of three major modules pe parser ooa rule generator and rule based classifier an ooafastfpgrowth algorithm is adapted to efficiently generate ooa rules for classification a comprehensive experimental study on a large collection of pe files obtained from the antivirus laboratory of kingsoft corporation is performed to compare various malware detection approaches promising experimental results demonstrate that the accuracy and efficiency of our imds system out perform popular antivirus software such as norton antivirus and mcafee virusscan as well as previous data mining based detection systems which employed naive bayes support vector machine svm and decision tree techniques
truth discovery with multiple conflicting information providers on the web truth discovery with multiple conflicting information providers on the web the worldwide web has become the most important information source for most of us unfortunately there is no guarantee for the correctness of information on the web moreover different web sites often provide conflicting information on a subject such as different specifications for the same product in this paper we propose a new problem called veracity ie conformity to truth which studies how to find true facts from a large amount of conflicting information on many subjects that is provided by various web sites we design a general framework for the veracity problem and invent an algorithm called truthfinder which utilizes the relationships between web sites and their information ie a web site is trustworthy if it provides many pieces of true information and a piece of information is likely to be true if it is provided by many trustworthy web sites our experiments show that truthfinder successfully finds true facts among conflicting information and identifies trustworthy web sites better than the popular search engines
data mining at the crossroads data mining at the crossroads since the workshop on knowledge discovery in databases the field has seen sustained growth and interest and has attained significant maturity the main objectives of this panel will be to reflect on the successes and failures in the field of data mining over the last eighteen years and to examine what insights we can take with us as we move forward
internet advertising and optimal auction design internet advertising and optimal auction design this talk describes the optimal revenue maximizing auction for sponsored search advertising we show that a search engines optimal reserve price is independent of the number of bidders using simulations we consider the changes that result from a search engines choice of reserve price and from changes in the number of participating advertisers
large scale data analysis and modelling in online services and advertising large scale data analysis and modelling in online services and advertising note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
regularization paths and coordinate descent regularization paths and coordinate descent in a statistical world faced with an explosion of data regularization has become an important ingredient in a wide variety of problems we have many more input features than observations and the lasso penalty and its hybrids have become increasingly useful for both feature selection and regularization this talk presents some effective algorithms based on coordinate descent for fitting large scale regularization paths for a variety of problems
the future of image search the future of image search there are billions of images on the internet today searching for a desired image is largely based on textual data such as filename or associated text on the web page not much use is made of the image content there are good reasons for this the field of contentbased image retrieval which emerged during the s focused primarily on color and texture cues these were easier to model than shape but they turned out to be much less useful than originally hoped i shall review some of the recent developments in the field of visual object recognition in the computer vision community that offer greater promise much better image features for characterizing shape advances in machine learning techniques and the availability of large amounts of training data lie at the heart of these approaches
genesis of postal address reading current state and future prospects genesis of postal address reading current state and future prospects abstract
influence and correlation in social networks influence and correlation in social networks in many online social systems social ties between users play an important role in dictating their behavior one of the ways this can happen is through social influence the phenomenon that the actions of a user can induce hisher friends to behave in a similar way in systems where social influence exists ideas modes of behavior or new technologies can diffuse through the network like an epidemic therefore identifying and understanding social influence is of tremendous interest from both analysis and design points of view
efficient semistreaming algorithms for local triangle counting in massive graphs efficient semistreaming algorithms for local triangle counting in massive graphs in this paper we study the problem of local triangle counting in large graphs namely given a large graph g ve we want to estimate as accurately as possible the number of triangles incident to every node v in the graph the problem of computing the global number of triangles in a graph has been considered before but to our knowledge this is the first paper that addresses the problem of local triangle counting with a focus on the efficiency issues arising in massive graphs the distribution of the local number of triangles and the related local clustering coefficient can be used in many interesting applications for example we show that the measures we compute can help to detect the presence of spamming activity in largescale web graphs as well as to provide useful features to assess content quality in social networks
structured entity identification and document categorization structured entity identification and document categorization traditionally research in identifying structured entities in documents has proceeded independently of document categorization research in this paper we observe that these two tasks have much to gain from each other apart from direct references to entities in a database such as names of person entities documents often also contain words that are correlated with discriminative entity attributes such agegroup and incomelevel of persons this happens naturally in many enterprise domains such as crm banking etc then entity identification which is typically vulnerable against noise and incompleteness in direct references to entities in documents can benefit from document categorization with respect to such attributes in return entity identification enables documents to be categorized according to different labelsets arising from entity attributes without requiring any supervision in this paper we propose a probabilistic generative model for joint entity identification and document categorization we show how the parameters of the model can be estimated using an em algorithm in an unsupervised fashion using extensive experiments over real and semisynthetic data we demonstrate that the two tasks can benefit immensely from each other when performed jointly using the proposed model
mining adaptively frequent closed unlabeled rooted trees in data streams mining adaptively frequent closed unlabeled rooted trees in data streams closed patterns are powerful representatives of frequent patterns since they eliminate redundant information we propose a new approach for mining closed unlabeled rooted trees adaptively from data streams that change over time our approach is based on an efficient representation of trees and a low complexity notion of relaxed closed trees and leads to an online strategy and an adaptive sliding window technique for dealing with changes over time more precisely we first present a general methodology to identify closed patterns in a data stream using galois lattice theory using this methodology we then develop three closed tree mining algorithms an incremental one inctreenat a slidingwindow based one wintreenat and finally one that mines closed trees adaptively from data streams adatreenat to the best of our knowledge this is the first work on mining frequent closed trees in streaming data varying with time we give a first experimental evaluation of the proposed algorithms
effective label acquisition for collective classification effective label acquisition for collective classification information diffusion viral marketing and collective classification all attempt to model and exploit the relationships in a network to make inferences about the labels of nodes a variety of techniques have been introduced and methods that combine attribute information and neighboring label information have been shown to be effective for collective labeling of the nodes in a network however in part because of the correlation between node labels that the techniques exploit it is easy to find cases in which once a misclassification is made incorrect information propagates throughout the network this problem can be mitigated if the system is allowed to judiciously acquire the labels for a small number of nodes unfortunately under relatively general assumptions determining the optimal set of labels to acquire is intractable here we propose an acquisition method that learns the cases when a given collective classification algorithm makes mistakes and suggests acquisitions to correct those mistakes we empirically show on both real and synthetic datasets that this method significantly outperforms a greedy approximate inference approach a viral marketing approach and approaches based on network structural measures such as node degree and network clustering in addition to significantly improving accuracy with just a small amount of labeled data our method is tractable on large networks
topical query decomposition topical query decomposition we introduce the problem of query decomposition where we are given a query and a document retrieval system and we want to produce a small set of queries whose union of resulting documents corresponds approximately to that of the original query ideally these queries should represent coherent conceptually wellseparated topics
unsupervised feature selection for principal components analysis unsupervised feature selection for principal components analysis principal components analysis pca is the predominant linear dimensionality reduction technique and has been widely applied on datasets in all scientific domains we consider both theoretically and empirically the topic of unsupervised feature selection for pca by leveraging algorithms for the socalled column subset selection problem cssp in words the cssp seeks the best subset of exactly k columns from an m x n data matrix a and has been extensively studied in the numerical linear algebra community we present a novel twostage algorithm for the cssp from a theoretical perspective for small to moderate values of k this algorithm significantly improves upon the best previouslyexisting results for the cssp from an empirical perspective we evaluate this algorithm as an unsupervised feature selection strategy in three application domains of modern statistical data analysis finance documentterm data and genetics we pay particular attention to how this algorithm may be used to select representative or landmark features from an objectfeature matrix in an unsupervised manner in all three application domains we are able to identify k landmark features ie columns of the data matrix that capture nearly the same amount of information as does the subspace that is spanned by the top k eigenfeatures
the cost of privacy the cost of privacy reidentification is a major privacy threat to public datasets containing individual records many privacy protection algorithms rely on generalization and suppression of quasiidentifier attributes such as zip code and birthdate their objective is usually syntactic sanitization for example kanonymity requires that each quasiidentifier tuple appear in at least k records while ldiversity requires that the distribution of sensitive attributes for each quasiidentifier have high entropy the utility of sanitized data is also measured syntactically by the number of generalization steps applied or the number of records with the same quasiidentifier in this paper we ask whether generalization and suppression of quasiidentifiers offer any benefits over trivial sanitization which simply separates quasiidentifiers from sensitive attributes previous work showed that kanonymous databases can be useful for data mining but kanonymization does not guarantee any privacy by contrast we measure the tradeoff between privacy how much can the adversary learn from the sanitized records and utility measured as accuracy of datamining algorithms executed on the same sanitized records
generating succinct titles for web urls generating succinct titles for web urls how can a search engine automatically provide the best and most appropriate title for a result url linktitle so that users will be persuaded to click on the url we consider the problem of automatically generating linktitles for urls and propose a general statistical framework for solving this problem the framework is based on using information from a diverse collection of sources each of which can be thought of as contributing one or more candidate linktitles for the url it can also incorporate the context in which the linktitle will be used along with constraints on its length our framework is applicable to several scenarios obtaining succinct titles for displaying quicklinks obtaining titles for urls that lack a good title constructing succinct sitemaps etc extensive experiments show that our method is very effective producing results that are at least better than nontrivial baselines
structured learning for nonsmooth ranking losses structured learning for nonsmooth ranking losses learning to rank from relevance judgment is an active research area itemwise score regression pairwise preference satisfaction and listwise structured learning are the major techniques in use listwise structured learning has been applied recently to optimize important nondecomposable ranking criteria like auc area under roc curve and map mean average precision we propose new almostlineartime algorithms to optimize for two other criteria widely used to evaluate search systems mrr mean reciprocal rank and ndcg normalized discounted cumulative gain in the maxmargin structured learning framework we also demonstrate that for different ranking criteria one may need to use different feature maps search applications should not be optimized in favor of a single criterion because they need to cater to a variety of queries eg mrr is best for navigational queries while ndcg is best for informational queries a key contribution of this paper is to fold multiple ranking loss functions into a multicriteria maxmargin optimization the result is a single robust ranking model that is close to the best accuracy of learners trained on individual criteria in fact experiments over the popular letor and trec data sets show that contrary to conventional wisdom a test criterion is often not best served by training with the same individual criterion
partitioned logistic regression for spam filtering partitioned logistic regression for spam filtering naive bayes and logistic regression perform well in different regimes while the former is a very simple generative model which is efficient to train and performs well empirically in many applicationsthe latter is a discriminative model which often achieves better accuracy and can be shown to outperform naive bayes asymptotically in this paper we propose a novel hybrid model partitioned logistic regression which has several advantages over both naive bayes and logistic regression this model separates the original feature space into several disjoint feature groups individual models on these groups of features are learned using logistic regression and their predictions are combined using the naive bayes principle to produce a robust final estimation we show that our model is better both theoretically and empirically in addition when applying it in a practical application email spam filtering it improves the normalized auc score at falsepositive rate by and compared to naive bayes and logistic regression when using the exact same training examples
learning subspace kernels for classification learning subspace kernels for classification kernel methods have been applied successfully in many data mining tasks subspace kernel learning was recently proposed to discover an effective lowdimensional subspace of a kernel feature space for improved classification in this paper we propose to construct a subspace kernel using the hilbertschmidt independence criterion hsic we show that the optimal subspace kernel can be obtained efficiently by solving an eigenvalue problem one limitation of the existing subspace kernel learning formulations is that the kernel learning and classification are independent and the subspace kernel may not be optimally adapted for classification to overcome this limitation we propose a joint optimization framework in which we learn the subspace kernel and subsequent classifiers simultaneously in addition we propose a novel learning formulation that extracts an uncorrelated subspace kernel to reduce the redundant information in a subspace kernel following the idea from multiple kernel learning we extend the proposed formulations to the case when multiple kernels are available and need to be combined we show that the integration of subspace kernels can be formulated as a semidefinite program sdp which is computationally expensive to improve the efficiency of the sdp formulation we propose an equivalent semiinfinite linear program silp formulation which can be solved efficiently by the column generation technique experimental results on a collection of benchmark data sets demonstrate the effectiveness of the proposed algorithms
combinational collaborative filtering for personalized community recommendation combinational collaborative filtering for personalized community recommendation rapid growth in the amount of data available on social networking sites has made information retrieval increasingly challenging for users in this paper we propose a collaborative filtering method combinational collaborative filtering ccf to perform personalized community recommendations by considering multiple types of cooccurrences in social data at the same time this filtering method fuses semantic and user information then applies a hybrid training strategy that combines gibbs sampling and expectationmaximization algorithm to handle the largescale dataset parallel computing is used to speed up the model training through an empirical study on the orkut dataset we show ccf to be both effective and scalable
fast fast the class imbalance problem is encountered in a large number of practical applications of machine learning and data mining for example information retrieval and filtering and the detection of credit card fraud it has been widely realized that this imbalance raises issues that are either nonexistent or less severe compared to balanced class cases and often results in a classifiers suboptimal performance this is even more true when the imbalanced data are also high dimensional in such cases feature selection methods are critical to achieve optimal performance in this paper we propose a new feature selection method feature assessment by sliding thresholds fast which is based on the area under a roc curve generated by moving the decision boundary of a single feature classifier with thresholds placed using an evenbin distribution fast is compared to two commonlyused feature selection methods correlation coefficient and relevance in estimating features relief for imbalanced data classification the experimental results obtained on text mining mass spectrometry and microarray data sets showed that the proposed method outperformed both relief and correlation methods on skewed data sets and was comparable on balanced data sets when small number of features is preferred the classification performance of the proposed method was significantly improved compared to correlation and reliefbased methods
semisupervised learning with data calibration for longterm time series forecasting semisupervised learning with data calibration for longterm time series forecasting many time series prediction methods have focused on single step or short term prediction problems due to the inherent difficulty in controlling the propagation of errors from one prediction step to the next step yet there is a broad range of applications such as climate impact assessments and urban growth planning that require long term forecasting capabilities for strategic decision making training an accurate model that produces reliable long term predictions would require an extensive amount of historical data which are either unavailable or expensive to acquire for some of these domains there are alternative ways to generate potential scenarios for the future using computerdriven simulation models such as global climate and traffic demand models however the data generated by these models are currently utilized in a supervised learning setting where a predictive model trained on past observations is used to estimate the future values in this paper we present a semisupervised learning framework for longterm time series forecasting based on hidden markov model regression a covariance alignment method is also developed to deal with the issue of inconsistencies between historical and model simulation data we evaluated our approach on data sets from a variety of domains including climate modeling our experimental results demonstrate the efficacy of the approach compared to other supervised learning methods for longterm time series forecasting
reconstructing chemical reaction networks reconstructing chemical reaction networks we present an approach to reconstructing chemical reaction networks from time series measurements of the concentrations of the molecules involved our solution strategy combines techniques from numerical sensitivity analysis and probabilistic graphical models by modeling a chemical reaction system as a markov network undirected graphical model we show how systematically probing for sensitivities between molecular species can identify the topology of the network given the topology our approach next uses detailed sensitivity profiles to characterize properties of reactions such as reversibility enzymecatalysis and the precise stoichiometries of the reactants and products we demonstrate applications to reconstructing key biological systems including the yeast cell cycle in addition to network reconstruction our algorithm finds applications in model reduction and model comprehension we argue that our reconstruction algorithm can serve as an important primitive for data mining in systems biology applications
automatic record linkage using seeded nearest neighbour and support vector machine classification automatic record linkage using seeded nearest neighbour and support vector machine classification the task of linking databases is an important step in an increasing number of data mining projects because linked data can contain information that is not available otherwise or that would require timeconsuming and expensive collection of specific data the aim of linking is to match and aggregate all records that refer to the same entity one of the major challenges when linking large databases is the efficient and accurate classification of record pairs into matches and nonmatches while traditionally classification was based on manuallyset thresholds or on statistical procedures many of the more recently developed classification methods are based on supervised learning techniques they therefore require training data which is often not available in real world situations or has to be prepared manually an expensive cumbersome and timeconsuming process
feedback effects between similarity and social influence in online communities feedback effects between similarity and social influence in online communities a fundamental open question in the analysis of social networks is to understand the interplay between similarity and social ties people are similar to their neighbors in a social network for two distinct reasons first they grow to resemble their current friends due to social influence and second they tend to form new links to others who are already like them a process often termed selection by sociologists while both factors are present in everyday social processes they are in tension social influence can push systems toward uniformity of behavior while selection can lead to fragmentation as such it is important to understand the relative effects of these forces and this has been a challenge due to the difficulty of isolating and quantifying them in real settings
anomaly pattern detection in categorical datasets anomaly pattern detection in categorical datasets we propose a new method for detecting patterns of anomalies in categorical datasets we assume that anomalies are generated by some underlying process which affects only a particular subset of the data our method consists of two steps we first use a local anomaly detector to identify individual records with anomalous attribute values and then detect patterns where the number of anomalous records is higher than expected given the set of anomalies flagged by the local anomaly detector we search over all subsets of the data defined by any set of fixed values of a subset of the attributes in order to detect selfsimilar patterns of anomalies we wish to detect any such subset of the test data which displays a significant increase in anomalous activity as compared to the normal behavior of the system as indicated by the training data we perform significance testing to determine if the number of anomalies in any subset of the test data is significantly higher than expected and propose an efficient algorithm to perform this test over all such subsets of the data we show that this algorithm is able to accurately detect anomalous patterns in realworld hospital container shipping and network intrusion data
bypass rates bypass rates we introduce a new approach to analyzing click logs by examining both the documents that are clicked and those that are bypasseddocuments returned higher in the ordering of the search results but skipped by the user this approach complements the popular clickthrough rate analysis and helps to draw negative inferences in the click logs we formulate a natural objective that finds sets of results that are unlikely to be collectively bypassed by a typical user this is closely related to the problem of reducing query abandonment we analyze a greedy approach to optimizing this objective and establish theoretical guarantees of its performance we evaluate our approach on a large set of queries and demonstrate that it compares favorably to the maximal marginal relevance approach on a number of metrics including mean average precision and mean reciprocal rank
deduping urls via rewrite rules deduping urls via rewrite rules a large fraction of the urls on the web contain duplicate or nearduplicate content deduping urls is an extremely important problem for search engines since all the principal functions of a search engine including crawling indexing ranking and presentation are adversely impacted by the presence of duplicate urls traditionally the deduping problem has been addressed by fetching and examining the content of the url our approach here is different given a set of urls partitioned into equivalence classes based on the content urls in the same equivalence class have similar content we address the problem of mining this set and learning url rewrite rules that transform all urls of an equivalence class to the same canonical form these rewrite rules can then be applied to eliminate duplicates among urls that are encountered for the first time during crawling even without fetching their content
structured metric learning for high dimensional problems structured metric learning for high dimensional problems the success of popular algorithms such as kmeans clustering or nearest neighbor searches depend on the assumption that the underlying distance functions reflect domainspecific notions of similarity for the problem at hand the distance metric learning problem seeks to optimize a distance function subject to constraints that arise from fullysupervised or semisupervised information several recent algorithms have been proposed to learn such distance functions in low dimensional settings one major shortcoming of these methods is their failure to scale to high dimensional problems that are becoming increasingly ubiquitous in modern data mining applications in this paper we present metric learning algorithms that scale linearly with dimensionality permitting efficient optimization storage and evaluation of the learned metric this is achieved through our main technical contribution which provides a framework based on the logdeterminant matrix divergence which enables efficient optimization of structured lowparameter mahalanobis distances experimentally we evaluate our methods across a variety of high dimensional domains including text statistical software analysis and collaborative filtering showing that our methods scale to data sets with tens of thousands or more features we show that our learned metric can achieve excellent quality with respect to various criteria for example in the context of metric learning for nearest neighbor classification we show that our methods achieve higher accuracy over the baseline distance additionally our methods yield very good precision while providing recall measures up to higher than other baseline methods such as latent semantic analysis
constraint programming for itemset mining constraint programming for itemset mining the relationship between constraintbased mining and constraint programming is explored by showing how the typical constraints used in pattern mining can be formulated for use in constraint programming environments the resulting framework is surprisingly flexible and allows us to combine a wide range of mining constraints in different ways we implement this approach in offtheshelf constraint programming systems and evaluate it empirically the results show that the approach is not only very expressive but also works well on complex benchmark problems
learning classifiers from only positive and unlabeled data learning classifiers from only positive and unlabeled data the input to an algorithm that learns a binary classifier normally consists of two sets of examples where one set consists of positive examples of the concept to be learned and the other set consists of negative examples however it is often the case that the available training data are an incomplete set of positive examples and a set of unlabeled examples some of which are positive and some of which are negative the problem solved in this paper is how to learn a standard binary classifier given a nontraditional training set of this nature
locality sensitive hash functions based on concomitant rank order statistics locality sensitive hash functions based on concomitant rank order statistics locality sensitive hash functions are invaluable tools for approximate near neighbor problems in high dimensional spaces in this work we are focused on lsh schemes where the similarity metric is the cosine measure the contribution of this work is a new class of locality sensitive hash functions for the cosine similarity measure based on the theory of concomitants which arises in order statistics consider n iid sample pairs x y x y xn yn obtained from a bivariate distribution fx y concomitant theory captures the relation between the order statistics of x and y in the form of a rank distribution given by probrankyijrankxik we exploit properties of the rank distribution towards developing a locality sensitive hash family that has excellent collision rate properties for the cosine measure
direct mining of discriminative and essential frequent patterns via modelbased search tree direct mining of discriminative and essential frequent patterns via modelbased search tree frequent patterns provide solutions to datasets that do not have wellstructured feature vectors however frequent pattern mining is nontrivial since the number of unique patterns is exponential but many are nondiscriminative and correlated currently frequent pattern mining is performed in two sequential steps enumerating a set of frequent patterns followed by feature selection although many methods have been proposed in the past few years on how to perform each separate step efficiently there is still limited success in eventually finding highly compact and discriminative patterns the culprit is due to the inherent nature of this widely adopted twostep approach this paper discusses these problems and proposes a new and different method it builds a decision tree that partitions the data onto different nodes then at each node it directly discovers a discriminative pattern to further divide its examples into purer subsets since the number of examples towards leaf level is relatively small the new approach is able to examine patterns with extremely low global support that could not be enumerated on the whole dataset by the twostep method the discovered feature vectors are more accurate on some of the most difficult graph as well as frequent itemset problems than most recently proposed algorithms but the total size is typically or more smaller importantly the minimum support of some discriminative patterns can be extremely low eg in order to enumerate these low support patterns stateoftheart frequent pattern algorithm either cannot finish due to huge memory consumption or have to enumerate to times more patterns before they can even be found software and datasets are available by contacting the author
scaling up text classification for large file systems scaling up text classification for large file systems we combine the speed and scalability of information retrieval with the generally superior classification accuracy offered by machine learning yielding a twophase text classifier that can scale to very large document corpora we investigate the effect of different methods of formulating the query from the training set as well as varying the query size in empirical tests on the reuters rcv corpus of documents we find runtime was easily reduced by a factor of x with a somewhat surprising gain in fmeasure compared with traditional text classification
spiral spiral hidden markov models hmms have received considerable attention in various communities eg speech recognition neurology and bioinformatic since many applications that use hmm have emerged the goal of this work is to identify efficiently and correctly the model in a given dataset that yields the state sequence with the highest likelihood with respect to the query sequence we propose spiral a fast search method for hmm datasets to reduce the search cost spiral efficiently prunes a significant number of search candidates by applying successive approximations when estimating likelihood we perform several experiments to verify the effectiveness of spiral the results show that spiral is more than times faster than the naive method
using ghost edges for classification in sparsely labeled networks using ghost edges for classification in sparsely labeled networks we address the problem of classification in partially labeled networks aka withinnetwork classification where observed class labels are sparse techniques for statistical relational learning have been shown to perform well on network classification tasks by exploiting dependencies between class labels of neighboring nodes however relational classifiers can fail when unlabeled nodes have too few labeled neighbors to support learning during training phase andor inference during testing phase this situation arises in realworld problems when observed labels are sparse
composition attacks and auxiliary information in data privacy composition attacks and auxiliary information in data privacy privacy is an increasingly important aspect of data publishing reasoning about privacy however is fraught with pitfalls one of the most significant is the auxiliary information also called external knowledge background knowledge or side information that an adversary gleans from other channels such as the web public records or domain knowledge this paper explores how one can reason about privacy in the face of rich realistic sources of auxiliary information specifically we investigate the effectiveness of current anonymization schemes in preserving privacy when multiple organizations independently release anonymized data about overlapping populations
entity categorization over large document collections entity categorization over large document collections extracting entities such as people movies from documents and identifying the categories such as painter writer they belong to enable structured querying and data analysis over unstructured document collections in this paper we focus on the problem of categorizing extracted entities most prior approaches developed for this task only analyzed the local document context within which entities occur in this paper we significantly improve the accuracy of entity categorization by i considering an entitys context across multiple documents containing it and ii exploiting existing large lists of related entities eg lists of actors directors books these approaches introduce computational challenges because a the context of entities has to be aggregated across several documents and b the lists of related entities may be very large we develop techniques to address these challenges we present a thorough experimental study on real data sets that demonstrates the increase in accuracy and the scalability of our approaches
knowledge transfer via multiple model local structure mapping knowledge transfer via multiple model local structure mapping the effectiveness of knowledge transfer using classification algorithms depends on the difference between the distribution that generates the training examples and the one from which test examples are to be drawn the task can be especially difficult when the training examples are from one or several domains different from the test domain in this paper we propose a locally weighted ensemble framework to combine multiple models for transfer learning where the weights are dynamically assigned according to a models predictive power on each test example it can integrate the advantages of various learning algorithms and the labeled information from multiple training domains into one unified classification model which can then be applied on a different domain importantly different from many previously proposed methods none of the base learning method is required to be specifically designed for transfer learning we show the optimality of a locally weighted ensemble framework as a general approach to combine multiple models for domain transfer we then propose an implementation of the local weight assignments by mapping the structures of a model onto the structures of the test domain and then weighting each model locally according to its consistency with the neighborhood structure around the test example experimental results on text classification spam filtering and intrusion detection data sets demonstrate significant improvements in classification accuracy gained by the framework on a transfer learning task of newsgroup message categorization the proposed locally weighted ensemble framework achieves accuracy when the best single model predicts correctly only on of the test examples in summary the improvement in accuracy is over and up to across different problems
banded structure in binary matrices banded structure in binary matrices a matrix has a banded structure if both rows and columns can be permuted so that the nonzero entries exhibit a staircase pattern of overlapping rows the concept of banded matrices has its origins in numerical analysis where entries can be viewed as descriptions between the problem variables the bandedness corresponds to variables that are coupled over short distances banded data occurs also in other applications for example in the physical mapping problem of the human genome in paleontological data in network data and in the discovery of overlapping communities without cycles
quantitative evaluation of approximate frequent pattern mining algorithms quantitative evaluation of approximate frequent pattern mining algorithms traditional association mining algorithms use a strict definition of support that requires every item in a frequent itemset to occur in each supporting transaction in reallife datasets this limits the recovery of frequent itemset patterns as they are fragmented due to random noise and other errors in the data hence a number of methods have been proposed recently to discover approximate frequent itemsets in the presence of noise these algorithms use a relaxed definition of support and additional parameters such as row and column error thresholds to allow some degree of error in the discovered patterns though these algorithms have been shown to be successful in finding the approximate frequent itemsets a systematic and quantitative approach to evaluate them has been lacking in this paper we propose a comprehensive evaluation framework to compare different approximate frequent pattern mining algorithms the key idea is to select the optimal parameters for each algorithm on a given dataset and use the itemsets generated with these optimal parameters in order to compare different algorithms we also propose simple variations of some of the existing algorithms by introducing an additional postprocessing step subsequently we have applied our proposed evaluation framework to a wide variety of synthetic datasets with varying amounts of noise and a real dataset to compare existing and our proposed variations of the approximate pattern mining algorithms source code and the datasets used in this study are made publicly available
unsupervised deduplication using crossfield dependencies unsupervised deduplication using crossfield dependencies recent work in deduplication has shown that collective deduplication of different attribute types can improve performance but although these techniques cluster the attributes collectively they do not model them collectively for example in citations in the research literature canonical venue strings and title strings are dependent because venues tend to focus on a few research areas but this dependence is not modeled by current unsupervised techniques we call this dependence between fields in a record a crossfield dependence in this paper we present an unsupervised generative model for the deduplication problem that explicitly models crossfield dependence our model uses a single set of latent variables to control two disparate clustering models a dirichletmultinomial model over titles and a nonexchangeable stringedit model over venues we show that modeling crossfield dependence yields a substantial improvement in performance a reduction in error over a standard dirichlet process mixture 
permupattern permupattern pattern discovery in sequences is an important problem in many applications especially in computational biology and text mining however due to the noisy nature of data the traditional sequential pattern model may fail to reflect the underlying characteristics of sequence data in these applications there are two challenges first the mutation noise exists in the data and therefore symbols may be misrepresented by other symbols secondly the order of symbols in sequences could be permutated to address the above problems in this paper we propose a new sequential pattern model called mutable permutation patterns since the apriori property does not hold for our permutation pattern model a novel permupattern algorithm is devised to mine frequent mutable permutation patterns from sequence databases a reachability property is identified to prune the candidate set last but not least we apply the permutation pattern model to a real genome dataset to discover gene clusters which shows the effectiveness of the model a large amount of synthetic data is also utilized to demonstrate the efficiency of the permupattern algorithm
simultaneous tensor subspace selection and clustering simultaneous tensor subspace selection and clustering singular value decomposition svdprincipal component analysis pca have played a vital role in finding patterns from many datasets recently tensor factorization has been used for data mining and pattern recognition in high indexorder data high order svd hosvd is a commonly used tensor factorization method and has recently been used in numerous applications like graphs videos social networks etc
bridging centrality bridging centrality despite the pervasiveness of networks as models for real world systems ranging from the internet the world wide web to gene regulation and scientific collaborations only a limited number of metrics capable of characterizing these systems are available the existing metrics for characterizing networks have broad specificity and lack the selectivity for many applications the purpose of this paper is to identify and critically evaluate a metric termed bridging centrality which is highly selective for identifying bridges in networks the properties of bridges are unique compared to the other network metrics for a diverse range of data sets we found that networks are highly susceptible to disruption but robust to loss structural integrity upon targeted deletion of bridging nodes a novel graph clustering approach termed bridge cut utilizing bridging edges as module boundary is also proposed the modules identified by the bridge cut algorithm are more effective than the other graph clustering methods thus bridging centrality is a network metric with unique properties that may aid in network analysis from element to group level in various areas including systems biology and national security applications
interpretable nonnegative matrix decompositions interpretable nonnegative matrix decompositions a matrix decomposition expresses a matrix as a product of at least two factor matrices equivalently it expresses each column of the input matrix as a linear combination of the columns in the first factor matrix the interpretability of the decompositions is a key issue in many dataanalysis tasks we propose two new matrixdecomposition problems the nonnegative cx and nonnegative cur problems that give naturally interpretable factors they extend the recentlyproposed column and columnrow based decompositions and are aimed to be used with nonnegative matrices our decompositions represent the input matrix as a nonnegative linear combination of a subset of its columns or columns and rows
fast logistic regression for text categorization with variablelength ngrams fast logistic regression for text categorization with variablelength ngrams a common representation used in text categorization is the bag of words model aka unigram model learning with this particular representation involves typically some preprocessing eg stopwordsremoval stemming this results in one explicit tokenization of the corpus in this work we introduce a logistic regression approach where learning involves automatic tokenization this allows us to weaken the apriori required knowledge about the corpus and results in a tokenization with variablelength word or character ngrams as basic tokens we accomplish this by solving logistic regression using gradient ascent in the space of all ngrams we show that this can be done very efficiently using a branch and bound approach which chooses the maximum gradient ascent direction projected onto a single dimension ie candidate feature although the space is very large our method allows us to investigate variablelength ngram learning we demonstrate the efficiency of our approach compared to stateoftheart classifiers used for text categorization such as cyclic coordinate descent logistic regression and support vector machines
probabilistic latent semantic visualization probabilistic latent semantic visualization we propose a visualization method based on a topic model for discrete data such as documents unlike conventional visualization methods based on pairwise distances such as multidimensional scaling we consider a mapping from the visualization space into the space of documents as a generative process of documents in the model both documents and topics are assumed to have latent coordinates in a two or threedimensional euclidean space or visualization space the topic proportions of a document are determined by the distances between the document and the topics in the visualization space and each word is drawn from one of the topics according to its topic proportions a visualization ie latent coordinates of documents can be obtained by fitting the model to a given set of documents using the em algorithm resulting in documents with similar topics being embedded close together we demonstrate the effectiveness of the proposed model by visualizing document and movie data sets and quantitatively compare it with conventional visualization methods
automatic identification of quasiexperimental designs for discovering causal knowledge automatic identification of quasiexperimental designs for discovering causal knowledge researchers in the social and behavioral sciences routinely rely on quasiexperimental designs to discover knowledge from large databases quasiexperimental designs qeds exploit fortuitous circumstances in nonexperimental data to identify situations sometimes called natural experiments that provide the equivalent of experimental control and randomization qeds allow researchers in domains as diverse as sociology medicine and marketing to draw reliable inferences about causal dependencies from nonexperimental data unfortunately identifying and exploiting qeds has remained a painstaking manual activity requiring researchers to scour available databases and apply substantial knowledge of statistics however recent advances in the expressiveness of databases and increases in their size and complexity provide the necessary conditions to automatically identify qeds in this paper we describe the first system to discover knowledge by applying quasiexperimental designs that were identified automatically we demonstrate that qeds can be identified in a traditional database schema and that such identification requires only a small number of extensions to that schema knowledge about quasiexperimental design encoded in firstorder logic and a theoremproving engine we describe several key innovations necessary to enable this system including methods for automatically constructing appropriate experimental units and for creating aggregate variables on those units we show that applying the resulting designs can identify important causal dependencies in real domains and we provide examples from academic publishing movie making and marketing and peerproduction systems finally we discuss the integration of qeds with other approaches to causal discovery including joint modeling and directed experimentation
extracting shared subspace for multilabel classification extracting shared subspace for multilabel classification multilabel problems arise in various domains such as multitopic document categorization and protein function prediction one natural way to deal with such problems is to construct a binary classifier for each label resulting in a set of independent binary classification problems since the multiple labels share the same input space and the semantics conveyed by different labels are usually correlated it is essential to exploit the correlation information contained in different labels in this paper we consider a general framework for extracting shared structures in multilabel classification in this framework a common subspace is assumed to be shared among multiple labels we show that the optimal solution to the proposed formulation can be obtained by solving a generalized eigenvalue problem though the problem is nonconvex for highdimensional problems direct computation of the solution is expensive and we develop an efficient algorithm for this case one appealing feature of the proposed framework is that it includes several wellknown algorithms as special cases thus elucidating their intrinsic relationships we have conducted extensive experiments on eleven multitopic web page categorization tasks and results demonstrate the effectiveness of the proposed formulation in comparison with several representative algorithms
mining preferences from superior and inferior examples mining preferences from superior and inferior examples mining user preferences plays a critical role in many important applications such as customer relationship management crm product and service recommendation and marketing campaigns in this paper we identify an interesting and practical problem of mining user preferences in a multidimensional space where the user preferences on some categorical attributes are unknown from some superior and inferior examples provided by a user can we learn about the users preferences on those categorical attributes we model the problem systematically and show that mining user preferences from superior and inferior examples is challenging although the problem has great potential in practice to the best of our knowledge it has not been explored systematically before as the first attempt to tackle the problem we propose a greedy method and show that our method is practical using real data sets and synthetic data sets
effective and efficient itemset pattern summarization effective and efficient itemset pattern summarization in this paper we propose a set of novel regressionbased approaches to effectively and efficiently summarize frequent itemset patterns specifically we show that the problem of minimizing the restoration error for a set of itemsets based on a probabilistic model corresponds to a nonlinear regression problem we show that under certain conditions we can transform the nonlinear regression problem to a linear regression problem we propose two new methods kregression and treeregression to partition the entire collection of frequent itemsets in order to minimize the restoration error the kregression approach employing a kmeans type clustering method guarantees that the total restoration error achieves a local minimum the treeregression approach employs a decisiontree type of topdown partition process in addition we discuss alternatives to estimate the frequency for the collection of itemsets being covered by the k representative itemsets the experimental evaluation on both real and synthetic datasets demonstrates that our approaches significantly improve the summarization performance in terms of both accuracy restoration error and computational cost
a sequential dual method for large scale multiclass linear svms a sequential dual method for large scale multiclass linear svms efficient training of direct multiclass formulations of linear support vector machines is very useful in applications such as text classification with a huge number examples as well as features this paper presents a fast dual method for this training the main idea is to sequentially traverse through the training set and optimize the dual variables associated with one example at a time the speed of training is enhanced further by shrinking and cooling heuristics experiments indicate that our method is much faster than state of the art solvers such as bundle cutting plane and exponentiated gradient methods
constructing comprehensive summaries of large event sequences constructing comprehensive summaries of large event sequences event sequences capture system and user activity over time prior research on sequence mining has mostly focused on discovering local patterns though interesting these patterns reveal local associations and fail to give a comprehensive summary of the entire event sequence moreover the number of patterns discovered can be large in this paper we take an alternative approach and build short summaries that describe the entire sequence while revealing local associations among events
factorization meets the neighborhood factorization meets the neighborhood recommender systems provide users with personalized suggestions for products or services these systems often rely on collaborating filtering cf where past transactions are analyzed in order to establish connections between users and products the two more successful approaches to cf are latent factor models which directly profile both users and products and neighborhood models which analyze similarities between products or users in this work we introduce some innovations to both approaches the factor and neighborhood models can now be smoothly merged thereby building a more accurate combined model further accuracy improvements are achieved by extending the models to exploit both explicit and implicit feedback by the users the methods are tested on the netflix data results are better than those previously published on that dataset in addition we suggest a new evaluation metric which highlights the differences among methods based on their performance at a topk recommendation task
the structure of information pathways in a social communication network the structure of information pathways in a social communication network social networks are of interest to researchers in part because they are thought to mediate the flow of information in communities and organizations here we study the temporal dynamics of communication using online data including email communication among the faculty and staff of a large university over a twoyear period we formulate a temporal notion of distance in the underlying social network by measuring the minimum time required for information to spread from one node to another a concept that draws on the notion of vectorclocks from the study of distributed computing systems we find that such temporal measures provide structural insights that are not apparent from analyses of the pure social network topology in particular we define the network backbone to be the subgraph consisting of edges on which information has the potential to flow the quickest we find that the backbone is a sparse graph with a concentration of both highly embedded edges and longrange bridges a finding that sheds new light on the relationship between tie strength and connectivity in social networks
anglebased outlier detection in highdimensional data anglebased outlier detection in highdimensional data detecting outliers in a large set of data objects is a major data mining task aiming at finding different mechanisms responsible for different groups of objects in a data set all existing approaches however are based on an assessment of distances sometimes indirectly by assuming certain distributions in the fulldimensional euclidean data space in highdimensional data these approaches are bound to deteriorate due to the notorious curse of dimensionality in this paper we propose a novel approach named abod anglebased outlier detection and some variants assessing the variance in the angles between the difference vectors of a point to the other points this way the effects of the curse of dimensionality are alleviated compared to purely distancebased approaches a main advantage of our new approach is that our method does not rely on any parameter selection influencing the quality of the achieved ranking in a thorough experimental evaluation we compare abod to the wellestablished distancebased method lof for various artificial and a real world data set and show abod to perform especially well on highdimensional data
stream prediction using a generative model based on frequent episodes in event sequences stream prediction using a generative model based on frequent episodes in event sequences this paper presents a new algorithm for sequence prediction over long categorical event streams the input to the algorithm is a set of target event types whose occurrences we wish to predict the algorithm examines windows of events that precede occurrences of the target event types in historical data the set of significant frequent episodes associated with each target event type is obtained based on formal connections between frequent episodes and hidden markov models hmms each significant episode is associated with a specialized hmm and a mixture of such hmms is estimated for every target event type the likelihoods of the current window of events under these mixture models are used to predict future occurrences of target events in the data the only userdefined model parameter in the algorithm is the length of the windows of events used during model estimation we first evaluate the algorithm on synthetic data that was generated by embedding in varying levels of noise patterns which are preselected to characterize occurrences of target events we then present an application of the algorithm for predicting targeted userbehaviors from large volumes of anonymous search session interaction logs from a commerciallydeployed web browser toolbar
microscopic evolution of social networks microscopic evolution of social networks we present a detailed study of network evolution by analyzing four large online social networks with full temporal information about node and edge arrivals for the first time at such a large scale we study individual node arrival and edge creation processes that collectively lead to macroscopic properties of networks using a methodology based on the maximumlikelihood principle we investigate a wide variety of network formation strategies and show that edge locality plays a critical role in evolution of networks our findings supplement earlier network models based on the inherently nonlocal preferential attachment
cutandstitch cutandstitch multicore processors with ever increasing number of cores per chip are becoming prevalent in modern parallel computing our goal is to make use of the multicore as well as multiprocessor architectures to speed up data mining algorithms specifically we present a parallel algorithm for approximate learning of linear dynamical systems lds also known as kalman filters kf ldss are widely used in time series analysis such as motion capture modeling visual tracking etc we propose cutandstitch cas a novel method to handle the data dependencies from the chain structure of hidden variables in lds so as to parallelize the embased parameter learning algorithm we implement the algorithm using openmp on both a supercomputer and a quadcore commercial desktop the experimental results show that parallel algorithms using cutandstitch achieve comparable accuracy and almost linear speedups over the serial version in addition cutandstitch can be generalized to other models with similar linear structures such as hidden markov models hmm and switching kalman filters skf
active learning with direct query construction active learning with direct query construction active learning may hold the key for solving the data scarcity problem in supervised learning ie the lack of labeled data indeed labeling data is a costly process yet an active learner may request labels of only selected instances thus reducing labeling work dramatically most previous works of active learning are however poolbased that is a pool of unlabeled examples is given and the learner can only select examples from the pool to query for their labels this type of active learning has several weaknesses in this paper we propose novel active learning algorithms that construct examples directly to query for labels we study both a specific active learner based on the decision tree algorithm and a general active learner that can work with any base learning algorithm as there is no restriction on what examples to be queried our methods are shown to often query fewer examples to reduce the predictive error quickly this casts doubt on the usefulness of the pool in poolbased active learning nevertheless our methods can be easily adapted to work with a given pool of unlabeled examples
spectral domaintransfer learning spectral domaintransfer learning traditional spectral classification has been proved to be effective in dealing with both labeled and unlabeled data when these data are from the same domain in many real world applications however we wish to make use of the labeled data from one domain called indomain to classify the unlabeled data in a different domain outofdomain this problem often happens when obtaining labeled data in one domain is difficult while there are plenty of labeled data from a related but different domain in general this is a transfer learning problem where we wish to classify the unlabeled data through the labeled data even though these data are not from the same domain in this paper we formulate this domaintransfer learning problem under a novel spectral classification framework where the objective function is introduced to seek consistency between the indomain supervision and the outofdomain intrinsic structure through optimization of the cost function the label information from the indomain data is effectively transferred to help classify the unlabeled data from the outofdomain we conduct extensive experiments to evaluate our method and show that our algorithm achieves significant improvements on classification performance over many stateoftheart algorithms
mining multifaceted overviews of arbitrary topics in a text collection mining multifaceted overviews of arbitrary topics in a text collection a common task in many text mining applications is to generate a multifaceted overview of a topic in a text collection such an overview not only directly serves as an informative summary of the topic but also provides a detailed view of navigation to different facets of the topic existing work has cast this problem as a categorization problem and requires training examples for each facet this has three limitations all facets are predefined which may not fit the need of a particular user training examples for each facet are often unavailable such an approach only works for a predefined type of topics in this paper we break these limitations and study a more realistic new setup of the problem in which we would allow a user to flexibly describe each facet with keywords for an arbitrary topic and attempt to mine a multifaceted overview in an unsupervised way we attempt a probabilistic approach to solve this problem empirical experiments on different genres of text data show that our approach can effectively generate a multifaceted overview for arbitrary topics the generated overviews are comparable with those generated by supervised methods with training examples they are also more informative than unstructured flat summaries the method is quite general thus can be applied to multiple text mining tasks in different application domains
multiclass costsensitive boosting with pnorm loss functions multiclass costsensitive boosting with pnorm loss functions we propose a family of novel costsensitive boosting methods for multiclass classification by applying the theory of gradient boosting to pnorm based cost functionals we establish theoretical guarantees including proof of convergence and convergence rates for the proposed methods our theoretical treatment provides interpretations for some of the existing algorithms in terms of the proposed family including a generalization of the costing algorithm dse and gbset and the average cost method we also experimentally evaluate the performance of our new algorithms against existing methods of cost sensitive boosting including adacost csb and adaboostm with costsensitive weight initialization we show that our proposed scheme generally achieves superior results in terms of cost minimization and with the use of higher order pnorm loss in certain cases consistently outperforms the comparison methods thus establishing its empirical advantage
on updates that constrain the features connections during learning on updates that constrain the features connections during learning in many multiclass learning scenarios the number of classes is relatively large thousands or the space and time efficiency of the learning system can be crucial we investigate two online update techniques especially suited to such problems these updates share a sparsity preservation capacity they allow for constraining the number of prediction connections that each feature can make we show that one method exponential moving average is solving a discrete regression problem for each feature changing the weights in the direction of minimizing the quadratic loss we design the other method to improve a hinge loss subject to constraints for better accuracy we empirically explore the methods and compare performance to previous indexing techniques developed with the same goals as well as other online algorithms based on prototype learning we observe that while the classification accuracies are very promising improving over previous indexing techniques the scalability benefits are preserved
weighted graphs and disconnected components weighted graphs and disconnected components the vast majority of earlier work has focused on graphs which are both connected typically by ignoring all but the giant connected component and unweighted here we study numerous real weighted graphs and report surprising discoveries on the way in which new nodes join and form links in a social network the motivating questions were the following how do connected components in a graph form and change over time what happens after new nodes join a network how common are repeated edges we study numerous diverse real graphs citation networks networks in social media internet traffic and others and make the following contributions a we observe that the nongiant connected components seem to stabilize in size b we observe the weights on the edges follow several power laws with surprising exponents and c we propose an intuitive generative model for graph growth that obeys observed patterns
finding nonredundant statistically significant regions in high dimensional data finding nonredundant statistically significant regions in high dimensional data projected and subspace clustering algorithms search for clusters of points in subsets of attributes projected clustering computes several disjoint clusters plus outliers so that each cluster exists in its own subset of attributes subspace clustering enumerates clusters of points in all subsets of attributes typically producing many overlapping clusters one problem of existing approaches is that their objectives are stated in a way that is not independent of the particular algorithm proposed to detect such clusters a second problem is the definition of cluster density based on userdefined parameters which makes it hard to assess whether the reported clusters are an artifact of the algorithm or whether they actually stand out in the data in a statistical sense
joint latent topic models for text and citations joint latent topic models for text and citations in this work we address the problem of joint modeling of text and citations in the topic modeling framework we present two different models called the pairwiselinklda and the linkplsalda models
classification with partial labels classification with partial labels in this paper we address the problem of learning when some cases are fully labeled while other cases are only partially labeled in the form of partial labels partial labels are represented as a set of possible labels for each training example one of which is the correct label we introduce a discriminative learning approach that incorporates partial label information into the conventional marginbased learning framework the partial label learning problem is formulated as a convex quadratic optimization minimizing the lnorm regularized empirical risk using hinge loss we also present an efficient algorithm for classification in the presence of partial labels experiments with different data sets show that partial label information improves the performance of classification when there is traditional fullylabeled data and also yields reasonable performance in the absence of any fully labeled data
discriminationaware data mining discriminationaware data mining in the context of civil rights law discrimination refers to unfair or unequal treatment of people based on membership to a category or a minority without regard to individual merit rules extracted from databases by data mining techniques such as classification or association rules when used for decision tasks such as benefit or credit approval can be discriminatory in the above sense in this paper the notion of discriminatory classification rules is introduced and studied providing a guarantee of nondiscrimination is shown to be a non trivial task a naive approach like taking away all discriminatory attributes is shown to be not enough when other background knowledge is available our approach leads to a precise formulation of the redlining problem along with a formal result relating discriminatory rules with apparently safe ones by means of background knowledge an empirical assessment of the results on the german credit dataset is also provided
fast collapsed gibbs sampling for latent dirichlet allocation fast collapsed gibbs sampling for latent dirichlet allocation in this paper we introduce a novel collapsed gibbs sampling method for the widely used latent dirichlet allocation lda model our new method results in significant speedups on real world text corpora conventional gibbs sampling schemes for lda require ok operations per sample where k is the number of topics in the model our proposed method draws equivalent samples but requires on average significantly less then k operations per sample on realword corpora fastlda can be as much as times faster than the standard collapsed gibbs sampler for lda no approximations are necessary and we show that our fast sampling scheme produces exactly the same results as the standard but slower sampling scheme experiments on four real world data sets demonstrate speedups for a wide range of collection sizes for the pubmed collection of over million documents with a required computation time of cpu months for lda our speedup of can save cpu months of computation
partial least squares regression for graph mining partial least squares regression for graph mining attributed graphs are increasingly more common in many application domains such as chemistry biology and text processing a central issue in graph mining is how to collect informative subgraph patterns for a given learning task we propose an iterative mining method based on partial least squares regression pls to apply pls to graph data a sparse version of pls is developed first and then it is combined with a weighted pattern mining algorithm the mining algorithm is iteratively called with different weight vectors creating one latent component per one mining call our method graph pls is efficient and easy to implement because the weight vector is updated with elementary matrix calculations in experiments our graph pls algorithm showed competitive prediction accuracies in many chemical datasets and its efficiency was significantly superior to graph boosting gboost and the naive method based on frequent graph mining
knowledge discovery of semantic relationships between words using nonparametric bayesian graph model knowledge discovery of semantic relationships between words using nonparametric bayesian graph model we developed a model based on nonparametric bayesian modeling for automatic discovery of semantic relationships between words taken from a corpus it is aimed at discovering semantic knowledge about words in particular domains which has become increasingly important with the growing use of text mining information retrieval and speech recognition the subjectpredicate structure is taken as a syntactic structure with the noun as the subject and the verb as the predicate this structure is regarded as a graph structure the generation of this graph can be modeled using the hierarchical dirichlet process and the pitmanyor process the probabilistic generative model we developed for this graph structure consists of subjectpredicate structures extracted from a corpus evaluation of this model by measuring the performance of graph clustering based on wordnet similarities demonstrated that it outperforms other baseline models
mobile call graphs mobile call graphs we analyze a massive social network gathered from the records of a large mobile phone operator with more than a million users and tens of millions of calls we examine the distributions of the number of phone calls per customer the total talk minutes per customer and the distinct number of calling partners per customer we find that these distributions are skewed and that they significantly deviate from what would be expected by powerlaw and lognormal distributions
efficient ticket routing by resolution sequence mining efficient ticket routing by resolution sequence mining it problem management calls for quick identification of resolvers to reported problems the efficiency of this process highly depends on ticket routingtransferring problem ticket among various expert groups in search of the right resolver to the ticket to achieve efficient ticket routing wise decision needs to be made at each step of ticket transfer to determine which expert group is likely to be or to lead to the resolver
get another label improving data quality and data mining using multiple noisy labelers get another label improving data quality and data mining using multiple noisy labelers this paper addresses the repeated acquisition of labels for data items when the labeling is imperfect we examine the improvement or lack thereof in data quality via repeated labeling and focus especially on the improvement of training labels for supervised induction with the outsourcing of small tasks becoming easier for example via rentacoder or amazons mechanical turk it often is possible to obtain lessthanexpert labeling at low cost with lowcost labeling preparing the unlabeled part of the data can become considerably more expensive than labeling we present repeatedlabeling strategies of increasing complexity and show several main results i repeatedlabeling can improve label quality and model quality but not always ii when labels are noisy repeated labeling can be preferable to single labeling even in the traditional setting where labels are not particularly cheap iii as soon as the cost of processing the unlabeled data is not free even the simple strategy of labeling everything multiple times can give considerable advantage iv repeatedly labeling a carefully chosen set of points is generally preferable and we present a robust technique that combines different notions of uncertainty to select data points for which quality should be improved the bottom line the results show clearly that when labeling is not perfect selective acquisition of multiple labels is a strategy that data miners should have in their repertoire for certain labelqualitycost regimes the benefit is substantial
isax isax current research in indexing and mining time series data has produced many interesting algorithms and representations however the algorithms and the size of data considered have generally not been representative of the increasingly massive datasets encountered in science engineering and business domains in this work we show how a novel multiresolution symbolic representation can be used to index datasets which are several orders of magnitude larger than anything else considered in the literature our approach allows both fast exact search and ultra fast approximate search we show how to exploit the combination of both types of search as subroutines in data mining algorithms allowing for the exact mining of truly massive real world datasets containing millions of time series
efficient computation of personal aggregate queries on blogs efficient computation of personal aggregate queries on blogs there is an exploding amount of usergenerated content on theweb due to the emergence of web services such as bloggermyspace flickr and delicious the participation of a large number of users in sharing their opinion on the web has inspired researchers to build an effective information filter by aggregating these independent opinions however given the diverse groups of users on the web nowadays the global aggregation of the information may not be of much interest to different groups of users in this paper we explore the possibility of computing personalized aggregation over the opinions expressed on the web based on a users indication of trust over the information sources the hope is that by employing such personalized aggregation we can make the recommendation more likely to be interesting to the users we address the challenging scalability issues by proposing an efficient method that utilizes two core techniques nonnegative matrix factorization and threshold algorithm to compute personalized aggregations when there are potentially millions of users and millions of sources within a system we show that through experiments on reallife dataset our personalized aggregation approach indeed makes a significant difference in the items that are recommended and it reduces the query computational cost significantly often more than while the result of personalized aggregation is kept accurate enough
semisupervised approach to rapid and reliable labeling of large data sets semisupervised approach to rapid and reliable labeling of large data sets in this paper we propose a method where the labeling of the data set is carried out in a semisupervised manner with userspecified guarantees about the quality of the labeling in our scheme we assume that for each class we have some heuristics available each of which can identify instances of one particular class the heuristics are assumed to have reasonable performance but they do not need to cover all instances of the class nor do they need to be perfectly reliable we further assume that we have an infallible expert who is willing to manually label a few instances the aim of the algorithm is to exploit the cluster structure of the problem the predictions by the imperfect heuristics and the limited perfect labels provided by the expert to classify label the instances of the data set with guaranteed precision specificed by the user with regards to each class the specified precision is not always attainable so the algorithm is allowed to classify some instances as dontknow the algorithm is evaluated by the number of instances labeled by the expert the number of dontknow instances global coverage and the achieved quality of the labeling on the kdd cup network intrusion data set containing instances we managed to label of the instances while guaranteeing a nominal precision of with confidence by having the expert label instances and by having the expert label instances we managed to guarantee nominal precision while labeling of the data we also provide a case study of applying our scheme to label the network traffic collected at a large campus network
relational learning via collective matrix factorization relational learning via collective matrix factorization relational learning is concerned with predicting unknown values of a relation given a database of entities and observed relations among entities an example of relational learning is movie rating prediction where entities could include users movies genres and actors relations encode users ratings of movies movies genres and actors roles in movies a common prediction technique given one pairwise relation for example a users x movies ratings matrix is lowrank matrix factorization in domains with multiple relations represented as multiple matrices we may improve predictive accuracy by exploiting information from one relation while predicting another to this end we propose a collective matrix factorization model we simultaneously factor several matrices sharing parameters among factors when an entity participates in multiple relations each relation can have a different value type and error distribution so we allow nonlinear relationships between the parameters and outputs using bregman divergences to measure error we extend standard alternating projection algorithms to our model and derive an efficient newton update for the projection furthermore we propose stochastic optimization methods to deal with large sparse matrices our model generalizes several existing matrix factorization methods and therefore yields new largescale optimization algorithms for these problems our model can handle any pairwise relational schema and a wide variety of error models we demonstrate its efficiency as well as the benefit of sharing parameters among relations
a bayesian mixture model with linear regression mixing proportions a bayesian mixture model with linear regression mixing proportions classic mixture models assume that the prevalence of the various mixture components is fixed and does not vary over time this presents problems for applications where the goal is to learn how complex data distributions evolve we develop models and bayesian learning algorithms for inferring the temporal trends of the components in a mixture model as a function of time we show the utility of our models by applying them to the reallife problem of tracking changes in the rates of antibiotic resistance in escherichia coli and staphylococcus aureus the results show that our methods can derive meaningful temporal antibiotic resistance patterns
hypergraph spectral learning for multilabel classification hypergraph spectral learning for multilabel classification a hypergraph is a generalization of the traditional graph in which the edges are arbitrary nonempty subsets of the vertex set it has been applied successfully to capture highorder relations in various domains in this paper we propose a hypergraph spectral learning formulation for multilabel classification where a hypergraph is constructed to exploit the correlation information among different labels we show that the proposed formulation leads to an eigenvalue problem which may be computationally expensive especially for largescale problems to reduce the computational cost we propose an approximate formulation which is shown to be equivalent to a least squares problem under a mild condition based on the approximate formulation efficient algorithms for solving least squares problems can be applied to scale the formulation to very large data sets in addition existing regularization techniques for least squares can be incorporated into the model for improved generalization performance we have conducted experiments using largescale benchmark data sets and experimental results show that the proposed hypergraph spectral learning formulation is effective in capturing the highorder relations in multilabel problems results also indicate that the approximate formulation is much more efficient than the original one while keeping competitive classification performance
community evolution in dynamic multimode networks community evolution in dynamic multimode networks a multimode network typically consists of multiple heterogeneous social actors among which various types of interactions could occur identifying communities in a multimode network can help understand the structural properties of the network address the data shortage and unbalanced problems and assist tasks like targeted marketing and finding influential actors within or between groups in general a network and the membership of groups often evolve gradually in a dynamic multimode network both actor membership and interactions can evolve which poses a challenging problem of identifying community evolution in this work we try to address this issue by employing the temporal information to analyze a multimode network a spectral framework and its scalability issue are carefully studied experiments on both synthetic data and realworld large scale networks demonstrate the efficacy of our algorithm and suggest its generality in solving problems with complex relationships
colibri colibri lowrank approximations of the adjacency matrix of a graph are essential in finding patterns such as communities and detecting anomalies additionally it is desirable to track the lowrank structure as the graph evolves over time efficiently and within limited storage real graphs typically have thousands or millions of nodes but are usually very sparse however standard decompositions such as svd do not preserve sparsity this has led to the development of methods such as cur and cmd which seek a nonorthogonal basis by sampling the columns andor rows of the sparse matrix
can complex network metrics predict the behavior of nba teams can complex network metrics predict the behavior of nba teams the united states national basketball association nba is one of the most popular sports league in the world and is well known for moving a millionary betting market that uses the countless statistical data generated after each game to feed the wagers this leads to the existence of a rich historical database that motivates us to discover implicit knowledge in it in this paper we use complex network statistics to analyze the nba database in order to create models to represent the behavior of teams in the nba results of complex networkbased models are compared with box score statistics such as points rebounds and assists per game we show the box score statistics play a significant role for only a small fraction of the players in the league we then propose new models for predicting a team success based on complex network metrics such as clustering coefficient and node degree complex networkbased models present good results when compared to box score statistics which underscore the importance of capturing network relationships in a community such as the nba
modelbased document clustering with a collapsed gibbs sampler modelbased document clustering with a collapsed gibbs sampler modelbased algorithms are emerging as a preferred method for document clustering as computing resources improve methods such as gibbs sampling have become more common for parameter estimation in these models gibbs sampling is well understood for many applications but has not been extensively studied for use in document clustering we explore the convergence rate the possibility of label switching and chain summarization methodologies for document clustering on a particular model namely a mixture of multinomials model and show that fairly simple methods can be employed while still producing clusterings of superior quality compared to those produced with the em algorithm
building semantic kernels for text classification using wikipedia building semantic kernels for text classification using wikipedia document classification presents difficult challenges due to the sparsity and the high dimensionality of text data and to the complex semantics of the natural language the traditional document representation is a wordbased vector bag of words or bow where each dimension is associated with a term of the dictionary containing all the words that appear in the corpus although simple and commonly used this representation has several limitations it is essential to embed semantic information and conceptual patterns in order to enhance the prediction capabilities of classification algorithms in this paper we overcome the shortages of the bow approach by embedding background knowledge derived from wikipedia into a semantic kernel which is then used to enrich the representation of documents our empirical evaluation with real data sets demonstrates that our approach successfully achieves improved classification accuracy with respect to the bow technique and to other recently developed methods
a unified approach for schema matching coreference and canonicalization a unified approach for schema matching coreference and canonicalization the automatic consolidation of database records from many heterogeneous sources into a single repository requires solving several information integration tasks although tasks such as coreference schema matching and canonicalization are closely related they are most commonly studied in isolation systems that do tackle multiple integration problems traditionally solve each independently allowing errors to propagate from one task to another in this paper we describe a discriminativelytrained model that reasons about schema matching coreference and canonicalization jointly we evaluate our model on a realworld data set of people and demonstrate that simultaneously solving these tasks reduces errors over a cascaded or isolated approach our experiments show that a joint model is able to improve substantially over systems that either solve each task in isolation or with the conventional cascade we demonstrate nearly a error reduction for coreference and a error reduction for schema matching
information extraction from wikipedia information extraction from wikipedia not only is wikipedia a comprehensive source of quality information it has several kinds of internal structure eg relational summaries known as infoboxes which enable selfsupervised information extraction while previous efforts at extraction from wikipedia achieve high precision and recall on wellpopulated classes of articles they fail in a larger number of cases largely because incomplete articles and infrequent use of infoboxes lead to insufficient training data this paper presents three novel techniques for increasing recall from wikipedias long tail of sparse classes shrinkage over an automaticallylearned subsumption taxonomy a retraining technique for improving the training data and supplementing results by extracting from the broader web our experiments compare design variations and show that used in concert these techniques increase recall by a factor of to while maintaining or increasing precision
sail sail informationtheoretic clustering aims to exploit information theoretic measures as the clustering criteria a common practice on this topic is socalled infokmeans which performs kmeans clustering with the kldivergence as the proximity function while expert efforts on infokmeans have shown promising results a remaining challenge is to deal with highdimensional sparse data indeed it is possible that the centroids contain many zerovalue features for highdimensional sparse data this leads to infinite kldivergence values which create a dilemma in assigning objects to the centroids during the iteration process of kmeans to meet this dilemma in this paper we propose a summationbased incremental learning sail method for infokmeans clustering specifically by using an equivalent objective function sail replaces the computation of the kldivergence by the computation of the shannon entropy this can avoid the zerovalue dilemma caused by the use of the kldivergence our experimental results on various realworld document data sets have shown that with sail as a booster the clustering performance of kmeans can be significantly improved also sail leads to quick convergence and a robust clustering performance on highdimensional sparse data
asymmetric support vector machines asymmetric support vector machines many practical applications of classification require the classifier to produce a very low falsepositive rate although the support vector machine svm has been widely applied to these applications due to its superiority in handling high dimensional data there are relatively little effort other than setting a threshold or changing the costs of slacks to ensure the low falsepositive rate in this paper we propose the notion of asymmetric support vectormachine asvm that takes into account the falsepositives and the user tolerance in its objective such a new objective formulation allows us to raise the confidence in predicting the positives and therefore obtain a lower chance of falsepositives we study the effects of the parameters in asvm objective and address some implementation issues related to the sequential minimal optimization smo to cope with largescale data an extensive simulation is conducted and shows that asvm is able to yield either noticeable improvement in performance or reduction in training time as compared to the previous arts
succinct summarization of transactional databases succinct summarization of transactional databases transactional data are ubiquitous several methods including frequent itemsets mining and coclustering have been proposed to analyze transactional databases in this work we propose a new research problem to succinctly summarize transactional databases solving this problem requires linking the high level structure of the database to a potentially huge number of frequent itemsets we formulate this problem as a set covering problem using overlapped hyperrectangles we then prove that this problem and its several variations are nphard we develop an approximation algorithm hyper which can achieve a lnk approximation ratio in polynomial time we propose a pruning strategy that can significantly speed up the processing of our algorithm additionally we propose an efficient algorithm to further summarize the set of hyperrectangles by allowing false positive conditions a detailed study using both real and synthetic datasets shows the effectiveness and efficiency of our approaches in summarizing transactional databases
anonymizing transaction databases for publication anonymizing transaction databases for publication this paper considers the problem of publishing transaction data for research purposes each transaction is an arbitrary set of items chosen from a large universe detailed transaction data provides an electronic image of ones life this has two implications one transaction data are excellent candidates for data mining research two use of transaction data would raise serious concerns over individual privacy therefore before transaction data is released for data mining it must be made anonymous so that data subjects cannot be reidentified the challenge is that transaction data has no structure and can be extremely high dimensional traditional anonymization methods lose too much information on such data to date there has been no satisfactory privacy notion and solution proposed for anonymizing transaction data this paper proposes one way to address this issue
local peculiarity factor and its application in outlier detection local peculiarity factor and its application in outlier detection peculiarity oriented mining pom aiming to discover peculiarity rules hidden in a dataset is a new data mining method in the past few years many results and applications on pom have been reported however there is still a lack of theoretical analysis in this paper we prove that the peculiarity factor pf one of the most important concepts in pom can accurately characterize the peculiarity of data with respect to the probability density function of a normal distribution but is unsuitable for more general distributions thus we propose the concept of local peculiarity factor lpf it is proved that the lpf has the same ability as the pf for a normal distribution and is the socalled sensitive peculiarity description for general distributions to demonstrate the effectiveness of the lpf we apply it to outlier detection problems and give a new outlier detection algorithm called lpfoutlier experimental results show that lpfoutlier is an effective outlier detection algorithm
a family of dissimilarity measures between nodes generalizing both the shortestpath and the commutetime distances a family of dissimilarity measures between nodes generalizing both the shortestpath and the commutetime distances this work introduces a new family of linkbased dissimilarity measures between nodes of a weighted directed graph this measure called the randomized shortestpath rsp dissimilarity depends on a parameter and has the interesting property of reducing on one end to the standard shortestpath distance when is large and on the other end to the commutetime or resistance distance when is small near zero intuitively it corresponds to the expected cost incurred by a random walker in order to reach a destination node from a starting node while maintaining a constant entropy related to spread in the graph the parameter is therefore biasing gradually the simple random walk on the graph towards the shortestpath policy by adopting a statistical physics approach and computing a sum over all the possible paths discrete path integral it is shown that the rsp dissimilarity from every node to a particular node of interest can be computed efficiently by solving two linear systems of n equations where n is the number of nodes on the other hand the dissimilarity between every couple of nodes is obtained by inverting an n x n matrix the proposed measure can be used for various graph mining tasks such as computing betweenness centrality finding dense communities etc as shown in the experimental section
training structural svms with kernels using sampled cuts training structural svms with kernels using sampled cuts discriminative training for structured outputs has found increasing applications in areas such as natural language processing bioinformatics information retrieval and computer vision focusing on largemargin methods the most general in terms of loss function and model structure training algorithms known to date are based on cuttingplane approaches while these algorithms are very efficient for linear models their training complexity becomes quadratic in the number of examples when kernels are used to overcome this bottleneck we propose new training algorithms that use approximate cutting planes and random sampling to enable efficient training with kernels we prove that these algorithms have improved time complexity while providing approximation guarantees in empirical evaluations our algorithms produced solutions with training and test error rates close to those of exact solvers even on binary classification problems where highly optimized conventional training methods exist eg svmlight our methods are about an order of magnitude faster than conventional training methods on large datasets while remaining competitive in speed on datasets of medium size
stable feature selection via dense feature groups stable feature selection via dense feature groups many feature selection algorithms have been proposed in the past focusing on improving classification accuracy in this work we point out the importance of stable feature selection for knowledge discovery from highdimensional data and identify two causes of instability of feature selection algorithms selection of a minimum subset without redundant features and small sample size we propose a general framework for stable feature selection which emphasizes both good generalization and stability of feature selection results the framework identifies dense feature groups based on kernel density estimation and treats features in each dense group as a coherent entity for feature selection an efficient algorithm drags dense relevant attribute group selector is developed under this framework we also introduce a general measure for assessing the stability of feature selection algorithms our empirical study based on microarray data verifies that dense feature groups remain stable under random sample hold out and the drags algorithm is effective in identifying a set of feature groups which exhibit both high classification accuracy and stability
categorizing and mining concept drifting data streams categorizing and mining concept drifting data streams mining concept drifting data streams is a defining challenge for data mining research recent years have seen a large body of work on detecting changes and building prediction models from stream data with a vague understanding on the types of the concept drifting and the impact of different types of concept drifting on the mining algorithms in this paper we first categorize concept drifting into two scenarios loose concept drifting lcd and rigorous concept drifting rcd and then propose solutions to handle each of them separately for lcd data streams because concepts in adjacent data chunks are sufficiently close to each other we apply kernel mean matching kmm method to minimize the discrepancy of the data chunks in the kernel space such a minimization process will produce weighted instances to build classifier ensemble and handle concept drifting data streams for rcd data streams because genuine concepts in adjacent data chunks may randomly and rapidly change we propose a new optimal weights adjustment owa method to determine the optimum weight values for classifiers trained from the most recent uptodate data chunk such that those classifiers can form an accurate classifier ensemble to predict instances in the yettocome data chunk experiments on synthetic and realworld datasets will show that weighted instance approach is preferable when the concept drifting is mainly caused by the changing of the class prior probability whereas the weighted classifier approach is preferable when the concept drifting is mainly triggered by the changing of the conditional probability
fastanova fastanova studying the association between quantitative phenotype such as height or weight and single nucleotide polymorphisms snps is an important problem in biology to understand underlying mechanisms of complex phenotypes it is often necessary to consider joint genetic effects across multiple snps anova analysis of variance test is routinely used in association study important findings from studying genegene snppair interactions are appearing in the literature however the number of snps can be up to millions evaluating joint effects of snps is a challenging task even for snppairs moreover with large number of snps correlated permutation procedure is preferred over simple bonferroni correction for properly controlling familywise error rate and retaining mapping power which dramatically increases the computational cost of association study
cutsvm cutsvm semisupervised support vector machine svm attempts to learn a decision boundary that traverses through low data density regions by maximizing the margin over labeled and unlabeled examples traditionally svm is formulated as a nonconvex integer programming problem and is thus difficult to solve in this paper we propose the cutting plane semisupervised support vector machine cutsvm algorithm to solve the svm problem specifically we construct a nested sequence of successively tighter relaxations of the original svm problem and each optimization problem in this sequence could be efficiently solved using the constrained concaveconvex procedure cccp moreover we prove theoretically that the cutsvm algorithm takes time osn to converge with guaranteed accuracy where n is the total number of samples in the dataset and s is the average number of nonzero features ie the sparsity experimental evaluations on several real world datasets show that cutsvm performs better than existing svm methods both in efficiency and accuracy
identifying biologically relevant genes via multiple heterogeneous data sources identifying biologically relevant genes via multiple heterogeneous data sources selection of genes that are differentially expressed and critical to a particular biological process has been a major challenge in postarray analysis recent development in bioinformatics has made various data sources available such as mrna and mirna expression profiles biological pathway and gene annotation etc efficient and effective integration of multiple data sources helps enrich our knowledge about the involved samples and genes for selecting genes bearing significant biological relevance in this work we studied a novel problem of multisource gene selection given multiple heterogeneous data sources or data sets select genes from expression profiles by integrating information from various data sources we investigated how to effectively employ information contained in multiple data sources to extract an intrinsic global geometric pattern and use it in covariance analysis for gene selection we designed and conducted experiments to systematically compare the proposed approach with representative methods in terms of statistical and biological significance and showed the efficacy and potential of the proposed approach with promising findings
volatile correlation computation volatile correlation computation recent years have witnessed increased interest in computing strongly correlated pairs in very large databases most previous studies have been focused on static data sets however in realworld applications input data are often dynamic and must continually be updated with such large and growing data sets new research efforts are expected to develop an incremental solution for correlation computing along this line in this paper we propose a checkpoint algorithm that can efficiently incorporate new transactions for correlation computing as they become available specifically we set a checkpoint to establish a computation buffer which can help us determine an upper bound for the correlation this checkpoint bound can be exploited to identify a list of candidate pairs which will be maintained and computed for correlations as new transactions are added into the database however if the total number of new transactions is beyond the buffer size a new upper bound is computed by the new checkpoint and a new list of candidate pairs is identified experimental results on realworld data sets show that checkpoint can significantly reduce the correlation computing cost in dynamic data sets and has the advantage of compacting the use of memory space
land cover change detection land cover change detection the study of land cover change is an important problem in the earth science domain because of its impacts on local climate radiation balance biogeochemistry hydrology and the diversity and abundance of terrestrial species most wellknown change detection techniques from statistics signal processing and control theory are not wellsuited for the massive highdimensional spatiotemporal data sets from earth science due to limitations such as high computational complexity and the inability to take advantage of seasonality and spatiotemporal autocorrelation inherent in earth science data in our work we seek to address these challenges with new change detection techniques that are based on data mining approaches specifically in this paper we have performed a case study for a new change detection technique for the land cover change detection problem we study land cover change in the state of california focusing on the san francisco bay area and perform an extended study on the entire state we also perform a comparative evaluation on forests in the entire state these results demonstrate the utility of data mining techniques for the land cover change detection problem
identifying authoritative actors in questionanswering forums identifying authoritative actors in questionanswering forums we consider the problem of identifying authoritative users in yahoo answers a common approach is to use link analysis techniques in order to provide a ranked list of users based on their degree of authority a major problem for such an approach is determining how many users should be chosen as authoritative from a ranked list to address this problem we propose a method for automatic identification of authoritative actors in our approach we propose to model the authority scores of users as a mixture of gamma distributions the number of components in the mixture is estimated by the bayesian information criterion bic while the parameters of each component are estimated using the expectationmaximization em algorithm this method allows us to automatically discriminate between authoritative and nonauthoritative users the suitability of our proposal is demonstrated in an empirical study using datasets from yahoo answers
contextaware query suggestion by mining clickthrough and session data contextaware query suggestion by mining clickthrough and session data query suggestion plays an important role in improving the usability of search engines although some recently proposed methods can make meaningful query suggestions by mining query patterns from search logs none of them are contextaware they do not take into account the immediately preceding queries as context in query suggestion in this paper we propose a novel contextaware query suggestion approach which is in two steps in the offine modellearning step to address data sparseness queries are summarized into concepts by clustering a clickthrough bipartite then from session data a concept sequence suffix tree is constructed as the query suggestion model in the online query suggestion step a users search context is captured by mapping the query sequence submitted by the user to a sequence of concepts by looking up the context in the concept sequence sufix tree our approach suggests queries to the user in a contextaware manner we test our approach on a largescale search log of a commercial search engine containing billion search queries billion clicks and million query sessions the experimental results clearly show that our approach outperforms two baseline methods in both coverage and quality of suggestions
the persuasive phase of visualization the persuasive phase of visualization research in visualization often revolves around visualizing information however visualization is a process that extends over time from initial exploration to hypothesis confirmation and even to result presentation it is rare that the final phases of visualization are solely about information in this paper we present a more biased kind of visualization in which there is a message or set of assumptions behind the presentation that is of interest to both the presenter and the viewer and emphasizes points that the presenter wants to convey to the viewer this kind of persuasive visualization presenting data in a way that emphasizes a point or message is not only common in visualization but also often expected by the viewer persuasive visualization is implicit in the deliberate emphasis on interestingness and also in the deliberate use of graphical elements that are processed preattentively by the human visual system which automatically groups these elements and guiding attention so that they stand out we discuss how these ideas have been implemented in the morpherspective system for automated generation of information graphics
detecting privacy leaks using corpusbased association rules detecting privacy leaks using corpusbased association rules detecting inferences in documents is critical for ensuring privacy when sharing information in this paper we propose a refined and practical model of inference detection using a reference corpus our model is inspired by association rule mining inferences are based on word cooccurrences using the model and taking the web as the reference corpus we can find inferences and measure their strength through webmining algorithms that leverage search engines such as google or yahoo
learning methods for lung tumor markerless gating in imageguided radiotherapy learning methods for lung tumor markerless gating in imageguided radiotherapy in an idealized gated radiotherapy treatment radiation is delivered only when the tumor is at the right position for gated lung cancer radiotherapy it is difficult to generate accurate gating signals due to the large uncertainties when using external surrogates and the risk of pneumothorax when using implanted fiducial markers in this paper we investigate machine learning algorithms for markerless gated radiotherapy with fluoroscopic images previous approach utilizes template matching to localize the tumor position here we investigate two ways to improve the precision of tumor target localization by applying an ensemble of templates where the representative templates are selected by gaussian mixture clustering and a support vector machine svm classifier with radial basis kernels template matching only considers images inside the gating window but images outside the gating window might provide additional information we take advantage of both states and recast the gating problem into a classification problem thus we are able to use the svm classifier for gated radiotherapy to verify the effectiveness of the two proposed techniques we apply them on five sequences of fluoroscopic images from five lung cancer patients against the gating signal of manually contoured tumors as ground truth our fivepatient case study shows that both ensemble template matching and svm are reasonable tools for imageguided markerless gated radiotherapy with an average of approximately precision in terms of delivered target dose at approximately duty cycle
text classification business intelligence and interactivity text classification business intelligence and interactivity text classification has matured as a research discipline over the last decade independently business intelligence over structured databases has long been a source of insights for enterprises in this work we bring the two together for customer satisfactioncsat analysis in the services industry we present itacs a solution combining text classification and business intelligence integrated with a novel interactive text labeling interface itacs has been deployed in multiple client accounts in contact centers it can be extended to any services industry setting to analyze unstructured text data and derive operational and business insights we highlight importance of interactivity in reallife text classification settings we bring out some unique research challenges about labelsets measuring accuracy and interpretability that need serious attention in both academic and industrial research we recount invaluable experiences and lessons learned as data mining researchers working toward seeing research technology deployed in the services industry
data mining using high performance data clouds data mining using high performance data clouds we describe the design and implementation of a high performance cloud that we have used to archive analyze and mine large distributed data sets by a cloud we mean an infrastructure that provides resources andor services over the internet a storage cloud provides storage services while a compute cloud provides compute services we describe the design of the sector storage cloud and how it provides the storage services required by the sphere compute cloud we also describe the programming paradigm supported by the sphere compute cloud sector and sphere are designed for analyzing large data sets using computer clusters connected with wide area high performance networks for example gbs we describe a distributed data mining application that we have developed using sector and sphere finally we describe some experimental studies comparing sectorsphere to hadoop
automated cyclone discovery and tracking using knowledge sharing in multiple heterogeneous satellite data automated cyclone discovery and tracking using knowledge sharing in multiple heterogeneous satellite data current techniques for cyclone detection and tracking employ ncep national centers for environmental prediction models from insitu measurements this solution does not provide true global coverage unlike remote satellite observations however it is impractical to use a single earth orbiting satellite to detect and track events such as cyclones in a continuous manner due to limited spatial and temporal coverage one solution to alleviate such persistent problems is to utilize heterogeneous sensor data from multiple orbiting satellites however this solution requires overcoming other new challenges such as varying spatial and temporal resolution between satellite sensor data the need to establish correspondence between features from different satellite sensors and the lack of definitive indicators for cyclone events in some sensor data
spotting out emerging artists using geoaware analysis of pp query strings spotting out emerging artists using geoaware analysis of pp query strings record label companies would like to identify potential artists as early as possible in their careers before other companies approach the artists with competing contracts the vast number of candidates makes the process of identifying the ones with high success potential time consuming and laborious this paper demonstrates how datamining of pp query strings can be used in order to mechanize most of this detection process using a unique intercepting system over the gnutella network we were able to capture an unprecedented amount of geographically identified geoaware queries allowing us to investigate the diffusion of music related queries in time and space our solution is based on the observation that emerging artists especially rappers have a discernible stronghold of fans in their hometown area where they are able to perform and market their music in a file sharing network this is reflected as a delta function spatial distribution of content queries using this observation we devised a detection algorithm for emerging artists that looks for performers with sharp increase in popularity in a small geographic region though still unnoticable nation wide the algorithm can suggest a short list of artists with breakthrough potential from which we showed that about translate the potential to national success
customer targeting models using activelyselected web content customer targeting models using activelyselected web content we consider the problem of predicting the likelihood that a company will purchase a new product from a seller the statistical models we have developed at ibm for this purpose rely on historical transaction data coupled with structured firmographic information like the company revenue number of employees and so on in this paper we extend this methodology to include additional textbased features based on analysis of the content on each companys website empirical results demonstrate that incorporating such web content can significantly improve customer targeting furthermore we present methods to actively select only the web content that is likely to improve our models while reducing the costs of acquisition and processing
anticipating annotations and emerging trends in biomedical literature anticipating annotations and emerging trends in biomedical literature the biojournalmonitor is a decision support system for the analysis of trends and topics in the biomedical literature its main goal is to identify potential diagnostic and therapeutic biomarkers for specific diseases several data sources are continuously integrated to provide the user with uptodate information on current research in this field stateoftheart text mining technologies are deployed to provide added value on top of the original content including named entity detection relation extraction classification clustering ranking summarization and visualization we present two novel technologies that are related to the analysis of temporal dynamics of text archives and associated ontologies currently the mesh ontology is used to annotate the scientific articles entering the pubmed database with medical terms both the maintenance of the ontology as well as the annotation of new articles is performed largely manually we describe how probabilistic topic models can be used to annotate recent articles with the most likely mesh terms this provides our users with a competitive advantage because when searching for mesh terms articles are found long before they are manually annotated we further present a study on how to predict the inclusion of new terms in the mesh ontology the results suggest that early prediction of emerging trends is possible the trend ranking functions are deployed in our system to enable interactive searches for the hottest new trends relating to a disease
temporal pattern discovery for trends and transient effects temporal pattern discovery for trends and transient effects we introduce a novel pattern discovery methodology for event history data focusing explicitly on the detailed temporal relationship between pairs of events at the core is a graphical statistical approach to summarising and visualising event history data which contrasts the observed to the expected incidence of the event of interest before and after an index event thus pattern discovery is not restricted to a specific time window of interest but encompasses extended parts of the underlying event histories in order to effectively screen large collections of event history data for interesting temporal relationships we introduce a new measure of temporal association the proposed measure contrasts the observedtoexpected ratio in a time period of interest to that in a predefined control period an important feature of both the observedtoexpected graph itself and the measure of association is a statistical shrinkage towards the null hypothesis of no association this provides protection against spurious associations and is an extension of the statistical shrinkage successfully applied to largescale screening for associations between events in crosssectional data such as large collections of adverse drug reaction reports we demonstrate the usefulness of the proposed pattern discovery methodology by a set of examples from a collection of over two million patient records in the united kingdom the identified patterns include temporal relationships between drug prescription and medical events suggestive of persistent or transient risks of adverse events as well as temporal relationships between prescriptions of different drugs
scalable and near realtime burst detection from ecommerce queries scalable and near realtime burst detection from ecommerce queries in large scale online systems like search ecommerce or social network applications user queries represent an important dimension of activities that can be used to study the impact on the system and even the business in this paper we describe how to detect characterize and classify bursts in user queries in a large scale ecommerce system we build upon the approaches discussed in kdd bursty and hierarchical structure in streams and apply them to a high volume industrial context we describe how to identify bursts on a near realtime basis classify them and apply them to build interesting merchandizing applications
identifying domain expertise of developers from source code identifying domain expertise of developers from source code we are interested in identifying the domain expertise of developers of a software system a developer gains expertise on the code base as well as the domain of the software system heshe develops this information forms a useful input in allocating software implementation tasks to developers domain concepts represented by the system are discovered by taking into account the linguistic information available in the source code the vocabulary contained in source code as identifiers such as class method variable names and comments are extracted concepts present in the code base are identified and grouped based on a well known text processing hypothesis words are similar to the extent to which they share similar words the developers association with the source code and the concepts it represents is arrived at using the version repository information in this line the analysis first derives documents from source code by discarding all the programming language constructs kmeans clustering is further used to cluster documents and extract closely related concepts the key concepts present in the documents authored by the developer determine hisher domain expertise to validate our approach we apply it on large software systems two of which are presented in detail in this paper
arnetminer arnetminer this paper addresses several key issues in the arnetminer system which aims at extracting and mining academic social networks specifically the system focuses on extracting researcher profiles automatically from the web integrating the publication data into the network from existing digital libraries modeling the entire academic network and providing search services for the academic network so far researcher profiles have been extracted using a unified tagging approach we integrate publications from online web databases and propose a probabilistic framework to deal with the name ambiguity problem furthermore we propose a unified modeling approach to simultaneously model topical aspects of papers authors and publication venues search services such as expertise search and people association search have been provided based on the modeling results in this paper we describe the architecture and main features of the system we also present the empirical evaluation of the proposed methods
tagmark tagmark radio frequency identification rfid promises optimization of commodity flows in all industry segments but due to physical constraints rfid technology cannot detect all rfid tags from an assembly of items this poses problems when integrating rfid data with enterprisebackend systems for tasks like inventory management or shelf replenishment in this paper we propose the tagmark method to accomplish this integration tagmark targets at a retailer scenario where it estimates the number of tagged items from samples like the sales history or the tags read by smart shelves the problem is challenging because most existing estimation methods depend on assumptions that do not hold in typical rfid applications eg static item sets simple random samples or the availability of samples with userdefined sizes tagmark adapts markrecapturemethods in order to provide guarantees for the accuracy of the estimation and bounds for the sample sizes it can be implemented as a database extension allowing seamless integration into existing enterprise backend systems a study with rfidequipped goods acknowledges that our approach is effective in realistic scenarios and database experiments with up to items confirm that it can be efficiently implemented finally we explore a broad range of extreme conditions that might stress tagmark including a thief who knows the location of unread items
experimental comparison of scalable online ad serving experimental comparison of scalable online ad serving online ad servers attempt to find best ads to serve for a given triggering user event the performance of ads may be measured in several ways we suggest a formulation in which the ad network tries to maximize revenue subject to relevance constraints we describe several algorithms for ad selection and review their complexity we tested these algorithms using microsoft ad network from october to february over billion impressions million combinations of triggers with ads and a number of algorithms were tested over this period we discover curious differences between adservers aimed at revenue versus clickthrough rate
a visualanalytic toolkit for dynamic interaction graphs a visualanalytic toolkit for dynamic interaction graphs in this article we describe a visualanalytic tool for the interrogation of evolving interaction network data such as those found in social bibliometric www and biological applications the tool we have developed incorporates common visualization paradigms such as zooming coarsening and filtering while naturally integrating information extracted by a previously described eventdriven framework for characterizing the evolution of such networks the visual frontend provides features that are specifically useful in the analysis of interaction networks capturing the dynamic nature of both individual entities as well as interactions among them the tool provides the user with the option of selecting multiple views designed to capture different aspects of the evolving graph from the perspective of a node a community or a subset of nodes of interest standard visual templates and cues are used to highlight critical changes that have occurred during the evolution of the network a key challenge we address in this work is that of scalability handling large graphs both in terms of the efficiency of the backend and in terms of the efficiency of the visual layout and rendering two case studies based on bibliometric and wikipedia data are presented to demonstrate the utility of the toolkit for visual knowledge discovery
heterogeneous data fusion for alzheimers disease study heterogeneous data fusion for alzheimers disease study effective diagnosis of alzheimers disease ad is of primary importance in biomedical research recent studies have demonstrated that neuroimaging parameters are sensitive and consistent measures of ad in addition genetic and demographic information have also been successfully used for detecting the onset and progression of ad the research so far has mainly focused on studying one type of data source only it is expected that the integration of heterogeneous data neuroimages demographic and genetic measures will improve the prediction accuracy and enhance knowledge discovery from the data such as the detection of biomarkers in this paper we propose to integrate heterogeneous data for ad prediction based on a kernel method we further extend the kernel framework for selecting features biomarkers from heterogeneous data sources the proposed method is applied to a collection of mri data from normal healthy controls and ad patients the mri data are preprocessed using tensor factorization in this study we treat the complementary voxelbased data and region of interest roi data from mri as two data sources and attempt to integrate the complementary information by the proposed method experimental results show that the integration of multiple data sources leads to a considerable improvement in the prediction accuracy results also show that the proposed algorithm identifies biomarkers that play more significant roles than others in ad diagnosis
privacypreserving cox regression for survival analysis privacypreserving cox regression for survival analysis privacypreserving data mining ppdm is an emergent research area that addresses the incorporation of privacy preserving concerns to data mining techniques in this paper we propose a privacypreserving pp cox model for survival analysis and consider a real clinical setting where the data is horizontally distributed among different institutions the proposed model is based on linearly projecting the data to a lower dimensional space through an optimal mapping obtained by solving a linear programming problem our approach differs from the commonly used random projection approach since it instead finds a projection that is optimal at preserving the properties of the data that are important for the specific problem at hand since our proposed approach produces an sparse mapping it also generates a pp mapping that not only projects the data to a lower dimensional space but it also depends on a smaller subset of the original features it provides explicit feature selection real data from several european healthcare institutions are used to test our model for survival prediction of nonsmallcell lung cancer patients these results are also confirmed using publicly available benchmark datasets our experimental results show that we are able to achieve a nearoptimal performance without directly sharing the data across different data sources this model makes it possible to conduct largescale multicentric survival analysis without violating privacypreserving requirements
using predictive analysis to improve invoicetocash collection using predictive analysis to improve invoicetocash collection it is commonly agreed that accounts receivable ar can be a source of financial difficulty for firms when they are not efficiently managed and are underperforming experience across multiple industries shows that effective management of ar and overall financial performance of firms are positively correlated in this paper we address the problem of reducing outstanding receivables through improvements in the collections strategy specifically we demonstrate how supervised learning can be used to build models for predicting the payment outcomes of newlycreated invoices thus enabling customized collection actions tailored for each invoice or customer our models can predict with high accuracy if an invoice will be paid on time or not and can provide estimates of the magnitude of the delay we illustrate our techniques in the context of realworld transaction data from multiple firms finally simulation results show that our approach can reduce collection time up to a factor of four compared to a baseline that is not modeldriven
learning from multitopic web documents for contextual advertisement learning from multitopic web documents for contextual advertisement contextual advertising on web pages has become very popular recently and it poses its own set of unique text mining challenges often advertisers wish to either target or avoid some specific content on web pages which may appear only in a small part of the page learning for these targeting tasks is difficult since most training pages are multitopic and need expensive human labeling at the subdocument level for accurate training in this paper we investigate ways to learn for subdocument classification when only page level labels are available these labels only indicate if the relevant content exists in the given page or not we propose the application of multipleinstance learning to this task to improve the effectiveness of traditional methods we apply subdocument classification to two different problems in contextual advertising one is sensitive content detection where the advertiser wants to avoid content relating to war violence pornography etc even if they occur only in a small part of a page the second problem involves opinion mining from review sites the advertiser wants to detect and avoid negative opinion about their product when positive negative and neutral sentiments coexist on a page in both these scenarios we present experimental results to show that our proposed system is able to get good block level labeling for free and improve the performance of traditional learning methods
social networks social networks by now online social networks have become an indispensable part of both online and offline lives of human beings a large fraction of time spent online by a user is directly influence by the social networks to which heshe belongs this calls for a deeper examination of social networks as largescale dynamic objects that foster efficient personperson interaction
an inductive database prototype based on virtual mining views an inductive database prototype based on virtual mining views we present a prototype of an inductive database our system enables the user to query not only the data stored in the database but also generalizations eg rules or trees over these data through the use of virtual mining views the mining views are relational tables that virtually contain the complete output of data mining algorithms executed over a given dataset the prototype implemented into postgresql currently integrates frequent itemset association rule and decision tree mining we illustrate the interactive and iterative capabilities of our system with a description of a complete data mining scenario
febrl febrl matching records that refer to the same entity across databases is becoming an increasingly important part of many data mining projects as often data from multiple sources needs to be matched in order to enrich data or improve its quality significant advances in record linkage techniques have been made in recent years however many new techniques are either implemented in research proofofconcept systems only or they are hidden within expensive black box commercial software this makes it difficult for both researchers and practitioners to experiment with new record linkage techniques and to compare existing techniques with new ones the febrl freely extensible biomedical record linkage system aims to fill this gap it contains many recently developed techniques for data cleaning deduplication and record linkage and encapsulates them into a graphical user interface gui febrl thus allows even inexperienced users to learn and experiment with both traditional and new record linkage techniques because febrl is written in python and its source code is available it is fairly easy to integrate new record linkage techniques into it therefore febrl can be seen as a tool that allows researchers to compare various existing record linkage techniques with their own ones enabling the record linkage research community to conduct their work more efficiently additionally febrl is suitable as a training tool for new record linkage users and it can also be used for practical linkage projects with data sets that contain up to several hundred thousand records
using tagflake for condensing navigable tag hierarchies from tag clouds using tagflake for condensing navigable tag hierarchies from tag clouds we present the tagflake system which supports semantically informed navigation within a tag cloud tagflake relies on tmine for organizing tags extracted from textual content in hierarchical organizations suitable for navigation visualization classification and tracking tmine extracts the most significant tagterms from text documents and maps them onto a hierarchy in such a way that descendant terms are contextually dependent on their ancestors within the given corpus of documents this provides tagflake with a mechanism for enabling navigation within the tag space and for classification of the text documents based on the contextual structure captured by the created hierarchy tagflake is language neutral since it does not rely on any natural language processing technique and is unsupervised
an integrated system for automatic customer satisfaction analysis in the services industry an integrated system for automatic customer satisfaction analysis in the services industry text classification has matured well as a research discipline over the years at the same time business intelligence over databases has long been a source of insights for enterprises with the growing importance of the services industry customer relationship management and contact center operations have become very important specifically the voice of the customer and customer satisfaction csat have emerged as invaluable sources of insights about how an enterprises products and services are percieved by customers
dimac dimac in some applications such as filling in a customer information form on the web some missing values may not be explicitly represented as such but instead appear as potentially valid data values such missing values are known as disguised missing data which may impair the quality of data analysis severely the very limited previous studies on cleaning disguised missing data highly rely on domain background knowledge in specific applications and may not work well for the cases where the disguise values are inliers
patternminer patternminer this demo presents patternminer an integrated environment for pattern management and mining that deals with the whole lifecycle of patterns from their generation using data mining techniques to their storage and querying putting also emphasis on the comparison between patterns and metamining operations over the extracted patterns pattern comparison comparing results of the data mining process and metamining are high level pattern operations that can be applied in a variety of applications from database change management to image comparison and retrieval
cro cro in this paper we present a system called cro chinese review observer for online product review structurization by structurization we mean identifying extracting and summarizing information from unstructured review text to a structured table the core tasks include review collection product feature and user opinion extraction and polarity analysis of opinions existing research in this area is mainly english text oriented to deal with chinese effectively we propose several novel approaches for fulfilling the core tasks then we integrated these approaches and implement the whole procedure of review structurization in the system cro running results for reviews of real products show its performance is satisfactory
morpheus morpheus data mining techniques extract interesting patterns out of large data resources meaningful visualization and interactive exploration of patterns are crucial for knowledge discovery visualization techniques exist for traditional clustering in low dimensional spaces in high dimensional data clusters typically only exist in subspace projections this subspace clustering however lacks interactive visualization tools challenges arise from typically large result sets in different subspace projections that hinder comparability visualization and understandability
a software system for buzzbased recommendations a software system for buzzbased recommendations in this paper we present an outline of a software system for buzzbased recommendations this system is based on a large source of queries in an ecommerce application the buzz events are detected based on query bursts linked to external entities like news and inventory information a semantic neighborhood of the chosen buzz query is selected and appropriate recommendations are made on products that relate to this neighborhood the system follows the paradigm of limited quantity merchandizing in the sense that on a perday basis the system shows recommendations around a single buzz query with the intent of increasing user curiosity and promoting user activity and stickiness the system demonstrates the deployment of an interesting application based on kdd principles applied to a high volume industrial context
pictor pictor we present a demonstration of an interactive wrapper induction system called pictor which is able to minimize labeling cost yet extract data with high accuracy from a website our demonstration will introduce two proposed technologies recordlevel wrappers and a wrapperassisted labeling strategy these approaches allow pictor to exploit previously generated wrappers in order to predict similar labels in a partially labeled webpage or a completely new webpage our experiment results show the effectiveness of the pictor system
a learning framework for nearest neighbor search a learning framework for nearest neighbor search can we leverage learning techniques to build a fast nearestneighbor nn retrieval data structure we present a general learning framework for the nn problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time andor the miss rate we explore the potential of this novel framework through two popular nn data structures kdtrees and the rectilinear structures employed by locality sensitive hashing we derive a generalization theory for these data structure classes and present simple learning algorithms for both experimental results reveal that learning often improves on the already strong performance of these data structures
a new view of automatic relevance determination a new view of automatic relevance determination automatic relevance determination ard and the closelyrelated sparse bayesian learning sbl framework are effective tools for pruning large numbers of irrelevant features however popular update rules used for this process are either prohibitively slow in practice andor heuristic in nature without proven convergence properties this paper furnishes an alternative means of optimizing a general ard cost function using an auxiliary function that can naturally be solved using a series of reweighted l problems the result is an efficient algorithm that can be implemented using standard convex programming toolboxes and is guaranteed to converge to a stationary point unlike existing methods the analysis also leads to additional insights into the behavior of previous ard updates as well as the ard cost function for example the standard fixedpoint updates of mackay are shown to be iteratively solving a particular minmax problem although they are not guaranteed to lead to a stationary point the analysis also reveals that ard is exactly equivalent to performing map estimation using a particular feature and noisedependent textitnonfactorial weight prior with several desirable properties over conventional priors with respect to feature selection in particular it provides a tighter approximation to the l quasinorm sparsity measure than the l norm overall these results suggests alternative cost functions and update procedures for selecting features and promoting sparse solutions
a probabilistic approach to language change a probabilistic approach to language change we present a probabilistic approach to language change in which word forms are represented by phoneme sequences that undergo stochastic edits along the branches of a phylogenetic tree our framework combines the advantages of the classical comparative method with the robustness of corpusbased probabilistic models we use this framework to explore the consequences of two different schemes for defining probabilistic models of phonological change evaluating these schemes using the reconstruction of ancient word forms in romance languages the result is an efficient inference procedure for automatically inferring ancient word forms from modern languages which can be generalized to support inferences about linguistic phylogenies
a unified model for content based image suggestion and feature selection a unified model for content based image suggestion and feature selection content based image retrieval systems provide techniques for representing indexing and searching images they address only the users short term needs expressed as queries from the importance of the visual information in many applications such as advertisements and security we motivate in this paper the emphcontent based image suggestion it targets the users long term needs as a recommendation of products based on the user preferences in different situations and on the visual content of images we propose a generative model in which the visual features and users are clustered into separate classes we identify the number of both user and image classes with the simultaneous selection of relevant visual features the goal is to ensure an accurate prediction of ratings for multidimensional images this model is learned using the minimum message length approach experiments with an image collection showed the merits of our approach
a unified nearoptimal estimator for dimension reduction in lalpha ltalphaleq using sta a unified nearoptimal estimator for dimension reduction in lalpha ltalphaleq using sta many tasks eg clustering in machine learning only require the lalpha distances instead of the original data for dimension reductions in the lalpha norm ltalphaleq the method of em stable random projections can efficiently compute the lalpha distances in massive datasets eg the web or massive data streams in one pass of the data the estimation task for em stable random projections has been an interesting topic we propose a simple estimator based on the em fractional power of the samples projected data which is surprisingly nearoptimal in terms of the asymptotic variance in fact it achieves the cramerrao bound when alpha and alpha this new result will be useful when applying em stable random projections to distancebased clustering classifications kernels massive data streams etc
active preference learning with discrete choice data active preference learning with discrete choice data we propose an active learning algorithm that learns a continuous valuation model from discrete preferences the algorithm automatically decides what items are best presented to an individual in order to find the item that they value highly in as few trials as possible and exploits quirks of human psychology to minimize time and cognitive burden to do this our algorithm maximizes the expected improvement at each query without accurately modelling the entire valuation surface which would be needlessly expensive the problem is particularly difficult because the space of choices is infinite we demonstrate the effectiveness of the new algorithm compared to related active learning methods we also embed the algorithm within a decision making tool for assisting digital artists in rendering materials the tool finds the best parameters while minimizing the number of queries
adaptive embedded subgraph algorithms using walksum analysis adaptive embedded subgraph algorithms using walksum analysis we consider the estimation problem in gaussian graphical models with arbitrary structure we analyze the embedded trees algorithm which solves a sequence of problems on tractable subgraphs thereby leading to the solution of the estimation problem on an intractable graph our analysis is based on the recently developed walksum interpretation of gaussian estimation we show that nonstationary iterations of the embedded trees algorithm using any sequence of subgraphs converge in walksummable models based on walksum calculations we develop adaptive methods that optimize the choice of subgraphs used at each iteration with a view to achieving maximum reduction in error these adaptive procedures provide a significant speedup in convergence over stationary iterative methods and also appear to converge in a larger class of models
automatic generation of social tags for music recommendation automatic generation of social tags for music recommendation social tags are usergenerated keywords associated with some resource on the web in the case of music social tags have become an important component of quotwebquot recommender systems allowing users to generate playlists based on usedependent terms such as quotchillquot or quotjoggingquot that have been applied to particular songs in this paper we propose a method for predicting these social tags directly from mp files using a set of boosted classifiers we map audio features onto social tags collected from the web the resulting automatic tags or quotautotagsquot furnish information about music that is otherwise untagged or poorly tagged allowing for insertion of previously unheard music into a social recommender this avoids the coldstart problem common in such systems autotags can also be used to smooth the tag space from which similarities and recommendations are made by providing a set of comparable baseline tags for all tracks in a recommender system
catching changepoints with lasso catching changepoints with lasso we propose a new approach for dealing with the estimation of the location of changepoints in onedimensional piecewise constant signals observed in white noise our approach consists in reframing this task in a variable selection context we use a penalized leastsquares criterion with a ltype penalty for this purpose we prove that in an appropriate asymptotic framework this method provides consistent estimators of the changepoints then we explain how to implement this method in practice by combining the lar algorithm and a reduced version of the dynamic programming algorithm and we apply it to synthetic and real data
competition adds complexity competition adds complexity it is known that determinining whether a decpomdp namely a cooperative partially observable stochastic game posg has a cooperative strategy with positive expected reward is complete for nexp it was not known until now how cooperation affected that complexity we show that for competitive posgs the complexity of determining whether one team has a positiveexpectedreward strategy is complete for the class nexp with an oracle for np
compressed regression compressed regression recent research has studied the role of sparsity in high dimensional regression and signal reconstruction establishing theoretical limits for recovering sparse models from sparse data in this paper we study a variant of this problem where the original n input variables are compressed by a random linear transformation to m ll n examples in p dimensions and establish conditions under which a sparse linear model can be successfully recovered from the compressed data a primary motivation for this compression procedure is to anonymize the data and preserve privacy by revealing little information about the original data we characterize the number of random projections that are required for ellregularized compressed regression to identify the nonzero coefficients in the true model with probability approaching one a property called sparsistence in addition we show that ellregularized compressed regression asymptotically predicts as well as an oracle linear model a property called persistence finally we characterize the privacy properties of the compression procedure in informationtheoretic terms establishing upper bounds on the rate of information communicated between the compressed and uncompressed data that decay to zero
computational equivalence of fixed points and no regret algorithms and convergence to equilibria computational equivalence of fixed points and no regret algorithms and convergence to equilibria we study the relation between notions of gametheoretic equilibria which are based on stability under a set of deviations and empirical equilibria which are reached by rational players rational players are modelled by players using no regret algorithms which guarantee that their payoff in the long run is almost as much as the most they could hope to achieve by consistently deviating from the algorithms suggested action we show that for a given set of deviations over the strategy set of a player it is possible to efficiently approximate fixed points of a given deviation if and only if there exist efficient no regret algorithms resistant to the deviations further we show that if all players use a no regret algorithm then the empirical distribution of their plays converges to an equilibrium
computing robust counterstrategies computing robust counterstrategies adaptation to other initially unknown agents often requires computing an effective counterstrategy in the bayesian paradigm one must find a good counterstrategy to the inferred posterior of the other agents behavior in the experts paradigm one may want to choose experts that are good counterstrategies to the other agents expected behavior in this paper we introduce a technique for computing robust counterstrategies for adaptation in multiagent scenarios under a variety of paradigms the strategies can take advantage of a suspected tendency in the decisions of the other agents while bounding the worstcase performance when the tendency is not observed the technique involves solving a modified game and therefore can make use of recently developed algorithms for solving very large extensive games we demonstrate the effectiveness of the technique in twoplayer texas holdem we show that the computed poker strategies are substantially more robust than best response counterstrategies while still exploiting a suspected tendency we also compose the generated strategies in an experts algorithm showing a dramatic improvement in performance over using simple best responses
configuration estimates improve pedestrian finding configuration estimates improve pedestrian finding fair discriminative pedestrian finders are now available in fact these pedestrian finders make most errors on pedestrians in configurations that are uncommon in the training data for example mounting a bicycle this is undesirable however the human configuration can itself be estimated discriminatively using structure learning we demonstrate a pedestrian finder which first finds the most likely human pose in the window using a discriminative procedure trained with structure learning on a small dataset we then present features local histogram of oriented gradient and local pca of gradient based on that configuration to an svm classifier we show using the inria person dataset that estimates of configuration significantly improve the accuracy of a discriminative pedestrian finder
convex relaxations of em convex relaxations of em we investigate a new convex relaxation of expectationmaximization em that approximates a standard objective while eliminating local minima first a cautionary result is presented showing that any convex relaxation of em over hidden variables must give trivial results if any dependence on the missing values is retained although this appears to be a strong negative outcome we then demonstrate how the problem can be bypassed by using equivalence relations instead of value assignments over hidden variables in particular we develop new algorithms for estimating exponential conditional models that only require equivalence relation information over the variable values this reformulation leads to an exact expression for em in a wide range of problems we then develop a semidefinite relaxation that yields effective global training by eliminating local minima
cpr for csps a probabilistic relaxation of constraint propagation cpr for csps a probabilistic relaxation of constraint propagation this paper proposes constraint propagation relaxation cpr a probabilistic approach to classical constraint propagation that provides another view on the whole parametric family of survey propagation algorithms spamp ranging from belief propagation amp to pure survey propagationamp more importantly the approach elucidates the implicit but fundamental assumptions underlying spamp thus shedding some light on its effectiveness and leading to applications beyond ksat
discriminative batch mode active learning discriminative batch mode active learning active learning sequentially selects unlabeled instances to label with the goal of reducing the effort needed to learn a good classifier most previous studies in active learning have focused on selecting one unlabeled instance at one time while retraining in each iteration however single instance selection systems are unable to exploit a parallelized labeler when one is available recently a few batch mode active learning approaches have been proposed that select a set of most informative unlabeled instances in each iteration guided by some heuristic scores in this paper we propose a discriminative batch mode active learning approach that formulates the instance selection task as a continuous optimization problem over auxiliary instance selection variables the optimization is formuated to maximize the discriminative classification performance of the target classifier while also taking the unlabeled data into account although the objective is not convex we can manipulate a quasinewton method to obtain a good local solution our empirical studies on uci datasets show that the proposed active learning is more effective than current stateofthe art batch mode active learning algorithms
discriminative keyword selection using support vector machines discriminative keyword selection using support vector machines many tasks in speech processing involve classification of long term characteristics of a speech segment such as language speaker dialect or topic a natural technique for determining these characteristics is to first convert the input speech into a sequence of tokens such as words phones etc from these tokens we can then look for distinctive phrases keywords that characterize the speech in many applications a set of distinctive keywords may not be known a priori in this case an automatic method of building up keywords from short context units such as phones is desirable we propose a method for construction of keywords based upon support vector machines we cast the problem of keyword selection as a feature selection problem for ngrams of phones we propose an alternating filterwrapper method that builds successively longer keywords application of this method on a language recognition task shows that the technique produces interesting and significant qualitative and quantitative results
efficient multiple hyperparameter learning for loglinear models efficient multiple hyperparameter learning for loglinear models using multiple regularization hyperparameters is an effective method for managing model complexity in problems where input features have varying amounts of noise while algorithms for choosing multiple hyperparameters are often used in neural networks and support vector machines they are not common in structured prediction tasks such as sequence labeling or parsing in this paper we consider the problem of learning regularization hyperparameters for loglinear models a class of probabilistic models for structured prediction tasks which includes conditional random fields crfs using an implicit differentiation trick we derive an efficient gradientbased method for learning gaussian regularization priors with multiple hyperparameters in both simulations and the realworld task of computational rna secondary structure prediction we find that multiple hyperparameter learning provides a significant boost in accuracy compared to models learned using only a single regularization hyperparameter
efficient principled learning of thin junction trees efficient principled learning of thin junction trees we present the first truly polynomial algorithm for learning the structure of boundedtreewidth junction trees an attractive subclass of probabilistic graphical models that permits both the compact representation of probability distributions and efficient exact inference for a constant treewidth our algorithm has polynomial time and sample complexity and provides strong theoretical guarantees in terms of kl divergence from the true distribution we also present a lazy extension of our approach that leads to very significant speed ups in practice and demonstrate the viability of our method empirically on several real world datasets one of our key new theoretical insights is a method for bounding the conditional mutual information of arbitrarily large sets of random variables with only a polynomial number of mutual information computations on fixedsize subsets of variables when the underlying distribution can be approximated by a bounded treewidth junction tree
evaluating search engines by modeling the relationship between relevance and clicks evaluating search engines by modeling the relationship between relevance and clicks we propose a model that leverages the millions of clicks received by web search engines to predict document relevance this allows the comparison of ranking functions when clicks are available but complete relevance judgments are not after an initial training phase using a set of relevance judgments paired with click data we show that our model can predict the relevance score of documents that have not been judged these predictions can be used to evaluate the performance of a search engine using our novel formalization of the confidence of the standard evaluation metric discounted cumulative gain dcg so comparisons can be made across time and datasets this contrasts with previous methods which can provide only pairwise relevance judgements between results shown for the same query when no relevance judgments are available we can identify the better of two ranked lists up to of the time and with only two relevance judgments for each query we can identify the better ranking up to of the time while our experiments are on sponsored search results which is the financial backbone of web search our method is general enough to be applicable to algorithmic web search results as well furthermore we give an algorithm to guide the selection of additional documents to judge to improve confidence
fast variational inference for largescale internet diagnosis fast variational inference for largescale internet diagnosis web servers on the internet need to maintain high reliability but the cause of intermittent failures of web transactions is nonobvious we use bayesian inference to diagnose problems with web services this diagnosis problem is far larger than any previously attempted it requires inference of possible faults from observations further such inference must be performed in less than a second inference can be done at this speed by combining a variational approximation a meanfield approximation and the use of stochastic gradient descent to optimize a variational cost function we use this fast inference to diagnose a time series of anomalous http requests taken from a real web service the inference is fast enough to analyze network logs with billions of entries in a matter of hours
fixing maxproduct convergent message passing algorithms for map lprelaxations fixing maxproduct convergent message passing algorithms for map lprelaxations we present a novel message passing algorithm for approximating the map problem in graphical models the algorithm is similar in structure to maxproduct but unlike maxproduct it always converges and can be proven to find the exact map solution in various settings the algorithm is derived via block coordinate descent in a dual of the lp relaxation of map but does not require any tunable parameters such as step size or tree weights we also describe a generalization of the method to cluster based potentials the new method is tested on synthetic and realworld problems and compares favorably with previous approaches
hmbitam bilingual topic exploration word alignment and translation hmbitam bilingual topic exploration word alignment and translation we present a novel paradigm for statistical machine translation smt based on joint modeling of word alignment and the topical aspects underlying bilingual document pairs via a hidden markov bilingual topic admixture hmbitam in this new paradigm parallel sentencepairs from a parallel documentpair are coupled via a certain semanticflow to ensure coherence of topical context in the alignment of matching words between languages during likelihoodbased training of topicdependent translational lexicons as well as topic representations in each language the resulting trained hmbitam can not only display topic patterns like other methods such as lda but now for bilingual corpora it also offers a principled way of inferring optimal translation in a contextdependent way our method integrates the conventional ibm models based on hmm a key component for most of the stateoftheart smt systems with the recently proposed bitam model and we report an extensive empirical analysis in many way complementary to the descriptionoriented of our method in three aspects word alignment bilingual topic representation and translation
learning bounds for domain adaptation learning bounds for domain adaptation empirical risk minimization offers wellknown learning guarantees when training and test data come from the same domain in the real world though we often wish to adapt a classifier from a source domain with a large amount of training data to different target domain with very little training data in this work we give uniform convergence bounds for algorithms that minimize a convex combination of source and target empirical risk the bounds explicitly model the inherent tradeoff between training on a large but inaccurate source data set and a small but accurate target training set our theory also gives results when we have multiple source domains each of which may have a different number of instances and we exhibit cases in which minimizing a nonuniform combination of source risks can achieve much lower target error than standard empirical risk minimization
learning the structure of manifolds using random projections learning the structure of manifolds using random projections we present a simple variant of the kd tree which automatically adapts to intrinsic low dimensional structure in data
linear programming analysis of loopy belief propagation for weighted matching linear programming analysis of loopy belief propagation for weighted matching loopy belief propagation has been employed in a wide variety of applications with great empirical success but it comes with few theoretical guarantees in this paper we investigate the use of the maxproduct form of belief propagation for weighted matching problems on general graphs we show that maxproduct converges to the correct answer if the linear programming lp relaxation of the weighted matching problem is tight and does not converge if the lp relaxation is loose this provides an exact characterization of maxproduct performance and reveals connections to the widely used optimization technique of lp relaxation in addition we demonstrate that maxproduct is effective in solving practical weighted matching problems in a distributed fashion by applying it to the problem of selforganization in sensor networks
local algorithms for approximate inference in minorexcluded graphs local algorithms for approximate inference in minorexcluded graphs we present a new local approximation algorithm for computing map and logpartition function for arbitrary exponential family distribution represented by a finitevalued pairwise markov random field mrf say g our algorithm is based on decomposing g into appropriately chosen small components computing estimates locally in each of these components and then producing a good global solution we prove that the algorithm can provide approximate solution within arbitrary accuracy when g excludes some finite sized graph as its minor and g has bounded degree all planar graphs with bounded degree are examples of such graphs the running time of the algorithm is thetan n is the number of nodes in g with constant dependent on accuracy degree of graph and size of the graph that is excluded as a minor constant for planar graphs our algorithm for minorexcluded graphs uses the decomposition scheme of klein plotkin and rao in general our algorithm works with any decomposition scheme and provides quantifiable approximation guarantee that depends on the decomposition scheme
loop series and bethe variational bounds in attractive graphical models loop series and bethe variational bounds in attractive graphical models variational methods are frequently used to approximate or bound the partition or likelihood function of a markov random field methods based on mean field theory are guaranteed to provide lower bounds whereas certain types of convex relaxations provide upper bounds in general loopy belief propagation bp provides often accurate approximations but not bounds we prove that for a class of attractive binary models the value specified by any fixed point of loopy bp always provides a lower bound on the true likelihood empirically this bound is much better than the naive mean field bound and requires no further work than running bp we establish these lower bounds using a loop series expansion due to chertkov and chernyak which we show can be derived as a consequence of the tree reparameterization characterization of bp fixed points
mining internetscale software repositories mining internetscale software repositories large repositories of source code create new challenges and opportunities for statistical machine learning here we first develop an infrastructure for the automated crawling parsing and database storage of open source software the infrastructure allows us to gather internetscale source code for instance in one experiment we gather java projects from sourceforge and apache totaling over million lines of code from developers simple statistical analyses of the data first reveal robust powerlaw behavior for package sloc and method call distributions we then develop and apply unsupervised authortopic probabilistic models to automatically discover the topics embedded in the code and extract topicword and authortopic distributions in addition to serving as a convenient summary for program function and developer activities these and other related distributions provide a statistical and informationtheoretic basis for quantifying and analyzing developer similarity and competence topic scattering and document tangling with direct applications to software engineering finally by combining software textual content with structural information captured by our coderank approach we are able to significantly improve software retrieval performance increasing the auc metric to roughly better than previous approaches based on text alone
modeling homophily and stochastic equivalence in symmetric relational data modeling homophily and stochastic equivalence in symmetric relational data this article discusses a latent variable model for inference and prediction of symmetric relational data the model based on the idea of the eigenvalue decomposition represents the relationship between two nodes as the weighted innerproduct of nodespecific vectors of latent characteristics this eigenmodel generalizes other popular latent variable models such as latent class and distance models it is shown mathematically that any latent class or distance model has a representation as an eigenmodel but not viceversa the practical implications of this are examined in the context of three real datasets for which the eigenmodel has as good or better outofsample predictive performance than the other two models
modeling image patches with a directed hierarchy of markov random fields modeling image patches with a directed hierarchy of markov random fields we describe an efficient learning procedure for multilayer generative models that combine the best aspects of markov random fields and deep directed belief nets the generative models can be learned one layer at a time and when learning is complete they have a very fast inference procedure for computing a good approximation to the posterior distribution in all of the hidden layers each hidden layer has its own mrf whose energy function is modulated by the topdown directed connections from the layer above to generate from the model each layer in turn must settle to equilibrium given its topdown input we show that this type of model is good at capturing the statistics of patches of natural images
multistage monte carlo approximation for fast generalized data summations multistage monte carlo approximation for fast generalized data summations in machine learning we often encounter computational bottlenecks in the form of generalized summations over datasets when the computational cost of these methods is on or higher applicability to large datasets is severely limited we present a multistage monte carlo method for approximating such summations with probabilistic relative error control this method differs from many previous scalability techniques such as treebased methods in that its error is stochastic but we derive conditions for error control and demonstrate that they work further we give a theoretical sample complexity for the method that is independent of dataset size and show that this appears to hold in experiments where speedups reach as high as on datasets with points numbering in the millions
multiple instance active learning multiple instance active learning in a multiple instance mi learning problem instances are naturally organized into bags and it is the bags instead of individual instances that are labeled for training mi learners assume that every instance in a bag labeled negative is actually negative whereas at least one instance in a bag labeled positive is actually positive we present a framework for active learning in the multipleinstance setting in particular we consider the case in which an mi learner is allowed to selectively query unlabeled instances in positive bags this approach is well motivated in domains in which it is inexpensive to acquire bag labels and possible but expensive to acquire instance labels we describe a method for learning from labels at mixed levels of granularity and introduce two active query selection strategies motivated by the mi setting our experiments show that learning from instance labels can significantly improve performance of a basic mi learning algorithm in two multipleinstance domains contentbased image recognition and text classification
multipleinstance pruning for learning efficient cascade detectors multipleinstance pruning for learning efficient cascade detectors cascade detectors have been shown to operate extremely rapidly with high accuracy and have important applications such as face detection driven by this success cascade earning has been an area of active research in recent years nevertheless there are still challenging technical problems during the training process of cascade detectors in particular determining the optimal target detection rate for each stage of the cascade remains an unsolved issue in this paper we propose the multiple instance pruning mip algorithm for soft cascades this algorithm computes a set of thresholds which aggressively terminate computation with no reduction in detection rate or increase in false positive rate on the training dataset the algorithm is based on two key insights i examples that are destined to be rejected by the complete classifier can be safely pruned early ii face detection is a multiple instance learning problem the mip process is fully automatic and requires no assumptions of probability distributions statistical independence or ad hoc intermediate rejection targets experimental results on the mitcmu dataset demonstrate significant performance advantages
on ranking in survival analysis bounds on the concordance index on ranking in survival analysis bounds on the concordance index in this paper we show that classical survival analysis involving censored data can naturally be cast as a ranking problem the concordance index ci which quantifies the quality of rankings is the standard performance measure for model emphassessment in survival analysis in contrast the standard approach to emphlearning the popular proportional hazard ph model is based on coxs partial likelihood in this paper we devise two bounds on cione of which emerges directly from the properties of ph modelsand optimize them emphdirectly our experimental results suggest that both methods perform about equally well with our new approach giving slightly better results than the coxs method we also explain why a method designed to maximize the coxs partial likelihood also ends up approximately maximizing the ci
online linear regression and its application to modelbased reinforcement learning online linear regression and its application to modelbased reinforcement learning we provide a provably efficient algorithm for learning markov decision processes mdps with continuous state and action spaces in the online setting specifically we take a modelbased approach and show that a special type of online linear regression allows us to learn mdps with possibly kernalized linearly parameterized dynamics this result builds on kearns and singhs work that provides a provably efficient algorithm for finite state mdps our approach is not restricted to the linear setting and is applicable to other classes of continuous mdps
optimal roc curve for a combination of classifiers optimal roc curve for a combination of classifiers we present a new analysis for the combination of binary classifiers we propose a theoretical framework based on the neymanpearson lemma to analyze combinations of classifiers in particular we give a method for finding the optimal decision rule for a combination of classifiers and prove that it has the optimal roc curve we also show how our method generalizes and improves on previous work on combining classifiers and generating roc curves
optimistic linear programming gives logarithmic regret for irreducible mdps optimistic linear programming gives logarithmic regret for irreducible mdps we present an algorithm called optimistic linear programming olp for learning to optimize average reward in an irreducible but otherwise unknown markov decision process mdp olp uses its experience so far to estimate the mdp it chooses actions by optimistically maximizing estimated future rewards over a set of nextstate transition probabilities that are close to the estimates a computation that corresponds to solving linear programs we show that the total expected reward obtained by olp up to time t is within cplog t of the reward obtained by the optimal policy where cp is an explicit mdpdependent constant olp is closely related to an algorithm proposed by burnetas and katehakis with four key differences olp is simpler it does not require knowledge of the supports of transition probabilities and the proof of the regret bound is simpler but our regret bound is a constant factor larger than the regret of their algorithm olp is also similar in flavor to an algorithm recently proposed by auer and ortner but olp is simpler and its regret bound has a better dependence on the size of the mdp
people tracking with the laplacian eigenmaps latent variable model people tracking with the laplacian eigenmaps latent variable model reliably recovering d human pose from monocular video requires constraints that bias the estimates towards typical human poses and motions we define priors for people tracking using a laplacian eigenmaps latent variable model lelvm lelvm is a probabilistic dimensionality reduction model that naturally combines the advantages of latent variable modelsdefinining a multimodal probability density for latent and observed variables and globally differentiable nonlinear mappings for reconstruction and dimensionality reductionwith those of spectral manifold learning methodsno local optima ability to unfold highly nonlinear manifolds and good practical scaling to latent spaces of high dimension lelvm is computationally efficient simple to learn from sparse training data and compatible with standard probabilistic trackers such as particle filters we analyze the performance of a lelvmbased probabilistic sigma point mixture tracker in several real and synthetic human motion sequences and demonstrate that lelvm provides sufficient constraints for robust operation in the presence of missing noisy and ambiguous image measurements
random projections for manifold learning random projections for manifold learning we propose a novel method for em linear dimensionality reduction of manifold modeled data first we show that with a small number m of em random projections of sample points in realsn belonging to an unknown kdimensional euclidean manifold the intrinsic dimension id of the sample set can be estimated to high accuracy second we rigorously prove that using only this set of random projections we can estimate the structure of the underlying manifold in both cases the number random projections required is linear in k and logarithmic in n meaning that kltmll n to handle practical situations we develop a greedy algorithm to estimate the smallest size of the projection space required to perform manifold learning our method is particularly relevant in distributed sensing systems and leads to significant potential savings in data acquisition storage and transmission costs
random sampling of states in dynamic programming random sampling of states in dynamic programming we combine two threads of research on approximate dynamic programming random sampling of states and using local trajectory optimizers to globally optimize a policy and associated value function this combination allows us to replace a dense multidimensional grid with a much sparser adaptive sampling of states our focus is on finding steady state policies for the deterministic time invariant discrete time control problems with continuous states and actions often found in robotics in this paper we show that we can now solve problems we couldnt solve previously with regular gridbased approaches
rapid inference on a novel andor graph detection segmentation and parsing of articulated deformable objects in cluttered backgrounds rapid inference on a novel andor graph detection segmentation and parsing of articulated deformable objects in cluttered backgrounds in this paper we formulate a novel andor graph representation capable of describing the different configurations of deformable articulated objects such as horses the representation makes use of the summarization principle so that lower level nodes in the graph only pass on summary statistics to the higher level nodes the probability distributions are invariant to position orientation and scale we develop a novel inference algorithm that combined a bottomup process for proposing configurations for horses together with a topdown process for refining and validating these proposalsthe algorithm was applied to the tasks of detecting segmenting and parsing horses to the best of our knowledge no computer vision algorithms are capable of solving all these tasks we demonstrate that the algorithm is fast and achieves the state of the art performance by evaluations on a challenging public dataset our approach can be applied to a range of other problems in machine intelligence
receding horizon differential dynamic programming receding horizon differential dynamic programming the control of highdimensional continuous nonlinear systems is a key problem in reinforcement learning and control local trajectorybased methods using techniques such as differential dynamic programming ddp are not directly subject to the curse of dimensionality but generate only local controllers in this paper we introduce receding horizon ddp rhddp an extension to the classic ddp algorithm which allows us to construct stable and robust controllers based on a library of localcontrol trajectories we demonstrate the effectiveness of our approach on a series of highdimensional control problems using a simulated multilink swimming robot these experiments show that our approach effectively circumvents dimensionality issues and is capable of dealing effectively with problems with at least state and action dimensions
scan strategies for meteorological radars scan strategies for meteorological radars we address the problem of adaptive sensor control in dynamic resourceconstrained sensor networks we focus on a meteorological sensing network comprising radars that can perform sector scanning rather than always scanning degrees we compare three sector scanning strategies the sitandspin strategy always scans degrees the limited lookahead strategy additionally uses the expected environmental state k decision epochs in the future as predicted from kalman filters in its decisionmaking the full lookahead strategy uses all expected future states by casting the problem as a markov decision process and using reinforcement learning to estimate the optimal scan strategy we show that the main benefits of using a lookahead strategy are when there are multiple meteorological phenomena in the environment and when the maximum radius of any phenomenon is sufficiently smaller than the radius of the radars we also show that there is a tradeoff between the average quality with which a phenomenon is scanned and the number of decision epochs before which a phenomenon is rescanned
sparse feature learning for deep belief networks sparse feature learning for deep belief networks unsupervised learning algorithms aim to discover the structure hidden in the data and to learn representations that are more suitable as input to a supervised machine than the raw input many unsupervised methods are based on reconstructing the input from the representation while constraining the representation to have certain desirable properties eg low dimension sparsity etc others are based on approximating density by stochastically reconstructing the input from the representation we describe a novel and efficient algorithm to learn sparse representations and compare it theoretically and experimentally with a similar machines trained probabilistically namely a restricted boltzmann machine we propose a simple criterion to compare and select different unsupervised machines based on the tradeoff between the reconstruction error and the information content of the representation we demonstrate this method by extracting features from a dataset of handwritten numerals and from a dataset of natural image patches we show that by stacking multiple levels of such machines and by training sequentially highorder dependencies between the input variables can be captured
spatial latent dirichlet allocation spatial latent dirichlet allocation in recent years the language model latent dirichlet allocation lda which clusters cooccurring words into topics has been widely appled in the computer vision field however many of these applications have difficulty with modeling the spatial and temporal structure among visual words since lda assumes that a document is a bagofwords it is also critical to properly design words and ldquodocumentsrdquo when using a language model to solve vision problems in this paper we propose a topic model spatial latent dirichlet allocation slda which better encodes spatial structure among visual words that are essential for solving many vision problems the spatial information is not encoded in the value of visual words but in the design of documents instead of knowing the partition of words into documents textita priori the worddocument assignment becomes a random hidden variable in slda there is a generative procedure where knowledge of spatial structure can be flexibly added as a prior grouping visual words which are close in space into the same document we use slda to discover objects from a collection of images and show it achieves better performance than lda
stability bounds for noniid processes stability bounds for noniid processes the notion of algorithmic stability has been used effectively in the past to derive tight generalization bounds a key advantage of these bounds is that they are de signed for specific learning algorithms exploiting their particular properties but as in much of learning theory existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed iid in many machine learning applications however this assumption does not hold the observations received by the learning algorithm often have some inherent temporal dependence which is clear in system diagnosis or time series prediction problems this paper studies the scenario where the observations are drawn from a station ary betamixing sequence which implies a dependence between observations that weaken over time it proves novel stabilitybased generalization bounds that hold even with this more general setting these bounds strictly generalize the bounds given in the iid case we also illustrate their application in the case of several general classes of learning algorithms including support vector regression and kernel ridge regression
supervised topic models supervised topic models we introduce supervised latent dirichlet allocation slda a statistical model of labelled documents the model accommodates a variety of response types we derive a maximumlikelihood procedure for parameter estimation which relies on variational approximations to handle intractable posterior expectations prediction problems motivate this research we use the fitted model to predict response values for new documents we test slda on two realworld problems movie ratings predicted from reviews and web page popularity predicted from text descriptions we illustrate the benefits of slda versus modern regularized regression as well as versus an unsupervised lda analysis followed by a separate regression
support vector machine classification with indefinite kernels support vector machine classification with indefinite kernels in this paper we propose a method for support vector machine classification using indefinite kernels instead of directly minimizing or stabilizing a nonconvex loss function our method simultaneously finds the support vectors and a proxy kernel matrix used in computing the loss this can be interpreted as a robust classification problem where the indefinite kernel matrix is treated as a noisy observation of the true positive semidefinite kernel our formulation keeps the problem convex and relatively large problems can be solved efficiently using the analytic center cutting plane method we compare the performance of our technique with other methods on several data sets
the epochgreedy algorithm for multiarmed bandits with side information the epochgreedy algorithm for multiarmed bandits with side information we present epochgreedy an algorithm for multiarmed bandits with observable side information epochgreedy has the following properties no knowledge of a time horizon t is necessary the regret incurred by epochgreedy is controlled by a sample complexity bound for a hypothesis class the regret scales as ot s or better sometimes much better here s is the complexity term in a sample complexity bound for standard supervised learning
the tradeoffs of large scale learning the tradeoffs of large scale learning this contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms the analysis shows distinct tradeoffs for the case of smallscale and largescale learning problems smallscale learning problems are subject to the usual approximationestimation tradeoff largescale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithms in nontrivial ways
theoretical analysis of heuristic search methods for online pomdps theoretical analysis of heuristic search methods for online pomdps planning in partially observable environments remains a challenging problem despite significant recent advances in offline approximation techniques a few online methods have also been proposed recently and proven to be remarkably scalable but without the theoretical guarantees of their offline counterparts thus it seems natural to try to unify offline and online techniques preserving the theoretical properties of the former and exploiting the scalability of the latter in this paper we provide theoretical guarantees on an anytime algorithm for pomdps which aims to reduce the error made by approximate offline value iteration algorithms through the use of an efficient online searching procedure the algorithm uses search heuristics based on an error analysis of lookahead search to guide the online search towards reachable beliefs with the most potential to reduce error we provide a general theorem showing that these search heuristics are admissible and lead to complete and epsilonoptimal algorithms this is to the best of our knowledge the strongest theoretical result available for online pomdp solution methods we also provide empirical evidence showing that our approach is also practical and can find provably nearoptimal solutions in reasonable time
topmoumoute online natural gradient algorithm topmoumoute online natural gradient algorithm guided by the goal of obtaining an optimization algorithm that is both fast and yielding good generalization we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error the surprising result is that from both the bayesian and frequentist perspectives this can yield the natural gradient direction although that direction can be very expensive to compute we develop an efficient general online approximation to the natural gradient descent which is suited to large scale problems we report experimental results showing much faster convergence in computation time and in number of iterations with tonga topmoumoute online natural gradient algorithm than with stochastic gradient descent even on very large datasets
using deep belief nets to learn covariance kernels for gaussian processes using deep belief nets to learn covariance kernels for gaussian processes we show how to use unlabeled data and a deep belief net dbn to learn a good covariance kernel for a gaussian process we first learn a deep generative model of the unlabeled data using the fast greedy algorithm introduced by hinton etal if the data is highdimensional and highlystructured a gaussian kernel applied to the top layer of features in the dbn works much better than a similar kernel applied to the raw input performance at both regression and classification can then be further improved by using backpropagation through the dbn to discriminatively finetune the covariance kernel
a general boosting method and its application to learning ranking functions for web search a general boosting method and its application to learning ranking functions for web search we present a general boosting method extending functional gradient boosting to optimize complex loss functions that are encountered in many machine learning problems our approach is based on optimization of quadratic upper bounds of the loss functions which allows us to present a rigorous convergence analysis of the algorithm more importantly this general framework enables us to use a standard regression base learner such as decision trees for fitting any loss function we illustrate an application of the proposed method in learning ranking functions for web search by combining both preference data and labeled data for training we present experimental results for web search using data from a commercial search engine that show significant improvements of our proposed methods over some existing methods
a randomized algorithm for large scale support vector learning a randomized algorithm for large scale support vector learning we propose a randomized algorithm for large scale svm learning which solves the problem by iterating over random subsets of the data crucial to the algorithm for scalability is the size of the subsets chosen in the context of text classification we show that by using ideas from random projections a sample size of olog n can be used to obtain a solution which is close to the optimal with a high probability experiments done on synthetic and real life data sets demonstrate that the algorithm scales up svm learners without loss in accuracy
a risk minimization principle for a class of parzen estimators a risk minimization principle for a class of parzen estimators this paper explores the use of a maximal average margin mam optimality principle for the design of learning algorithms it is shown that the application of this risk minimization principle results in a class of computationally simple learning machines similar to the classical parzen window classifier a direct relation with the rademacher complexities is established as such facilitating analysis and providing a notion of certainty of prediction this analysis is related to support vector machines by means of a margin transformation the power of the mam principle is illustrated further by application to ordinal regression tasks resulting in an on algorithm able to process large datasets in reasonable time
diffrac a discriminative and flexible framework for clustering diffrac a discriminative and flexible framework for clustering we present a novel linear clustering framework diffrac which relies on a linear discriminative cost function and a convex relaxation of a combinatorial optimization problem the large convex optimization problem is solved through a sequence of lower dimensional singular value decompositions this framework has several attractive properties although apparently similar to kmeans it exhibits superior clustering performance than kmeans in particular in terms of robustness to noise it can be readily extended to non linear clustering if the discriminative cost function is based on positive definite kernels and can then be seen as an alternative to spectral clustering prior information on the partition is easily incorporated leading to stateoftheart performance for semisupervised learning for clustering or classification we present empirical evaluations of our algorithms on synthetic and real mediumscale datasets
direct importance estimation with model selection and its application to covariate shift adaptation direct importance estimation with model selection and its application to covariate shift adaptation when training and test samples follow different input distributions ie the situation called emphcovariate shift the maximum likelihood estimator is known to lose its consistency for regaining consistency the loglikelihood terms need to be weighted according to the emphimportance ie the ratio of test and training input densities thus accurately estimating the importance is one of the key tasks in covariate shift adaptation a naive approach is to first estimate training and test input densities and then estimate the importance by the ratio of the density estimates however since density estimation is a hard problem this approach tends to perform poorly especially in high dimensional cases in this paper we propose a direct importance estimation method that does not require the input density estimates our method is equipped with a natural model selection procedure so tuning parameters such as the kernel width can be objectively optimized this is an advantage over a recently developed method of direct importance estimation simulations illustrate the usefulness of our approach
discriminative kmeans for clustering discriminative kmeans for clustering we present a theoretical study on the discriminative clustering framework recently proposed for simultaneous subspace selection via linear discriminant analysis lda and clustering empirical results have shown its favorable performance in comparison with several other popular clustering algorithms however the inherent relationship between subspace selection and clustering in this framework is not well understood due to the iterative nature of the algorithm we show in this paper that this iterative subspace selection and clustering is equivalent to kernel kmeans with a specific kernel gram matrix this provides significant and new insights into the nature of this subspace selection procedure based on this equivalence relationship we propose the discriminative kmeans diskmeans algorithm for simultaneous lda subspace selection and clustering as well as an automatic parameter estimation procedure we also present the nonlinear extension of diskmeans using kernels we show that the learning of the kernel matrix over a convex set of prespecified kernel matrices can be incorporated into the clustering formulation the connection between diskmeans and several other clustering algorithms is also analyzed the presented theories and algorithms are evaluated through experiments on a collection of benchmark data sets
efficient convex relaxation for transductive support vector machine efficient convex relaxation for transductive support vector machine we consider the problem of support vector machine transduction which involves a combinatorial problem with exponential computational complexity in the number of unlabeled examples although several studies are devoted to transductive svm they suffer either from the high computation complexity or from the solutions of local optimum to address this problem we propose solving transductive svm via a convex relaxation which converts the nphard problem to a semidefinite programming compared with the other sdp relaxation for transductive svm the proposed algorithm is computationally more efficient with the number of free parameters reduced from on to on where n is the number of examples empirical study with several benchmark data sets shows the promising performance of the proposed algorithm in comparison with other stateoftheart implementations of transductive svm
fast and scalable training of semisupervised crfs with application to activity recognition fast and scalable training of semisupervised crfs with application to activity recognition we present a new and efficient semisupervised training method for parameter estimation and feature selection in conditional random fields crfs in realworld applications such as activity recognition unlabeled sensor traces are relatively easy to obtain whereas labeled examples are expensive and tedious to collect furthermore the ability to automatically select a small subset of discriminatory features from a large pool can be advantageous in terms of computational speed as well as accuracy in this paper we introduce the semisupervised virtual evidence boosting sveb algorithm for training crfs a semisupervised extension to the recently developed virtual evidence boosting veb method for feature selection and parameter learning semisupervised veb takes advantage of the unlabeled data via minimum entropy regularization the objective function combines the unlabeled conditional entropy with labeled conditional pseudolikelihood the sveb algorithm reduces the overall system cost as well as the human labeling cost required during training which are both important considerations in building real world inference systems in a set of experiments on synthetic data and real activity traces collected from wearable sensors we illustrate that our algorithm benefits from both the use of unlabeled data and automatic feature selection and outperforms other semisupervised training approaches
fitted qiteration in continuous actionspace mdps fitted qiteration in continuous actionspace mdps we consider continuous state continuous action batch reinforcement learning where the goal is to learn a good policy from a sufficiently rich trajectory generated by another policy we study a variant of fitted qiteration where the greedy action selection is replaced by searching for a policy in a restricted set of candidate policies by maximizing the average action values we provide a rigorous theoretical analysis of this algorithm proving what we believe is the first finitetime bounds for valuefunction based algorithms for continuous state and actionspace problems
gaussian process models for link analysis and transfer learning gaussian process models for link analysis and transfer learning in this paper we develop a gaussian process gp framework to model a collection of reciprocal random variables defined on the emphedges of a network we show how to construct gp priors iecovariance functions on the edges of directed undirected and bipartite graphs the model suggests an intimate connection between emphlink prediction and emphtransfer learning which were traditionally considered two separate research topics though a straightforward gp inference has a very high complexity we develop an efficient learning algorithm that can handle a large number of observations the experimental results on several realworld data sets verify superior learning capacity
multitask learning via conic programming multitask learning via conic programming when we have several related tasks solving them simultaneously is shown to be more effective than solving them individually this approach is called multitask learning mtl and has been studied extensively existing approaches to mtl often treat all the tasks as emphuniformly related to each other and the relatedness of the tasks is controlled globally for this reason the existing methods can lead to undesired solutions when some tasks are not highly related to each other and some pairs of related tasks can have significantly different solutions in this paper we propose a novel mtl algorithm that can overcome these problems our method makes use of a task network which describes the relation structure among tasks this allows us to deal with intricate relation structures in a systematic way furthermore we control the relatedness of the tasks locally so all pairs of related tasks are guaranteed to have similar solutions we apply the above idea to support vector machines svms and show that the optimization problem can be cast as a second order cone program which is convex and can be solved efficiently the usefulness of our approach is demonstrated through simulations with protein superfamily classification and ordinal regression problems
on higherorder perceptron algorithms on higherorder perceptron algorithms a new algorithm for online learning linearthreshold functions is proposed which efficiently combines secondorder statistics about the data with the quotlogarithmic behaviorquot of multiplicativedualnorm algorithms an initial theoretical analysis is provided suggesting that our algorithm might be viewed as a standard perceptron algorithm operating on a transformed sequence of examples with improved margin properties we also report on experiments carried out on datasets from diverse domains with the goal of comparing to known perceptron algorithms firstorder secondorder additive multiplicative our learning procedure seems to generalize quite well and converges faster than the corresponding multiplicative baseline algorithms
parallelizing support vector machines on distributed computers parallelizing support vector machines on distributed computers support vector machines svms suffer from a widely recognized scalability problem in both memory use and computational time to improve scalability we have developed a parallel svm algorithm psvm which reduces memory use through performing a rowbased approximate matrix factorization and which loads only essential data to each machine to perform parallel computation let n denote the number of training instances p the reduced matrix dimension after factorization p is significantly smaller than n and m the number of machines psvm reduces the memory requirement from mon to monpm and improves computation time to monpm empirical studies on up to computers shows psvm to be effective
predictive matrixvariate t models predictive matrixvariate t models it is becoming increasingly important to learn from a partiallyobserved random matrix and predict its missing elements we assume that the entire matrix is a single sample drawn from a matrixvariate t distribution and suggest a matrixvariate t model mvtm to predict those missing elements we show that mvtm generalizes a range of known probabilistic models and automatically performs model selection to encourage sparse predictive models due to the nonconjugacy of its prior it is difficult to make predictions by computing the mode or mean of the posterior distribution we suggest an optimization method that sequentially minimizes a convex upperbound of the loglikelihood which is very efficient and scalable the experiments on a toy data and eachmovie dataset show a good predictive accuracy of the model
progressive mixture rules are deviation suboptimal progressive mixture rules are deviation suboptimal we consider the learning task consisting in predicting as well as the best function in a finite reference set g up to the smallest possible additive term if rg denotes the generalization error of a prediction function g under reasonable assumptions on the loss function typically satisfied by the least square loss when the output is bounded it is known that the progressive mixture rule gn satisfies e rgn lt ming in g rg cst loggn where n denotes the size of the training set e denotes the expectation wrt the training set distribution this work shows that surprisingly for appropriate reference sets g the deviation convergence rate of the progressive mixture rule is only no better than cst sqrtn and not the expected cst n it also provides an algorithm which does not suffer from this drawback
regularized boost for semisupervised learning regularized boost for semisupervised learning semisupervised inductive learning concerns how to learn a decision rule from a data set containing both labeled and unlabeled data several boosting algorithms have been extended to semisupervised learning with various strategies to our knowledge however none of them takes local smoothness constraints among data into account during ensemble learning in this paper we introduce a local smoothness regularizer to semisupervised boosting algorithms based on the universal optimization framework of margin cost functionals our regularizer is applicable to existing semisupervised boosting algorithms to improve their generalization and speed up their training comparative results on synthetic benchmark and real world tasks demonstrate the effectiveness of our local smoothness regularizer we discuss relevant issues and relate our regularizer to previous work
regulator discovery from gene expression time series of malaria parasites a hierachical approach regulator discovery from gene expression time series of malaria parasites a hierachical approach we introduce a hierarchical bayesian model for the discovery of putative regulators from gene expression data only the hierarchy incorporates the knowledge that there are just a few regulators that by themselves only regulate a handful of genes this is implemented through a socalled spikeandslab prior a mixture of gaussians with different widths with mixing weights from a hierarchical bernoulli model for efficient inference we implemented expectation propagation running the model on a malaria parasite data set we found four genes with significant homology to transcription factors in an amoebe one rna regulator and three genes of unknown function out of the top ten genes considered
robust regression with twinned gaussian processes robust regression with twinned gaussian processes we propose a gaussian process gp framework for robust inference in which a gp prior on the mixing weights of a twocomponent noise model augments the standard process over latent function values this approach is a generalization of the mixture likelihood used in traditional robust gp regression and a specialization of the gp mixture models suggested by tresp and rasmussen and ghahramani the value of this restriction is in its tractable expectation propagation updates which allow for faster inference and model selection and better convergence than the standard mixture an additional benefit over the latter method lies in our ability to incorporate knowledge of the noise domain to influence predictions and to recover with the predictive distribution information about the outlier distribution via the gating process the model has asymptotic complexity equal to that of conventional robust methods but yields more confident predictions on benchmark problems than classical heavytailed models and exhibits improved stability for data with clustered corruptions for which they fail altogether we show further how our approach can be used without adjustment for more smoothly heteroscedastic data and suggest how it could be extended to more general noise models we also address similarities with the work of goldberg et al and the more recent contributions of tresp and rasmussen and ghahramani
sparse overcomplete latent variable decomposition of counts data sparse overcomplete latent variable decomposition of counts data an important problem in many fields is the analysis of counts data to extract meaningful latent components methods like probabilistic latent semantic analysis plsa and latent dirichlet allocation lda have been proposed for this purpose however they are limited in the number of components they can extract and also do not have a provision to control the quotexpressivenessquot of the extracted components in this paper we present a learning formulation to address these limitations by employing the notion of sparsity we start with the plsa framework and use an entropic prior in a maximum a posteriori formulation to enforce sparsity we show that this allows the extraction of overcomplete sets of latent components which better characterize the data we present experimental evidence of the utility of such representations
temporal difference with eligibility traces derived from first principles temporal difference with eligibility traces derived from first principles we derive an equation for temporal difference learning from statistical first principles specifically we start with the variational principle and then bootstrap to produce an updating rule for discounted state value estimates the resulting equation is similar to the standard equation for temporal difference learning with eligibility traces so called tdamp however it lacks the parameter amp that specifies the learning rate in the place of this free parameter there is now an equation for the learning rate that is specific to each state transition we experimentally test this new learning rule against tdamp and find that it offers superior performance in various settings finally we make some preliminary investigations into how to extend our new temporal difference algorithm to reinforcement learning to do this we combine our update equation with both watkinrsquos qamp and sarsaamp and find that it again offers superior performance with fewer parameters
testing for homogeneity with kernel fisher discriminant analysis testing for homogeneity with kernel fisher discriminant analysis we propose to test for the homogeneity of two samples by using kernel fisher discriminant analysis this provides us with a consistent nonparametric test statistic for which we derive the asymptotic distribution under the null hypothesis we give experimental evidence of the relevance of our method on both artificial and real datasets
the distribution family of similarity distances the distribution family of similarity distances assessing similarity between features is a key step in object recognition and scene categorization tasks we argue that knowledge on the distribution of distances generated by similarity functions is crucial in deciding whether features are similar or not intuitively one would expect that similarities between features could arise from any distribution in this paper we will derive the contrary and report the theoretical result that lpnorms a class of commonly applied distance metrics from one feature vector to other vectors are weibulldistributed if the feature values are correlated and nonidentically distributed besides these assumptions being realistic for images we experimentally show them to hold for various popular feature extraction algorithms for a diverse range of images this fundamental insight opens new directions in the assessment of feature similarity with projected improvements in object and scene recognition algorithms
the infinite gammapoisson feature model the infinite gammapoisson feature model we address the problem of factorial learning which associates a set of latent causes or features with the observed data factorial models usually assume that each feature has a single occurrence in a given data point however there are data such as images where latent features have multiple occurrences eg a visual object class can have multiple instances shown in the same image to deal with such cases we present a probability model over nonnegative integer valued matrices with possibly unbounded number of columns this model can play the role of the prior in an nonparametric bayesian learning scenario where both the latent features and the number of their occurrences are unknown we use this prior together with a likelihood model for unsupervised learning from images using a markov chain monte carlo inference algorithm
variational inference for diffusion processes variational inference for diffusion processes diffusion processes are a family of continuoustime continuousstate stochastic processes that are in general only partially observed the joint estimation of the forcing parameters and the system noise volatility in these dynamical systems is a crucial but nontrivial task especially when the system is nonlinear and multimodal we propose a variational treatment of diffusion processes which allows us to estimate these parameters by simple gradient techniques and which is computationally less demanding than most mcmc approaches furthermore our parameter inference scheme does not break down when the time step gets smaller unlike most current approaches finally we show how a cheap estimate of the posterior over the parameters can be constructed based on the variational free energy
variational inference for markov jump processes variational inference for markov jump processes markov jump processes play an important role in a large number of application domains however realistic systems are analytically intractable and they have traditionally been analysed using simulation based techniques which do not provide a framework for statistical inference we propose a mean field approximation to perform posterior inference and parameter estimation the approximation allows a practical solution to the inference problem while still retaining a good degree of accuracy we illustrate our approach on two biologically motivated systems
the noisylogical distribution and its application to causal inference the noisylogical distribution and its application to causal inference we describe a novel noisylogical distribution for representing the distribution of a binary output variable conditioned on multiple binary input variables the distribution is represented in terms of noisyors and noisyandnots of causal features which are conjunctions of the binary inputs the standard noisyor and noisyandnot models used in causal reasoning and artificial intelligence are special cases of the noisylogical distribution we prove that the noisylogical distribution is complete in the sense that it can represent all conditional distributions provided a sufficient number of causal factors are used we illustrate the noisylogical distribution by showing that it can account for new experimental findings on how humans perform causal reasoning in more complex contexts finally we speculate on the use of the noisylogical distribution for causal reasoning and artificial intelligence
a bayesian model of conditioned perception a bayesian model of conditioned perception we propose an extended probabilistic model for human perception we argue that in many circumstances human observers simultaneously evaluate sensory evidence under different hypotheses regarding the underlying physical process that might have generated the sensory information within this context inference can be optimal if the observer weighs each hypothesis according to the correct belief in that hypothesis but if the observer commits to a particular hypothesis the belief in that hypothesis is converted into subjective certainty and subsequent perceptual behavior is suboptimal conditioned only on the chosen hypothesis we demonstrate that this framework can explain psychophysical data of a recently reported decisionestimation experiment the model well accounts for the data predicting the same estimation bias as a consequence of the preceding decision step the power of the framework is that it has no free parameters except the degree of the observers uncertainty about its internal sensory representation all other parameters are defined by the particular experiment which allows us to make quantitative predictions of human perception to two modifications of the original experiment
a complexity measure for intuitive theories a complexity measure for intuitive theories much of human knowledge is organized into sophisticated systems that are often called intuitive theories we propose that intuitive theories are mentally represented in a logical language and that the subjective complexity of a theory is determined by the length of its representation in this language this complexity measure helps to explain how theories are learned from relational data and how theories support inductive inferences about unobserved relations we describe two behavioral experiments that test our approach and show that our measure accounts better for subjective complexity than a measure developed by nelson goodman
a configurable analog vlsi neural network with spiking neurons and selfregulating plastic synapses a configurable analog vlsi neural network with spiking neurons and selfregulating plastic synapses we summarize the implementation of an analog vlsi chip hosting a network of integrateandfire if neurons with spikefrequency adaptation and hebbian plastic bistable spikedriven stochastic synapses endowed with a selfregulating mechanism which stops unnecessary synaptic changes the synaptic matrix can be flexibly configured and provides both recurrent and aerbased connectivity with external aer compliant devices we demonstrate the ability of the network to efficiently classify overlapping patterns thanks to the selfregulating mechanism
an online hebbian learning rule that performs independent component analysis an online hebbian learning rule that performs independent component analysis independent component analysis ica is a powerful method to decouple signals most of the algorithms performing ica do not consider the temporal correlations of the signal but only higher moments of its amplitude distribution moreover they require some preprocessing of the data whitening so as to remove second order correlations in this paper we are interested in understanding the neural mechanism responsible for solving ica we present an online learning rule that exploits delayed correlations in the input this rule performs ica by detecting joint variations in the firing rates of pre and postsynaptic neurons similar to a local ratebased hebbian learning rule
bayesian binning beats approximate alternatives estimating peristimulus time histograms bayesian binning beats approximate alternatives estimating peristimulus time histograms the peristimulus time historgram psth and its more continuous cousin the spike density function sdf are staples in the analytic toolkit of neurophysiologists the former is usually obtained by binning spiketrains whereas the standard method for the latter is smoothing with a gaussian kernel selection of a bin with or a kernel size is often done in an relatively arbitrary fashion even though there have been recent attempts to remedy this situation citeshimazakibinningnipsshimazakibinningneco we develop an exact bayesian generative model approach to estimating pshts and demonstate its superiority to competing methods further advantages of our scheme include automatic complexity control and error bars on its predictions
continuous time particle filtering for fmri continuous time particle filtering for fmri we construct a biologically motivated stochastic differential model of the neural and hemodynamic activity underlying the observed blood oxygen level dependent bold signal in functional magnetic resonance imaging fmri the model poses a difficult parameter estimation problem both theoretically due to the nonlinearity and divergence of the differential system and computationally due to its time and space complexity we adapt a particle filter and smoother to the task and discuss some of the practical approaches used to tackle the difficulties including use of sparse matrices and parallelisation results demonstrate the tractability of the approach in its application to an effective connectivity study
eegbased braincomputer interaction improved accuracy by automatic singletrial error detection eegbased braincomputer interaction improved accuracy by automatic singletrial error detection braincomputer interfaces bcis as any other interaction modality based on physiological signals and body channels eg muscular activity speech and gestures are prone to errors in the recognition of subjects intent an elegant approach to improve the accuracy of bcis consists in a verification procedure directly based on the presence of errorrelated potentials errp in the eeg recorded right after the occurrence of an error six healthy volunteer subjects with no prior bci experience participated in a new humanrobot interaction experiment where they were asked to mentally move a cursor towards a target that can be reached within a few steps using motor imagination this experiment confirms the previously reported presence of a new kind of errp these quotinteraction errpquot exhibit a first sharp negative peak followed by a positive peak and a second broader negative peak and ms after the feedback respectively but in order to exploit these errp we need to detect them in each single trial using a short window following the feedback associated to the response of the classifier embedded in the bci we have achieved an average recognition rate of correct and erroneous single trials of and respectively furthermore we have achieved an average recognition rate of the subjects intent while trying to mentally drive the cursor of these results show that its possible to simultaneously extract useful information for mental control to operate a brainactuated device as well as cognitive states such as error potentials to improve the quality of the braincomputer interaction finally using a wellknown inverse model sloreta we show that the main focus of activity at the occurrence of the errp are as expected in the presupplementary motor area and in the anterior cingulate cortex
estimating disparity with confidence from energy neurons estimating disparity with confidence from energy neurons binocular fusion takes place over a limited region smaller than one degree of visual angle panums fusional area which is on the order of the range of preferred disparities measured in populations of disparitytuned neurons in the visual cortex however the actual range of binocular disparities encountered in natural scenes ranges over tens of degrees this discrepancy suggests that there must be a mechanism for detecting whether the stimulus disparity is either inside or outside of the range of the preferred disparities in the population here we present a statistical framework to derive feature in a population of v disparity neuron to determine the stimulus disparity within the preferred disparity range of the neural population when optimized for natural images it yields a feature that can be explained by the normalization which is a common model in v neurons we further makes use of the feature to estimate the disparity in natural images our proposed model generates more correct estimates than coarsetofine multiple scales approaches and it can also identify regions with occlusion the approach suggests another critical role for normalization in robust disparity estimation
grift a graphical model for inferring visual classification features from human data grift a graphical model for inferring visual classification features from human data this paper describes a new model for human visual classification that enables the recovery of image features that explain human subjects performance on different visual classification tasks unlike previous methods this algorithm does not model their performance with a single linear classifier operating on raw image pixels instead it models classification as the combination of multiple feature detectors this approach extracts more information about human visual classification than has been previously possible with other methods and provides a foundation for further exploration
kernels on attributed pointsets with applications kernels on attributed pointsets with applications this paper introduces kernels on attributed pointsets which are sets of vectors embedded in an euclidean space the embedding gives the notion of neighborhood which is used to define positive semidefinite kernels on pointsets two novel kernels on neighborhoods are proposed one evaluating the attribute similarity and the other evaluating shape similarity shape similarity function is motivated from spectral graph matching techniques the kernels are tested on three real life applications face recognition photo album tagging and shot annotation in video sequences with encouraging results
learning horizontal connections in a sparse coding model of natural images learning horizontal connections in a sparse coding model of natural images it has been shown that adapting a dictionary of basis functions to the statistics of natural images so as to maximize sparsity in the coefficients results in a set of dictionary elements whose spatial properties resemble those of v primary visual cortex receptive fields however the resulting sparse coefficients still exhibit pronounced statistical dependencies thus violating the independence assumption of the sparse coding model here we propose a model that attempts to capture the dependencies among the basis function coefficients by including a pairwise coupling term in the prior over the coefficient activity states when adapted to the statistics of natural images the coupling terms learn a combination of facilitatory and inhibitory interactions among neighboring basis functions these learned interactions may offer an explanation for the function of horizontal connections in v and we discuss the implications of our findings for physiological experiments
learning the d topology of images learning the d topology of images we study the following question is the twodimensional structure of images a very strong prior or is it something that can be learned with a few examples of natural images if someone gave us a learning task involving images for which the twodimensional topology of pixels was not known could we discover it automatically and exploit it for example suppose that the pixels had been permuted in a fixed but unknown way could we recover the relative twodimensional location of pixels on images the surprising result presented here is that not only the answer is yes but that about as few as a thousand images are enough to approximately recover the relative locations of about a thousand pixels this is achieved using a manifold learning algorithm applied to pixels associated with a measure of distributional similarity between pixel intensities we compare different topologyextraction approaches and show how having the twodimensional topology can be exploited
locality and lowdimensions in the prediction of natural experience from fmri locality and lowdimensions in the prediction of natural experience from fmri functional magnetic resonance imaging fmri provides an unprecedented window into the complex functioning of the human brain typically detailing the activity of thousands of voxels during hundreds of sequential time points unfortunately the interpretation of fmri is complicated due both to the relatively unknown connection between the hemodynamic response and neural activity and the unknown spatiotemporal characteristics of the cognitive patterns themselves here we use data from the experience based cognition competition to compare global and local methods of prediction applying both linear and nonlinear techniques of dimensionality reduction we build global low dimensional representations of an fmri dataset using linear and nonlinear methods we learn a set of time series that are implicit functions of the fmri data and predict the values of these times series in the future from the knowledge of the fmri data only we find effective lowdimensional models based on the principal components of cognitive activity in classicallydefined anatomical regions the brodmann areas furthermore for some of the stimuli the top predictive regions were stable across subjects and episodes including wernickeotildes area for verbal instructions visual cortex for facial and body features and visualtemporal regions brodmann area for velocity these interpretations and the relative simplicity of our approach provide a transparent and conceptual basis upon which to build more sophisticated techniques for fmri decoding to our knowledge this is the first time that classical areas have been used in fmri for an effective prediction of complex natural experience
modeling natural sounds with modulation cascade processes modeling natural sounds with modulation cascade processes natural sounds are structured on many timescales a typical segment of speech for example contains features that span four orders of magnitude sentences s phonemes s glottal pulses s and formants lts the auditory system uses information from each of these timescales to solve complicated tasks such as auditory scene analysis one route toward understanding how auditory processing accomplishes this analysis is to build neuroscienceinspired algorithms which solve similar tasks and to compare the properties of these algorithms with properties of auditory processing there is however a discord current machineaudition algorithms largely concentrate on the shorter timescale structures in sounds and the longer structures are ignored the reason for this is twofold firstly it is a difficult technical problem to construct an algorithm that utilises both sorts of information secondly it is computationally demanding to simultaneously process data both at high resolution to extract short temporal information and for long duration to extract long temporal information the contribution of this work is to develop a new statistical model for natural sounds that captures structure across a wide range of timescales and to provide efficient learning and inference algorithms we demonstrate the success of this approach on a missing data task
modelling motion primitives and their timing in biologically executed movements modelling motion primitives and their timing in biologically executed movements biological movement is built up of subblocks or motion primitives such primitives provide a compact representation of movement which is also desirable in robotic control applications we analyse handwriting data to gain a better understanding of use of primitives and their timings in biological movements inference of the shape and the timing of primitives can be done using a factorial hmm based model allowing the handwriting to be represented in primitive timing space this representation provides a distribution of spikes corresponding to the primitive activations which can also be modelled using hmm architectures we show how the coupling of the low level primitive model and the higher level timing model during inference can produce good reconstructions of handwriting with shared primitives for all characters modelled this coupled model also captures the variance profile of the dataset which is accounted for by spike timing jitter the timing code provides a compact representation of the movement while generating a movement without an explicit timing model produces a scribbling style of output
on sparsity and overcompleteness in image models on sparsity and overcompleteness in image models computational models of visual cortex and in particular those based on sparse coding have enjoyed much recent attention despite this currency the question of how sparse or how overcomplete a sparse representation should be has gone without principled answer here we use bayesian modelselection methods to address these questions for a sparsecoding model based on a studentt prior having validated our methods on toy data we find that natural images are indeed best modelled by extremely sparse distributions although for the studentt prior the associated optimal basis size is only modestly overcomplete
optimal models of sound localization by barn owls optimal models of sound localization by barn owls sound localization by barn owls is commonly modeled as a matching procedure where localization cues derived from auditory inputs are compared to stored templates while the matching models can explain properties of neural responses no model explains how the owl resolves spatial ambiguity in the localization cues to produce accurate localization near the center of gaze here we examine two models for the barn owls sound localization behavior first we consider a maximum likelihood estimator in order to further evaluate the cue matching model second we consider a maximum a posteriori estimator to test if a bayesian model with a prior that emphasizes directions near the center of gaze can reproduce the owls localization behavior we show that the maximum likelihood estimator can not reproduce the owls behavior while the maximum a posteriori estimator is able to match the behavior this result suggests that the standard cue matching model will not be sufficient to explain sound localization behavior in the barn owl the bayesian model provides a new framework for analyzing sound localization in the barn owl and leads to predictions about the owls localization behavior
predicting brain states from fmri data incremental functional principal component regression predicting brain states from fmri data incremental functional principal component regression we propose a method for reconstruction of human brain states directly from functional neuroimaging data the method extends the traditional multivariate regression analysis of discretized fmri data to the domain of stochastic functional measurements facilitating evaluation of brain responses to naturalistic stimuli and boosting the power of functional imaging the method searches for sets of voxel timecourses that optimize a multivariate functional linear model in terms of rsquarestatistic population based incremental learning is used to search for spatially distributed voxel clusters taking into account the variation in haemodynamic lag across brain areas and among subjects by voxelwise nonlinear registration of stimuli to fmri data the method captures spatially distributed brain responses to naturalistic stimuli without attempting to localize function application of the method for prediction of naturalistic stimuli from new and unknown fmri data shows that the approach is capable of identifying distributed clusters of brain locations that are highly predictive of a specific stimuli
predicting human gaze using lowlevel saliency combined with face detection predicting human gaze using lowlevel saliency combined with face detection under natural viewing conditions human observers shift their gaze to allocate processing resources to subsets of the visual input many computational models have aimed at predicting such voluntary attentional shifts although the importance of high level stimulus properties higher order statistics semantics stands undisputed most models are based on lowlevel features of the input alone in this study we recorded eyemovements of human observers while they viewed photographs of natural scenes about two thirds of the stimuli contained at least one person we demonstrate that a combined model of face detection and lowlevel saliency clearly outperforms a lowlevel model in predicting locations humans fixate this is reflected in our finding fact that observes even when not instructed to look for anything particular fixate on a face with a probability of over within their first two fixations ms remarkably the models predictive performance in images that do not contain faces is not impaired by spurious face detector responses which is suggestive of a bottomup mechanism for face detection in summary we provide a novel computational approach which combines high level object knowledge in our case face locations with lowlevel features to successfully predict the allocation of attentional resources
receptive fields without spiketriggering receptive fields without spiketriggering stimulus selectivity of sensory neurons is often characterized by estimating their receptive field properties such as orientation selectivity receptive fields are usually derived from the mean or covariance of the spiketriggered stimulus ensemble this approach treats each spike as an independent message but does not take into account that information might be conveyed through patterns of neural activity that are distributed across space or time can we find a concise description for the processing of a whole population of neurons analogous to the receptive field for single neurons here we present a generalization of the linear receptive field which is not bound to be triggered on individual spikes but can be meaningfully linked to distributed response patterns more precisely we seek to identify those stimulus features and the corresponding patterns of neural activity that are most reliably coupled we use an extension of reversecorrelation methods based on canonical correlation analysis the resulting population receptive fields span the subspace of stimuli that is most informative about the population response we evaluate our approach using both neuronal models and multielectrode recordings from rabbit retinal ganglion cells we show how the model can be extended to capture nonlinear stimulusresponse relationships using kernel canonical correlation analysis which makes it possible to test different coding mechanisms our technique can also be used to calculate receptive fields from multidimensional neural measurements such as those obtained from dynamic imaging methods
second order bilinear discriminant analysis for single trial eeg analysis second order bilinear discriminant analysis for single trial eeg analysis traditional analysis methods for singletrial classification of electroencephalography eeg focus on two types of paradigms phase locked methods in which the amplitude of the signal is used as the feature for classification ie event related potentials and second order methods in which the feature of interest is the power of the signal ie event related desynchronization the process of deciding which paradigm to use is ad hoc and is driven by knowledge of neurological findings here we propose a unified method in which the algorithm learns the best first and second order spatial and temporal features for classification of eeg based on a bilinear model the efficiency of the method is demonstrated in simulated and real eeg from a benchmark data set for brain computer interface
simplified rules and theoretical analysis for information bottleneck optimization and pca with spiking neurons simplified rules and theoretical analysis for information bottleneck optimization and pca with spiking neurons we show that under suitable assumptions primarily linearization a simple and perspicuous online learning rule for information bottleneck optimization with spiking neurons can be derived this rule performs on common benchmark tasks as well as a rather complex rule that has previously been proposed citeklampfletalb furthermore the transparency of this new learning rule makes a theoretical analysis of its convergence properties feasible a variation of this learning rule with sign changes provides a theoretically founded method for performing principal component analysis pca with spiking neurons by applying this rule to an ensemble of neurons different principal components of the input can be extracted in addition it is possible to preferentially extract those principal components from incoming signals x that are related or are not related to some additional target signal yt in a biological interpretation this target signal yt also called relevance variable could represent proprioceptive feedback input from other sensory modalities or topdown signals
sparse deep belief net model for visual area v sparse deep belief net model for visual area v motivated in part by the hierarchical organization of cortex a number of algorithms have recently been proposed that try to learn hierarchical or deep structure from unlabeled data while several authors have formally or informally compared their algorithms to computations performed in visual area v and the cochlea little attempt has been made thus far to evaluate these algorithms in terms of their fidelity for mimicking computations at deeper levels in the cortical hierarchy this paper presents an unsupervised learning model that faithfully mimics certain properties of visual area v specifically we develop a sparse variant of the deep belief networks of hinton et al we learn two layers of nodes in the network and demonstrate that the first layer similar to prior work on sparse coding and ica results in localized oriented edge filters similar to the gabor functions known to model v cell receptive fields further the second layer in our model encodes correlations of the first layer responses in the data specifically it picks up both collinear contour features as well as corners and junctions more interestingly in a quantitative comparison the encoding of these more complex corner features matches well with the results from the ito amp komatsus study of biological v responses this suggests that our sparse variant of deep belief networks holds promise for modeling more higherorder features
the discriminant centersurround hypothesis for bottomup saliency the discriminant centersurround hypothesis for bottomup saliency the classical hypothesis that bottomup saliency is a centersurround process is combined with a more recent hypothesis that all saliency decisions are optimal in a decisiontheoretic sense the combined hypothesis is denoted as discriminant centersurround saliency and the corresponding optimal saliency architecture is derived this architecture equates the saliency of each image location to the discriminant power of a set of features with respect to the classification problem that opposes stimuli at center and surround at that location it is shown that the resulting saliency detector makes accurate quantitative predictions for various aspects of the psychophysics of human saliency including nonlinear properties beyond the reach of previous saliency models furthermore it is shown that discriminant centersurround saliency can be easily generalized to various stimulus modalities such as color orientation and motion and provides optimal solutions for many other saliency problems of interest for computer vision optimal solutions under this hypothesis are derived for a number of the former including static natural images dense motion fields and even dynamic textures and applied to a number of the latter the prediction of human eye fixations motionbased saliency in the presence of egomotion and motionbased saliency in the presence of highly dynamic backgrounds in result discriminant saliency is shown to predict eye fixations better than previous models and produce background subtraction algorithms that outperform the stateoftheart in computer vision
unconstrained online handwriting recognition with recurrent neural networks unconstrained online handwriting recognition with recurrent neural networks online handwriting recognition is unusual among sequence labelling tasks in that the underlying generator of the observed data ie the movement of the pen is recorded directly however the raw data can be difficult to interpret because each letter is spread over many pen locations as a consequence sophisticated preprocessing is required to obtain inputs suitable for conventional sequence labelling algorithms such as hmms in this paper we describe a system capable of directly transcribing raw online handwriting data the system consists of a recurrent neural network trained for sequence labelling combined with a probabilistic language model in experiments on an unconstrained online database we record excellent results using either raw or preprocessed data well outperforming a benchmark hmm in both cases
not sparse coding not sparse coding prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian l that promotes sparsity we show that a prior based on minimizing kldivergence preserves the benefits of these sparse priors while adding stability to the maximum aposteriori map estimate that makes it more useful for prediction problems additionally we show how to calculate the derivative of the map estimate efficiently with implicit differentiation and demonstrate how online optimization of the parameters of the klregularized model can significantly improve performance on a wide variety of applications
a quotshape aware model for semisupervised learning of objects and its context a quotshape aware model for semisupervised learning of objects and its context integrating semantic and syntactic analysis is essential for document analysis using an analogous reasoning we present an approach that combines bagofwords and spatial models to perform semantic and syntactic analysis for recognition of an object based on its internal appearance and its context we argue that while object recognition requires modeling relative spatial locations of image features within the object a bagofword is sufficient for representing context learning such a model from weakly labeled data involves labeling of features into two classes foregroundobject or informative backgroundcontext labeling we present a shapeaware model which utilizes contour information for efficient and accurate labeling of features in the image our approach iterates between an mcmcbased labeling and contour based labeling of features to integrate cooccurrence of features and shape similarity
a bayesian approach for extracting state transition dynamics from multiple spike trains a bayesian approach for extracting state transition dynamics from multiple spike trains neural activity is nonstationary and varies across time hidden markov models hmms have been used to track the state transition among quasistationary discrete neural states within this context an independent poisson model has been used for the output distribution of hmms hence the model is incapable of tracking the change in correlation without modulating the firing rate to achieve this we applied a multivariate poisson distribution with a correlation term for the output distribution of hmms we formulated a variational bayes vb inference for the model the vb could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem we developed an efficient algorithm for computing posteriors using the recursive relationship of a multivariate poisson distribution we demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird
a computational model of hippocampal function in trace conditioning a computational model of hippocampal function in trace conditioning we present a new reinforcementlearning model for the role of the hippocampus in classical conditioning focusing on the differences between trace and delay conditioning in the model all stimuli are represented both as unindividuated wholes and as a series of temporal elements with varying delays these two stimulus representations interact producing different patterns of learning in trace and delay conditioning the model proposes that hippocampal lesions eliminate longlatency temporal elements but preserve shortlatency temporal elements for trace conditioning with no contiguity between stimulus and reward these longlatency temporal elements are vital to learning adaptively timed responses for delay conditioning in contrast the continued presence of the stimulus supports conditioned responding and the shortlatency elements suppress responding early in the stimulus in accord with the empirical data simulated hippocampal damage impairs trace conditioning but not delay conditioning at mediumlength intervals with longer intervals learning is impaired in both procedures and with shorter intervals in neither in addition the model makes novel predictions about the response topography with extended stimuli or posttraining lesions these results demonstrate how temporal contiguity as in delay conditioning changes the timing problem faced by animals rendering it both easier and less susceptible to disruption by hippocampal lesions
a convergent on temporaldifference algorithm for offpolicy learning with linear function approxi a convergent on temporaldifference algorithm for offpolicy learning with linear function approxi we introduce the first temporaldifference learning algorithm that is stable with linear function approximation and offpolicy training for any finite markov decision process target policy and exciting behavior policy and whose complexity scales linearly in the number of parameters we consider an iid policyevaluation setting in which the data need not come from onpolicy experience the gradient temporaldifference gtd algorithm estimates the expected update vector of the td algorithm and performs stochastic gradient descent on its l norm our analysis proves that its expected update is in the direction of the gradient assuring convergence under the usual stochastic approximation conditions to the same leastsquares solution as found by the lstd but without its quadratic computational complexity gtd is online and incremental and does not involve multiplying by products of likelihood ratios as in importancesampling methods
a convex upper bound on the logpartition function a convex upper bound on the logpartition function we consider the problem of bounding from above the logpartition function corresponding to secondorder ising models for binary distributions we introduce a new bound the cardinality bound which can be computed via convex optimization the corresponding error on the logpartition function is bounded above by twice the distance in model parameter space to a class of standard ising models for which variable interdependence is described via a simple mean field term in the context of maximumlikelihood using the new bound instead of the exact logpartition function while constraining the distance to the class of standard ising models leads not only to a good approximation to the logpartition function but also to a model that is parsimonious and easily interpretable we compare our bound with the logdeterminant bound introduced by wainwright and jordan and show that when the lnorm of the model parameter vector is small enough the latter is outperformed by the new bound
a general framework for investigating how far the decoding process in the brain can be simplified a general framework for investigating how far the decoding process in the brain can be simplified how is information decoded in the brain is one of the most difficult and important questions in neuroscience whether neural correlation is important or not in decoding neural activities is of special interest we have developed a general framework for investigating how far the decoding process in the brain can be simplified first we hierarchically construct simplified probabilistic models of neural responses that ignore more than kthorder correlations by using a maximum entropy principle then we compute how much information is lost when information is decoded using the simplified models ie mismatched decoders we introduce an information theoretically correct quantity for evaluating the information obtained by mismatched decoders we applied our proposed framework to spike data for vertebrate retina we used ms natural movies as stimuli and computed the information contained in neural activities about these movies we found that the information loss is negligibly small in population activities of ganglion cells even if all orders of correlation are ignored in decoding we also found that if we assume stationarity for long durations in the information analysis of dynamically changing stimuli like natural movies pseudo correlations seem to carry a large portion of the information
a hierarchical image model for polynomialtime d parsing a hierarchical image model for polynomialtime d parsing language and image understanding are two major goals of artificial intelligence which can both be conceptually formulated in terms of parsing the input signal into a hierarchical representation natural language researchers have made great progress by exploiting the d structure of language to design efficient polynomialtime parsing algorithms by contrast the twodimensional nature of images makes it much harder to design efficient image parsers and the form of the hierarchical representations is also unclear attempts to adapt representations and algorithms from natural language have only been partially successful in this paper we propose a hierarchical image model him for d image parsing which outputs image segmentation and object recognition this him has multiple layers five in this paper and has advantages for representation inference and learning firstly the him has a coarsetofine representation which is capable of capturing longrange dependency and exploiting different levels of contextual information secondly the structure of the him allows us to design a rapid inference algorithm based on dynamic programming which enables us to parse the image rapidly in polynomial time thirdly we can learn the him efficiently in a discriminative manner from a labeled dataset we demonstrate that him outperforms other stateoftheart methods by evaluation on the challenging public msrc image dataset finally we sketch how the him architecture can be extended to model more complex image phenomena
a massively parallel digital learning processor a massively parallel digital learning processor we present a new massively parallel architecture for accelerating machine learning algorithms based on arrays of variableresolution arithmetic vector processing elements vpe groups of vpes operate in simd single instruction multiple data mode and each group is connected to an independent memory bank in this way memory bandwidth scales with the number of vpe and the main data flows are local keeping power dissipation low with vpes implemented on two fpga field programmable gate array chips we obtain a sustained speed of gmacs billion multiplyaccumulate per sec for svm training and gmacs for svm classification this performance is more than an order of magnitude higher than that of any fpga implementation reported so far the speed on one fpga is similar to the fastest speeds published on a graphics processor for the mnist problem despite a clock rate of the fpga that is six times lower high performance at low clock rates makes this massively parallel architecture particularly attractive for embedded applications where low power dissipation is critical tests with convolutional neural networks and other learning algorithms are under way now
a mixture model for the evolution of gene expression in nonhomogeneous datasets a mixture model for the evolution of gene expression in nonhomogeneous datasets we address the challenge of assessing conservation of gene expression in complex nonhomogeneous datasets recent studies have demonstrated the success of probabilistic models in studying the evolution of gene expression in simple eukaryotic organisms such as yeast for which measurements are typically scalar and independent models capable of studying expression evolution in much more complex organisms such as vertebrates are particularly important given the medical and scientific interest in species such as human and mouse we present a statistical model that makes a number of significant extensions to previous models to enable characterization of changes in expression among highly complex organisms we demonstrate the efficacy of our method on a microarray dataset containing diverse tissues from multiple vertebrate species we anticipate that the model will be invaluable in the study of gene expression patterns in other diverse organisms as well such as worms and insects
a rational model of preference learning and choice prediction by children a rational model of preference learning and choice prediction by children young children demonstrate the ability to make inferences about the preferences of other agents based on their choices however there exists no overarching account of what children are doing when they learn about preferences or how they use that knowledge we use a rational model of preference learning drawing on ideas from economics and computer science to explain the behavior of children in several recent experiments specifically we show how a simple econometric model can be extended to capture two to fouryearoldsacirc use of statistical information in inferring preferences and their generalization of these preferences
a scalable hierarchical distributed language model a scalable hierarchical distributed language model neural probabilistic language models nplms have been shown to be competitive with and occasionally superior to the widelyused ngram language models the main drawback of nplms is their extremely long training and testing times morin and bengio have proposed a hierarchical language model built around a binary tree of words that was two orders of magnitude faster than the nonhierarchical language model it was based on however it performed considerably worse than its nonhierarchical counterpart in spite of using a word tree created using expert knowledge we introduce a fast hierarchical language model along with a simple featurebased algorithm for automatic construction of word trees from the data we then show that the resulting models can outperform nonhierarchical models and achieve stateoftheart performance
a spatially varying twosample recombinant coalescent with applications to hiv escape response a spatially varying twosample recombinant coalescent with applications to hiv escape response statistical evolutionary models provide an important mechanism for describing and understanding the escape response of a viral population under a particular therapy we present a new hierarchical model that incorporates spatially varying mutation and recombination rates at the nucleotide level it also maintains sep arate parameters for treatment and control groups which allows us to estimate treatment effects explicitly we use the model to investigate the sequence evolu tion of hiv populations exposed to a recently developed antisense gene therapy as well as a more conventional drug therapy the detection of biologically rele vant and plausible signals in both therapy studies demonstrates the effectiveness of the method
a transductive bound for the voted classifier with an application to semisupervised learning a transductive bound for the voted classifier with an application to semisupervised learning in this paper we present two transductive bounds on the risk of the majority vote estimated over partially labeled training sets our first bound is tight when the additional unlabeled training data are used in the cases where the voted classifier makes its errors on low margin observations and where the errors of the associated gibbs classifier can accurately be estimated in semisupervised learning considering the margin as an indicator of confidence constitutes the working hypothesis of algorithms which search the decision boundary on low density regions in this case we propose a second bound on the joint probability that the voted classifier makes an error over an example having its margin over a fixed threshold as an application we are interested on selflearning algorithms which assign iteratively pseudolabels to unlabeled training examples having margin above a threshold obtained from this bound empirical results on different datasets show the effectiveness of our approach compared to the same algorithm and the tsvm in which the threshold is fixed manually
accelerating bayesian inference over nonlinear differential equations with gaussian processes accelerating bayesian inference over nonlinear differential equations with gaussian processes identification and comparison of nonlinear dynamical systems using noisy and sparse experimental data is a vital task in many fields however current methods are computationally expensive and prone to error due in part to the nonlinear nature of the likelihood surfaces induced we present an accelerated sampling procedure which enables bayesian inference of parameters in nonlinear ordinary and delay differential equations via the novel use of gaussian processes gp our method involves gp regression over timeseries data and the resulting derivative and time delay estimates make parameter inference possible without solving the dynamical system explicitly resulting in dramatic savings of computational time we demonstrate the speed and statistical accuracy of our approach using examples of both ordinary and delay differential equations and provide a comprehensive comparison with current state of the art methods
adapting to a market shock optimal sequential marketmaking adapting to a market shock optimal sequential marketmaking we study the profitmaximization problem of a monopolistic marketmaker who sets twosided prices in an asset market the sequential decision problem is hard to solve because the state space is a function we demonstrate that the belief state is well approximated by a gaussian distribution we prove a key monotonicity property of the gaussian state update which makes the problem tractable yielding the first optimal sequential marketmaking algorithm in an established model the algorithm leads to a surprising insight an optimal monopolist can provide more liquidity than perfectly competitive marketmakers in periods of extreme uncertainty because a monopolist is willing to absorb initial losses in order to learn a new valuation rapidly so she can extract higher profits later
adaptive forwardbackward greedy algorithm for sparse learning with linear models adaptive forwardbackward greedy algorithm for sparse learning with linear models consider linear prediction models where the target function is a sparse linear combination of a set of basis functions we are interested in the problem of identifying those basis functions with nonzero coefficients and reconstructing the target function from noisy observations two heuristics that are widely used in practice are forward and backward greedy algorithms first we show that neither idea is adequate second we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneficial we prove strong theoretical results showing that this procedure is effective in learning sparse representations experimental results support our theory
adaptive martingale boosting adaptive martingale boosting in recent work long and servedio lsshort presented a martingale boosting algorithm that works by constructing a branching program over weak classifiers and has a simple analysis based on elementary properties of random walks lsshort showed that this martingale booster can tolerate random classification noise when it is run with a noisetolerant weak learner however a drawback of the algorithm is that it is not adaptive ie it cannot effectively take advantage of variation in the quality of the weak classifiers it receives in this paper we present a variant of the original martingale boosting algorithm and prove that it is adaptive this adaptiveness is achieved by modifying the original algorithm so that the random walks that arise in its analysis have different step size depending on the quality of the weak learner at each stage the new algorithm inherits the desirable properties of the original lsshort algorithm such as random classification noise tolerance and has several other advantages besides adaptiveness it requires polynomially fewer calls to the weak learner than the original algorithm and it can be used with confidencerated weak hypotheses that output real values rather than boolean predictions
adaptive template matching with shiftinvariant seminmf adaptive template matching with shiftinvariant seminmf how does one extract unknown but stereotypical events that are linearly superimposed within a signal with variable latencies and variable amplitudes one could think of using template matching or matching pursuit to find the arbitrarily shifted linear components however traditional matching approaches require that the templates be known a priori to overcome this restriction we use instead semi nonnegative matrix factorization seminmf that we extend to allow for time shifts when matching the templates to the signal the algorithm estimates templates directly from the data along with their nonnegative amplitudes the resulting method can be thought of as an adaptive template matching procedure we demonstrate the procedure on the task of extracting spikes from single channel extracellular recordings on these data the algorithm essentially performs spike detection and unsupervised spike clustering results on simulated data and extracellular recordings indicate that the method performs well for signaltonoise ratios of db or higher and that spike templates are recovered accurately provided they are sufficiently different
algorithms for infinitely manyarmed bandits algorithms for infinitely manyarmed bandits we consider multiarmed bandit problems where the number of arms is larger than the possible number of experiments we make a stochastic assumption on the meanreward of a new selected arm which characterizes its probability of being a nearoptimal arm our assumption is weaker than in previous works we describe algorithms based on upperconfidencebounds applied to a restricted set of randomly selected arms and provide upperbounds on the resulting expected regret we also derive a lowerbound which matchs up to logarithmic factors the upperbound in some cases
an algorithm for microchip spike sorting an algorithm for microchip spike sorting a new spike feature extraction algorithm which enables realtime spike sorting and facilitates miniaturized microchip implementation is presented the proposed algorithm has been evaluated on synthesized waveforms and experimentally recorded sequences from different animals when compared with many spike sorting approaches our algorithm demonstrates significantly improved speed accuracy and allows unsupervised execution a preliminary hardware implementation has been realized using an integrated microchip interfaced with a personal computer
an efficient sequential monte carlo algorithm for coalescent clustering an efficient sequential monte carlo algorithm for coalescent clustering we propose an efficient sequential monte carlo inference scheme for the recently proposed coalescent clustering model teh et al our algorithm has a quadratic runtime while those in teh et al is cubic in experiments we were surprised to find that in addition to being more efficient it is also a better sequential monte carlo sampler than the best in teh et al when measured in terms of variance of estimated likelihood and effective sample size
an empirical analysis of domain adaptation algorithms for genomic sequence analysis an empirical analysis of domain adaptation algorithms for genomic sequence analysis we study the problem of domain transfer for a supervised classification task in mrna splicing we consider a number of recent domain transfer methods from machine learning including some that are novel and evaluate them on genomic sequence data from model organisms of varying evolutionary distance we find that in cases where the organisms are not closely related the use of domain adaptation methods can help improve classification performance
an extended level method for efficient multiple kernel learning an extended level method for efficient multiple kernel learning we consider the problem of multiple kernel learning mkl which can be formulated as a convexconcave problem in the past two efficient methods ie semiinfinite linear programming silp and subgradient descent sd have been proposed for largescale multiple kernel learning despite their success both methods have their own shortcomings a the sd method utilizes the gradient of only the current solution and b the silp method does not regularize the approximate solution obtained from the cutting plane model in this work we extend the level method which was originally designed for optimizing nonsmooth objective functions to convexconcave optimization and apply it to multiple kernel learning the extended level method overcomes the drawbacks of silp and sd by exploiting all the gradients computed in past iterations and by regularizing the solution via a projection to a level set empirical study with eight uci datasets shows that the extended level method can significantly improve efficiency by saving on average of computational time over the silp method and over the sd method
an homotopy algorithm for the lasso with online observations an homotopy algorithm for the lasso with online observations it has been shown that the problem of ellpenalized leastsquare regression commonly referred to as the lasso or basis pursuit denoising leads to solutions that are sparse and therefore achieves model selection we propose in this paper an algorithm to solve the lasso with online observations we introduce an optimization problem that allows us to compute an homotopy from the current solution to the solution after observing a new data point we compare our method to lars and present an application to compressed sensing with sequential observations our approach can also be easily extended to compute an homotopy from the current solution to the solution after removing a data point which leads to an efficient algorithm for leaveoneout crossvalidation
an ideal observer model of infant object perception an ideal observer model of infant object perception before the age of months infants make inductive inferences about the motions of physical objects developmental psychologists have provided verbal accounts of the knowledge that supports these inferences but often these accounts focus on categorical rather than probabilistic principles we propose that infant object perception is guided in part by probabilistic principles like persistence things tend to remain the same and when they change they do so gradually to illustrate this idea we develop an ideal observer model that includes probabilistic formulations of rigidity and inertia like previous researchers we suggest that rigid motions are expected from an early age but we challenge the previous claim that expectations consistent with inertia are relatively slow to develop spelke et al we support these arguments by modeling four experiments from the developmental literature
an improved estimator of variance explained in the presence of noise an improved estimator of variance explained in the presence of noise a crucial part of developing mathematical models of how the brain works is the quantification of their success one of the most widelyused metrics yields the percentage of the variance in the data that is explained by the model unfortunately this metric is biased due to the intrinsic variability in the data this variability is in principle unexplainable by the model we derive a simple analytical modification of the traditional formula that significantly improves its accuracy as measured by bias with similar or better precision as measured by meansquare error in estimating the true underlying variance explained by the model class our estimator advances on previous work by a accounting for the uncertainty in the noise estimate b accounting for overfitting due to free model parameters mitigating the need for a separate validation data set and c adding a conditioning term we apply our new estimator to binocular disparity tuning curves of a set of macaque v neurons and find that on a population level almost all of the variance unexplained by gabor functions is attributable to noise
an interiorpoint stochastic approximation method and an lregularized delta rule an interiorpoint stochastic approximation method and an lregularized delta rule the stochastic approximation method is behind the solution to many important activelystudied problems in machine learning despite its farreaching application there is almost no work on applying stochastic approximation to learning problems with constraints the reason for this we hypothesize is that no robust widelyapplicable stochastic approximation method exists for handling such problems we propose that interiorpoint methods are a natural solution we establish the stability of a stochastic interiorpoint approximation method both analytically and empirically and demonstrate its utility by deriving an online learning algorithm that also performs feature selection via l regularization
an online algorithm for maximizing submodular functions an online algorithm for maximizing submodular functions we present an algorithm for solving a broad class of online resource allocation problems our online algorithm can be applied in environments where abstract jobs arrive one at a time and one can complete the jobs by investing time in a number of abstract activities according to some schedule we assume that the fraction of jobs completed by a schedule is a monotone submodular function of a set of pairs vt where t is the time invested in activity v under this assumption our online algorithm performs nearoptimally according to two natural metrics i the fraction of jobs completed within time t for some fixed deadline t gt and ii the average time required to complete each job we evaluate our algorithm experimentally by using it to learn online a schedule for allocating cpu time among solvers entered in the sat solver competition
analyzing human feature learning as nonparametric bayesian inference analyzing human feature learning as nonparametric bayesian inference almost all successful machine learning algorithms and cognitive models require powerful representations capturing the features that are relevant to a particular problem we draw on recent work in nonparametric bayesian statistics to define a rational model of human feature learning that forms a featural representation from raw sensory data without prespecifying the number of features by comparing how the human perceptual system and our rational model use distributional and category information to infer feature representations we seek to identify some of the forces that govern the process by which people separate and combine sensory primitives to form features
analyzing the monotonic feature abstraction for text classification analyzing the monotonic feature abstraction for text classification is accurate classification possible in the absence of handlabeled data this paper introduces the monotonic feature mf abstractionwhere the probability of class membership increases monotonically with the mfs value the paper proves that when an mf is given pac learning is possible with no handlabeled data under certain assumptions we argue that mfs arise naturally in a broad range of textual classification applications on the classic quot newsgroupsquot data set a learner given an mf and unlabeled data achieves classification accuracy equal to that of a stateoftheart semisupervised learner relying on handlabeled examples even when mfs are not given as input their presence or absence can be determined from a small amount of handlabeled data which yields a new semisupervised learning method that reduces error by on the newsgroups data
artificial olfactory brain for mixture identification artificial olfactory brain for mixture identification the odor transduction process has a large time constant and is susceptible to various types of noise therefore the olfactory code at the sensorreceptor level is in general a slow and highly variable indicator of the input odor in both natural and artificial situations insects overcome this problem by using a neuronal device in their antennal lobe al which transforms the identity code of olfactory receptors to a spatiotemporal code this transformation improves the decision of the mushroom bodies mbs the subsequent classifier in both speed and accuracyhere we propose a rate model based on two intrinsic mechanisms in the insect al namely integration and inhibition then we present a mb classifier model that resembles the sparse and random structure of insect mb a local hebbian learning procedure governs the plasticity in the model these formulations not only help to understand the signal conditioning and classification methods of insect olfactory systems but also can be leveraged in synthetic problems among them we consider here the discrimination of odor mixtures from pure odors we show on a set of records from metaloxide gas sensors that the cascade of these two new models facilitates fast and accurate discrimination of even highly imbalanced mixtures from pure odors
asynchronous distributed learning of topic models asynchronous distributed learning of topic models distributed learning is a problem of fundamental interest in machine learning and cognitive science in this paper we present asynchronous distributed learning algorithms for two wellknown unsupervised learning frameworks latent dirichlet allocation lda and hierarchical dirichlet processes hdp in the proposed approach the data are distributed across p processors and processors independently perform gibbs sampling on their local data and communicate their information in a local asynchronous manner with other processors we demonstrate that our asynchronous algorithms are able to learn global topic models that are statistically as accurate as those learned by the standard lda and hdp samplers but with significant improvements in computation time and memory we show speedup results on a millionword text corpus using processors and we provide perplexity results for up to virtual processors as a stepping stone in the development of asynchronous hdp a parallel hdp sampler is also introduced
automatic online tuning for fast gaussian summation automatic online tuning for fast gaussian summation many machine learning algorithms require the summation of gaussian kernel functions an expensive operation if implemented straightforwardly several methods have been proposed to reduce the computational complexity of evaluating such sums including tree and analysis based methods these achieve varying speedups depending on the bandwidth dimension and prescribed error making the choice between methods difficult for machine learning tasks we provide an algorithm that combines tree methods with the improved fast gauss transform ifgt as originally proposed the ifgt suffers from two problems the taylor series expansion does not perform well for very low bandwidths and parameter selection is not trivial and can drastically affect performance and ease of use we address the first problem by employing a tree data structure resulting in four evaluation methods whose performance varies based on the distribution of sources and targets and input parameters such as desired accuracy and bandwidth to solve the second problem we present an online tuning approach that results in a black box method that automatically chooses the evaluation method and its parameters to yield the best performance for the input data desired accuracy and bandwidth in addition the new ifgt parameter selection approach allows for tighter error bounds our approach chooses the fastest method at negligible additional cost and has superior performance in comparisons with previous approaches
bayesian experimental design of magnetic resonance imaging sequences bayesian experimental design of magnetic resonance imaging sequences we show how improved sequences for magnetic resonance imaging can be found through automated optimization of bayesian design scores combining recent advances in approximate bayesian inference and natural image statistics with highperformance numerical computation we propose the first scalable bayesian experimental design framework for this problem of high relevance to clinical and brain research our solution requires approximate inference for dense nongaussian models on a scale seldom addressed before we propose a novel scalable variational inference algorithm and show how powerful methods of numerical mathematics can be modified to compute primitives in our framework our approach is evaluated on a realistic setup with raw data from a t mr scanner
bayesian exponential family pca bayesian exponential family pca principal components analysis pca has become established as one of the key tools for dimensionality reduction when dealing with real valued data approaches such as exponential family pca and nonnegative matrix factorisation have successfully extended pca to nongaussian data types but these techniques fail to take advantage of bayesian inference and can suffer from problems of overfitting and poor generalisation this paper presents a fully probabilistic approach to pca which is generalised to the exponential family based on hybrid monte carlo sampling we describe the model which is based on a factorisation of the observed data matrix and show performance of the model on both synthetic and real data
bayesian kernel shaping for learning control bayesian kernel shaping for learning control in kernelbased regression learning optimizing each kernel individually is useful when the data density curvature of regression surfaces or decision boundaries or magnitude of output noise ie heteroscedasticity varies spatially unfortunately it presents a complex computational problem as the danger of overfitting is high and the individual optimization of every kernel in a learning system may be overly expensive due to the introduction of too many open learning parameters previous work has suggested gradient descent techniques or complex statistical hypothesis methods for local kernel shaping typically requiring some amount of manual tuning of meta parameters in this paper we focus on nonparametric regression and introduce a bayesian formulation that with the help of variational approximations results in an emlike algorithm for simultaneous estimation of regression and kernel parameters the algorithm is computationally efficient suitable for large data sets requires no sampling automatically rejects outliers and has only one prior to be specified it can be used for nonparametric regression with local polynomials or as a novel method to achieve nonstationary regression with gaussian processes our methods are particularly useful for learning control where reliable estimation of local tangent planes is essential for adaptive controllers and reinforcement learning we evaluate our methods on several synthetic data sets and on an actual robot which learns a tasklevel control law
bayesian model of behaviour in economic games bayesian model of behaviour in economic games classical game theoretic approaches that make strong rationality assumptions have difficulty modeling observed behaviour in economic games of human subjects we investigate the role of finite levels of iterated reasoning and nonselfish utility functions in a partially observable markov decision process model that incorporates game theoretic notions of interactivity our generative model captures a broad class of characteristic behaviours in a multiround investment game we invert the generative process for a recognition model that is used to classify subjects playing an investortrustee game against randomly matched opponents
bayesian network score approximation using a metagraph kernel bayesian network score approximation using a metagraph kernel many interesting problems including bayesian network structuresearch can be cast in terms of finding the optimum value of a function over the space of graphs however this function is often expensive to compute exactly we here present a method derived from the study of reproducingkernel hilbert spaces which takes advantage of the regular structure of the space of all graphs on a fixed number of nodes to obtain approximations to the desired function quickly and with reasonable accuracy we then test this method on both a small testing set and a realworld bayesian network the results suggest that not only is this method reasonably accurate but that the bde score itself varies quadratically over the space of all graphs
bayesian synchronous grammar induction bayesian synchronous grammar induction we present a novel method for inducing synchronous context free grammars scfgs from a corpus of parallel string pairs scfgs can model equivalence between strings in terms of substitutions insertions and deletions and the reordering of substrings we develop a nonparametric bayesian model and apply it to a machine translation task using priors to replace the various heuristics commonly used in this field using a variational bayes training procedure we learn the latent structure of translation equivalence through the induction of synchronous grammar categories for phrasal translations showing improvements in translation performance over previously proposed maximum likelihood models
beyond novelty detection incongruent events when general and specific classifiers disagree beyond novelty detection incongruent events when general and specific classifiers disagree unexpected stimuli are a challenge to any machine learning algorithm here we identify distinct types of unexpected events focusing on incongruent events when general level and specific level classifiers give conflicting predictions we define a formal framework for the representation and processing of incongruent events starting from the notion of label hierarchy we show how partial order on labels can be deduced from such hierarchies for each event we compute its probability in different ways based on adjacent levels according to the partial order in the label hierarchy an incongruent event is an event where the probability computed based on some more specific level in accordance with the partial order is much smaller than the probability computed based on some more general level leading to conflicting predictions we derive algorithms to detect incongruent events from different types of hierarchies corresponding to class membership or part membership respectively we show promising results with real data on two specific problems out of vocabulary words in speech recognition and the identification of a new subclass eg the face of a new individual in audiovisual facial object recognition
biasing approximate dynamic programming with a lower discount factor biasing approximate dynamic programming with a lower discount factor most algorithms for solving markov decision processes rely on a discount factor which ensures their convergence in fact it is often used in problems with is no intrinsic motivation in this paper we show that when used in approximate dynamic programming an artificially low discount factor may significantly improve the performance on some problems such as tetris we propose two explanations for this phenomenon our first justification follows directly from the standard approximation error bounds using a lower discount factor may decrease the approximation error bounds however we also show that these bounds are loose a thus their decrease does not entirely justify a better practical performance we thus propose another justification when the rewards are received only sporadically as it is the case in tetris we can derive tighter bounds which support a significant performance increase with a decrease in the discount factor
bioinspired real time sensory map realignment in a robotic barn owl bioinspired real time sensory map realignment in a robotic barn owl the visual and auditory map alignment in the superior colliculus sc of barn owl is important for its accurate localization for prey behavior prism learning or blindness may interfere this alignment and cause loss of the capability of accurate prey however juvenile barn owl could recover its sensory map alignment by shifting its auditory map the adaptation of this map alignment is believed based on activity dependent axon developing in inferior colliculus ic a model is built to explore this mechanism in this model axon growing process is instructed by an inhibitory network in sc while the strength of the inhibition adjusted by spike timing dependent plasticity stdp we test and analyze this mechanism by application of the neural structures involved in spatial localization in a robotic system
bounding performance loss in approximate mdp homomorphisms bounding performance loss in approximate mdp homomorphisms we define a metric for measuring behavior similarity between states in a markov decision process mdp in which action similarity is taken into account we show that the kernel of our metric corresponds exactly to the classes of states defined by mdp homomorphisms ravindran amp barto we prove that the difference in the optimal value function of different states can be upperbounded by the value of this metric and that the bound is tighter than that provided by bisimulation metrics ferns et al our results hold both for discrete and for continuous actions we provide an algorithm for constructing approximate homomorphisms by using this metric to identify states that can be grouped together as well as actions that can be matched previous research on this topic is based mainly on heuristics
bounds on marginal probability distributions bounds on marginal probability distributions we propose a novel bound on singlevariable marginal probability distributions in factor graphs with discrete variables the bound is obtained by propagating bounds convex sets of probability distributions over a subtree of the factor graph rooted in the variable of interest by construction the method not only bounds the exact marginal probability distribution of a variable but also its approximate belief propagation marginal belief thus apart from providing a practical means to calculate bounds on marginals our contribution also lies in providing a better understanding of the error made by belief propagation we show that our bound outperforms the stateoftheart on some inference problems arising in medical diagnosis
breaking audio captchas with machine learning techniques breaking audio captchas with machine learning techniques captchas are computergenerated tests that humans can pass but current computer systems cannot captchas provide a method for automatically distinguishing a human from a computer program and therefore can protect web services from abuse by socalled acircbotsacirc most captchas consist of distorted images usually text for which a user must provide some description unfortunately visual captchas limit access to the millions of visually impaired people using the web audio captchas were created to solve this accessibility issue however the security of audio captchas was never formally tested many visual captchas have been broken using machine learning techniques and we propose using similar ideas to test the security of audio captchas audio captchas are generally composed of a set of words to be identified layered on top of noise we analyzed the security of current audio captchas from popular web sites by using adaboost svm and knn and achieved correct solutions for test samples with accuracy up to such accuracy is enough to consider these captchas broken training several different machine learning algorithms on different types of audio captchas allowed us to analyze the strengths and weaknesses of the algorithms so that we could create a design for a more robust audio captcha
cascaded classification models combining models for holistic scene understanding cascaded classification models combining models for holistic scene understanding one of the original goals of computer vision was to fully understand a natural scene this requires solving several problems simultaneously including object detection labeling of meaningful regions and d reconstruction while great progress has been made in tackling each of these problems in isolation only recently have researchers again been considering the difficult task of assembling various methods to the mutual benefit of all we consider learning a set of such classification models in such a way that they both solve their own problem and help each other we develop a framework known as cascaded classification models ccm where repeated instantiations of these classifiers are coupled by their inputoutput variables in a cascade that improves performance at each level our method requires only a limited acircblack boxacirc interface with the models allowing us to use very sophisticated stateoftheart classifiers without having to look under the hood we demonstrate the effectiveness of our method on a large set of natural images by combining the subtasks of scene categorization object detection multiclass image segmentation and d scene reconstruction
cell assemblies in large sparse inhibitory networks of biologically realistic spiking neurons cell assemblies in large sparse inhibitory networks of biologically realistic spiking neurons cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum and hippocampus ca here we address the question of how coherent dynamically switching assemblies appear in large networks of biologically realistic spiking neurons interacting deterministically we show by numerical simulations of large asymmetric inhibitory networks with fixed external excitatory drive that if the network has intermediate to sparse connectivity the individual cells are in the vicinity of a bifurcation between a quiescent and firing state and the network inhibition varies slowly on the spiking timescale then cells form assemblies whose members show strong positive correlation while members of different assemblies show strong negative correlation we show that cells and assemblies switch between firing and quiescent states with time durations consistent with a powerlaw our results are in good qualitative agreement with the experimental studies the deterministic dynamical behaviour is related to winnerless competition shown in small closed loop inhibitory networks with heteroclinic cycles connecting saddlepoints
characteristic kernels on groups and semigroups characteristic kernels on groups and semigroups embeddings of random variables in reproducing kernel hilbert spaces rkhss may be used to conduct statistical inference based on higher order moments for sufficiently rich characteristic rkhss each probability distribution has a unique embedding allowing all statistical properties of the distribution to be taken into consideration necessary and sufficient conditions for an rkhs to be characteristic exist for rn in the present work conditions are established for an rkhs to be characteristic on groups and semigroups illustrative examples are provided including characteristic kernels on periodic domains rotation matrices and rn
characterizing neural dependencies with poisson copula models characterizing neural dependencies with poisson copula models the coding of information by neural populations depends critically on the statistical dependencies between neuronal responses however there is no simple model that combines the observations that marginal distributions over singleneuron spike counts are often approximately poisson and joint distributions over the responses of multiple neurons are often strongly dependent here we show that both marginal and joint properties of neural responses can be captured using poisson copula models copulas are joint distributions that allow random variables with arbitrary marginals to be combined while incorporating arbitrary dependencies between them different copulas capture different kinds of dependencies allowing for a richer and more detailed description of dependencies than traditional summary statistics such as correlation coefficients we explore a variety of poisson copula models for joint neural response distributions and derive an efficient maximum likelihood procedure for estimating them we apply these models to neuronal data collected in and macaque motor cortex and quantify the improvement in coding accuracy afforded by incorporating the dependency structure between pairs of neurons
clustered multitask learning a convex formulation clustered multitask learning a convex formulation in multitask learning several related tasks are considered simultaneously with the hope that by an appropriate sharing of information across tasks each task may benefit from the others in the context of learning linear functions for supervised classification or regression this can be achieved by including a priori information about the weight vectors associated with the tasks and how they are expected to be related to each other in this paper we assume that tasks are clustered into groups which are unknown beforehand and that tasks within a group have similar weight vectors we design a new spectral norm that encodes this a priori assumption without the prior knowledge of the partition of tasks into groups resulting in a new convex optimization formulation for multitask learning we show in simulations on synthetic examples and on the iedb mhci binding dataset that our approach outperforms wellknown convex methods for multitask learning as well as related non convex methods dedicated to the same problem
clustering via lpbased stabilities clustering via lpbased stabilities a novel centerbased clustering algorithm is proposed in this paper we first formulate clustering as an nphard linear integer program and we then use linear programming and the duality theory to derive the solution of this optimization problem this leads to an efficient and very general algorithm which works in the dual domain and can cluster data based on an arbitrary set of distances despite its generality it is independent of initialization unlike emlike methods such as kmeans has guaranteed convergence and can also provide online optimality bounds about the quality of the estimated clustering solutions to deal with the most critical issue in a centerbased clustering algorithm selection of cluster centers we also introduce the notion of stability of a cluster center which is a well defined lpbased quantity that plays a key role to our algorithms success furthermore we also introduce what we call the margins another key ingredient in our algorithm which can be roughly thought of as dual counterparts to stabilities and allow us to obtain computationally efficient approximations to the latter promising experimental results demonstrate the potentials of our method
clusters and coarse partitions in lp relaxations clusters and coarse partitions in lp relaxations we propose a new class of consistency constraints for linear programming lp relaxations for finding the most probable map configuration in graphical models usual clusterbased lp relaxations enforce joint consistency of the beliefs of a cluster of variables with computational cost increasing exponentially with the size of the clusters by partitioning the state space of a cluster and enforcing consistency only across partitions we obtain a class of constraints which although less tight are computationally feasible for large clusters we show how to solve the cluster selection and partitioning problem monotonically in the dual lp using the current beliefs to guide these choices we obtain a dual messagepassing algorithm and apply it to protein design problems where the variables have large state spaces and the usual clusterbased relaxations are very costly
comparing model predictions of response bias and variance in cue combination comparing model predictions of response bias and variance in cue combination we explore a recently proposed mixture model approach to understanding interactions between conflicting sensory cues alternative model formulations differing in their sensory noise models and inference methods are compared based on their fit to experimental data heavytailed sensory likelihoods yield a better description of the interesting response behavior than standard gaussian noise models we study the underlying case for this result and then present several testable predictions of these models
competing rbm density models for classification of fmri images competing rbm density models for classification of fmri images neuroimaging datasets often have a very large number of voxels and a very small number of training cases which means that overfitting of models for this data can become a very serious problem working with a set of fmri images from a study on stroke recovery we consider a classification task for which logistic regression performs poorly even when l or l regularised we show that much better discrimination can be achieved by fitting a generative model to each separate condition and then seeing which model is most likely to have generated the data we use discriminative fitting of exactly the same set of models to demonstrate that the superior discrimination performance is caused by the generative fitting rather than the type of model we used restricted boltzmann machines as our generative models but our results suggest that many other generative models should be tried for discriminating different conditions in neuroimaging data
continuouslyadaptive discretization for messagepassing algorithms continuouslyadaptive discretization for messagepassing algorithms continuouslyadaptive discretization for messagepassing cadmp is a new messagepassing algorithm employing adaptive discretization most previous messagepassing algorithms approximated arbitrary continuous probability distributions using either a family of continuous distributions such as the exponential family a particleset of discrete samples or a fixed uniform discretization in contrast cadmp uses a discretization that is i nonuniform and ii adaptive the nonuniformity allows cadmp to localize interesting features such as sharp peaks in the marginal belief distributions with time complexity that scales logarithmically with precision as opposed to uniform discretization which scales at best linearly we give a principled method for altering the nonuniform discretization according to informationbased measures cadmp is shown in experiments on simulated data to estimate marginal beliefs much more precisely than competing approaches for the same computational expense
convergence and rate of convergence of a manifoldbased dimension reduction convergence and rate of convergence of a manifoldbased dimension reduction we study the convergence and the rate of convergence of a particular manifoldbased learning algorithm local tangent space alignment ltsa the main technical tool is the perturbation analysis on the linear invariant subspace that corresponds to the solution of ltsa we derive the upper bound for errors under the worst case for ltsa it naturally leads to a convergence result we then derive the rate of convergence for ltsa in a special case
correlated bigram lsa for unsupervised language model adaptation correlated bigram lsa for unsupervised language model adaptation we propose using correlated bigram lsa for unsupervised lm adaptation for automatic speech recognition the model is trained using efficient variational em and smoothed using the proposed fractional kneserney smoothing which handles fractional counts our approach can be scalable to large training corpora via bootstrapping of bigram lsa from unigram lsa for lm adaptation unigram and bigram lsa are integrated into the background ngram lm via marginal adaptation and linear interpolation respectively experimental results show that applying unigram and bigram lsa together yields relative perplexity reduction and absolute character error rates cer reduction compared to applying only unigram lsa on the mandarin rt test set comparing with the unadapted baseline our approach reduces the absolute cer by 
counting solution clusters using belief propagation counting solution clusters using belief propagation we show that an important and hardtocompute solutionspace feature of a constraint satisfaction problem csp namely the number of clusters of solutions can be accurately estimated by a technique very similar counting the number of solutions this cluster counting approach can be naturally written in terms of a factor graph and using a variant of the belief propagation inference framework we can accurately approximate cluster counts in random csp problems we illustrate the algorithm on random graph coloring instances of sizes up to vertices moreover we supply a methodology to calculate number of clusters exactly using advanced techniques from knowledge compilation domain which scale up to several hundred variables
covariance estimation for high dimensional data vectors using the sparse matrix transform covariance estimation for high dimensional data vectors using the sparse matrix transform covariance estimation for high dimensional vectors is a classically difficult problem in statistical analysis and machine learning due to limited sample size in this paper we propose a new approach to covariance estimation which is based on constrained maximum likelihood ml estimation of the covariance specifically the covariance is constrained to have an eigen decomposition which can be represented as a sparse matrix transform smt the smt is formed by a product of pairwise coordinate rotations known as givens rotations using this framework the covariance can be efficiently estimated using greedy minimization of the log likelihood function and the number of givens rotations can be efficiently computed using a crossvalidation procedure the estimator obtained using this method is always positive definite and wellconditioned even with limited sample size experiments on hyperspectral data show that smt covariance estimation results in consistently better estimates of the covariance for a variety of different classes and sample sizes compared to traditional shrinkage estimators
cyclizing clusters via zeta function of a graph cyclizing clusters via zeta function of a graph detecting underlying clusters from largescale data plays a central role in machine learning research in this paper we attempt to tackle clustering problems for complex data of multiple distributions and large multiscales to this end we develop an algorithm named zeta llinks or zell which consists of two parts zeta merging with a similarity graph and an initial set of small clusters derived from local llinks of the graph more specifically we propose to structurize a cluster using cycles in the associated subgraph a mathematical tool zeta function of a graph is introduced for the integration of all cycles leading to a structural descriptor of the cluster in determinantal form the popularity character of the cluster is conceptualized as the global fusion of variations of the structural descriptor by means of the leaveoneout strategy in the cluster zeta merging proceeds in the agglomerative fashion according to the maximum incremental popularity among all pairwise clusters experiments on toy data real imagery data and real sensory data show the promising performance of zell the accuracy in the sense of the normalized mutual information is obtained on the frgc face data of samples and facial clusters the matlab codes of zell will be made publicly available for peer evaluation
deep learning with kernel regularization for visual recognition deep learning with kernel regularization for visual recognition in this paper we focus on training deep neural networks for visual recognition tasks one challenge is the lack of an informative regularization on the network parameters to imply a meaningful control on the computed function we propose a training strategy that takes advantage of kernel methods where an existing kernel function represents useful prior knowledge about the learning task of interest we derive an efficient algorithm using stochastic gradient descent and demonstrate very positive results in a wide range of visual recognition tasks
deflation methods for sparse pca deflation methods for sparse pca in analogy to the pca setting the sparse pca problem is often solved by iteratively alternating between two subtasks cardinalityconstrained rankone variance maximization and matrix deflation while the former has received a great deal of attention in the literature the latter is seldom analyzed and is typically borrowed without justification from the pca context in this work we demonstrate that the standard pca deflation procedure is seldom appropriate for the sparse pca setting to rectify the situation we first develop several heuristic deflation alternatives with more desirable properties we then reformulate the sparse pca optimization problem to explicitly reflect the maximum additional variance objective on each round the result is a generalized deflation procedure that typically outperforms more standard techniques on realworld datasets
dependence of orientation tuning on recurrent excitation and inhibition in a network model of v dependence of orientation tuning on recurrent excitation and inhibition in a network model of v one major role of primary visual cortex v in vision is the encoding of the orientation of lines and contours the role of the local recurrent network in these computations is however still a matter of debate to address this issue we analyze intracellular recording data of cat v which combine measuring the tuning of a range of neuronal properties with a precise localization of the recording sites in the orientation preference map for the analysis we consider a network model of hodgkinhuxley type neurons arranged according to a biologically plausible twodimensional topographic orientation preference map we then systematically vary the strength of the recurrent excitation and inhibition relative to the strength of the afferent input each parametrization gives rise to a different model instance for which the tuning of model neurons at different locations of the orientation map is compared to the experimentally measured orientation tuning of membrane potential spike output excitatory and inhibitory conductances a quantitative analysis shows that the data provides strong evidence for a network model in which the afferent input is dominated by strong balanced contributions of recurrent excitation and inhibition this recurrent regime is close to a regime of instability where strong selfsustained activity of the network occurs the firing rate of neurons in the bestfitting network is particularly sensitive to small modulations of model parameters which could be one of the functional benefits of a network operating in this particular regime
dependent dirichlet process spike sorting dependent dirichlet process spike sorting in this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violations accounts for action potential waveform drift and can handle appearance and disappearance of neurons our approach is to augment a known timevarying dirichlet process that ties together a sequence of infinite gaussian mixture models one per action potential waveform observation with an interspikeintervaldependent likelihood that prohibits refractory period violations we demonstrate this model by showing results from sorting two publicly available neural data recordings for which the a partial ground truth labeling is known
depression an rl formulation and a behavioural test depression an rl formulation and a behavioural test abstract virtually all pharmacological treatments with clinical efficacy in psychiatry have effects on neuromodulatory systems reinforcement learning has provided a very detailed and rich ground for the understanding of neuromodulators effects in normal behaviour here we report a formulation of very specific notions in psychiatry in reinforcement learning terms and a decision making task testing them behaviourally we choose depression a mood disorder with intuitive links to affective learning as a first test case we argue for a very simple definition of reward sensitivity and behavioural control in a goaldirected setting design a behavioural decision making task and show that it allows related cognitive constructs to be captured in a parametric and behaviourally specific manner finally we show that these tasks allow classification of subjects into healthy and depressed groups based purely on a behavioural measure we discuss the relevance of this approach to psychitric diagnosis
designing neurophysiology experiments to optimally constrain receptive field models along parametric designing neurophysiology experiments to optimally constrain receptive field models along parametric sequential optimal design methods hold great promise for improving the efficiency of neurophysiology experiments however previous methods for optimal experimental design have incorporated only weak prior information about the underlying neural system eg the sparseness or smoothness of the receptive field here we describe how to use stronger prior information in the form of parametric models of the receptive field in order to construct optimal stimuli and further improve the efficiency of our experiments for example if we believe that the receptive field is wellapproximated by a gabor function then our method constructs stimuli that optimally constrain the gabor parameters orientation spatial frequency etc using as few experimental trials as possible more generally we may believe a priori that the receptive field lies near a known submanifold of the full parameter space in this case our method chooses stimuli in order to reduce the uncertainty along the tangent space of this submanifold as rapidly as possible applications to simulated and real data indicate that these methods may in many cases improve the experimental efficiency by an order of magnitude
diffeomorphic dimensionality reduction diffeomorphic dimensionality reduction this paper introduces a new approach to constructing meaningful lower dimensional representations of sets of data points we argue that constraining the mapping between the high and low dimensional spaces to be a diffeomorphism is a natural way of ensuring that pairwise distances are approximately preserved accordingly we develop an algorithm which diffeomorphically maps the data near to a lower dimensional subspace and then projects onto that subspace the problem of solving for the mapping is transformed into one of solving for an eulerian flow field which we compute using ideas from kernel methods we demonstrate the efficacy of our approach on various real world data sets
dimensionality reduction for data in multiple feature representations dimensionality reduction for data in multiple feature representations in solving complex visual learning tasks adopting multiple descriptors to more precisely characterize the data has been a feasible way for improving performance these representations are typically high dimensional and assume diverse forms thus finding a way to transform them into a unified space of lower dimension generally facilitates the underlying tasks such as object recognition or clustering we describe an approach that incorporates multiple kernel learning with dimensionality reduction mkldr while the proposed framework is flexible in simultaneously tackling data in various feature representations the formulation itself is general in that it is established upon graph embedding it follows that any dimensionality reduction techniques explainable by graph embedding can be generalized by our method to consider data in multiple feature representations
disclda discriminative learning for dimensionality reduction and classification disclda discriminative learning for dimensionality reduction and classification probabilistic topic models and their extensions have become popular as models of latent structures in collections of text documents or images these models are usually treated as generative models and trained using maximum likelihood estimation an approach which may be suboptimal in the context of an overall classification problem in this paper we describe disclda a discriminative learning framework for such models as latent dirichlet allocation lda in the setting of dimensionality reduction with supervised side information in disclda a classdependent linear transformation is introduced on the topic mixture proportions this parameter is estimated by maximizing the conditional likelihood using monte carlo em by using the transformed topic mixture proportions as a new representation of documents we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classification we compare the predictive power of the latent structure of disclda with unsupervised lda on the newsgroup ocument classification task
domain adaptation with multiple sources domain adaptation with multiple sources this paper presents a theoretical analysis of the problem of adaptation with multiple sources for each source domain the distribution over the input points as well as a hypothesis with error at most epsilon are given the problem consists of combining these hypotheses to derive a hypothesis with small error with respect to the target domain we present several theoretical results relating to this problem in particular we prove that standard convex combinations of the source hypotheses may in fact perform very poorly and that instead combinations weighted by the source distributions benefit from favorable theoretical guarantees our main result shows that remarkably for any fixed target function there exists a distribution weighted combining rule that has a loss of at most epsilon with respect to any target mixture of the source distributions we further generalize the setting from a single target function to multiple consistent target functions and show the existence of a combining rule with error at most epsilon finally we report empirical results for a multiple source adaptation problem with a realworld dataset
dynamic visual attention searching for coding length increments dynamic visual attention searching for coding length increments a visual attention system should respond placidly when common stimuli are presented while at the same time keep alert to anomalous visual inputs in this paper a dynamic visual attention model based on the rarity of features is proposed we introduce the incremental coding length icl to measure the perspective entropy gain of each feature the objective of our model is to maximize the entropy of the sampled visual features in order to optimize energy consumption the limit amount of energy of the system is redistributed amongst features according to their incremental coding length by selecting features with large coding length increments the computational system can achieve attention selectivity in both static and dynamic scenes we demonstrate that the proposed model achieves superior accuracy in comparison to mainstream approaches in static saliency map generation moreover we also show that our model captures several lessreported dynamic visual search behaviors such as attentional swing and inhibition of return
effects of stimulus type and of errorcorrecting code design on bci speller performance effects of stimulus type and of errorcorrecting code design on bci speller performance from an informationtheoretic perspective a noisy transmission system such as a visual braincomputer interface bci speller could benefit from the use of errorcorrecting codes however optimizing the code solely according to the maximal minimumhammingdistance criterion tends to lead to an overall increase in target frequency of target stimuli and hence a significantly reduced average targettotarget interval tti leading to difficulties in classifying the individual eventrelated potentials erps due to overlap and refractory effects clearly any change to the stimulus setup must also respect the possible psychophysiological consequences here we report new eeg data from experiments in which we explore stimulus types and codebooks in a withinsubject design finding an interaction between the two factors our data demonstrate that the traditional rowcolumn code has particular spatial properties that lead to better performance than one would expect from its ttis and hammingdistances alone but nonetheless errorcorrecting codes can improve performance provided the right stimulus type is used
efficient direct density ratio estimation for nonstationarity adaptation and outlier detection efficient direct density ratio estimation for nonstationarity adaptation and outlier detection we address the problem of estimating the ratio of two probability density functions akathe importance the importance values can be used for various succeeding tasks such as nonstationarity adaptation or outlier detection in this paper we propose a new importance estimation method that has a closedform solution the leaveoneout crossvalidation score can also be computed analytically therefore the proposed method is computationally very efficient and numerically stable we also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bound numerical experiments show that the proposed method is comparable to the best existing method in accuracy while it is computationally more efficient than competing approaches
efficient exact inference in planar ising models efficient exact inference in planar ising models we present polynomialtime algorithms for the exact computation of lowest energy states worst margin violators partition functions and marginals in binary undirected graphical models our approach provides an interesting alternative to the wellknown graph cut paradigm in that it does not impose any submodularity constraints instead we require planarity to establish a correspondence with perfect matchings in an expanded dual graph maximummargin parameter estimation for a boundary detection task shows our approach to be efampcient and effective
efficient inference in phylogenetic indel trees efficient inference in phylogenetic indel trees accurate and efficient inference in evolutionary trees is a central problem in computational biology realistic models require tracking insertions and deletions along the phylogenetic tree making inference challenging we propose new sampling techniques that speed up inference and improve the quality of the samples we compare our method to previous approaches and show performance improvement on metrics evaluating multiple sequence alignment and reconstruction of ancestral sequences
efficient sampling for gaussian process inference using control variables efficient sampling for gaussian process inference using control variables sampling functions in gaussian process gp models is challenging because of the highly correlated posterior distribution we describe an efficient markov chain monte carlo algorithm for sampling from the posterior process of the gp model this algorithm uses control variables which are auxiliary function values that provide a low dimensional representation of the function at each iteration the algorithm proposes new values for the control variables and generates the function from the conditional gp prior the control variable input locations are found by continuously minimizing an objective function we demonstrate the algorithm on regression and classification problems and we use it to estimate the parameters of a differential equation model of gene regulation
empirical performance maximization for linear rank statistics empirical performance maximization for linear rank statistics the roc curve is known to be the golden standard for measuring performance of a testscoring statistic regarding its capacity of discrimination between two populations in a wide variety of applications ranging from anomaly detection in signal processing to information retrieval through medical diagnosis most practical performance measures used in scoring applications such as the auc the local auc the pnorm push the dcg and others can be seen as summaries of the roc curve this paper highlights the fact that many of these empirical criteria can be expressed as conditional linear rank statistics we investigate the properties of empirical maximizers of such performance criteria and provide preliminary results for the concentration properties of a novel class of random variables that we will call a linear rank process
estimating robust query models with convex optimization estimating robust query models with convex optimization query expansion is a longstudied approach for improving retrieval effectiveness by enhancing the useracircs original query with additional related terms current algorithms for automatic query expansion have been shown to consistently improve retrieval accuracy on average but are highly unstable and have bad worstcase performance for individual queries we introduce a novel risk framework that formulates query model estimation as a constrained metric labeling problem on a graph of term relations themodel combines assignment costs based on a baseline feedback algorithm edge weights based on term similarity and simple constraints to enforce aspect balance aspect coverage and term centrality results across multiple standard test collections show consistent and dramatic reductions in the number and magnitude of expansion failures while retaining the strong positive gains of the baseline algorithm
estimating the location and orientation of complex correlated neural activity using meg estimating the location and orientation of complex correlated neural activity using meg the synchronous brain activity measured via meg or eeg can be interpreted as arising from a collection possibly large of current dipoles or sources located throughout the cortex estimating the number location and orientation of these sources remains a challenging task one that is significantly compounded by the effects of source correlations and the presence of interference from spontaneous brain activity sensor noise and other artifacts this paper derives an empirical bayesian method for addressing each of these issues in a principled fashion the resulting algorithm guarantees descent of a cost function uniquely designed to handle unknown orientations and arbitrary correlations robust interference suppression is also easily incorporated in a restricted setting the proposed method is shown to have theoretically zero bias estimating both the location and orientation of multicomponent dipoles even in the presence of correlations unlike a variety of existing bayesian localization methods or common signal processing techniques such as beamforming and sloreta empirical results on both simulated and real data sets verify the efficacy of this approach
estimating vector fields using sparse basis field expansions estimating vector fields using sparse basis field expansions we introduce a novel framework for estimating vector fields using sparse basis field expansions sflex the notion of basis fields which are an extension of scalar basis functions arises naturally in our framework from a rotational invariance requirement we consider a regression setting as well as inverse problems all variants discussed lead to secondorder cone programming formulations while our framework is generally applicable to any type of vector field we focus in this paper on applying it to solving the eegmeg inverse problem it is shown that significantly more precise and neurophysiologically more plausible location and shape estimates of cerebral current sources from eegmeg measurements become possible with our method when comparing to the stateoftheart
estimation of information theoretic measures for continuous random variables estimation of information theoretic measures for continuous random variables we analyze the estimation of information theoretic measures of continuous random variables such as differential entropy mutual information or kullbackleibler divergence the objective of this paper is twofold first we prove that the information theoretic measure estimates using the knearestneighbor density estimation with fixed k converge almost surely even though the knearestneighbor density estimation with fixed k does not converge to its true measure second we show that the information theoretic measure estimates do not converge for k growing linearly with the number of samples nevertheless these nonconvergent estimates can be used for solving the twosample problem and assessing if two random variables are independent we show that the twosample and independence tests based on these nonconvergent estimates compare favorably with the maximum mean discrepancy test and the hilbert schmidt independence criterion respectively
evaluating probabilities under highdimensional latent variable models evaluating probabilities under highdimensional latent variable models we present a simple new monte carlo algorithm for evaluating probabilities of observations in complex latent variable models such as deep belief networks while the method is based on markov chains estimates based on short runs are formally unbiased in expectation the log probability of a test set will be underestimated and this could form the basis of a probabilistic bound the method is much cheaper than goldstandard annealingbased methods and only slightly more expensive than the cheapest monte carlo methods we give examples of the new method substantially improving simple variational bounds at modest extra cost
exact convex confidenceweighted learning exact convex confidenceweighted learning confidenceweighted cw learning an online learning method for linear classifiers maintains a gaussian distributions over weight vectors with a covariance matrix that represents uncertainty about weights and correlations confidence constraints ensure that a weight vector drawn from the hypothesis distribution correctly classifies examples with a specified probability within this framework we derive a new convex form of the constraint and analyze it in the mistake bound model empirical evaluation with both synthetic and text data shows our version of cw learning achieves lower cumulative and outofsample errors than commonly used firstorder and secondorder online methods
exploring large feature spaces with hierarchical multiple kernel learning exploring large feature spaces with hierarchical multiple kernel learning for supervised and unsupervised learning positive definite kernels allow to use large and potentially infinite dimensional feature spaces with a computational cost that only depends on the number of observations this is usually done through the penalization of predictor functions by euclidean or hilbertian norms in this paper we explore penalizing by sparsityinducing norms such as the lnorm or the block lnorm we assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework in polynomial time in the number of selected kernels this framework is naturally applied to non linear variable selection our extensive simulations on synthetic datasets and datasets from the uci repository show that efficiently exploring the large feature space through sparsityinducing norms leads to stateoftheart predictive performance
extended grassmann kernels for subspacebased learning extended grassmann kernels for subspacebased learning subspacebased learning problems involve data whose elements are linear subspaces of a vector space to handle such data structures grassmann kernels have been proposed and used previously in this paper we analyze the relationship between grassmann kernels and probabilistic similarity measures firstly we show that the kl distance in the limit yields the projection kernel on the grassmann manifold whereas the bhattacharyya kernel becomes trivial in the limit and is suboptimal for subspacebased problems secondly based on our analysis of the kl distance we propose extensions of the projection kernel which can be extended to the set of affine as well as scaled subspaces we demonstrate the advantages of these extended kernels for classification and recognition tasks with support vector machines and kernel discriminant analysis using synthetic and real image databases
fast computation of posterior mode in multilevel hierarchical models fast computation of posterior mode in multilevel hierarchical models multilevel hierarchical models provide an attractive framework for incorporating correlations induced in a response variable organized in a hierarchy model fitting is challenging especially for hierarchies with large number of nodes we provide a novel algorithm based on a multiscale kalman filter that is both scalable and easy to implement for nongaussian responses quadratic approximation to the loglikelihood results in biased estimates we suggest a bootstrap strategy to correct such biases our method is illustrated through simulation studies and analyses of real world data sets in health care and online advertising
fast highdimensional kernel summations using the monte carlo multipole method fast highdimensional kernel summations using the monte carlo multipole method we propose a new fast gaussian summation algorithm for highdimensional datasets with high accuracy first we extend the original fast multipoletype methods to use approximation schemes with both hard and probabilistic error second we utilize a new data structure called subspace tree which maps each data point in the node to its lower dimensional mapping as determined by any linear dimension reduction method such as pca this new data structure is suitable for reducing the cost of each pairwise distance computation the most dominant cost in many kernel methods our algorithm guarantees probabilistic relative error on each kernel sum and can be applied to highdimensional gaussian summations which are ubiquitous inside many kernel methods as the key computational bottleneck we provide empirical speedup results on low to highdimensional datasets up to dimensions
fast prediction on a tree fast prediction on a tree given an nvertex weighted tree with structural diameter s and a subset of m vertices we present a technique to compute a corresponding m times m gram matrix of the pseudoinverse of the graph laplacian in on m m s time we discuss the application of this technique to fast label prediction on a generic graph we approximate the graph with a spanning tree and then we predict with the kernel perceptron we address the approximation of the graph with either a minimum spanning tree or a shortest path tree the fast computation of the pseudoinverse enables us to address prediction problems on large graphs to this end we present experiments on two webspam classification tasks one of which includes a graph with nodes and more than edges the results indicate that the accuracy of our technique is competitive with previous methods using the full graph information
fast rates for regularized objectives fast rates for regularized objectives we show that the empirical minimizer of a stochastic strongly convex objective where the stochastic component is linear converges to the population minimizer with rate on the result applies in particular to the svm objective thus we get a rate of on on the convergence of the svm objective to its infinite data limit we demonstrate how this is essential for obtaining tight oracle inequalities for svms the results extend also to strong convexity with respect to other ellnormp norms and so also to objectives regularized using other norms
finding latent causes in causal networks an efficient approach based on markov blankets finding latent causes in causal networks an efficient approach based on markov blankets causal structurediscovery techniques usually assume that all causes of more than one variable are observed this is the socalled causal sufficiency assumption in practice it is untestable and often violated in this paper we present an efficient causal structurelearning algorithm suited for causally insufficient data similar to algorithms such as ic and fci the proposed approach drops the causal sufficiency assumption and learns a structure that indicates potential latent causes for pairs of observed variables assuming a constant local density of the datagenerating graph our algorithm makes a quadratic number of conditionalindependence tests wrt the number of variables we show with experiments that our algorithm is comparable to the stateoftheart fci algorithm in accuracy while being several orders of magnitude faster on large problems we conclude that mbcs makes a new range of causally insufficient problems computationally tractable
fitted qiteration by advantage weighted regression fitted qiteration by advantage weighted regression recently fitted qiteration fqi based methods have become more popular due to their increased sample efficiency a more stable learning process and the higher quality of the resulting policy however these methods remain hard to use for continuous action spaces which frequently occur in realworld tasks eg in robotics and other technical applications the greedy action selection commonly used for the policy improvement step is particularly problematic as it is expensive for continuous actions can cause an unstable learning process introduces an optimization bias and results in highly nonsmooth policies unsuitable for realworld systems in this paper we show that by using a softgreedy action selection the policy improvement step used in fqi can be simplified to an inexpensive advantageweighted regression with this result we are able to derive a new computationally efficient fqi algorithm which can even deal with high dimensional action spaces
from online to batch learning with cutoffaveraging from online to batch learning with cutoffaveraging we present quotcutoff averagingquot a technique for converting any conservative online learning algorithm into a batch learning algorithm most onlinetobatch conversion techniques work well with certain types of online learning algorithms and not with others whereas cutoff averaging explicitly tries to adapt to the characteristics of the online algorithm being converted an attractive property of our technique is that it preserves the efficiency of the original online algorithm making it approporiate for largescale learning problems we provide a statistical analysis of our technique and back our theoretical claims with experimental results
gates gates gates are a new notation for representing mixture models and contextsensitive independence in factor graphs factor graphs provide a natural representation for messagepassing algorithms such as expectation propagation however message passing in mixture models is not well captured by factor graphs unless the entire mixture is represented by one factor because the message equations have a containment structure gates capture this containment structure graphically allowing both the independences and the messagepassing equations for a model to be readily visualized different variational approximations for mixture models can be understood as different ways of drawing the gates in a model we present general equations for expectation propagation and variational message passing in the presence of gates
gaussianprocess factor analysis for lowdimensional singletrial analysis of neural population activity gaussianprocess factor analysis for lowdimensional singletrial analysis of neural population activity we consider the problem of extracting smooth lowdimensional neural trajectories that summarize the activity recorded simultaneously from tens to hundreds of neurons on individual experimental trials beyond the benefit of visualizing the highdimensional noisy spiking activity in a compact denoised form such trajectories can offer insight into the dynamics of the neural circuitry underlying the recorded activity current methods for extracting neural trajectories involve a twostage process the data are first denoised by smoothing over time then a static dimensionality reduction technique is applied we first describe extensions of the twostage methods that allow the degree of smoothing to be chosen in a principled way and account for spiking variability that may vary both across neurons and across time we then present a novel method for extracting neural trajectories gaussianprocess factor analysis gpfa which unifies the smoothing and dimensionality reduction operations in a common probabilistic framework we applied these methods to the activity of neurons recorded simultaneously in macaque premotor and motor cortices during reach planning and execution by adopting a goodnessoffit metric that measures how well the activity of each neuron can be predicted by all other recorded neurons we found that gpfa provided a better characterization of the population activity than the twostage methods from the extracted singletrial neural trajectories we directly observed a convergence in neural state during motor planning an effect suggestive of attractor dynamics that was shown indirectly by previous studies
generative and discriminative learning with unknown labeling bias generative and discriminative learning with unknown labeling bias we apply robust bayesian decision theory to improve both generative and discriminative learners under bias in class proportions in labeled training data when the true class proportions are unknown for the generative case we derive an entropybased weighting that maximizes expected log likelihood under the worstcase true class proportions for the discriminative case we derive a multinomial logistic model that minimizes worstcase conditional log loss we apply our theory to the modeling of species geographic distributions from presence data an extreme case of label bias since there is no absence data on a benchmark dataset we find that entropybased weighting offers an improvement over constant estimates of class proportions consistently reducing log loss on unbiased test data
global ranking using continuous conditional random fields global ranking using continuous conditional random fields this paper studies global ranking problem by learning to rank methods conventional learning to rank methods are usually designed for local ranking in the sense that the ranking model is defined on a single object for example a document in information retrieval for many applications this is a very loose approximation relations always exist between objects and it is better to define the ranking model as a function on all the objects to be ranked ie the relations are also included this paper refers to the problem as global ranking and proposes employing a continuous conditional random fields crf for conducting the learning task the continuous crf model is defined as a conditional probability distribution over ranking scores of objects conditioned on the objects it can naturally represent the content information of objects as well as the relation information between objects necessary for global ranking taking two specific information retrieval tasks as examples the paper shows how the continuous crf method can perform global ranking better than baselines
goaldirected decision making in prefrontal cortex a computational framework goaldirected decision making in prefrontal cortex a computational framework research in animal learning and behavioral neuroscience has distinguished between two forms of action control a habitbased form which relies on stored action values and a goaldirected form which forecasts and compares action outcomes based on a model of the environment while habitbased control has been the subject of extensive computational research the computational principles underlying goaldirected control in animals have so far received less attention in the present paper we advance a computational framework for goaldirected control in animals and humans we take three empirically motivated points as founding premises neurons in dorsolateral prefrontal cortex represent action policies neurons in orbitofrontal cortex represent rewards and neural computation across domains can be appropriately understood as performing structured probabilistic inference on a purely computational level the resulting account relates closely to previous work using bayesian inference to solve markov decision problems but extends this work by introducing a new algorithm which provably converges on optimal plans on a cognitive and neuroscientific level the theory provides a unifying framework for several different forms of goaldirected action selection placing emphasis on a novel form within which orbitofrontal reward representations directly drive policy selection
grouping contours via a related image grouping contours via a related image contours have been established in the biological and computer vision literatures as a compact yet descriptive representation of object shape while individual contours provide structure they lack the large spatial support of region segments which lack internal structure we present a method for further grouping of contours in an image using their relationship to the contours of a second related image stereo motion and similarity all provide cues that can aid this task contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group to find matches for contours we rely only on shape which applies directly to all three modalities without modification in constrant to the specialized approaches developed for each independently visually salient contours are extracted in each image along with a set of candidate transformations for aligning subsets of them for each transformation groups of contours with matching shape across the two images are identified to provide a context for evaluating matches of individual contour points across the images the resulting contexts of contours are used to perform a final grouping on contours in the original image while simultaneously finding matches in the related image again by shape matching we demonstrate grouping results on image pairs consisting of stereo motion and similar images our method also produces qualitatively better results against a baseline method that does not use the inferred contexts
hebbian learning of bayes optimal decisions hebbian learning of bayes optimal decisions uncertainty is omnipresent when we perceive or interact with our environment and the bayesian framework provides computational methods for dealing with it mathematical models for bayesian decision making typically require datastructures that are hard to implement in neural networks this article shows that even the simplest and experimentally best supported type of synaptic plasticity hebbian learning in combination with a sparse redundant neural code can in principle learn to infer optimal bayesian decisions we present a concrete hebbian learning rule operating on logprobability ratios modulated by rewardsignals this hebbian plasticity rule also provides a new perspective for understanding how bayesian inference could support fast reinforcement learning in the brain in particular we show that recent experimental results by yang and shadlen on reinforcement learning of probabilistic inference in primates can be modeled in this way
hierarchical conditional random fields for recursive sequential data hierarchical conditional random fields for recursive sequential data inspired by the hierarchical hidden markov models hhmm we present the hierarchical conditional random field hcrf a generalisation of embedded undirected markov chains to model complex hierarchical nested markov processes it is parameterised in a discriminative framework and has polynomial time algorithms for learning and inference importantly we consider partiallysupervised learning and propose algorithms for generalised partiallysupervised learning and constrained inference we demonstrate the hcrf in two applications i recognising human activities of daily living adls from indoor surveillance cameras and ii nounphrase chunking we show that the hcrf is capable of learning rich hierarchical models with reasonable accuracy in both fully and partially observed data cases
hierarchical fisher kernels for longitudinal data hierarchical fisher kernels for longitudinal data we develop new techniques for time series classification based on hierarchical bayesian generative models called mixedeffect models and the fisher kernel derived from them a key advantage of the new formulation is that one can compute the fisher information matrix despite varying sequence lengths and sampling times we therefore can avoid the ad hoc replacement of fisher information matrix with the identity matrix commonly used in literature which destroys the geometrical grounding of the kernel construction in contrast our construction retains the proper geometric structure resulting in a kernel that is properly invariant under change of coordinates in the model parameter space experiments on detecting cognitive decline show that classifiers based on the proposed kernel outperform those based on generative models and other feature extraction routines
highdimensional union support recovery in multivariate regression highdimensional union support recovery in multivariate regression we study the behavior of block ll regularization for multivariate regression where a kdimensional response vector is regressed upon a fixed set of p covariates we are interested in sparse multivariate regression specifically our goal is to recover the subset of covariates that are active in at least one of the regression problems we refer to this as the union support recovery problem studying this problem under highdimensional scaling where the number p of problem parameters as well as sample size n tend to infinity simultaneously our main result is to show that exact recovery is possible once the sample complexity parameter given by thetan p sb npsib logps exceeds a critical threshold here n is the sample size p is the ambient dimension of the regression model s is the size of the union of supports and psib is a sparsityoverlap function that measures a combination of the sparsities and overlaps of the kregression coefficient vectors that constitute the model this sparsityoverlap function reveals that under some structural assumptions block ll regularization for multivariate regression never harms performance relative to a naive lapproach and can yield substantial improvements in sample complexity up to a factor of k when the regression vectors are suitably orthogonal although our statements are made in an asymptotic form for clarity the analysis itself is nonasymptotic in nature yielding explicit bounds on loss for finite choices of the triple n p s we complement our theoretical results with simulations that demonstrate the sharpness of the result even for relatively small problems egp 
how memory biases affect information transmission a rational analysis of serial reproduction how memory biases affect information transmission a rational analysis of serial reproduction many human interactions involve pieces of information being passed from one person to another raising the question of how this process of information transmission is affected by the capacities of the agents involved in the s sir frederic bartlett explored the influence of memory biases in acircserial reproductionacirc of information in which one personacircs reconstruction of a stimulus from memory becomes the stimulus seen by the next person these experiments were done using relatively uncontrolled stimuli such as pictures and stories but suggested that serial reproduction would transform information in a way that reflected the biases inherent in memory we formally analyze serial reproduction using a bayesian model of reconstruction from memory giving a general result characterizing the effect of memory biases on information transmission we then test the predictions of this account in two experiments using simple onedimensional stimuli our results provide theoretical and empirical justification for the idea that serial reproduction reflects memory biases
human active learning human active learning we investigate a topic at the interface of machine learning and cognitive science human active learning where learners can actively query the world for information is contrasted with passive learning from random examples furthermore we compare human active learning performance with predictions from statistical learning theory we conduct a series of human category learning experiments inspired by a machine learning task for which active and passive learning error bounds are well understood and dramatically distinct our results indicate that humans are capable of actively selecting informative queries and in doing so learn better and faster than if they are given random training data as predicted by learning theory however the improvement over passive learning is not as dramatic as that achieved by machine active learning algorithms to the best of our knowledge this is the first quantitative study comparing human category learning in active versus passive settings
ica based on a smooth estimation of the differential entropy ica based on a smooth estimation of the differential entropy in this paper we introduce the meannn approach for estimation of main information theoretic measures such as differential entropy mutual information and divergence as opposed to other nonparametric approaches the meannn results in smooth differentiable functions of the data samples with clear geometrical interpretation then we apply the proposed estimators to the ica problem and obtain a smooth expression for the mutual information that can be analytically optimized by gradient descent methods the improved performance on the proposed ica algorithm is demonstrated on standard tests in comparison with stateoftheart techniques
implicit mixtures of restricted boltzmann machines implicit mixtures of restricted boltzmann machines we present a mixture model whose components are restricted boltzmann machines rbms this possibility has not been considered before because computing the partition function of an rbm is intractable which appears to make learning a mixture of rbms intractable as well surprisingly when formulated as a thirdorder boltzmann machine such a mixture model can be learned tractably using contrastive divergence the energy function of the model captures threeway interactions among visible units hidden units and a single hidden multinomial unit that represents the cluster labels the distinguishing feature of this model is that unlike other mixture models the mixing proportions are not explicitly parameterized instead they are defined implicitly via the energy function and depend on all the parameters in the model we present results for the mnist and norb datasets showing that the implicit mixture of rbms learns clusters that reflect the class structure in the data
improved moves for truncated convex models improved moves for truncated convex models we consider the problem of obtaining the approximate maximum a posteriori estimate of a discrete random field characterized by pairwise potentials that form a truncated convex model for this problem we propose an improved stmincut based move making algorithm unlike previous move making approaches which either provide a loose bound or no bound on the quality of the solution in terms of the corresponding gibbs energy our algorithm achieves the same guarantees as the standard linear programming lp relaxation compared to previous approaches based on the lp relaxation eg interiorpoint algorithms or treereweighted message passing trw our method is faster as it uses only the efficient stmincut algorithm in its design furthermore it directly provides us with a primal solution unlike trw and other related methods which attempt to solve the dual of the lp we demonstrate the effectiveness of the proposed approach on both synthetic and standard real data problems our analysis also opens up an interesting question regarding the relationship between move making algorithms such as alphaexpansion and the algorithms presented in this paper and the randomized rounding schemes used with convex relaxations we believe that further explorations in this direction would help design efficient algorithms for more complex relaxations
improving on expectation propagation improving on expectation propagation we develop as series of corrections to expectation propagation ep which is one of the most popular methods for approximate probabilistic inference these corrections can lead to improvements of the inference approximation or serve as a sanity check indicating when ep yields unrealiable results
inferring rankings under constrained sensing inferring rankings under constrained sensing motivated by applications such as elections webpage ranking revenue maximization in gambling etc we consider the question of inferring popular rankings in many of these applications the number of distinct popular rankings are usually very few however the data available for inferring them is highly constrained as the first main result of this paper we provide a simple and novel algorithm to recover the popular rankings and their popularity exactly under a natural set of assumptions specifically under a natural stochastic model our algorithm recovers them exactly as long as the number of distinct popular rankings up to on over n candidates or elements in certain applications like ranked election the interest is only in recovering the most popular or mode ranking as the second result we provide an algorithm based on fourier transform over symmetric group to recover the most popular ranking under natural majority condition interestingly enough this algorithm becomes a maximum weight matching on an appropriately defined weighted bipartite graph the questions considered in this paper are thematically related to the currently popular topic of compressed sensing the existing results in the literature do not apply to our problem as the information is constrained and can not be sampled randomly thus the results of this paper correspond to constrained compressed sensing
influence of graph construction on graphbased clustering measures influence of graph construction on graphbased clustering measures graph clustering methods such as spectral clustering are defined for general weighted graphs in machine learning however data often is not given in form of a graph but in terms of similarity or distance values between points in this case first a neighborhood graph is constructed using the similarities between the points and then a graph clustering algorithm is applied to this graph in this paper we investigate the influence of the construction of the similarity graph on the clustering results from a theoretical point of view we first study the convergence of graph clustering criteria such as the normalized cut ncut as the sample size tends to infinity we find that the limit expressions are different for different types of graph for example the rneighborhood graph or the knearest neighbor graph in plain words ncut on a knn graph does something systematically different than ncut on an rneighborhood graph this finding shows that graph clustering criteria cannot be studied independently of the kind of graph they will be applied to we also provide examples which show how those differences lead to big differences in clustering results in practice
integrating locally learned causal structures with overlapping variables integrating locally learned causal structures with overlapping variables in many domains data are distributed among datasets that share only some variables other recorded variables may occur in only one dataset there are several asymptotically correct informative algorithms that search for causal information given a single dataset even with missing values and hidden variables there are however no such reliable procedures for distributed data with overlapping variables and only a single heuristic procedure structural em this paper describes an asymptotically correct procedure ion that provides all the information about structure obtainable from the marginal independence relations using simulated and real data the accuracy of ion is compared with that of structural em and with inference on complete unified data
interpreting the neural code with formal concept analysis interpreting the neural code with formal concept analysis we propose a novel application of formal concept analysis fca to neural decoding instead of just trying to figure out which stimulus was presented we demonstrate how to explore the semantic relationships between the neural representation of large sets of stimuli fca provides a way of displaying and interpreting such relationships via concept lattices we explore the effects of neural code sparsity on the lattice we then analyze neurophysiological data from highlevel visual cortical area stsa using an exact bayesian approach to construct the formal context needed by fca prominent features of the resulting concept lattices are discussed including indications for a productofexperts code in real neurons
kernel changepoint analysis kernel changepoint analysis we introduce a kernelbased method for changepoint analysis within a sequence of temporal observations changepoint analysis of an unlabelled sample of observations consists in first testing whether a change in the distribution occurs within the sample and second if a change occurs estimating the changepoint instant after which the distribution of the observations switches from one distribution to another different distribution we propose a test statistics based upon the maximum kernel fisher discriminant ratio as a measure of homogeneity between segments we derive its limiting distribution under the null hypothesis no change occurs and establish the consistency under the alternative hypothesis a change occurs this allows to build a statistical hypothesis testing procedure for testing the presence of changepoint with a prescribed falsealarm probability and detection probability tending to one in the largesample setting if a change actually occurs the test statistics also yields an estimator of the changepoint location promising experimental results in temporal segmentation of mental tasks from bci data and pop song indexation are presented
kernel measures of independence for noniid data kernel measures of independence for noniid data many machine learning algorithms can be formulated in the framework of statistical independence such as the hilbert schmidt independence criterion in this paper we extend this criterion to deal with with structured and interdependent observations this is achieved by modeling the structures using undirected graphical models and comparing the hilbert space embeddings of distributions we apply this new criterion to independent component analysis and sequence clustering
kernelarma for hand tracking and brainmachine interfacing during d motor control kernelarma for hand tracking and brainmachine interfacing during d motor control using machine learning algorithms to decode intended behavior from neural activity serves a dual purpose first these tools can be used to allow patients to interact with their environment through a brainmachine interface bmi second analysis of the characteristics of such methods can reveal the significance of various features of neural activity stimuli and responses to the encodingdecoding task in this study we adapted implemented and tested a machine learning method called kernel autoregressive moving average karma for the task of inferring movements from neural activity in primary motor cortex our version of this algorithm is used in an online learning setting and is updated when feedback from the last inferred sequence become available we first used it to track real hand movements executed by a monkey in a standard d motor control task we then applied it in a closedloop bmi setting to infer intended movement while arms were restrained allowing a monkey to perform the task using the bmi alone karma is a recurrent method that learns a nonlinear model of output dynamics it uses similarity functions termed kernels to compare between inputs these kernels can be structured to incorporate domain knowledge into the method we compare karma to various stateoftheart methods by evaluating tracking performance and present results from the karma based bmi experiments
kernelized sorting kernelized sorting object matching is a fundamental operation in data analysis it typically requires the definition of a similarity measure between the classes of objects to be matched instead we develop an approach which is able to perform matching by requiring a similarity measure only within each of the classes this is achieved by maximizing the dependency between matched pairs of observations by means of the hilbert schmidt independence criterion this problem can be cast as one of maximizing a quadratic assignment problem with special structure and we present a simple algorithm for finding a locally optimal solution
large margin taxonomy embedding for document categorization large margin taxonomy embedding for document categorization applications of multiclass classification such as document categorization often appear in costsensitive settings recent work has significantly improved the state of the art by moving beyond flat classification through incorporation of class hierarchies cai and hoffman we present a novel algorithm that goes beyond hierarchical classification and estimates the latent semantic space that underlies the class hierarchy in this space each class is represented by a prototype and classification is done with the simple nearest neighbor rule the optimization of the semantic space incorporates large margin constraints that ensure that for each instance the correct class prototype is closer than any other we show that our optimization is convex and can be solved efficiently for large data sets experiments on the ohsumed medical journal data base yield stateoftheart results on topic categorization
learning a discriminative hidden part model for human action recognition learning a discriminative hidden part model for human action recognition we present a discriminative partbased approach for human action recognition from video sequences using motion features our model is based on the recently proposed hidden conditional random fieldhcrf for object recognition similar to hcrf for object recognition we model a human action by a flexible constellation of parts conditioned on image observations different from object recognition our model combines both largescale global features and local patch features to distinguish various actions our experimental results show that our model is comparable to other stateoftheart approaches in action recognition in particular our experimental results demonstrate that combining largescale global features and local patch features performs significantly better than directly applying hcrf on local patches alone
learning bounded treewidth bayesian networks learning bounded treewidth bayesian networks with the increased availability of data for complex domains it is desirable to learn bayesian network structures that are sufficiently expressive for generalization while also allowing for tractable inference while the method of thin junction trees can in principle be used for this purpose its fully greedy nature makes it prone to overfitting particularly when data is scarce in this work we present a novel method for learning bayesian networks of bounded treewidth that employs global structure modifications and that is polynomial in the size of the graph and the treewidth bound at the heart of our method is a triangulated graph that we dynamically update in a way that facilitates the addition of chain structures that increase the bound on the models treewidth by at most one we demonstrate the effectiveness of our treewidthfriendly method on several reallife datasets importantly we also show that by using global operators we are able to achieve better generalization even when learning bayesian networks of unbounded treewidth
learning hybrid models for image annotation with partially labeled data learning hybrid models for image annotation with partially labeled data extensive labeled data for image annotation systems which learn to assign class labels to image regions is difficult to obtain we explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction we propose three alternative formulations for imposing a spatial smoothness prior on the image labels tests of the new models and some baseline approaches on two real image datasets demonstrate the effectiveness of incorporating the latent structure
learning taxonomies by dependence maximization learning taxonomies by dependence maximization we introduce a family of unsupervised algorithms numerical taxonomy clustering to simultaneously cluster data and to learn a taxonomy that encodes the relationship between the clusters the algorithms work by maximizing the dependence between the taxonomy and the original data the resulting taxonomy is a more informative visualization of complex data than simple clustering in addition taking into account the relations between different clusters is shown to substantially improve the quality of the clustering when compared with stateoftheart algorithms in the literature both spectral clustering and a previous dependence maximization approach we demonstrate our algorithm on image and text data
learning the semantic correlation an alternative way to gain from unlabeled text learning the semantic correlation an alternative way to gain from unlabeled text in this paper we address the question of what kind of knowledge is generally transferable from unlabeled text we suggest and analyze the semantic correlation of words as a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model this semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameters for any specific task in the same space through regularization in an empirical study we construct different text classification tasks from a realworld benchmark and the unlabeled documents are a mixture from all these tasks we test the ability of various algorithms to use the mixed unlabeled text to enhance all classification tasks empirical results show that the proposed approach is a reliable and scalable method for semisupervised learning regardless of the source of unlabeled data the specific task to be enhanced and the prediction model used
learning to use working memory in partially observable environments through dopaminergic reinforcement learning to use working memory in partially observable environments through dopaminergic reinforcement working memory is a central topic of cognitive neuroscience because it is critical for solving real world problems in which information from multiple temporally distant sources must be combined to generate appropriate behavior however an often neglected fact is that learning to use working memory effectively is itself a difficult problem the quotgatingquot framework is a collection of psychological models that show how dopamine can train the basal ganglia and prefrontal cortex to form useful working memory representations in certain types of problems we bring together gating with ideas from machine learning about using finite memory systems in more general problems thus we present a normative gating model that learns by online temporal difference methods to use working memory to maximize discounted future rewards in general partially observable settings the model successfully solves a benchmark working memory problem and exhibits limitations similar to those observed in human experiments moreover the model introduces a concise normative definition of high level cognitive concepts such as working memory and cognitive control in terms of maximizing discounted future rewards
learning transformational invariants from timevarying natural images learning transformational invariants from timevarying natural images we describe a hierarchical probabilistic model that learns to extract complex motion from movies of the natural environment the model consists of two hidden layers the first layer produces a sparse representation of the image that is expressed in terms of local amplitude and phase variables the second layer learns the higherorder structure among the timevarying phase variables after training on natural movies the top layer units discover the structure of phaseshifts within the first layer we show that the top layer units encode transformational invariants they are selective for the speed and direction of a moving pattern but are invariant to its spatial structure orientationspatialfrequency the diversity of units in both the intermediate and top layers of the model provides a set of testable predictions for representations that might be found in v and mt in addition the model on of synchronous grammar categories for phrasal translations showing improvements in translation performance over previously proposed maximum likelihood models we present a novel method for inducing synchronous context free grammars scfgs from a corpus of parallel string pairs scfgs can model equivalence between strings in terms of substitutions insertions and deletions and the reordering of substrings we develop a nonparametric bayesian model and apply it to a machine translation task using priors to replace the various heuristics commonly used in this field using a variational bayes training procedure we learn the latent structure of translation equivalence through the induction of synchronous grammar categories for phrasal translations showing improvements in translation performance over previously proposed maximum likelihood models
onsiderations and on the intuition that a good kernelbased inductive function should be consistent with both the data and the kernel a novel learning scheme is proposed the advantages of this scheme lie in its corresponding representer theorem its strong interpretation ability about what kind of functions should not be penalized and its promising accuracy improvements shown in a number of experiments furthermore we provide a detailed technical description about heat kernels which serves as an example for the readers to apply similar techniques for other kernels our work provides a preliminary step in a new direction to explore the varying consistency between inductive functions and kernels under various distributions
learning with locally linear feature regularization learning with locally linear feature regularization many semisupervised learning problems have very highdimensional feature spaces that make even the local distances required by semisupervised techniques such as manifold regularization difficult to estimate this paper presents locally linear feature regularization llfr a framework that exploits relationships among features to constrain model parameter space llfr works by constructing a neighborhood graph on features then penalizing the parameter for each feature based on the difference to the average parameter values of its neighbors the feature graphs we exploit can be computed either from unlabeled data or from declarative human knowledge we show that for text classification with a graph on features constructed from unlabeled feature cooccurrences llfr outperforms manifold regularization and compares favorably to more recent semisupervised methods for sentiment analysis both semisupervised and manually constructed feature graphs significantly improve prediction accuracy
linear classification and selective sampling under low noise conditions linear classification and selective sampling under low noise conditions we provide a new analysis of an efficient marginbased algorithm for selective sampling in classification problems using the socalled tsybakov low noise condition to parametrize the instance distribution we show bounds on the convergence rate to the bayes risk of both the fully supervised and the selective sampling versions of the basic algorithm our analysis reveals that excluding logarithmic factors the average risk of the selective sampler converges to the bayes risk at rate nalphaalpha with labels being sampled at the same rate here n denotes the sample size and alpha gt is the exponent in the low noise condition we compare this convergence rate to the rate nalphaalpha achieved by the fully supervised algorithm using all labels experiments on textual data reveal that simple variants of the proposed selective sampler perform much better than popular and similarly efficient competitors
load and attentional bayes load and attentional bayes selective attention is a most intensively studied psychological phenomenon rife with theoretical suggestions and schisms a critical idea is that of limited capacity the allocation of which has produced half a centurys worth of conflict about such phenomena as early and late selection an influential resolution of this debate is based on the notion of perceptual load lavie tics which suggests that lowload easy tasks because they underuse the total capacity of attention mandatorily lead to the processing of stimuli that are irrelevant to the current attentional set whereas highload difficult tasks grab all resources for themselves leaving distractors high and dry we argue that this theory presents a challenge to bayesian theories of attention and suggest an alternative statistical account of key supporting data
local gaussian process regression for real time online model learning local gaussian process regression for real time online model learning learning in realtime applications eg online approximation of the inverse dynamics model for modelbased robot control requires fast online regression techniques inspired by local learning we propose a method to speed up standard gaussian process regression gpr with local gp models lgp the training data is partitioned in local regions for each an individual gp model is trained the prediction for a query point is performed by weighted estimation using nearby local models unlike other gp approximations such as mixtures of experts we use a distance based measure for partitioning of the data and weighted prediction the proposed method achieves online learning and prediction in realtime comparisons with other nonparametric regression methods show that lgp has higher accuracy than lwpr and close to the performance of standard gpr and nusvr
localized sliced inverse regression localized sliced inverse regression we developed localized sliced inverse regression for supervised dimension reduction it has the advantages of preventing degeneracy increasing estimation accuracy and automatic subclass discovery in classification problems a semisupervised version is proposed for the use of unlabeled data the utility is illustrated on simulated as well as real data sets
loops localizing object outlines using probabilistic shape loops localizing object outlines using probabilistic shape discriminative tasks including object categorization and detection are central components of highlevel computer vision sometimes however we are interested in more refined aspects of the object in an image such as pose or particular regions in this paper we develop a method loops for learning a shape and image feature model that can be trained on a particular object class and used to outline instances of the class in novel images furthermore while the training data consists of uncorresponded outlines the resulting loops model contains a set of landmark points that appear consistently across instances and can be accurately localized in an image the resulting localization can then be used to address a range of tasks including descriptive classification search and clustering
mas a multiplicative approximation scheme for probabilistic inference mas a multiplicative approximation scheme for probabilistic inference we propose a multiplicative approximation scheme mas for inference problems in graphical models which can be applied to various inference algorithms the method uses epsilondecompositions which decompose functions used throughout the inference procedure into functions over smaller sets of variables with a known error epsilon mas translates these local approximations into bounds on the accuracy of the results we show how to optimize epsilondecompositions and provide a fast closedform solution for an l approximation applying mas to the variable elimination inference algorithm we introduce an algorithm we call dynadecomp which is extremely fast in practice and provides guaranteed error bounds on the result the superior accuracy and efficiency of dynadecomp is demonstrated
mcboost multiple classifier boosting for perceptual coclustering of images and visual features mcboost multiple classifier boosting for perceptual coclustering of images and visual features we present a new coclustering problem of images and visual features the problem involves a set of nonobject images in addition to a set of object images and features to be coclustered coclustering is performed in a way of maximising discrimination of object images from nonobject images thus emphasizing discriminative features this provides a way of obtaining perceptual jointclusters of object images and features we tackle the problem by simultaneously boosting multiple strong classifiers which compete for images by their expertise each boosting classifier is an aggregation of weaklearners ie simple visual features the obtained classifiers are useful for multicategory and multiview object detection tasks experiments on a set of pedestrian images and a face data set demonstrate that the method yields intuitive image clusters with associated features and is much superior to conventional boosting classifiers in object detection tasks
mdps with nondeterministic policies mdps with nondeterministic policies markov decision processes mdps have been extensively studied and used in the context of planning and decisionmaking and many methods exist to find the optimal policy for problems modelled as mdps although finding the optimal policy is sufficient in many domains in certain applications such as decision support systems where the policy is executed by a human rather than a machine finding all possible nearoptimal policies might be useful as it provides more flexibility to the person executing the policy in this paper we introduce the new concept of nondeterministic mdp policies and address the question of finding nearoptimal nondeterministic policies we propose two solutions to this problem one based on a mixed integer program and the other one based on a search algorithm we include experimental results obtained from applying this framework to optimize treatment choices in the context of a medical decision support system
measures of clustering quality a working set of axioms for clustering measures of clustering quality a working set of axioms for clustering aiming towards the development of a general clustering theory we discuss abstract axiomatization for clustering in this respect we follow up on the work of kelinberg kleinberg that showed an impossibility result for such axiomatization we argue that an impossibility result is not an inherent feature of clustering but rather to a large extent it is an artifact of the specific formalism used in kleinberg as opposed to previous work focusing on clustering functions we propose to address clustering quality measures as the primitive object to be axiomatized we show that principles like those formulated in kleinbergs axioms can be readily expressed in the latter framework without leading to inconsistency a clusteringquality measure is a function that given a data set and its partition into clusters returns a nonnegative real number representing how strong or conclusive the clustering is we analyze what clusteringquality measures should look like and introduce a set of requirements axioms that express these requirement and extend the translation of kleinbergs axioms to our framework we propose several natural clustering quality measures all satisfying the proposed axioms in addition we show that the proposed clustering quality can be computed in polynomial time
mind the duality gap logarithmic regret algorithms for online optimization mind the duality gap logarithmic regret algorithms for online optimization we describe a primaldual framework for the design and analysis of online strongly convex optimization algorithms our framework yields the tightest known logarithmic regret bounds for followtheleader and for the gradient descent algorithm proposed in hazankakaag we then show that one can interpolate between these two extreme cases in particular we derive a new algorithm that shares the computational simplicity of gradient descent but achieves lower regret in many practical situations finally we further extend our framework for generalized strongly convex functions
mixed membership stochastic blockmodels mixed membership stochastic blockmodels observations consisting of measurements on relationships for pairs of objects arise in many settings such as protein interaction and gene regulatory networks collections of authorrecipient email and social networks analyzing such data with probabilisic models can be delicate because the simple exchangeability assumptions underlying many boilerplate models no longer hold in this paper we describe a class of latent variable models of such data called mixed membership stochastic blockmodels this model extends blockmodels for relational data to ones which capture mixed membership latent relational structure thus providing an objectspecific lowdimensional representation we develop a general variational inference algorithm for fast approximate posterior inference we explore applications to social networks and protein interaction networks
model selection and velocity estimation using novel priors for motion patterns model selection and velocity estimation using novel priors for motion patterns psychophysical experiments show that humans are better at perceiving rotation and expansion than translation these findings are inconsistent with standard models of motion integration which predict best performance for translation to explain this discrepancy our theory formulates motion perception at two levels of inference we first perform model selection between the competing models eg translation rotation and expansion and then estimate the velocity using the selected model we define novel prior models for smooth rotation and expansion using techniques similar to those in the slowandsmooth model eg green functions of differential operators the theory gives good agreement with the trends observed in human experiments
model selection in gaussian graphical models highdimensional consistency of ellregularizedmle model selection in gaussian graphical models highdimensional consistency of ellregularizedmle we study the performance of the ellregularized maximum likelihood estimator for the problem of estimating the graph structure associated with a gaussian markov random field gmrf we consider the performance of the estimator in the highdimensional setting where the number of nodes in the graph p the number of edges in the graph s and the maximum node degree d are allowed to grow as a function of the number of samples n our main result provides sufficient conditions on the quadruple npsd for the ellregularized mle estimator to recover all the edges of the graph with high probability under some conditions on the model covariance we show that model selection can be achieved for sample sizes n omegamaxsdlogp with the error decaying as oexpclogp for some constant c empirical simulations suggest that the rate is tight for graphs where the maximum node degree d scales linearly with p but could be tightened for graphs with d o
modeling human function learning with gaussian processes modeling human function learning with gaussian processes accounts of how people learn functional relationships between continuous variables have tended to focus on two possibilities that people are estimating explicit functions or that they are simply performing associative learning supported by similarity we provide a rational analysis of function learning drawing on work on regression in machine learning and statistics using the equivalence of bayesian linear regression and gaussian processes we show that learning explicit rules and using similarity can be seen as two views of one solution to this problem we use this insight to define a gaussian process model of human function learning that combines the strengths of both approaches
modeling shortterm noise dependence of spike counts in macaque prefrontal cortex modeling shortterm noise dependence of spike counts in macaque prefrontal cortex correlations between spike counts are often used to analyze neural coding the noise is typically assumed to be gaussian yet this assumption is often inappropriate especially for low spike counts in this study we present copulas as an alternative approach with copulas it is possible to use arbitrary marginal distributions such as poisson or negative binomial that are better suited for modeling noise distributions of spike counts furthermore copulas place a wide range of dependence structures at the disposal and can be used to analyze higher order interactions we develop a framework to analyze spike count data by means of copulas methods for parameter inference based on maximum likelihood estimates and for computation of shannon entropy are provided we apply the method to our data recorded from macaque prefrontal cortex the data analysis leads to three significant findings copulabased distributions provide better fits than discretized multivariate normal distributions negative binomial margins fit the data better than poisson margins and a dependence model that includes only pairwise interactions overestimates the information entropy by at least compared to the model with higher order interactions
modeling the effects of memory on human online sentence processing with particle filters modeling the effects of memory on human online sentence processing with particle filters language comprehension in humans is significantly constrained by memory yet rapid highly incremental and capable of utilizing a wide range of contextual information to resolve ambiguity and form expectations about future input in contrast most of the leading psycholinguistic models and fielded algorithms for natural language parsing are nonincremental have run time superlinear in input length andor enforce structural locality constraints on probabilistic dependencies between events we present a new limitedmemory model of sentence comprehension which involves an adaptation of the particle filter a sequential monte carlo method to the problem of incremental parsing we show that this model can reproduce classic results in online sentence comprehension and that it naturally provides the first rational account of an outstanding problem in psycholinguistics in which the preferred alternative in a syntactic ambiguity seems to grow more attractive over time even in the absence of strong disambiguating information
mortal multiarmed bandits mortal multiarmed bandits we formulate and study a new variant of the karmed bandit problem motivated by ecommerce applications in our model arms have stochastic lifetime after which they expire in this setting an algorithm needs to continuously explore new arms in contrast to the standard karmed bandit model in which arms are available indefinitely and exploration is reduced once an optimal arm is identified with nearcertainty the main motivation for our setting is onlineadvertising where ads have limited lifetime due to for example the nature of their content and their campaign budget an algorithm needs to choose among a large collection of ads more than can be fully explored within the ads lifetime we present an optimal algorithm for the stateaware deterministic reward function case and build on this technique to obtain an algorithm for the stateoblivious stochastic reward function case empirical studies on various reward distributions including one derived from a realworld ad serving application show that the proposed algorithms significantly outperform the standard multiarmed bandit approaches applied to these settings
multiagent filtering with infinitely nested beliefs multiagent filtering with infinitely nested beliefs in partially observable worlds with many agents nested beliefs are formed when agents simultaneously reason about the unknown state of the world and the beliefs of the other agents the multiagent filtering problem is to efficiently represent and update these beliefs through time as the agents act in the world in this paper we formally define an infinite sequence of nested beliefs about the state of the world at the current time t and present a filtering algorithm that maintains a finite representation which can be used to generate these beliefs in some cases this representation can be updated exactly in constant time we also present a simple approximation scheme to compact beliefs if they become too complex in experiments we demonstrate efficient filtering in a range of multiagent domains
multilabel multiple kernel learning multilabel multiple kernel learning we present a multilabel multiple kernel learning mkl formulation in which the data are embedded into a lowdimensional space directed by the instancelabel correlations encoded into a hypergraph we formulate the problem in the kernelinduced feature space and propose to learn the kernel matrix as a linear combination of a given collection of kernel matrices in the mkl framework the proposed learning formulation leads to a nonsmooth minmax problem and it can be cast into a semiinfinite linear program silp we further propose an approximate formulation with a guaranteed error bound which involves an unconstrained and convex optimization problem in addition we show that the objective function of the approximate formulation is continuously differentiable with lipschitz gradient and hence existing methods can be employed to compute the optimal solution efficiently we apply the proposed formulation to the automated annotation of drosophila gene expression pattern images and promising results have been reported in comparison with representative algorithms
multilevel active prediction of useful image annotations for recognition multilevel active prediction of useful image annotations for recognition we introduce a framework for actively learning visual categories from a mixture of weakly and strongly labeled image examples we propose to allow the categorylearner to strategically choose what annotations it receivesbased on both the expected reduction in uncertainty as well as the relative costs of obtaining each annotation we construct a multipleinstance discriminative classifier based on the initial training data then all remaining unlabeled and weakly labeled examples are surveyed to actively determine which annotation ought to be requested next after each request the current classifier is incrementally updated unlike previous work our approach accounts for the fact that the optimal use of manual annotation may call for a combination of labels at multiple levels of granularity eg a full segmentation on some images and a presentabsent flag on others as a result it is possible to learn more accurate category models with a lower total expenditure of manual annotation effort
multiresolution exploration in continuous spaces multiresolution exploration in continuous spaces the essence of exploration is acting to try to decrease uncertainty we propose a new methodology for representing uncertainty in continuousstate control problems our approach multiresolution exploration mre uses a hierarchical mapping to identify regions of the state space that would benefit from additional samples we demonstrate mres broad utility by using it to speed up learning in a prototypical modelbased and valuebased reinforcementlearning method empirical results show that mre improves upon stateoftheart exploration approaches
multistage convex relaxation for learning with sparse regularization multistage convex relaxation for learning with sparse regularization we study learning formulations with nonconvex regularizaton that are natural for sparse linear models there are two approaches to this problem heuristic methods such as gradient descent that only find a local minimum a drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution convex relaxation such as lregularization that solves the problem under some conditions however it often leads to suboptimal sparsity in reality this paper tries to remedy the above gap between theory and practice in particular we investigate a multistage convex relaxation scheme for solving problems with nonconvex regularization theoretically we analyze the behavior of a resulting twostage relaxation scheme for the cappedl regularization our performance bound shows that the procedure is superior to the standard l convex relaxation for learning sparse targets experiments confirm the effectiveness of this method on some simulation and real data
multitask gaussian process learning of robot inverse dynamics multitask gaussian process learning of robot inverse dynamics the inverse dynamics problem for a robotic manipulator is to compute the torques needed at the joints to drive it along a given trajectory it is beneficial to be able to learn this function for adaptive control a given robot manipulator will often need to be controlled while holding different loads in its end effector giving rise to a multitask learning problem we show how the structure of the inverse dynamics problem gives rise to a multitask gaussian process prior over functions where the intertask similarity depends on the underlying dynamic parameters experiments demonstrate that this multitask formulation generally improves performance over either learning only on single tasks or pooling the data over all tasks
multiscale random fields with application to contour grouping multiscale random fields with application to contour grouping we introduce a new interpretation of multiscale random fields msrfs that admits efficient optimization in the framework of regular single level random fields rfs it is based on a new operator called append that combines sets of random variables rvs to single rvs we assume that a msrf can be decomposed into disjoint trees that link rvs at different pyramid levels the append operator is then applied to map rvs in each tree structure to a single rv we demonstrate the usefulness of the proposed approach on a challenging task involving grouping contours of target shapes in images msrfs provide a natural representation of multiscale contour models which are needed in order to cope with unstable contour decompositions the append operator allows us to find optimal image labels using the classical framework of relaxation labeling alternative methods like markov chain monte carlo mcmc could also be used
natural image denoising with convolutional networks natural image denoising with convolutional networks we present an approach to lowlevel vision that combines two main ideas the use of convolutional networks as an image processing architecture and an unsupervised learning procedure that synthesizes training samples from specific noise models we demonstrate this approach on the challenging problem of natural image denoising using a test set with a hundred natural images we find that convolutional networks provide comparable and in some cases superior performance to state of the art wavelet and markov random field mrf methods moreover we find that a convolutional network offers similar performance in the blind denoising setting as compared to other techniques in the nonblind setting we also show how convolutional networks are mathematically related to mrf approaches by presenting a mean field theory for an mrf specially designed for image denoising although these approaches are related convolutional networks avoid computational difficulties in mrf approaches that arise from probabilistic learning and inference this makes it possible to learn image processing architectures that have a high degree of representational power we train models with over parameters but whose computational expense is significantly less than that associated with inference in mrf approaches with even hundreds of parameters
nearminimax recursive density estimation on the binary hypercube nearminimax recursive density estimation on the binary hypercube this paper describes a recursive estimation procedure for multivariate binary densities using orthogonal expansions for d covariates there are d basis coefficients to estimate which renders conventional approaches computationally prohibitive when d is large however for a wide class of densities that satisfy a certain sparsity condition our estimator runs in probabilistic polynomial time and adapts to the unknown sparsity of the underlying density in two key ways it attains nearminimax meansquared error and the computational complexity is lower for sparser densities our method also allows for flexible control of the tradeoff between meansquared error and computational complexity
nearoptimal regret bounds for reinforcement learning nearoptimal regret bounds for reinforcement learning for undiscounted reinforcement learning in markov decision processes mdps we consider the total regret of a learning algorithm with respect to an optimal policy in order to describe the transition structure of an mdp we propose a new parameter an mdp has diameter d if for any pair of states ss there is a policy which moves from s to s in at most d steps on average we present a reinforcement learning algorithm with total regret odsat after t steps for any unknown mdp with s states a actions per state and diameter d this bound holds with high probability we also present a corresponding lower bound of omegadsat on the total regret of any learning algorithm both bounds demonstrate the utility of the diameter as structural parameter of the mdp
nonparametric regression between manifolds nonparametric regression between manifolds this paper discusses nonparametric regression between riemannian manifolds this learning problem arises frequently in many application areas ranging from signal processing computer vision over robotics to computer graphics we present a new algorithmic scheme for the solution of this general learning problem based on regularized empirical risk minimization the regularization functional takes into account the geometry of input and output manifold and we show that it implements a prior which is particularly natural moreover we demonstrate that our algorithm performs well in a difficult surface registration problem
nonstationary dynamic bayesian networks nonstationary dynamic bayesian networks a principled mechanism for identifying conditional dependencies in timeseries data is provided through structure learning of dynamic bayesian networks dbns an important assumption of dbn structure learning is that the data are generated by a stationary processacircan assumption that is not true in many important settings in this paper we introduce a new class of graphical models called nonstationary dynamic bayesian networks in which the conditional dependence structure of the underlying datageneration process is permitted to change over time nonstationary dynamic bayesian networks represent a new framework for studying problems in which the structure of a network is evolving over time we define the nonstationary dbn model present an mcmc sampling algorithm for learning the structure of the model from timeseries data under different assumptions and demonstrate the effectiveness of the algorithm on both simulated and biological data
nonlinear causal discovery with additive noise models nonlinear causal discovery with additive noise models the discovery of causal relationships between a set of observed variables is a fundamental problem in science for continuousvalued data linear acyclic causal models are often used because these models are well understood and there are wellknown methods to fit them to data in reality of course many causal relationships are more or less nonlinear raising some doubts as to the applicability and usefulness of purely linear methods in this contribution we show that in fact the basic linear framework can be generalized to nonlinear models with additive noise in this extended framework nonlinearities in the datagenerating process are in fact a blessing rather than a curse as they typically provide information on the underlying causal system and allow more aspects of the true datagenerating mechanisms to be identified in addition to theoretical results we show simulations and some simple real data experiments illustrating the identification power provided by nonlinearities
nonparametric bayesian learning of switching linear dynamical systems nonparametric bayesian learning of switching linear dynamical systems many nonlinear dynamical phenomena can be effectively modeled by a system that switches among a set of conditionally linear dynamical modes we consider two such models the switching linear dynamical system slds and the switching vector autoregressive var process in this paper we present a nonparametric approach to the learning of an unknown number of persistent smooth dynamical modes by utilizing a hierarchical dirichlet process prior we develop a sampling algorithm that combines a truncated approximation to the dirichlet process with an efficient joint sampling of the mode and state sequences the utility and flexibility of our model are demonstrated on synthetic data sequences of dancing honey bees and the ibovespa stock index
nonparametric bayesian sparse hierarchical factor modeling and regression nonparametric bayesian sparse hierarchical factor modeling and regression we propose a nonparametric bayesian sparse factor analysis model that accounts for uncertainty in the number of factors and the relationship between factors to accomplish this we propose a sparse variant of the indian buffet process and couple this with a hierarchical model over factors based on kingmans coalescent we apply this model to two problems in geneexpression analysis
nonparametric regression and classification with joint sparsity constraints nonparametric regression and classification with joint sparsity constraints we propose new families of models and algorithms for highdimensional nonparametric learning with joint sparsity constraints our approach is based on a regularization method that enforces common sparsity patterns across different function components in a nonparametric additive model the algorithms employ a coordinate descent approach that is based on a functional softthresholding operator the framework yields several new models including multitask sparse additive models multiresponse sparse additive models and sparse additive multicategory logistic regression the methods are illustrated with experiments on synthetic data and gene microarray data
nonparametric sparse hierarchical models describe v fmri responses to natural images nonparametric sparse hierarchical models describe v fmri responses to natural images we propose a novel hierarchical nonlinear model that predicts brain activity in area v evoked by natural images in the study reported here brain activity was measured by means of functional magnetic resonance imaging fmri a noninvasive technique that provides an indirect measure of neural activity pooled over a small volume mm cube of brain tissue our model which we call the spam v model is based on the reasonable assumption that fmri measurements reflect the possibly nonlinearly pooled rectified output of a large population of simple and complex cells in v it has a hierarchical filtering stage that consists of three layers model simple cells model complex cells and a third layer in which the complex cells are linearly pooled called acircpooledcomplexacirc cells the pooling stage then obtains the measured fmri signals as a sparse additive model spam in which a sparse nonparametric nonlinear combination of model complex cell and model pooledcomplex cell outputs are summed our results show that the spam v model predicts fmri responses evoked by natural images better than a benchmark model that only provides linear pooling of model complex cells furthermore the spatial receptive fields frequency tuning and orientation tuning curves of the spam v model estimated for each voxel appears to be consistent with the known properties of v and with previous analyses of this data set a visualization procedure applied to the spam v model shows that most of the nonlinear pooling consists of simple compressive or saturating nonlinearities
nonrigid structure from motion in trajectory space nonrigid structure from motion in trajectory space existing approaches to nonrigid structure from motion assume that the instantaneous d shape of a deforming object is a linear combination of basis shapes which have to be estimated anew for each video sequence in contrast we propose that the evolving d structure be described by a linear combination of basis trajectories the principal advantage of this lateral approach is that we do not need to estimate any basis vectors during computation instead we show that generic bases over trajectories such as the discrete cosine transform dct bases can be used to effectively describe most real motions this results in a significant reduction in unknowns and corresponding stability in estimation we report empirical performance quantitatively using motion capture data and qualitatively on several video sequences exhibiting nonrigid motions including piecewise rigid motion articulated motion partially nonrigid motion such as a facial expression and highly nonrigid motion such as a person dancing
offline handwriting recognition with multidimensional recurrent neural networks offline handwriting recognition with multidimensional recurrent neural networks offline handwriting recognitionthe transcription of images of handwritten textis an interesting task in that it combines computer vision with sequence learning in most systems the two elements are handled separately with sophisticated preprocessing techniques used to extract the image features and sequential models such as hmms used to provide the transcriptions by combining two recent innovations in neural networksmultidimensional recurrent neural networks and connectionist temporal classificationthis paper introduces a globally trained offline handwriting recogniser that takes raw pixel data as input unlike competing systems it does not require any alphabet specific preprocessing and can therefore be used unchanged for any language evidence of its generality and power is provided by data from a recent international arabic recognition competition where it outperformed all entries accuracy compared to for the competition winner despite the fact that neither author understands a word of arabic
on bootstrapping the roc curve on bootstrapping the roc curve this paper is devoted to thoroughly investigating how to bootstrap the roc curve a widely used visual tool for evaluating the accuracy of testscoring statistics in the bipartite setup the issue of confidence bands for the roc curve is considered and a resampling procedure based on a smooth version of the empirical distribution called the quotsmoothed bootstrapquot is introduced theoretical arguments and simulation results are presented to show that the quotsmoothed bootstrapquot is preferable to a quotnaivequot bootstrap in order to construct accurate confidence bands
on computational power and the orderchaos phase transition in reservoir computing on computational power and the orderchaos phase transition in reservoir computing randomly connected recurrent neural circuits have proven to be very powerful models for online computations when a trained memoryless readout function is appended such reservoir computing rc systems are commonly used in two flavors with analog or binary spiking neurons in the recurrent circuits previous work showed a fundamental difference between these two incarnations of the rc idea the performance of a rc system build from binary neurons seems to depend strongly on the network connectivity structure in networks of analog neurons such dependency has not been observed in this article we investigate this apparent dichotomy in terms of the indegree of the circuit nodes our analyses based amongst others on the lyapunov exponent reveal that the phase transition between ordered and chaotic network behavior of binary circuits qualitatively differs from the one in analog circuits this explains the observed decreased computational performance of binary circuits of high node indegree furthermore a novel meanfield predictor for computational performance is introduced and shown to accurately predict the numerically obtained results
on the complexity of linear prediction risk bounds margin bounds and regularization on the complexity of linear prediction risk bounds margin bounds and regularization we provide sharp bounds for rademacher and gaussian complexities of constrained linear classes these bounds make short work of providing a number of corollaries including risk bounds for linear prediction including settings where the weight vectors are constrained by either l or l constraints margin bounds including both l and l margins along with more general notions based on relative entropy a proof of the pacbayes theorem and l covering numbers with lp norm constraints and relative entropy constraints in addition to providing a unified analysis the results herein provide some of the sharpest risk and margin bounds improving upon a number of previous results interestingly our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction up to a constant factor of 
on the design of loss functions for classification theory robustness to outliers and savageboost on the design of loss functions for classification theory robustness to outliers and savageboost the machine learning problem of classifier design is studied from the perspective of probability elicitation in statistics this shows that the standard approach of proceeding from the specification of a loss to the minimization of conditional risk is overly restrictive it is shown that a better alternative is to start from the specification of a functional form for the minimum conditional risk and derive the loss function this has various consequences of practical interest such as showing that the widely adopted practice of relying on convex loss functions is unnecessary and many new losses can be derived for classification problems these points are illustrated by the derivation of a new loss which is not convex but does not compromise the computational tractability of classifier design and is robust to the contamination of data with outliers a new boosting algorithm savageboost is derived for the minimization of this loss experimental results show that it is indeed less sensitive to outliers than conventional methods such as ada real or logitboost and converges in fewer iterations
on the efficient minimization of classification calibrated surrogates on the efficient minimization of classification calibrated surrogates bartlett et al recently proved that a ground condition for convex surrogates classification calibration ties up the minimization of the surrogates and classification risks and left as an important problem the algorithmic questions about the minimization of these surrogates in this paper we propose an algorithm which provably minimizes any classification calibrated surrogate strictly convex and differentiable a set whose losses span the exponential logistic and squared losses with boostingtype guaranteed convergence rates under a weak learning assumption a particular subclass of these surrogates that we call balanced convex surrogates has a key rationale that ties it to maximum likelihood estimation zerosum games and the set of losses that satisfy some of the most common requirements for losses in supervised learning we report experiments on more than readily available domains of flavors of the algorithm that shed light on new surrogates and the potential of data dependent strategies to tune surrogates
on the equivalence between td learning and differential hebbian learning using a local third factor on the equivalence between td learning and differential hebbian learning using a local third factor in this theoretical contribution we provide mathematical proof that two of the most important classes of network learning correlationbased differential hebbian learning and rewardbased temporal difference learning are asymptotically equivalent when timing the learning with a local modulatory signal this opens the opportunity to consistently reformulate most of the abstract reinforcement learning framework from a correlation based perspective that is more closely related to the biophysics of neurons
on the generalization ability of online strongly convex programming algorithms on the generalization ability of online strongly convex programming algorithms this paper examines the generalization properties of online convex programming algorithms when the loss function is lipschitz and strongly convex our main result is a sharp bound that holds with high probability on the excess risk of the output of an online algorithm in terms of the average regret this allows one to use recent algorithms with logarithmic cumulative regret guarantees to achieve fast convergence rates for the excess risk with high probability the bound also solves an open problem regarding the convergence rate of pegasos a recently proposed method for solving the svm optimization problem
on the reliability of clustering stability in the large sample regime on the reliability of clustering stability in the large sample regime clustering stability is an increasingly popular family of methods for performing model selection in data clustering the basic idea is that the chosen model should be stable under perturbation or resampling of the data despite being reasonably effective in practice these methods are not well understood theoretically and present some difficulties in particular when the data is assumed to be sampled from an underlying distribution the solutions returned by the clustering algorithm will usually become more and more stable as the sample size increases this raises a potentially serious practical difficulty with these methods because it means there might be some hardtocompute sample size beyond which clustering stability estimators break down and become unreliable in detecting the most stable model namely all models will be relatively stable with differences in their stability measures depending mostly on random and meaningless sampling artifacts in this paper we provide a set of general sufficient conditions which ensure the reliability of clustering stability estimators in the large sample regime in contrast to previous work which concentrated on specific toy distributions or specific idealized clustering frameworks here we make no such assumptions we then exemplify how these conditions apply to several important families of clustering algorithms such as maximum likelihood clustering certain types of kernel clustering and centroidbased clustering with any bregman divergence in addition we explicitly derive the nontrivial asymptotic behavior of these estimators for any framework satisfying our conditions this can help us understand what is considered a stable model by these estimators at least for large enough samples
online prediction on large diameter graphs online prediction on large diameter graphs current online learning algorithms for predicting the labelling of a graph have an important limitation in the case of large diameter graphs the number of mistakes made by such algorithms may be proportional to the square root of the number of vertices even when tackling simple problems we overcome this problem with an efficient algorithm which achieves a logarithmic mistake bound furthermore current algorithms are optimised for data which exhibits clusterstructure we give an additional algorithm which performs well locally in the presence of cluster structure and on large diameter graphs
one sketch for all theory and application of conditional random sampling one sketch for all theory and application of conditional random sampling conditional random sampling crs was originally proposed for efficiently computing pairwise l l distances in static largescale and sparse data sets such as text and web data it was previously presented using a heuristic argument this study extends crs to handle dynamic or streaming data which much better reflect the realworld situation than assuming static data compared with other known sketching algorithms for dimension reductions such as stable random projections crs exhibits a significant advantage in that it is onesketchforall in particular we demonstrate that crs can be applied to efficiently compute the lp distance and the hilbertian metrics both are popular in machine learning although a fully rigorous analysis of crs is difficult we prove that with a simple modification crs is rigorous at least for an important application of computing hamming norms a generic estimator and an approximate variance formula are provided and tested on various applications for computing hamming norms hamming distances and chi distances
online metric learning and fast similarity search online metric learning and fast similarity search metric learning algorithms can provide useful distance functions for a variety of domains and recent work has shown good accuracy for problems where the learner can access all distance constraints at once however in many real applications constraints are only available incrementally thus necessitating methods that can perform online updates to the learned metric existing online algorithms offer bounds on worstcase performance but typically do not perform well in practice as compared to their offline counterparts we present a new online metric learning algorithm that updates a learned mahalanobis metric based on logdet regularization and gradient descent we prove theoretical worstcase performance bounds and empirically compare the proposed method against existing online metric learning algorithms to further boost the practicality of our approach we develop an online localitysensitive hashing scheme which leads to efficient updates for approximate similarity search data structures we demonstrate our algorithm on multiple datasets and show that it outperforms relevant baselines
online models for content optimization online models for content optimization we describe a new content publishing system that selects articles to serve to a user choosing from an editorially programmed pool that is frequently refreshed it is now deployed on a major internet portal and selects articles to serve to hundreds of millions of user visits per day significantly increasing the number of user clicks over the original manual approach in which editors periodically selected articles to display some of the challenges we face include a dynamic content pool short article lifetimes nonstationary clickthrough rates and extremely high traffic volumes the fundamental problem we must solve is to quickly identify which items are popularperhaps within different user segments and to exploit them while they remain current we must also explore the underlying pool constantly to identify promising alternatives quickly discarding poor performers our approach is based on tracking per article performance in near real time through online models we describe the characteristics and constraints of our application setting discuss our design choices and show the importance and effectiveness of coupling online models with a simple randomization procedure we discuss the challenges encountered in a production online contentpublishing environment and highlight issues that deserve careful attention our analysis of this application also suggests a number of future research avenues
online optimization in xarmed bandits online optimization in xarmed bandits we consider a generalization of stochastic bandit problems where the set of arms x is allowed to be a generic topological space we constraint the meanpayoff function with a dissimilarity function over x in a way that is more general than lipschitz we construct an arm selection policy whose regret improves upon previous result for a large class of problems in particular our results imply that if x is the unit hypercube in a euclidean space and the meanpayoff function has a finite number of global maxima around which the behavior of the function is locally houmllder with a known exponent then the expected regret is bounded up to a logarithmic factor by n ie the rate of the growth of the regret is independent of the dimension of the space moreover we prove the minimax optimality of our algorithm for the class of meanpayoff functions we consider
optimal response initiation why recent experience matters optimal response initiation why recent experience matters in most cognitive and motor tasks speedaccuracy tradeoffs are observed individuals can respond slowly and accurately or quickly yet be prone to errors control mechanisms governing the initiation of behavioral responses are sensitive not only to task instructions and the stimulus being processed but also to the recent stimulus history when stimuli can be characterized on an easyhard dimension eg word frequency in a naming task items preceded by easy trials are responded to more quickly and with more errors than items preceded by hard trials we propose a rationally motivated mathematical model of this sequential adaptation of control based on a diffusion model of the decision process in which difficulty corresponds to the drift rate for the correct response the model assumes that responding is based on the posterior distribution over which response is correct conditioned on the accumulated evidence we derive this posterior as a function of the drift rate and show that higher estimates of the drift rate lead to normatively faster responding trialbytrial tracking of difficulty thus leads to sequential effects in speed and accuracy simulations show the model explains a variety of phenomena in human speeded decision making we argue this passive statistical mechanism provides a more elegant and parsimonious account than extant theories based on elaborate control structures
optimization on a budget a reinforcement learning approach optimization on a budget a reinforcement learning approach many popular optimization algorithms like the levenbergmarquardt algorithm lma use heuristicbased quotcontrollers that modulate the behavior of the optimizer during the optimization process for example in the lma a damping parameter is dynamically modified based on a set rules that were developed using various heuristic arguments reinforcement learning rl is a machine learning approach to learn optimal controllers by examples and thus is an obvious candidate to improve the heuristicbased controllers implicit in the most popular and heavily used optimization algorithms improving the performance of offtheshelf optimizers is particularly important for timeconstrained optimization problems for example the lma algorithm has become popular for many realtime computer vision problems including object tracking from video where only a small amount of time can be allocated to the optimizer on each incoming video frame here we show that a popular modern reinforcement learning technique using a very simply state space can dramatically improve the performance of general purpose optimizers like the lma most surprisingly the controllers learned for a particular domain appear to work very well also on very different optimization domains for example we used rl methods to train a new controller for the damping parameter of the lma this controller was trained on a collection of classic relatively small nonlinear regression problems the modified lma performed better than the standard lma on these problems most surprisingly it also dramatically outperformed the standard lma on a difficult large scale computer vision problem for which it had not been trained before thus the controller appeared to have extracted control rules that were not just domain specific but generalized across a wide range of optimization domains
overlaying classifiers a practical approach for optimal ranking overlaying classifiers a practical approach for optimal ranking roc curves are one of the most widely used displays to evaluate performance of scoring functions in the paper we propose a statistical method for directly optimizing the roc curve the target is known to be the regression function up to an increasing transformation and this boils down to recovering the level sets of the latter we propose to use classifiers obtained by empirical risk minimization of a weighted classification error and then to construct a scoring rule by overlaying these classifiers we show the consistency and rate of convergence to the optimal roc curve of this procedure in terms of supremum norm and also as a byproduct of the analysis we derive an empirical estimate of the optimal roc curve
partially observed maximum entropy discrimination markov networks partially observed maximum entropy discrimination markov networks learning graphical models with hidden variables can offer semantic insights to complex data and lead to salient structured predictors without relying on expensive sometime unattainable fully annotated training data while likelihoodbased methods have been extensively explored to our knowledge learning structured prediction models with latent variables based on the maxmargin principle remains largely an open problem in this paper we present a partially observed maximum entropy discrimination markov network pomen model that attempts to combine the advantages of bayesian and margin based paradigms for learning markov networks from partially labeled data pomen leads to an averaging prediction rule that resembles a bayes predictor that is more robust to overfitting but is also built on the desirable discriminative laws resemble those of the mn we develop an emstyle algorithm utilizing existing convex optimization algorithms for mn as a subroutine we demonstrate competent performance of pomen over existing methods on a realworld web data extraction task
particle filterbased policy gradient in pomdps particle filterbased policy gradient in pomdps our setting is a partially observable markov decision process with continuous state observation and action spaces decisions are based on a particle filter for estimating the belief state given past observations we consider a policy gradient approach for parameterized policy optimization for that purpose we investigate sensitivity analysis of the performance measure with respect to the parameters of the policy focusing on finite difference fd techniques we show that the naive fd is subject to variance explosion because of the nonsmoothness of the resampling procedure we propose a more sophisticated fd method which overcomes this problem and establish its consistency
performance analysis for l kernel classification performance analysis for l kernel classification we provide statistical performance guarantees for a recently introduced kernel classifier that optimizes the l or integrated squared error ise of a difference of densities the classifier is similar to a support vector machine svm in that it is the solution of a quadratic program and yields a sparse classifier unlike svms however the l kernel classifier does not involve a regularization parameter we prove a distribution free concentration inequality for a crossvalidation based estimate of the ise and apply this result to deduce an oracle inequality and consistency of the classifier on the sense of both ise and probability of error our results can also be specialized to give performance guarantees for an existing method of l kernel density estimation
phase transitions for highdimensional joint support recovery phase transitions for highdimensional joint support recovery we consider the following instance of transfer learning given a pair of regression problems suppose that the regression coefficients share a partially common support parameterized by the overlap fraction overlap between the two supports this setup suggests the use of inftyregularized linear regression for recovering the support sets of both regression vectors our main contribution is to provide a sharp characterization of the sample complexity of this infty relaxation exactly pinning down the minimal sample size n required for joint support recovery as a function of the model dimension pdim support size spindex and overlap overlap in for measurement matrices drawn from standard gaussian ensembles we prove that the joint inftyregularized method undergoes a phase transition characterized by order parameter orparnumobs pdim spindex overlap numobs overlap s logpoverlaps more precisely the probability of successfully recovering both supports converges to for scalings such that orpar gt and converges to to scalings for which orpar lt an implication of this threshold is that use of inftyregularization leads to gains in sample complexity if the overlap parameter is large enough overlap gt but performs worse than a naive approach if overlap lt we illustrate the close agreement between these theoretical predictions and the actual behavior in simulations thus our results illustrate both the benefits and dangers associated with blockinfty regularization in highdimensional inference
playing pinball with noninvasive bci playing pinball with noninvasive bci compared to invasive braincomputer interfaces bci noninvasive bci systems based on electroencephalogram eeg signals have not been applied successfully for complex control tasks in the present study however we demonstrate this is possible and report on the interaction of a human subject with a complex real device a pinball machine first results in this single subject study clearly show that fast and welltimed control well beyond chance level is possible even though the environment is extremely rich and requires complex predictive behavior using machine learning methods for mental state decoding bcibased pinball control is possible within the first session without the necessity to employ lengthy subject training while the current study is still of anecdotal nature it clearly shows that very compelling control with excellent timing and dynamics is possible for a noninvasive bci
policy search for motor primitives in robotics policy search for motor primitives in robotics many motor skills in humanoid robotics can be learned using parametrized motor primitives as done in imitation learning however most interesting motor learning problems are highdimensional reinforcement learning problems often beyond the reach of current methods in this paper we extend previous work on policy learning from the immediate reward case to episodic reinforcement learning we show that this results into a general common framework also connected to policy gradient methods and yielding a novel algorithm for policy learning by assuming a form of exploration that is particularly wellsuited for dynamic motor primitives the resulting algorithm is an eminspired algorithm applicable in complex motor learning tasks we compare this algorithm to alternative parametrized policy search methods and show that it outperforms previous methods we apply it in the context of motor learning and show that it can learn a complex ballinacup task using a real barrett wam robot arm
posterior consistency of the silverman gprior in bayesian model choice posterior consistency of the silverman gprior in bayesian model choice kernel supervised learning methods can be unified by utilizing the tools from regularization theory the duality between regularization and prior leads to interpreting regularization methods in terms of maximum a posteriori estimation and has motivated bayesian interpretations of kernel methods in this paper we pursue a bayesian interpretation of sparsity in the kernel setting by making use of a mixture of a pointmass distribution and prior that we refer to as silvermans gprior we provide a theoretical analysis of the posterior consistency of a bayesian model choice procedure based on this prior we also establish the asymptotic relationship between this procedure and the bayesian information criterion
predicting the geometry of metal binding sites from protein sequence predicting the geometry of metal binding sites from protein sequence metal binding is important for the structural and functional characterization of proteins previous prediction efforts have only focused on bonding state ie deciding which protein residues act as metal ligands in some binding site identifying the geometry of metalbinding sites ie deciding which residues are jointly involved in the coordination of a metal ion is a new prediction problem that has been never attempted before from protein sequence alone in this paper we formulate it in the framework of learning with structured outputs our solution relies on the fact that from a graph theoretical perspective metal binding has the algebraic properties of a matroid enabling the application of greedy algorithms for learning structured outputs on a data set of nonredundant metalloproteins we obtained precisionrecall levels of correct ligandion assignments which improves to in the setting where the metal binding state is known
predictive indexing for fast search predictive indexing for fast search we tackle the computational problem of queryconditioned search given a machinelearned scoring rule and a query distribution we build a predictive index by precomputing lists of potential results sorted based on an expected score of the result over future queries the predictive index datastructure supports an anytime algorithm for approximate retrieval of the top elements the general approach is applicable to webpage ranking internet advertisement and approximate nearest neighbor search it is particularly effective in settings where standard techniques eg inverted indices are intractable we experimentally find substantial improvement over existing methods for internet advertisement and approximate nearest neighbors
privacypreserving logistic regression privacypreserving logistic regression this paper addresses the important tradeoff between privacy and learnability when designing algorithms for learning from private databases first we apply an idea of dwork et al to design a specific privacypreserving machine learning algorithm logistic regression this involves bounding the sensitivity of logistic regression and perturbing the learned classifier with noise proportional to the sensitivity noting that the approach of dwork et al has limitations when applied to other machine learning algorithms we then present another privacypreserving logistic regression algorithm the algorithm is based on solving a perturbed objective and does not depend on the sensitivity we prove that our algorithm preserves privacy in the model due to dwork et al and we provide a learning performance guarantee our work also reveals an interesting connection between regularization and privacy
probabilistic detection of short events with application to critical care monitoring probabilistic detection of short events with application to critical care monitoring we describe an application of probabilistic modeling and inference technology to the problem of analyzing sensor data in the setting of an intensive care unit icu in particular we consider the arterialline blood pressure sensor which is subject to frequent data artifacts that cause false alarms in the icu and make the raw data almost useless for automated decision making the problem is complicated by the fact that the sensor data are acquired at fixed intervals whereas the events causing data artifacts may occur at any time and have durations that may be significantly shorter than the data collection interval we show that careful modeling of the sensor combined with a general technique for detecting subinterval events and estimating their duration enables effective detection of artifacts and accurate estimation of the underlying blood pressure values
psdboost matrixgeneration linear programming for positive semidefinite matrices learning psdboost matrixgeneration linear programming for positive semidefinite matrices learning in this work we consider the problem of learning a positive semidefinite matrix the critical issue is how to preserve positive semidefiniteness during the course of learning our algorithm is mainly inspired by lpboost and the general greedy convex optimization framework of zhang we demonstrate the essence of the algorithm termed psdboost positive semidefinite boosting by focusing on a few different applications in machine learning the proposed psdboost algorithm extends traditional boosting algorithms in that its parameter is a positive semidefinite matrix with trace being one instead of a classifier psdboost is based on the observation that any traceone positive semidefinitematrix can be decomposed into linear convex combinations of traceone rankone matrices which serve as base learners of psdboost numerical experiments are presented
quicsvd fast svd using cosine trees quicsvd fast svd using cosine trees the singular value decomposition is a key operation in many machine learning methods its computational cost however makes it unscalable and impractical for the massivesized datasets becoming common in applications we present a new method quicsvd for fast approximation of the full svd with automatic sample size minimization and empirical relative error control previous monte carlo approaches have not addressed the full svd nor benefited from the efficiency of automatic empiricallydriven sample sizing our empirical tests show speedups of several orders of magnitude over exact svd such scalability should enable quicsvd to meet the needs of a wide array of methods and applications
rademacher complexity bounds for noniid processes rademacher complexity bounds for noniid processes this paper presents the first datadependent generalization bounds for noniid settings based on the notion of rademacher complexity our bounds extend to the noniid case existing rademacher complexity bounds derived for the iid setting these bounds provide a strict generalization of the ones found in the iid case and can also be used within the standard iid scenario they apply to the standard scenario of betamixing stationary sequences examined in many previous studies of noniid settings and benefit form the crucial advantages of rademacher complexity over other measures of the complexity of hypothesis classes in particular they are datadependent and measure the complexity of a class of hypotheses based on the training sample the empirical rademacher complexity can be estimated from finite samples and lead to tighter bounds
reconciling real scores with binary comparisons a unified logistic model for ranking reconciling real scores with binary comparisons a unified logistic model for ranking the problem of ranking arises ubiquitously in almost every aspect of life a statistical model for ranking predicts how humans rank subsets v of some universe u in this work we define a statistical model for ranking that has the following desirable properties i it is parametrized by a real valued function quotscorequot on u ii the score parameters can be efficiently learnt from sample data consisting of a sequence of comparison bits either quota precedes bquot or quotb precedes aquot iii for any subset v of u the ranking which sorts the elements by score is the mode most probable outcome of the model distribution iv it predicts the empirical strong independence between human response to comparison questions and the context in which the questions are asked and v it admits an efficient onlog n time sampling algorithm for subsets of size n this is the first theoretical justification to the score based approach often found in machine learning ranking systems most notably in information retrieval in addition it gives rise to a new promising logistic regression based approach to learning how to rank for which the score and comparison based approaches are dual views this offers a reconciliation between the two approaches whereas in previous work on ranking the two were intermixed eg by acquiring comparison labels and optimizing over scores with only heuristic justification our model is built rigorously and axiomatically based on very simple desirable properties defined locally for comparisons and automatically implies the existence of a global score function serving as a natural model parameter which can be efficiently fitted to data by solving a convex optimization problem
reducing statistical dependencies in natural signals using radial gaussianization reducing statistical dependencies in natural signals using radial gaussianization we consider the problem of efficiently encoding a signal by transforming it to a new representation whose components are statistically independent a widely studied linear solution independent components analysis ica exists for the case when the signal is generated as a linear transformation of independent non gaussian sources here we examine a complementary case in which the source is nongaussian but elliptically symmetric in this case no linear transform suffices to properly decompose the signal into independent components but we show that a simple nonlinear transformation which we call radial gaussianization rg is able to remove all dependencies we then demonstrate this methodology in the context of natural signal statistics we first show that the joint distributions of bandpass filter responses for both sound and images are better described as elliptical than linearly transformed independent sources consistent with this we demonstrate that the reduction in dependency achieved by applying rg to either pairs or blocks of bandpass filter responses is significantly greater than that achieved by pca or ica
regularized coclustering with dual supervision regularized coclustering with dual supervision by attempting to simultaneously partition both the rows examples and columns features of a data matrix coclustering algorithms often demonstrate surpris ingly impressive performance improvements over traditional onesided row clustering techniques a good clustering of features may be seen as a combinatorial transformation of the data matrix effectively enforcing a form of regularization that may lead to a better clustering of examples and viceversa in many applications partial supervision in the form of a few row labels as well as column labels may be available to potentially assist coclustering in this paper we develop two novel semisupervised multiclass classification algorithms motivated respectively by spectral bipartite graph partitioning and matrix approximation eg nonnegative matrix factorization formulations for coclustering these algorithms i support dual supervision in the form of labels for both examples andor features ii provide principled predictive capability on outofsample test data and iii arise naturally from the classical representer theorem applied to regularization problems posed on a collection of reproducing kernel hilbert spaces empirical results demonstrate the effectiveness and utility of our algorithms
regularized policy iteration regularized policy iteration in this paper we consider approximate policyiterationbased reinforcement learning algorithms in order to implement a flexible function approximation scheme we propose the use of nonparametric methods with regularization providing a convenient way to control the complexity of the function approximator we propose two novel regularized policy iteration algorithms by adding lregularization to two widelyused policy evaluation methods bellman residual minimization brm and leastsquares temporal difference learning lstd we derive efficient implementation for our algorithms when the approximate valuefunctions belong to a reproducing kernel hilbert space we also provide finitesample performance bounds for our algorithms and show that they are able to achieve optimal rates of convergence under the studied conditions
relative margin machines relative margin machines in classification problems support vector machines maximize the margin of separation between two classes while the paradigm has been successful the solution obtained by svms is dominated by the directions with large data spread and biased to separate the classes by cutting along large spread directions this article proposes a novel formulation to overcome such sensitivity and maximizes the margin relative to the spread of the data the proposed formulation can be efficiently solved and experiments on digit datasets show drastic performance improvements over svms
relative performance guarantees for approximate inference in latent dirichlet allocation relative performance guarantees for approximate inference in latent dirichlet allocation hierarchical probabilistic modeling of discrete data has emerged as a powerful tool for text analysis posterior inference in such models is intractable and practitioners rely on approximate posterior inference methods such as variational inference or gibbs sampling there has been much research in designing better approximations but there is yet little theoretical understanding of which of the available techniques are appropriate and in which data analysis settings in this paper we provide the beginnings of such understanding we analyze the improvement that the recently proposed collapsed variational inference cvb provides over mean field variational inference vb in latent dirichlet allocation we prove that the difference in the tightness of the bound on the likelihood of a document decreases as ok log m m where k is the number of topics in the model and m is the number of words in a document as a consequence the advantage of cvb over vb is lost for long documents but increases with the number of topics we demonstrate empirically that the theory holds using simulated text data and two text corpora we provide practical guidelines for choosing an approximation
resolution limits of sparse coding in high dimensions resolution limits of sparse coding in high dimensions recent research suggests that neural systems employ sparse coding however there is limited theoretical understanding of fundamental resolution limits in such sparse coding this paper considers a general sparse estimation problem of detecting the sparsity pattern of a ksparse vector in rn from m random noisy measurements our main results provide necessary and sufficient conditions on the problem dimensions m n and k and the signaltonoise ratio snr for asymptoticallyreliable detection we show a necessary condition for perfect recovery at any given snr for all algorithms regardless of complexity is m omegaklognk measurements this is considerably stronger than all previous necessary conditions we also show that the scaling of omegaklognk measurements is sufficient for a trivial maximum correlation estimator to succeed hence this scaling is optimal and does not require lasso matching pursuit or more sophisticated methods and the optimal scaling can thus be biologically plausible
risk bounds for randomized sample compressed classifiers risk bounds for randomized sample compressed classifiers we derive risk bounds for the randomized classifiers in sample compressions settings where the classifierspecification utilizes two sources of information viz the compression set and the message string by extending the recently proposed occamacircs hammer principle to the datadependent settings we derive pointwise versions of the bounds on the stochastic sample compressed classifiers and also recover the corresponding classical pacbayes bound we further show how these compare favorably to the existing results
robust kernel principal component analysis robust kernel principal component analysis kernel principal component analysis kpca is a popular generalization of linear pca that allows nonlinear feature extraction in kpca data in the input space is mapped to higher usually dimensional feature space where the data can be linearly modeled the feature space is typically induced implicitly by a kernel function and linear pca in the feature space is performed via the kernel trick however due to the implicitness of the feature space some extensions of pca such as robust pca cannot be directly generalized to kpca this paper presents a technique to overcome this problem and extends it to a unified framework for treating noise missing data and outliers in kpca our method is based on a novel cost function to perform inference in kpca extensive experiments in both synthetic and real data show that our algorithm outperforms existing methods
robust nearisometric matching via structured learning of graphical models robust nearisometric matching via structured learning of graphical models models for nearrigid shape matching are typically based on distancerelated features in order to infer matches that are consistent with the isometric assumption however real shapes from image datasets even when expected to be related by quotalmost isometricquot transformations are actually subject not only to noise but also to some limited degree to variations in appearance and scale in this paper we introduce a graphical model that parameterises appearance distance and angle features and we learn all of the involved parameters via structured prediction the outcome is a model for nearrigid shape matching which is robust in the sense that it is able to capture the possibly limited but still important scale and appearance variations our experimental results reveal substantial improvements upon recent successful models while maintaining similar running times
robust regression and lasso robust regression and lasso we consider robust leastsquares regression with featurewise disturbance we show that this formulation leads to tractable convex optimization problems and we exhibit a particular uncertainty set for which the robust problem is equivalent to ell regularized regression lasso this provides an interpretation of lasso from a robust optimization perspective we generalize this robust formulation to consider more general uncertainty sets which all lead to tractable convex optimization problems therefore we provide a new methodology for designing regression algorithms which generalize known formulations the advantage is that robustness to disturbance is a physical property that can be exploited in addition to obtaining new formulations we use it directly to show sparsity properties of lasso as well as to prove a general consistency result for robust regression problems including lasso from a unified robustness perspective
scalable algorithms for string kernels with inexact matching scalable algorithms for string kernels with inexact matching we present a new family of linear time algorithms based on sufficient statistics for string comparison with mismatches under the string kernels framework our algorithms improve theoretical complexity bounds of existing approaches while scaling well with respect to the sequence alphabet size the number of allowed mismatches and the size of the dataset in particular on large alphabets with loose mismatch constraints our algorithms are several orders of magnitude faster than the existing algorithms for string comparison under the mismatch similarity measure we evaluate our algorithms on synthetic data and real applications in music genre classification protein remote homology detection and protein fold prediction the scalability of the algorithms allows us to consider complex sequence transformations modeled using longer string features and larger numbers of mismatches leading to a stateoftheart performance with significantly reduced running times
sdl supervised dictionary learning sdl supervised dictionary learning it is now well established that sparse signal models are well suited to restoration tasks and can effectively be learned from audio image and video data recent research has been aimed at learning discriminative sparse models instead of purely reconstructive ones this paper proposes a new step in that direction with a novel sparse representation for signals belonging to different classes in terms of a shared dictionary and multiple decision functions it is shown that the linear variant of the model admits a simple probabilistic interpretation and that its most general variant also admits a simple interpretation in terms of kernels an optimization framework for learning all the components of the proposed model is presented along with experiments on standard handwritten digit and texture classification tasks
selforganization using dynamical synapses selforganization using dynamical synapses large networks of spiking neurons show abrupt changes in their collective dynamics resembling phase transitions studied in statistical physics an example of this phenomenon is the transition from irregular noisedriven dynamics to regular selfsustained behavior observed in networks of integrateandfire neurons as the interaction strength between the neurons increases in this work we show how a network of spiking neurons is able to selforganize toward a critical state for which the number of possible robust periods dynamic range is maximized selforganization occurs via synaptic dynamics the resulting plasticity rule is defined locally so that global homeostasis near the critical state is achieved by local regulation of individual synapses
semisupervised learning with weaklyrelated unlabeled data towards better text categorization semisupervised learning with weaklyrelated unlabeled data towards better text categorization the cluster assumption is exploited by most semisupervised learning ssl methods however if the unlabeled data is merely weakly related to the target classes it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classification in such case the cluster assumption may not be valid and consequently how to leverage this type of unlabeled data to enhance the classification accuracy becomes a challenge we introduce quotsemisupervised learning with weaklyrelated unlabeled dataquot sslw an inductive method that builds upon the maximummargin approach towards a better usage of weaklyrelated unlabeled information although the sslw could improve a wide range of classification tasks in this paper we focus on text categorization with a small training pool the key assumption behind this work is that even with different topics the word usage patterns across different corpora tends to be consistent to this end sslw estimates the optimal wordcorrelation matrix that is consistent with both the cooccurrence information derived from the weaklyrelated unlabeled documents and the labeled documents for empirical evaluation we present a direct comparison with a number of stateoftheart methods for inductive semisupervised learning and text categorization and we show that sslw results in a significant improvement in categorization accuracy equipped with a small training set and an unlabeled resource that is weakly related to the test beds
sequential effects superstition or rational behavior sequential effects superstition or rational behavior in a variety of behavioral tasks subjects exhibit an automatic and apparently suboptimal sequential effect they respond more rapidly and accurately to a stimulus if it reinforces a local pattern in stimulus history such as a string of repetitions or alternations compared to when it violates such a pattern this is often the case even if the local trends arise by chance in the context of a randomized design such that stimulus history has no predictive power in this work we use a normative bayesian framework to examine the hypothesis that such idiosyncrasies may reflect the inadvertent engagement of fundamental mechanisms critical for adapting to changing statistics in the natural environment we show that prior belief in nonstationarity can induce experimentally observed sequential effects in an otherwise bayesoptimal algorithm the bayesian algorithm is shown to be well approximated by linearexponential filtering of past observations a feature also apparent in the behavioral data we derive an explicit relationship between the parameters and computations of the exact bayesian algorithm and those of the approximate linearexponential filter since the latter is equivalent to a leakyintegration process a commonly used model of neuronal dynamics underlying perceptual decisionmaking and trialtotrial dependencies our model provides a principled account of why such dynamics are useful we also show that nearoptimal tuning of the leakyintegration process is possible using stochastic gradient descent based only on the noisy binary inputs this is a proof of concept that not only can neurons implement nearoptimal prediction based on standard neuronal dynamics but that they can also learn to tune the processing parameters without explicitly representing probabilities
shared segmentation of natural scenes using dependent pitmanyor processes shared segmentation of natural scenes using dependent pitmanyor processes we develop a statistical framework for the simultaneous unsupervised segmentation and discovery of visual object categories from image databases examining a large set of manually segmented scenes we use chisquare tests to show that object frequencies and segment sizes both follow power law distributions which are well modeled by the pitmanyor py process this nonparametric prior distribution leads to learning algorithms which discover an unknown set of objects and segmentation methods which automatically adapt their resolution to each image generalizing previous applications of py processes we use gaussian processes to discover spatially contiguous segments which respect image boundaries using a novel family of variational approximations our approach produces segmentations which compare favorably to stateoftheart methods while simultaneously discovering categories shared among natural scenes
shortterm depression in vlsi stochastic synapse shortterm depression in vlsi stochastic synapse we report a compact realization of shortterm depression std in a vlsi stochastic synapse the behavior of the circuit is based on a subtractive single release model of std experimental results agree well with simulation and exhibit expected std behavior the transmitted spike train has negative autocorrelation and lower power spectral density at low frequencies which can remove redundancy in the input spike train and the mean transmission probability is inversely proportional to the input spike rate which has been suggested as an automatic gain control mechanism in neural systems the dynamic stochastic synapse could potentially be a powerful addition to existing deterministic vlsi spiking neural systems
signaltonoise ratio analysis of policy gradient algorithms signaltonoise ratio analysis of policy gradient algorithms policy gradient pg reinforcement learning algorithms have strong local convergence guarantees but their learning performance is typically limited by a large variance in the estimate of the gradient in this paper we formulate the variance reduction problem by describing a signaltonoise ratio snr for policy gradient algorithms and evaluate this snr carefully for the popular weight perturbation wp algorithm we confirm that snr is a good predictor of longterm learning performance and that in our episodic formulation the costtogo function is indeed the optimal baseline we then propose two modifications to traditional modelfree policy gradient algorithms in order to optimize the snr first we examine wp using anisotropic sampling distributions which introduces a bias into the update but increases the snr this bias can be interpretted as following the natural gradient of the cost function second we show that nongaussian distributions can also increase the snr and argue that the optimal isotropic distribution is a acircshellacirc distribution with a constant magnitude and uniform distribution in direction we demonstrate that both modifications produce substantial improvements in learning performance in challenging policy gradient experiments
simple local models for complex dynamical systems simple local models for complex dynamical systems we present a novel mathematical formalism for the idea of a quotlocal model a model of a potentially complex dynamical system that makes only certain predictions in only certain situations as a result of its restricted responsibilities a local model may be far simpler than a complete model of the system we then show how one might combine several local models to produce a more detailed model we demonstrate our ability to learn a collection of local models on a largescale example and do a preliminary empirical comparison of learning a collection of local models and some other model learning methods
skill characterization based on betweenness skill characterization based on betweenness we present a characterization of a useful class of skills based on a graphical representation of an agents interaction with its environment our characterization uses betweenness a measure of centrality on graphs it may be used directly to form a set of skills suitable for a given environment more importantly it serves as a useful guide for developing online incremental skill discovery algorithms that do not rely on knowing or representing the environment graph in its entirety
sparse convolved gaussian processes for multiouptut regression sparse convolved gaussian processes for multiouptut regression we present a sparse approximation approach for dependent output gaussian processes gp employing a latent function framework we apply the convolution process formalism to establish dependencies between output variables where each latent function is represented as a gp based on these latent functions we establish an approximation scheme using a conditional independence assumption between the output processes leading to an approximation of the full covariance which is determined by the locations at which the latent functions are evaluated we show results of the proposed methodology for synthetic data and real world applications on pollution prediction and a sensor network
sparse online learning via truncated gradient sparse online learning via truncated gradient we propose a general method called truncated gradient to induce sparsity in the weights of onlinelearning algorithms with convex loss this method has several essential properties first the degree of sparsity is continuousa parameter controls the rate of sparsification from no sparsification to total sparsification second the approach is theoretically motivated and an instance of it can be regarded as an online counterpart of the popular lregularization method in the batch setting we prove that small rates of sparsification result in only small additional regret with respect to typical onlinelearning guarantees finally the approach works well empirically we apply it to several datasets and find that for datasets with large numbers of features substantial sparsity is discoverable
sparse probabilistic projections sparse probabilistic projections we present a generative model for performing sparse probabilistic projections which includes sparse principal component analysis and sparse canonical correlation analysis as special cases sparsity is enforced by means of automatic relevance determination or by imposing appropriate prior distributions such as generalised hyperbolic distributions we derive a variational expectationmaximisation algorithm for the estimation of the hyperparameters and show that our novel probabilistic approach compares favourably to existing techniques we illustrate how the proposed method can be applied in the context of cryptoanalysis as a preprocessing tool for the construction of template attacks
sparse signal recovery using markov random fields sparse signal recovery using markov random fields compressive sensing cs combines sampling and compression into a single subnyquist linear measurement process for sparse and compressible signals in this paper we extend the theory of cs to include signals that are concisely represented in terms of a graphical model in particular we use markov random fields mrfs to represent sparse signals whose nonzero coefficients are clustered our new modelbased reconstruction algorithm dubbed lattice matching pursuit lamp stably recovers mrfmodeled signals using many fewer measurements and computations than the current stateoftheart algorithms
sparsity of svms that use the epsiloninsensitive loss sparsity of svms that use the epsiloninsensitive loss in this paper lower and upper bounds for the number of support vectors are derived for support vector machines svms based on the epsiloninsensitive loss function it turns out that these bounds are asymptotically tight under mild assumptions on the data generating distribution finally we briefly discuss a tradeoff in epsilon between sparsity and accuracy if the svm is used to estimate the conditional median
spectral clustering with perturbed data spectral clustering with perturbed data spectral clustering is useful for a wideranging set of applications in areas such as biological data analysis image processing and data mining however the computational andor communication resources required by the method in processing largescale data sets are often prohibitively high and practitioners are often required to perturb the original data in various ways quantization downsampling etc before invoking a spectral algorithm in this paper we use stochastic perturbation theory to study the effects of data perturbation on the performance of spectral clustering we show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the laplacian matrix from this result we derive approximate upper bounds on the clustering error we show that this bound is tight empirically across a wide range of problems suggesting that it can be used in practical settings to determine the amount of data reduction allowed in order to meet a specification of permitted loss in clustering performance
spectral hashing spectral hashing semantic hashing seeks compact binary codes of datapoints so that the hamming distance between codewords correlates with semantic similarity hinton et al used a clever implementation of autoencoders to find such codes in this paper we show that the problem of finding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be np hard by relaxing the original problem we obtain a spectral method whose solutions are simply a subset of thresh olded eigenvectors of the graph laplacian by utilizing recent results on convergence of graph laplacian eigenvectors to the laplacebeltrami eigen functions of manifolds we show how to efficiently calculate the code of a novel datapoint taken together both learning the code and applying it to a novel point are extremely simple our experiments show that our codes significantly outperform the stateofthe art
stochastic relational models for largescale dyadic data using mcmc stochastic relational models for largescale dyadic data using mcmc stochastic relational models provide a rich family of choices for learning and predicting dyadic data between two sets of entities it generalizes matrix factorization to a supervised learning problem that utilizes attributes of objects in a hierarchical bayesian framework previously empirical bayesian inference was applied which is however not scalable when the size of either object sets becomes tens of thousands in this paper we introduce a markov chain monte carlo mcmc algorithm to scale the model to very largescale dyadic data both superior scalability and predictive accuracy are demonstrated on a collaborative filtering problem which involves tens of thousands users and a half million items
stress noradrenaline and realistic prediction of mouse behaviour using reinforcement learning stress noradrenaline and realistic prediction of mouse behaviour using reinforcement learning suppose we train an animal in a conditioning experiment can one predict how a given animal under given experimental conditions would perform the task since various factors such as stress motivation genetic background and previous errors in task performance can influence animal behavior this appears to be a very challenging aim reinforcement learning rl models have been successful in modeling animal and human behavior but their success has been limited because of uncertainty as to how to set metaparameters such as learning rate exploitationexploration balance and future reward discount factor that strongly influence model performance we show that a simple rl model whose metaparameters are controlled by an artificial neural network fed with inputs such as stress affective phenotype previous task performance and even neuromodulatory manipulations can successfully predict mouse behavior in the quotholeboxquot a simple conditioning task our results also provide important insights on how stress and anxiety affect animal learning performance accuracy and discounting of future rewards and on how noradrenergic systems can interact with these processes
structure learning in human sequential decisionmaking structure learning in human sequential decisionmaking we use graphical models and structure learning to explore how people learn policies in sequential decision making tasks studies of sequential decisionmaking in humans frequently find suboptimal performance relative to an ideal actor that knows the graph model that generates reward in the environment we argue that the learning problem humans face also involves learning the graph structure for reward generation in the environment we formulate the structure learning problem using mixtures of reward models and solve the optimal action selection problem using bayesian reinforcement learning we show that structure learning in one and two armed bandit problems produces many of the qualitative behaviors deemed suboptimal in previous studies our argument is supported by the results of experiments that demonstrate humans rapidly learn and exploit new reward structure
structured ranking learning using cumulative distribution networks structured ranking learning using cumulative distribution networks ranking is at the heart of many information retrieval applications unlike standard regression or classification in which we predict outputs independently in ranking we are interested in predicting structured outputs so that misranking one object can significantly affect whether we correctly rank the other objects in practice the problem of ranking involves a large number of objects to be ranked and either approximate structured prediction methods are required or assumptions of independence between object scores must be made in order to make the problem tractable we present a probabilistic method for learning to rank using the graphical modelling framework of cumulative distribution networks cdns where we can take into account the structure inherent to the problem of ranking by modelling the joint cumulative distribution functions cdfs over multiple pairwise preferences we apply our framework to the problem of document retrieval in the case of the ohsumed benchmark dataset we will show that the ranknet listnet and listmle probabilistic models can be viewed as particular instances of cdns and that our proposed framework allows for the exploration of a broad class of flexible structured loss functionals for ranking learning
supervised bipartite graph inference supervised bipartite graph inference we formulate the problem of bipartite graph inference as a supervised learning problem and propose a new method to solve it from the viewpoint of distance metric learning the method involves the learning of two mappings of the heterogeneous objects to a unified euclidean space representing the network topology of the bipartite graph where the graph is easy to infer the algorithm can be formulated as an optimization problem in a reproducing kernel hilbert space we report encouraging results on the problem of compoundprotein interaction network reconstruction from chemical structure data and genomic sequence data
supervised exponential family principal component analysis via convex optimizatio supervised exponential family principal component analysis via convex optimizatio recently supervised dimensionality reduction has been gaining attention owing to the realization that data labels are often available and strongly suggest important underlying structures in the data in this paper we present a novel convex supervised dimensionality reduction approach based on exponential family pca and provide a simple but novel form to project new testing data into the embedded space this convex approach successfully avoids the local optima of the em learning moreover by introducing a samplebased multinomial approximation to exponential family models it avoids the limitation of the prevailing gaussian assumptions of standard pca and produces a kernelized formulation for nonlinear supervised dimensionality reduction a training algorithm is then devised based on a subgradient bundle method whose scalability can be gained through a coordinate descent procedure the advantage of our global optimization approach is demonstrated by empirical results over both synthetic and real data
suppport vector machines with a reject option suppport vector machines with a reject option we consider the problem of binary classification where the classifier may abstain instead of classifying each observation the bayes decision rule for this setup known as chows rule is defined by two thresholds on posterior probabilities from simple desiderata namely the consistency and the sparsity of the classifier we derive the double hinge loss function that focuses on estimating conditional probabilities only in the vicinity of the threshold points of the optimal decision rule we show that for suitable kernel machines our approach is universally consistent we cast the problem of minimizing the double hinge loss as a quadratic program akin to the standard svm optimization problem and propose an active set method to solve it efficiently we finally provide preliminary experimental results illustrating the interest of our constructive approach to devising loss functions
syntactic topic models syntactic topic models we develop name stm a nonparametric bayesian model of parsed documents shortname generates words that are both thematically and syntactically constrained which combines the semantic insights of topic models with the syntactic information available from parse trees each word of a sentence is generated by a distribution that combines documentspecific topic weights and parsetree specific syntactic transitions words are assumed generated in an order that respects the parse tree we derive an approximate posterior inference method based on variational methods for hierarchical dirichlet processes and we report qualitative and quantitative results on both synthetic data and handparsed documents
temporal difference based actor critic learning convergence and neural implementation temporal difference based actor critic learning convergence and neural implementation actorcritic algorithms for reinforcement learning are achieving renewed popularity due to their good convergence properties in situations where other approaches often fail eg when function approximation is involved interestingly there is growing evidence that actorcritic approaches based on phasic dopamine signals play a key role in biological learning through the cortical and basal ganglia we derive a temporal difference based actor critic learning algorithm for which convergence can be proved without assuming separate time scales for the actor and the critic the approach is demonstrated by applying it to networks of spiking neurons the established relation between phasic dopamine and the temporal difference signal lends support to the biological relevance of such algorithms
temporal dynamics of cognitive control temporal dynamics of cognitive control cognitive control refers to the flexible deployment of memory and attention in response to task demands and current goals control is often studied experimentally by presenting sequences of stimuli some demanding a response and others modulating the stimulusresponse mapping in these tasks participants must maintain information about the current stimulusresponse mapping in working memory prominent theories of cognitive control use recurrent neural nets to implement working memory and optimize memory utilization via reinforcement learning we present a novel perspective on cognitive control in which working memory representations are intrinsically probabilistic and control operations that maintain and update working memory are dynamically determined via probabilistic inference we show that our model provides a parsimonious account of behavioral and neuroimaging data and suggest that it offers an elegant conceptualization of control in which behavior can be cast as optimal subject to limitations on learning and the rate of information processing moreover our model provides insight into how task instructions can be directly translated into appropriate behavior and then efficiently refined with subsequent task experience
the conjoint effect of divisive normalization and orientation selectivity on redundancy reduction the conjoint effect of divisive normalization and orientation selectivity on redundancy reduction bandpass filtering orientation selectivity and contrast gain control are prominent features of sensory coding at the level of v simple cells while the effect of bandpass filtering and orientation selectivity can be assessed within a linear model contrast gain control is an inherently nonlinear computation here we employ the class of lp elliptically contoured distributions to investigate the extent to which the two featuresorientation selectivity and contrast gain controlare suited to model the statistics of natural images within this framework we find that contrast gain control can play a significant role for the removal of redundancies in natural images orientation selectivity in contrast has only a very limited potential for redundancy reduction
the gaussian process density sampler the gaussian process density sampler we present the gaussian process density sampler gpds an exchangeable generative model for use in nonparametric bayesian density estimation samples drawn from the gpds are consistent with exact independent samples from a fixed density function that is a transformation of a function drawn from a gaussian process prior our formulation allows us to infer an unknown density from data using markov chain monte carlo which gives samples from the posterior distribution over density functions and from the predictive distribution on data space we can also infer the hyperparameters of the gaussian process we compare this density modeling technique to several existing techniques on a toy problem and a skullreconstruction task
the infinite factorial hidden markov model the infinite factorial hidden markov model we introduces a new probability distribution over a potentially infinite number of binary markov chains which we call the markov indian buffet process this process extends the ibp to allow temporal dependencies in the hidden variables we use this stochastic process to build a nonparametric extension of the factorial hidden markov model after working out an inference scheme which combines slice sampling and dynamic programming we demonstrate how the infinite factorial hidden markov model can be used for blind source separation
the mondrian process the mondrian process we describe a novel stochastic process that can be used to construct a multidimensional generalization of the stickbreaking process and which is related to the classic stick breaking process described by sethuraman in one dimension we describe how the process can be applied to relational data modeling using the de finetti representation for infinitely and partially exchangeable arrays
the recurrent temporal restricted boltzmann machine the recurrent temporal restricted boltzmann machine the temporal restricted boltzmann machine trbm is a probabilistic model for sequences that is able to successfully model ie generate nicelooking samples of several very high dimensional sequences such as motion capture data and the pixels of low resolution videos of balls bouncing in a box the major disadvantage of the trbm is that exact inference is extremely hard since even computing a gibbs update for a single variable of the posterior is exponentially expensive this difficulty has necessitated the use of a heuristic inference procedure that nonetheless was accurate enough for successful learning in this paper we introduce the recurrent trbm which is a very slight modification of the trbm for which exact inference is very easy and exact gradient learning is almost tractable we demonstrate that the rtrbm is better than an analogous trbm at generating motion capture and videos of bouncing balls
theory of matching pursuit theory of matching pursuit we analyse matching pursuit for kernel principal components analysis by proving that the sparse subspace it produces is a sample compression scheme we show that this bound is tighter than the kpca bound of shawetaylor et al swck and highly predictive of the size of the subspace needed to capture most of the variance in the data we analyse a second matching pursuit algorithm called kernel matching pursuit kmp which does not correspond to a sample compression scheme however we give a novel bound that views the choice of subspace of the kmp algorithm as a compression scheme and hence provide a vc bound to upper bound its future loss finally we describe how the same bound can be applied to other matching pursuit related algorithms
tighter bounds for structured estimation tighter bounds for structured estimation largemargin structured estimation methods work by minimizing a convex upper bound of loss functions while they allow for efficient optimization algorithms these convex formulations are not tight and sacrifice the ability to accurately model the true loss we present tighter nonconvex bounds based on generalizing the notion of a ramp loss from binary classification to structured estimation we show that a small modification of existing optimization algorithms suffices to solve this modified problem on structured prediction tasks such as protein sequence alignment and web page ranking our algorithm leads to improved accuracy
tracking changing stimuli in continuous attractor neural networks tracking changing stimuli in continuous attractor neural networks continuous attractor neural networks canns are emerging as promising models for describing the encoding of continuous stimuli in neural systems due to the translational invariance of their neuronal interactions canns can hold a continuous family of neutrally stable states in this study we systematically explore how neutral stability of a cann facilitates its tracking performance a capacity believed to have wide applications in brain functions we develop a perturbative approach that utilizes the dominant movement of the network stationary states in the state space we quantify the distortions of the bump shape during tracking and study their effects on the tracking performance results are obtained on the maximum speed for a moving stimulus to be trackable and the reaction time to catch up an abrupt change in stimulus
transfer learning by distribution matching for targeted advertising transfer learning by distribution matching for targeted advertising we address the problem of learning classifiers for several related tasks that may differ in their joint distribution of input and output variables for each task small possibly even empty labeled samples and large unlabeled samples are available while the unlabeled samples reflect the target distribution the labeled samples may be biased we derive a solution that produces resampling weights which match the pool of all examples to the target distribution of any given task our work is motivated by the problem of predicting sociodemographic features for users of web portals based on the content which they have accessed here questionnaires offered to a small portion of each portals users produce biased samples transfer learning enables us to make predictions even for new portals with few or no training data and improves the overall prediction accuracy
translated learning translated learning this paper investigates a new machine learning strategy called translated learning unlike many previous learning tasks we focus on how to use labeled data from one feature space to enhance the classification of other entirely different learning spaces for example we might wish to use labeled text data to help learn a model for classifying image data when the labeled images are difficult to obtain an important aspect of translated learning is to build a bridge to link one feature space known as the source domain to another domain known as the target domain through a translator in order to migrate the knowledge from source to target the translated learning solution uses a language model to link the class labels to the features in the source spaces which in turn is translated to the features in the target spaces finally this chain of linkages is completed by tracing back to the instances in the target spaces we show that this path of linkage can be modeled using a markov chain and risk minimization through experiments on the textaided image classification and crosslanguage classification tasks we demonstrate that our translated learning framework can greatly outperform many stateoftheart baseline methods
understanding brain connectivity patterns during motor imagery for braincomputer interfacing understanding brain connectivity patterns during motor imagery for braincomputer interfacing eeg connectivity measures could provide a new type of feature space for inferring a subjects intention in braincomputer interfaces bcis however very little is known on eeg connectivity patterns for bcis in this study eeg connectivity during motor imagery mi of the left and right is investigated in a broad frequency range across the whole scalp by combining beamforming with transfer entropy and taking into account possible volume conduction effects observed connectivity patterns indicate that modulation intentionally induced by mi is strongest in the gammaband ie above hz furthermore modulation between mi and rest is found to be more pronounced than between mi of different hands this is in contrast to results on mi obtained with bandpower features and might provide an explanation for the so far only moderate success of connectivity features in bcis it is concluded that future studies on connectivity based bcis should focus on high frequency bands and consider experimental paradigms that maximally vary cognitive demands between conditions
unifying the sensory and motor components of sensorimotor adaptation unifying the sensory and motor components of sensorimotor adaptation adaptation of visually guided reaching movements in novel visuomotor environments eg wearing prism goggles comprises not only motor adaptation but also substantial sensory adaptation corresponding to shifts in the perceived spatial location of visual and proprioceptive cues previous computational models of the sensory component of visuomotor adaptation have assumed that it is driven purely by the discrepancy introduced between visual and proprioceptive estimates of hand position and is independent of any motor component of adaptation we instead propose a unified model in which sensory and motor adaptation are jointly driven by optimal bayesian estimation of the sensory and motor contributions to perceived errors our model is able to account for patterns of performance errors during visuomotor adaptation as well as the subsequent perceptual aftereffects this unified model also makes the surprising prediction that force field adaptation will elicit similar perceptual shifts even though there is never any discrepancy between visual and proprioceptive observations we confirm this prediction with an experiment
unlabeled data now it helps now it doesnt unlabeled data now it helps now it doesnt empirical evidence shows that in favorable situations semisupervised learning ssl algorithms can capitalize on the abundancy of unlabeled training data to improve the performance of a learning task in the sense that fewer labeled training data are needed to achieve a target error bound however in other situations unlabeled data do not seem to help recent attempts at theoretically characterizing the situations in which unlabeled data can help have met with little success and sometimes appear to conflict with each other and intuition in this paper we attempt to bridge the gap between practice and theory of semisupervised learning we develop a rigorous framework for analyzing the situations in which unlabeled data can help and quantify the improvement possible using finite sample error bounds we show that there are large classes of problems for which ssl can significantly outperform supervised learning in finite sample regimes and sometimes also in terms of error convergence rates
unsupervised bayesian parameter estimation for probabilistic grammars unsupervised bayesian parameter estimation for probabilistic grammars in this paper we explore bayesian approaches for the unsupervised estimation of probabilistic grammars a family of distributions over discrete structures that includes hidden markov models and probabilistic contextfree grammars we consider the use of dirichlet priors and we extend the correlated topic model framework to probabilistic grammars and derive a variational em algorithm for eampcient approximate inference we experiment with the task of unsupervised grammar induction for natural language dependency parsing and show that superior results can be achieved when using a logistic normal prior over probabilistic grammars
unsupervised learning of visual sense models for polysemous words unsupervised learning of visual sense models for polysemous words polysemy is a problem for methods that exploit image search engines to build object category models existing unsupervised approaches do not take word sense into consideration we propose a new method that uses a dictionary to learn models of visual word sense from a large collection of unlabeled web data the use of lda to discover a latent sense space makes the model robust despite the very limited nature of dictionary definitions the definitions are used to learn a distribution in the latent space that best represents a sense the algorithm then uses the text surrounding image links to retrieve images with high probability of a particular dictionary sense an object classifier is trained on the resulting sensespecific images we evaluate our method on a dataset obtained by searching the web for polysemous words category classification experiments show that our dictionarybased approach outperforms baseline methods
using bayesian dynamical systems for motion template libraries using bayesian dynamical systems for motion template libraries motor primitives or motion templates have become an important concept for both modeling human motor control as well as generating robot behaviors using imitation learning recent impressive results range from humanoid robot movement generation to timing models of human motions the automatic generation of skill libraries containing multiple motion templates is an important step in robot learning such a skill learning system needs to cluster similar movements together and represent each resulting motion template as a generative model which is subsequently used for the execution of the behavior by a robot system in this paper we show how human trajectories captured as multidimensional timeseries can be clustered using bayesian mixtures of linear gaussian statespace models based on the similarity of their dynamics the appropriate number of templates is automatically determined by enforcing a parsimonious parametrization as the resulting model is intractable we introduce a novel approximation method based on variational bayes which is especially designed to enable the use of efficient inference algorithms on recorded human balero movements this method is not only capable of finding reasonable motion templates but also yields a generative model which works well in the execution of this complex task on a simulated anthropomorphic sarcos arm
using matrices to model symbolic relationship using matrices to model symbolic relationship we describe a way of learning matrix representations of objects and relationships the goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships which is the main novelty of the method we demonstrate that this leads to excellent generalization in two different domains modular arithmetic and family relationships we show that the same system can learn firstorder propositions such as member or christopher penelopemember haswife and higherorder propositions such as member plus and member inverse or hashusband haswifein higheroppsex we further demonstrate that the system understands how higherorder propositions are related to firstorder ones by showing that it can correctly answer questions about firstorder propositions involving the relations or haswife even though it has not been trained on any firstorder examples involving these relations
variational mixture of gaussian process experts variational mixture of gaussian process experts mixture of gaussian processes models extended a single gaussian process with ability of modeling multimodal data and reduction of training complexity previous inference algorithms for these models are mostly based on gibbs sampling which can be very slow particularly for largescale data sets we present a new generative mixture of experts model each expert is still a gaussian process but is reformulated by a linear model this breaks the dependency among training outputs and enables us to use a much faster variational bayesian algorithm for training our gating network is more flexible than previous generative approaches as inputs for each expert are modeled by a gaussian mixture model the number of experts and number of gaussian components for an expert are inferred automatically a variety of tests show the advantages of our method
weighted sums of random kitchen sinks replacing minimization with randomization in learning weighted sums of random kitchen sinks replacing minimization with randomization in learning randomized neural networks are immortalized in this ai koan in the days when sussman was a novice minsky once came to him as he sat hacking at the pdp what are you doing asked minsky i am training a randomly wired neural net to play tictactoe sussman replied why is the net wired randomly asked minsky sussman replied i do not want it to have any preconceptions of how to play minsky then shut his eyes why do you close your eyes sussman asked his teacher so that the room will be empty replied minsky at that moment sussman was enlightened we analyze shallow random networks with the help of concentration of measure inequalities specifically we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlinearities we identify conditions under which these networks exhibit good classification performance and bound their test error in terms of the size of the dataset and the number of random nonlinearities
quantum haystacks quantum haystacks this acceptance talk is a curious mixture of personal history and developing ideas in the context of the growing field of ir covering several decades i want to concentrate on models and theories interpreted loosely and try and give an insight into where i have got to in my thinking where the ideas came from and where i believe we are goingin the last few years i have been working on the development of what might be coined as a design language for ir it takes its inspiration from quantum mechanics but by analogy only the mathematical objects represent documents these objects might be vectors or density operators in an ndimensional vector space usually a hilbert space a request for information or a query is taken as an observable and is represented as a linear operator on the space linear operators can be expressed as matrices such an operator hermitian has a set of eigenvectors forming a basis for the space which we interpret as a point of view or perspective from which to understand the space thus any documentvector can be located with respect to the basis and we can calculate an inner product between such a vector and any basis vector which may be interpreted as a probability of relevance the probability of observing any given eigenvector is now given by the square of that inner product assuming all vectors are normalised hence we connect the probability of observation to the geometry of the space furthermore the subspaces of the space make up a lattice structure which is equivalent to a logic this makes up the entire mathematical structure and the language for handling this structure is linear algebra vectors matrices projections innerproducts neatly captured by the dirac notation used in quantum mechanics our probability is slightly different from classical probability the same for logic we end up with quantum logic and quantum probabilitya commitment to this kind of mathematical structure with which to model objects and processes in ir depends on two critical assumptionsthe distances in the space between objects are a source of important relationships with respect to relevance and aboutnessthe observation of a property such as relevance or aboutness is user dependent in the sense that a potential interaction is specified by a user through an operator which when measured achieves outcomes with a probability determined by the geometry of the spacethe geometry of this mathematical structure and the probability defined on it are closely connected by the following theorem due to gleason one may summarise this theorem by saying that the probability of a subspace is given by a simple algorithm derived from a projection onto the subspace and a special kind of operator namely a statistical operator or density matrix and conversely that given a probability measure on the subspaces then we can encode that measure uniquely through such an algorithm this is a very powerful theorem and its consequences remain to be exploredso how did i get to this point and form of abstraction most of my research work can be divided into contributions to the following areasclusteringevaluationprobabilistic modelslogic modelsgeometryin all these areas i have attempted to search for underlying mathematical structures that would lead to computations these topics have in common that they depend on the construction of measures on a space which in some sense determines the usefulness or effectiveness of the structure for clustering one considers mapping from metric spaces to ultrametic spaces and measure the closeness of fit in the case of evaluation one starts with a relational conjoint structure and imposes some constraints given by what is to be measured one then constructs a numerical representation of this structure leading to such measures as f or e for probabilistic models the main difficulty is concerned with deciding on an appropriate event space on which to define the right probability measures for me the most significant example in this context was the attempt to construct a logical uncertainty principle which formulated a measure of uncertainty on incomplete logical constructs this attempt left unspecified the exact form of the measure in the geometry of ir i finally managed to formulate that measure as a projectionvalued measurethis way of thinking did not appear out of nowhere it was heavily influenced by the work of fairthorne whose work on brouwerian logic an intuitionistic logic was picked up by salton in his early book on ir at an earlier stage mackay wrote a paper that opened with this paper relates to the borderline linking experimental and theoretical physics with mathematical logic and covers at several points ground which is common to the theory of communication he goes on to define an informationoperator which is very similar in scope and intent to the hermitian operator above maron who collaborated with mackay stated in his paper therefore it can be argued that index descriptions should not be viewed as properties of documents they function to relate documents and users one can see that the development of these early ideas was continued to the construction of the geometry of irwhat does it leave to be done an attempt should be made to use this design language to build an ir system on the theoretical front it is worth considering whether it would be better to start with a transition probability space rather than a hilbert space as von neumann did in translated in the assumption that closed linear subspaces will be the elements of our logic can be challenged as perhaps a construction with different elements is possible it is not obvious what the best form of conditional probability might be in these spaces agreeing on a form of conditionalisation is intimately tied up with how to model contextuality there is some evidence to suggest that contextuality plays a role in modelling the conjuncton of concepts widdows such contexts have been modelled in quantum theory almost from the beginning for example gleasons theorem precludes noncontextual hidden variable theories 
learning user interaction models for predicting web search result preferences learning user interaction models for predicting web search result preferences evaluating user preferences of web search results is crucial for search engine development deployment and maintenance we present a realworld study of modeling the behavior of web search users to predict web search result preferences accurate modeling and interpretation of user behavior has important applications to ranking click spam detection web search personalization and other tasks our key insight to improving robustness of interpreting implicit feedback is to model querydependent deviations from the expected noisy user behavior we show that our model of clickthrough interpretation improves prediction accuracy over stateoftheart clickthrough methods we generalize our approach to model user behavior beyond clickthrough which results in higher preference prediction accuracy than models based on clickthrough information alone we report results of a largescale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods 
user performance versus precision measures for simple search tasks user performance versus precision measures for simple search tasks several recent studies have demonstrated that the type of improvements in information retrieval system effectiveness reported in forums such as sigir and trec do not translate into a benefit for users two of the studies used an instance recall task and a third used a question answering task so perhaps it is unsurprising that the precision based measures of ir system effectiveness on oneshot query evaluation do not correlate with user performance on these tasks in this study we evaluate two different information retrieval tasks on trec webtrack data a precisionbased user task measured by the length of time that users need to find a single document that is relevant to a trec topic and a simple recallbased task represented by the total number of relevant documents that users can identify within five minutes users employ search engines with controlled mean average precision map of between and our results show that there is no significant relationship between system effectiveness measured by map and the precisionbased task a significant but weak relationship is present for the precision at one document returned metric a weak relationship is present between map and the simple recallbased task 
improving web search ranking by incorporating user behavior information improving web search ranking by incorporating user behavior information we show that incorporating user behavior data can significantly improve ordering of top results in real web search setting we examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features we report results of a large scale evaluation over queries and million user interactions with a popular web search engine we show that incorporating implicit feedback can augment other features improving the accuracy of a competitive web search ranking algorithms by as much as relative to the original performance 
contextual search and name disambiguation in email using graphs contextual search and name disambiguation in email using graphs similarity measures for text have historically been an important tool for solving information retrieval problems in many interesting settings however documents are often closely connected to other documents as well as other nontextual objects for instance email messages are connected to other messages via header information in this paper we consider extended similarity metrics for documents and other objects embedded in graphs facilitated via a lazy graph walk we provide a detailed instantiation of this framework for email data where content social networks and a timeline are integrated in a structural graph the suggested framework is evaluated for two emailrelated problems disambiguating names in email documents and threading we show that reranking schemes based on the graphwalk similarity measures often outperform baseline methods and that further improvements can be obtained by use of appropriate learning methods 
thread detection in dynamic text message streams thread detection in dynamic text message streams text message stream is a newly emerging type of web data which is produced in enormous quantities with the popularity of instant messaging and internet relay chat it is beneficial for detecting the threads contained in the text stream for various applications including information retrieval expert recognition and even crime prevention despite its importance not much research has been conducted so far on this problem due to the characteristics of the data in which the messages are usually very short and incomplete in this paper we present a stringent definition of the thread detection task and our preliminary solution to it we propose three variations of a singlepass clustering algorithm for exploiting the temporal information in the streams an algorithm based on linguistic features is also put forward to exploit the discourse structure information we conducted several experiments to compare our approaches with some existing algorithms on a real dataset the results show that all three variations of the singlepass algorithm outperform the basic singlepass algorithm our proposed algorithm based on linguistic features improves the performance relatively by and when compared with the basic singlepass algorithm and the best variation algorithm in terms of f respectively 
formal models for expert finding in enterprise corpora formal models for expert finding in enterprise corpora searching an organizations document repositories for experts provides a cost effective solution for the task of expert finding we present two general strategies to expert searching given a document collection which are formalized using generative probabilistic models the first of these directly models an experts knowledge based on the documents that they are associated with whilst the second locates documents on topic and then finds the associated expert forming reliable associations is crucial to the performance of expert finding systems consequently in our evaluation we compare the different approaches exploring a variety of associations along with other operational parameters such as topicality using the trec enterprise corpora we show that the second strategy consistently outperforms the first a comparison against other unsupervised techniques reveals that our second model delivers excellent performance 
spoken document retrieval from callcenter conversations spoken document retrieval from callcenter conversations we are interested in retrieving information from conversational speech corpora such as callcenter data this data comprises spontaneous speech conversations with low recording quality which makes automatic speech recognition asr a highly difficult task for typical callcenter data even stateoftheart large vocabulary continuous speech recognition systems produce a transcript with word error rate of or higher in addition to the output transcript advanced systems provide word confusion networks wcns a compact representation of word lattices associating each word hypothesis with its posterior probability our work exploits the information provided by wcns in order to improve retrieval performance in this paper we show that the mean average precision map is improved using wcns compared to the raw word transcripts finally we analyze the effect of increasing asr word error rate on search effectiveness we show that map is still reasonable even under extremely high error rate 
towards efficient automated singer identification in large music databases towards efficient automated singer identification in large music databases automated singer identification is important in organising browsing and retrieving data in large music databases in this paper we propose a novel scheme called hybrid singer identifier hsi for automated singer recognition hsi can effectively use multiple lowlevel features extracted from both vocal and nonvocal music segments to enhance the identification process with a hybrid architecture and build profiles of individual singer characteristics based on statistical mixture models extensive experimental results conducted on a large music database demonstrate the superiority of our method over stateoftheart approaches 
music structure based vector space retrieval music structure based vector space retrieval this paper proposes a novel framework for music content indexing and retrieval the music structure information ie timing harmony and music region content is represented by the layers of the music structure pyramid we begin by extracting this layered structure information we analyze the rhythm of the music and then segment the signal proportional to the interbeat intervals thus the timing information is incorporated in the segmentation process which we call beat space segmentation to describe harmony events we propose a twolayer hierarchical approach to model the music chords we also model the progression of instrumental and vocal content as acoustic events after information extraction we propose a vector space modeling approach which uses these events as the indexing terms in querybyexample music retrieval a query is represented by a vector of the statistics of the ngram events we then propose two effective retrieval models a hardindexing scheme and a softindexing scheme experiments show that the vector space modeling is effective in representing the layered music information achieving top retrieval accuracy using sec music clips as the queries the softindexing outperforms hardindexing in general 
aggregaterank aggregaterank since the website is one of the most important organizational structures of the web how to effectively rank websites has been essential to many web applications such as web search and crawling in order to get the ranks of websites researchers used to describe the interconnectivity among websites with a socalled hostgraph in which the nodes denote websites and the edges denote linkages between websites if and only if there are hyperlinks from the pages in one website to the pages in the other there will be an edge between these two websites and then adopted the random walk model in the hostgraph however as pointed in this paper the random walk over such a hostgraph is not reasonable because it is not in accordance with the browsing behavior of web surfers therefore the derivate rank cannot represent the true probability of visiting the corresponding websitein this work we mathematically proved that the probability of visiting a website by the random web surfer should be equal to the sum of the pagerank values of the pages inside that website nevertheless since the number of web pages is much larger than that of websites it is not feasible to base the calculation of the ranks of websites on the calculation of pagerank to tackle this problem we proposed a novel method named aggregaterank rooted in the theory of stochastic complement which cannot only approximate the sum of pagerank accurately but also have a lower computational complexity than pagerank both theoretical analysis and experimental evaluation show that aggregaterank is a better method for ranking websites than previous methods 
respect my authority respect my authority we present an approach to improving the precision of an initial document ranking wherein we utilize cluster information within a graphbased framework the main idea is to perform reranking based on centrality within bipartite graphs of documents on one side and clusters on the other side on the premise that these are mutually reinforcing entities links between entities are created via consideration of language models induced from themwe find that our clusterdocument graphs give rise to much better retrieval performance than previously proposed documentonly graphs do for example authoritybased reranking of documents via a hitsstyle clusterbased approach outperforms a previouslyproposed pagerankinspired algorithm applied to solelydocument graphs moreover we also show that computing authority scores for clusters constitutes an effective method for identifying clusters containing a large percentage of relevant documents 
topical link analysis for web search topical link analysis for web search traditional web linkbased ranking schemes use a single score to measure a pages authority without concern of the community from which that authority is derived as a result a resource that is highly popular for one topic may dominate the results of another topic in which it is less authoritative to address this problem we suggest calculating a score vector for each page to distinguish the contribution from different topics using a random walk model that probabilistically combines page topic distribution and link structure we show how to incorporate the topical model within both pagerank and hits without affecting the overall property and still render insight into topiclevel transition experiments on multiple datasets indicate that our technique outperforms other ranking approaches that incorporate textual analysis 
the role of knowledge in conceptual retrieval the role of knowledge in conceptual retrieval despite its intuitive appeal the hypothesis that retrieval at the level of concepts should outperform purely termbased approaches remains unverified empirically in addition the use of knowledge has not consistently resulted in performance gains after identifying possible reasons for previous negative results we present a novel framework for conceptual retrieval that articulates the types of knowledge that are important for information seeking we instantiate this general framework in the domain of clinical medicine based on the principles of evidencebased medicine ebm experiments show that an ebmbased scoring algorithm dramatically outperforms a stateoftheart baseline that employs only term statistics ablation studies further yield a better understanding of the performance contributions of different components finally we discuss how other domains can benefit from knowledgebased approaches 
a parallel derivation of probabilistic information retrieval models a parallel derivation of probabilistic information retrieval models this paper investigates in a stringent athematical formalism the parallel derivation of three grand probabilistic retrieval models binary independent retrieval bir poisson model pm and language modelling lmthe investigation has been motivated by a number of questions firstly though sharing the same origin namely the probability of relevance the models differ with respect to event spaces how can this be captured in a consistent notation and can we relate the event spaces secondly bir and pm are closely related but how does lm fit in thirdly how are tfidf and probabilistic models related the parallel investigation of the models leads to a number of formalised results bir and pm assume the collection to be a set of nonrelevant documents whereas lm assumes the collection to be a set of terms from relevant documentspm can be viewed as a bridge connecting bir and lma birlm equivalence explains bir as a special lm casepm explains tfidf and both bir and lm probabilities express tfidf in a dual way 
semantic term matching in axiomatic approaches to information retrieval semantic term matching in axiomatic approaches to information retrieval a common limitation of many retrieval models including the recently proposed axiomatic approaches is that retrieval scores are solely based on exact ie syntactic matching of terms in the queries and documents without allowing distinct but semantically related terms to match each other and contribute to the retrieval score in this paper we show that semantic term matching can be naturally incorporated into the axiomatic retrieval model through defining the primitive weighting function based on a semantic similarity function of terms we define several desirable retrieval constraints for semantic term matching and use such constraints to extend the axiomatic model to directly support semantic term matching based on the mutual information of terms computed on some document set we show that such extension can be efficiently implemented as query expansion experiment results on several representative data sets show that with mutual information computed over the documents in either the target collection for retrieval or an external collection such as the web our semantic expansion consistently and substantially improves retrieval accuracy over the baseline axiomatic retrieval model as a pseudo feedback method our method also outperforms a stateoftheart language modeling feedback method 
online spam filter fusion online spam filter fusion we show that a set of independently developed spam filters may be combined in simple ways to provide substantially better filtering than any of the individual filters the results of fiftythree spam filters evaluated at the trec spam track were combined posthoc so as to simulate the parallel online operation of the filters the combined results were evaluated using the trec methodology yielding more than a factor of two improvement over the best filter the simplest method averaging the binary classifications returned by the individual filters yields a remarkably good result a new method averaging logodds estimates based on the scores returned by the individual filters yields a somewhat better result and provides input to svm and logisticregressionbased stacking methods the stacking methods appear to provide further improvement but only for very large corpora of the stacking methods logistic regression yields the better result finally we show that it is possible to select a priori small subsets of the filters that when combined still outperform the best individual filter by a substantial margin 
building bridges for web query classification building bridges for web query classification web query classification qc aims to classify web users queries which are often short and ambiguous into a set of target categories qc has many applications including page ranking in web search targeted advertisement in response to queries and personalization in this paper we present a novel approach for qc that outperforms the winning solution of the acm kddcup competition whose objective is to classify real user queries in our approach we first build a bridging classifier on an intermediate taxonomy in an offline mode this classifier is then used in an online mode to map user queries to the target categories via the above intermediate taxonomy a major innovation is that by leveraging the similarity distribution over the intermediate taxonomy we do not need to retrain a new classifier for each new set of target categories and therefore the bridging classifier needs to be trained only once in addition we introduce category selection as a new method for narrowing down the scope of the intermediate taxonomy based on which we classify the queries category selection can improve both efficiency and effectiveness of the online classification by combining our algorithm with the winning solution of kddcup we made an improvement by and in terms of precision and f respectively compared with the best results of kddcup 
probfuse probfuse data fusion is the combination of the results of independent searches on a document collection into one single output result set it has been shown in the past that this can greatly improve retrieval effectiveness over that of the individual resultsthis paper presents probfuse a probabilistic approach to data fusion probfuse assumes that the performance of the individual input systems on a number of training queries is indicative of their future performance the fused result set is based on probabilities of relevance calculated during this training process retrieval experiments using data from the trec ad hoc collection demonstrate that probfuse achieves results superior to that of the popular combmnz fusion algorithm 
using webgraph distance for relevance feedback in web search using webgraph distance for relevance feedback in web search we study the effect of user supplied relevance feedback in improving web search results rather than using query refinement or document similarity measures to rerank results we show that the webgraph distance between two documents is a robust measure of their relative relevancy we demonstrate how the use of this metric can improve the rankings of result urls even when the user only rates one document in the dataset our research suggests that such interactive systems can significantly improve search results 
improving the estimation of relevance models using large external corpora improving the estimation of relevance models using large external corpora information retrieval algorithms leverage various collection statistics to improve performance because these statistics are often computed on a relatively small evaluation corpus we believe using larger nonevaluation corpora should improve performance specifically we advocate incorporating external corpora based on language modeling we refer to this process as external expansion when compared to traditional pseudorelevance feedback techniques external expansion is more stable across topics and up to more effective in terms of mean average precision our results show that using a high quality corpus that is comparable to the evaluation corpus can be as if not more effective than using the web our results also show that external expansion outperforms simulated relevance feedback in addition we propose a method for predicting the extent to which external expansion will improve retrieval performance our new measure demonstrates positive correlation with improvements in mean average precision 
regularized estimation of mixture models for robust pseudorelevance feedback regularized estimation of mixture models for robust pseudorelevance feedback pseudorelevance feedback has proven to be an effective strategy for improving retrieval accuracy in all retrieval models however the performance of existing pseudo feedback methods is often affected significantly by some parameters such as the number of feedback documents to use and the relative weight of original query terms these parameters generally have to be set by trialanderror without any guidance in this paper we present a more robust method for pseudo feedback based on statistical language models our main idea is to integrate the original query with feedback documents in a single probabilistic mixture model and regularize the estimation of the language model parameters in the model so that the information in the feedback documents can be gradually added to the original query unlike most existing feedback methods our new method has no parameter to tune experiment results on two representative data sets show that the new method is significantly more robust than a stateoftheart baseline language modeling approach for feedback with comparable or better retrieval accuracy 
contextsensitive semantic smoothing for the language modeling approach to genomic ir contextsensitive semantic smoothing for the language modeling approach to genomic ir semantic smoothing which incorporates synonym and sense information into the language models is effective and potentially significant to improve retrieval performance the implemented semantic smoothing models such as the translation model which statistically maps document terms to query terms and a number of works that have followed have shown good experimental results however these models are unable to incorporate contextual information thus the resulting translation might be mixed and fairly general to overcome this limitation we propose a novel contextsensitive semantic smoothing method that decomposes a document or a query into a set of weighted contextsensitive topic signatures and then translate those topic signatures into query terms in detail we solve this problem through choosing concept pairs as topic signatures and adopting an ontologybased approach to extract concept pairs estimating the translation model for each topic signature using the em algorithm and expanding document and query models based on topic signature translations the new smoothing method is evaluated on trec genomics track collections and significant improvements are obtained the map mean average precision achieves a maximal gain over the simple language model as well as a gain over the language model with contextinsensitive semantic smoothing 
ldabased document models for adhoc retrieval ldabased document models for adhoc retrieval search algorithms incorporating some form of topic model have a long history in information retrieval for example clusterbased retrieval has been studied since the s and has recently produced good results in the language model framework an approach to building topic models based on a formal generative model of documents latent dirichlet allocation lda is heavily cited in the machine learning literature but its feasibility and effectiveness in information retrieval is mostly unknown in this paper we study how to efficiently use lda to improve adhoc retrieval we propose an ldabased document model within the language modeling framework and evaluate it on several trec collections gibbs sampling is employed to conduct approximate inference in lda and the computational complexity is analyzed we show that improvements over retrieval using clusterbased models can be obtained with reasonable efficiency 
adapting ranking svm to document retrieval adapting ranking svm to document retrieval the paper is concerned with applying learning to rank to document retrieval ranking svm is a typical method of learning to rank we point out that there are two factors one must consider when applying ranking svm in general a learning to rank method to document retrieval first correctly ranking documents on the top of the result list is crucial for an information retrieval system one must conduct training in a way that such ranked results are accurate second the number of relevant documents can vary from query to query one must avoid training a model biased toward queries with a large number of relevant documents previously when existing methods that include ranking svm were applied to document retrieval none of the two factors was taken into consideration we show it is possible to make modifications in conventional ranking svm so it can be better used for document retrieval specifically we modify the hinge loss function in ranking svm to deal with the problems described above we employ two methods to conduct optimization on the loss function gradient descent and quadratic programming experimental results show that our method referred to as ranking svm for ir can outperform the conventional ranking svm and other existing methods for document retrieval on two datasets 
a study of statistical models for query translation a study of statistical models for query translation this paper presents a study of three statistical query translation models that use different units of translation we begin with a review of a wordbased translation model that uses cooccurrence statistics for resolving translation ambiguities the translation selection problem is then formulated under the framework of graphic model resorting to which the modeling assumptions and limitations of the cooccurrence model are discussed and the research of finding better translation units is motivated then two other models that use larger linguistically motivated translation units ie noun phrase and dependency triple are presented for each model the modeling and training methods are described in detail all query translation models are evaluated using trec collections results show that larger translation units lead to more specific models that usually achieve better translation and crosslanguage information retrieval results 
combining bidirectional translation and synonymy for crosslanguage information retrieval combining bidirectional translation and synonymy for crosslanguage information retrieval this paper introduces a general framework for the use of translation probabilities in crosslanguage information retrieval based on the notion that information retrieval fundamentally requires matching what the searcher means with what the author of a document meant that perspective yields a computational formulation that provides a natural way of combining what have been known as query and document translation two wellrecognized techniques are shown to be a special case of this model under restrictive assumptions crosslanguage search results are reported that are statistically indistinguishable from strong monolingual baselines for both french and chinese documents 
social networks incentives and search social networks incentives and search the role of network structure has grown in significance over the past ten years in the field of information retrieval stimulated to a great extent by the importance of link analysis in the development of web search techniques this body of work has focused primarily on the network that is most clearly visible on the web the network of hyperlinks connecting documents to documents but the web has always contained a second network less explicit but equally important and this is the social network on its users with latent persontoperson links encoding a variety of relationships including friendship information exchange and influence developments over the past few years including the emergence of social networking systems and rich social media as well as the availability of largescale email and instant messenging datasets have highlighted the crucial role played by online social networks and at the same time have made them much easier to uncover and analyze there is now a considerable opportunity to exploit the information content inherent in these networks and this prospect raises a number of interesting research challengewithin this context we focus on some recent efforts to formalize the problem of searching a social network the goal is to capture the issues underlying a variety of related scenarios a member of a social networking system such as myspace seeks a piece of information that may be held by a friend of a friend an employee in a large company searches his or her network of colleagues for expertise in a particular subject a node in a decentralized peertopeer filesharing system queries for a file that is likely to be a small number of hops away or a user in a distributed ir or federated search setting traverses a network of distributed resources connected by links that may not just be informational but also economic or contractual in their most basic forms these scenarios have some essential features in common a node in a network without global knowledge must find a short path to a desired target node or to one of several possible target nodesto frame the underlying problem we go back to one of the most wellknown pieces of empirical social network analysis stanley milgrams research into the smallworld phenomenon also known as the six degrees of separation the form of milgrams experiments in which randomly chosen starters had to forward a letter to a designated target individual established not just that short chains connecting farflung pairs of people are abundant in large social networks but also that the individuals in these networks operating with purely local information about their own friends and acquaintances are able to actually find these chains the milgram experiments thus constituted perhaps the earliest indication that largescale social networks are structured to support this type of decentralized search within a family of randomgraph models proposed by watts and strogatz we have shown that the ability of a network to support this type of decentralized search depends in subtle ways on how its longrange connections are correlated with the underlying spatial or organizational structure in which it is embedded recent studies using data on communication within organizations and the friendships within large online communities have established the striking fact that real social networks closely match some of the structural features predicted by these mathematical modelsif one looks further at the online settings that provide the initial motivation for these issues there is clearly interest from many directions in their longterm economic implications essentially the consequences that follow from viewing distributed information retrieval applications peertopeer systems or socialnetworking sites as providing marketplaces for information and services how does the problem of decentralized search in a network change when the participants are not simply agents following a fixed algorithm but strategic actors who make decisions in their own selfinterest and may demand compensation for taking part in a protocol such considerations bring us into the realm of algorithmic game theory an active area of current research that uses gametheoretic notions to quantify the performance of systems in which the participants follow their own selfinterest in a simple model for decentralized search in the presence of incentives we find that performance depends crucially on both the rarity of the information and the richness of the network topology if the network is too structurally impoverished an enormous investment may be required to produce a path from a query to an answer 
probabilistic model for definitional question answering probabilistic model for definitional question answering this paper proposes a probabilistic model for definitional question answering qa that reflects the characteristics of the definitional question the intention of the definitional question is to request the definition about the question target therefore an answer for the definitional question should contain the content relevant to the topic of the target and have a representation form of the definition style modeling the problem of definitional qa from both the topic and definition viewpoints the proposed probabilistic model converts the task of answering the definitional questions into that of estimating the three language models topic language model definition language model and general language model the proposed model systematically combines several evidences in a probabilistic framework experimental results show that a definitional qa system based on the proposed probabilistic model is comparable to stateoftheart systems 
answering complex questions with random walk models answering complex questions with random walk models we present a novel framework for answering complex questions that relies on question decomposition complex questions are decomposed by a procedure that operates on a markov chain by following a random walk on a bipartite graph of relations established between concepts related to the topic of a complex question and subquestions derived from topicrelevant passages that manifest these relations decomposed questions discovered during this random walk are then submitted to a stateoftheart question answering qa system in order to retrieve a set of passages that can later be merged into a comprehensive answer by a multidocument summarization mds system in our evaluations we show that access to the decompositions generated using this method can significantly enhance the relevance and comprehensiveness of summarylength answers to complex questions 
a framework to predict the quality of answers with nontextual features a framework to predict the quality of answers with nontextual features new types of document collections are being developed by various web services the service providers keep track of nontextual features such as click counts in this paper we present a framework to use nontextual features to predict the quality of documents we also show our quality measure can be successfully incorporated into the language modelingbased retrieval model we test our approach on a collection of question and answer pairs gathered from a community based question answering service where people ask and answer questions experimental results using our quality measure show a significant improvement over our baseline 
latent semantic analysis for multipletype interrelated data objects latent semantic analysis for multipletype interrelated data objects cooccurrence data is quite common in many real applications latent semantic analysis lsa has been successfully used to identify semantic relations in such data however lsa can only handle a single cooccurrence relationship between two types of objects in practical applications there are many cases where multiple types of objects exist and any pair of these objects could have a pairwise cooccurrence relation all these cooccurrence relations can be exploited to alleviate data sparseness or to represent objects more meaningfully in this paper we propose a novel algorithm mlsa which conducts latent semantic analysis by incorporating all pairwise cooccurrences among multiple types of objects based on the mutual reinforcement principle mlsa identifies the most salient concepts among the cooccurrence data and represents all the objects in a unified semantic space mlsa is general and we show that several variants of lsa are special cases of our algorithm experiment results show that mlsa outperforms lsa on multiple applications including collaborative filtering text clustering and text categorization 
identifying comparative sentences in text documents identifying comparative sentences in text documents this paper studies the problem of identifying comparative sentences in text documents the problem is related to but quite different from sentimentopinion sentence identification or classification sentiment classification studies the problem of classifying a document or a sentence based on the subjective opinion of the author an important application area of sentimentopinion identification is business intelligence as a product manufacturer always wants to know consumers opinions on its products comparisons on the other hand can be subjective or objective furthermore a comparison is not concerned with an object in isolation instead it compares the object with others an example opinion sentence is the sound quality of cd player x is poor an example comparative sentence is the sound quality of cd player x is not as good as that of cd player y clearly these two sentences give different information their language constructs are quite different too identifying comparative sentences is also useful in practice because direct comparisons are perhaps one of the most convincing ways of evaluation which may even be more important than opinions on each individual object this paper proposes to study the comparative sentence identification problem it first categorizes comparative sentences into different types and then presents a novel integrated pattern discovery and supervised learning approach to identifying comparative sentences from text documents experiment results using three types of documents news articles consumer reviews of products and internet forum postings show a precision of and recall of more detailed results are given in the paper 
tackling concept drift by temporal inductive transfer tackling concept drift by temporal inductive transfer machine learning is the mainstay for text classification however even the most successful techniques are defeated by many realworld applications that have a strong timevarying component to advance research on this challenging but important problem we promote a natural experimental frameworkthe daily classification taskwhich can be applied to large timebased datasets such as reuters rcvin this paper we dissect concept drift into three main subtypes we demonstrate via a novel visualization that the recurrent themes subtype is present in rcv this understanding led us to develop a new learning model that transfers induced knowledge through time to benefit future classifier learning tasks the method avoids two main problems with existing work in inductive transfer scalability and the risk of negative transfer in empirical tests it consistently showed more than points fmeasure improvement for each of four reuters categories tested 
evaluation in xml information retrieval evaluation in xml information retrieval standard information retrieval ir metrics assume a simple model where documents are understood as independent units such an assumption is not adapted to new paradigms like xml or web ir where retrievable informations are parts of documents or sets of related documents moreover classical hypotheses assumes that the user ignores the structural or logical context of document elements and hence the possibility of navigation between units eprum is a generalisation of precisionrecall pr that aims at allowing the user to navigate or browse in the corpus structure like the cumulated gain metrics it is able to handle continuous valued relevance we apply and compare eprum in the context of xml retrieval a very active field for evaluation metrics we also explain how eprum can be used in other ir paradigms 
minimal test collections for retrieval evaluation minimal test collections for retrieval evaluation accurate estimation of information retrieval evaluation metrics such as average precision require large sets of relevance judgments building sets large enough for evaluation of realworld implementations is at best inefficient at worst infeasible in this work we link evaluation with test collection construction to gain an understanding of the minimal judging effort that must be done to have high confidence in the outcome of an evaluation a new way of looking at average precision leads to a natural algorithm for selecting documents to judge and allows us to estimate the degree of confidence by defining a distribution over possible document judgments a study with annotators shows that this method can be used by a small group of researchers to rank a set of systems in under three hours with confidence information retrieval metrics such as average precision require large sets of relevance judgments to be accurately estimated building these sets is infeasible and often inefficient for many realworld retrieval implementations we present a new way of looking at average precision that allows us to estimate the confidence in an evaluation based on the size of the test collection we use this to build an algorithm for selecting the best documents to judge to have maximum confidence in an evaluation with a minimal number of relevance judgments a study with annotators shows how the algorithm can be used by a small group of researchers to quickly rank a set of systems with confidence 
dynamic test collections dynamic test collections existing methods for measuring the quality of search algorithms use a static collection of documents a set of queries and a mapping from the queries to the relevant documents allow the experimenter to see how well different search engines or engine configurations retrieve the correct answers this methodology assumes that the document set and thus the set of relevant documents are unchanging in this paper we abandon the static collection requirement we begin with a recent trec collection created from a web crawl and analyze how the documents in that collection have changed over time we determine how decay of the document collection affects trec systems and present the results of an experiment using the decayed collection to measure a live web search system we employ novel measures of search effectiveness that are robust despite incomplete relevance information lastly we propose a methodology of collection maintenance which supports measuring search performance both for a single system and between systems run at different points in time 
finding nearduplicate web pages finding nearduplicate web pages broder et als shingling algorithm and charikars random projection based approach are considered stateoftheart algorithms for finding nearduplicate web pages both algorithms were either developed at or used by popular web search engines we compare the two algorithms on a very large scale namely on a set of b distinct web pages the results show that neither of the algorithms works well for finding nearduplicate pairs on the same site while both achieve high precision for nearduplicate pairs on different sites since charikars algorithm finds more nearduplicate pairs on different sites it achieves a better precision overall namely versus for broder et als algorithm we present a combined algorithm which achieves precision with of the recall of the other algorithms 
structuredriven crawler generation by example structuredriven crawler generation by example many web ir and digital library applications require a crawling process to collect pages with the ultimate goal of taking advantage of useful information available on web sites for some of these applications the criteria to determine when a page is to be present in a collection are related to the page content however there are situations in which the inner structure of the pages provides a better criteria to guide the crawling process than their content in this paper we present a structuredriven approach for generating web crawlers that requires a minimum effort from users the idea is to take as input a sample page and an entry point to a web site and generate a structuredriven crawler based on navigation patterns sequences of patterns for the links a crawler has to follow to reach the pages structurally similar to the sample page in the experiments we have carried out structuredriven crawlers generated by our new approach were able to collect all pages that match the samples given including those pages added after their generation 
building implicit links from content for forum search building implicit links from content for forum search the objective of web forums is to create a shared space for open communications and discussions of specific topics and issues the tremendous information behind forum sites is not fullyutilized yet most links between forum pages are automatically created which means the linkbased ranking algorithm cannot be applied efficiently in this paper we proposed a novel ranking algorithm which tries to introduce the content information into linkbased methods as implicit links the basic idea is derived from the more focused random surfer the surfer may more likely jump to a page which is similar to what he is reading currently in this manner we are allowed to introduce the content similarities into the link graph as a personalization bias our method named finegrained rank fgrank can be efficiently computed based on an automatically generated topic hierarchy not like the topicsensitive pagerank our method only need to compute single pagerank score for each page another contribution of this paper is to present a very efficient algorithm for automatically generating topic hierarchy and map each page in a largescale collection onto the computed hierarchy the experimental results show that the proposed method can improve retrieval performance and reveal that contentbased link graph is also important compared with the hyperlink graph 
generalizing pagerank generalizing pagerank this paper introduces a family of linkbased ranking algorithms that propagate page importance through links in these algorithms there is a damping function that decreases with distance so a direct link implies more endorsement than a link through a long path pagerank is the most widely known ranking function of this familythe main objective of this paper is to determine whether this family of ranking techniques has some interest per se and how different choices for the damping function impact on rank quality and on convergence speed even though our results suggest that pagerank can be approximated with other simpler forms of rankings that may be computed more efficiently our focus is of more speculative nature in that it aims at separating the kernel of pagerank that is linkbased importance propagation from the way propagation decays over pathswe focus on three damping functions having linear exponential and hyperbolic decay on the lengths of the paths the exponential decay corresponds to pagerank and the other functions are new our presentation includes algorithms analysis comparisons and experiments that study their behavior under different parameters in real web graph dataamong other results we show how to calculate a linear approximation that induces a page ordering that is almost identical to pageranks using a fixed small number of iterations comparisons were performed using kendalls on large domain datasets 
capturing collection size for distributed noncooperative retrieval capturing collection size for distributed noncooperative retrieval modern distributed information retrieval techniques require accurate knowledge of collection size in noncooperative environments where detailed collection statistics are not available the size of the underlying collections must be estimated while several approaches for the estimation of collection size have been proposed their accuracy has not been thoroughly evaluated an empirical analysis of past estimation approaches across a variety of collections demonstrates that their prediction accuracy is low motivated by ecological techniques for the estimation of animal populations we propose two new approaches for the estimation of collection size we show that our approaches are significantly more accurate that previous methods and are more efficient in use of resources required to perform the estimation 
probabilistic latent query analysis for combining multiple retrieval sources probabilistic latent query analysis for combining multiple retrieval sources combining the output from multiple retrieval sources over the same document collection is of great importance to a number of retrieval tasks such as multimedia retrieval web retrieval and metasearch to merge retrieval sources adaptively according to query topics we propose a series of new approaches called probabilistic latent query analysis plqa which can associate nonidentical combination weights with latent classes underlying the query space compared with previous query independent and queryclass based combination methods the proposed approaches have the advantage of being able to discover latent query classes automatically without using prior human knowledge to assign one query to a mixture of query classes and to determine the number of query classes under a model selection principle experimental results on two retrieval tasks ie multimedia retrieval and metasearch demonstrate that the proposed methods can uncover sensible latent classes from training data and can achieve considerable performance gains 
user modeling for fulltext federated search in peertopeer networks user modeling for fulltext federated search in peertopeer networks user modeling for information retrieval has mostly been studied to improve the effectiveness of information access in centralized repositories in this paper we explore user modeling in the context of fulltext federated search in peertopeer networks our approach models a users persistent longterm interests based on past queries and uses the model to improve search efficiency for future queries that represent interests similar to past queries our approach also enables queries representing a users transient adhoc interests to be automatically recognized so that search for these queries can rely on a relatively large search radius to avoid sacrificing effectiveness for efficiency experimental results demonstrate that our approach can significantly improve the efficiency of fulltext federated search without degrading its accuracy furthermore the proposed approach does not require a large amount of training data and is robust to a range of parameter values 
distributed query sampling distributed query sampling we present an adaptive distributed querysampling framework that is qualityconscious for extracting highquality text database samples the framework divides the querybased sampling process into an initial seed sampling phase and a qualityaware iterative sampling phase in the second phase the sampling process is dynamically scheduled based on estimated database size and quality parameters derived during the previous sampling process the unique characteristic of our adaptive querybased sampling framework is its selflearning and selfconfiguring ability based on the overall quality of all text databases under consideration we introduce three qualityconscious sampling schemes for estimating database quality and our initial results show that the proposed framework supports higherquality document sampling than existing approaches 
load balancing for termdistributed parallel retrieval load balancing for termdistributed parallel retrieval largescale web and text retrieval systems deal with amounts of data that greatly exceed the capacity of any single machine to handle the necessary data volumes and query throughput rates parallel systems are used in which the document and index data are split across tightlyclustered distributed computing systems the index data can be distributed either by document or by term in this paper we examine methods for load balancing in termdistributed parallel architectures and propose a suite of techniques for reducing net querying costs in combination the techniques we describe allow a improvement in query throughput when tested on an eightnode parallel computer system 
hybrid index maintenance for growing text collections hybrid index maintenance for growing text collections we present a new family of hybrid index maintenance strategies to be used in online index construction for monotonically growing text collections these new strategies improve upon recent results for hybrid index maintenance in dynamic text retrieval systems like previous techniques our new method distinguishes between short and long posting lists while short lists are maintained using a merge strategy long lists are kept separate and are updated inplace this way costly relocations of long posting lists are avoidedwe discuss the shortcomings of previous hybrid methods and give an experimental evaluation of the new technique showing that its index maintenance performance is superior to that of the earlier methods especially when the amount of main memory available to the indexing system is small we also present a complexity analysis which proves that under a zipfian term distribution the asymptotical number of disk accesses performed by the best hybrid maintenance strategy is linear in the size of the text collection implying the asymptotical optimality of the proposed strategy 
type less find more type less find more we consider the following fulltext search autocompletion feature imagine a user of a search engine typing a query then with every letter being typed we would like an instant display of completions of the last query word which would lead to good hits at the same time the best hits for any of these completions should be displayed known indexing data structures that apply to this problem either incur large processing times for a substantial class of queries or they use a lot of space we present a new indexing data structure that uses no more space than a stateoftheart compressed inverted index but with times faster query processing times even on the large trec terabyte collection which comprises over million documents we achieve on a single machine and with the index on disk average response times of one tenth of a second we have built a fullfledged interactive search engine that realizes the proposed autocompletion feature combined with support for proximity search semistructured xml text subword and phrase completion and semantic tags 
pruned query evaluation using precomputed impacts pruned query evaluation using precomputed impacts exhaustive evaluation of ranked queries can be expensive particularly when only a small subset of the overall ranking is required or when queries contain common terms this concern gives rise to techniques for dynamic query pruning that is methods for eliminating redundant parts of the usual exhaustive evaluation yet still generating a demonstrably good enough set of answers to the query in this work we propose new pruning methods that make use of impactsorted indexes compared to exhaustive evaluation the new methods reduce the amount of computation performed reduce the amount of memory required for accumulators reduce the amount of data transferred from disk and at the same time allow performance guarantees in terms of precision and mean average precision these strong claims are backed by experiments using the trec terabyte collection and queries 
mining dependency relations for query expansion in passage retrieval mining dependency relations for query expansion in passage retrieval classical query expansion techniques such as the local context analysis lca make use of term cooccurrence statistics to incorporate additional contextual terms for enhancing passage retrieval however relevant contextual terms do not always cooccur frequently with the query terms and vice versa hence the use of such methods often brings in noise which leads to reduced precision previous studies have demonstrated the importance of relationship analysis for natural language queries in passage retrieval however they found that without query expansion the performance is not satisfactory for short queries in this paper we present two novel query expansion techniques that make use of dependency relation analysis to extract contextual terms and relations from external corpuses the techniques are used to enhance the performance of density based and relation based passage retrieval frameworks respectively we compare the performance of the resulting systems with lca in a density based passage retrieval system dbs and a relation based system without any query expansion rbs using the factoid questions from the trec qa task the results show that in terms of mrr scores our relation based term expansion method with dbs outperforms the lca by while our relation expansion method outperforms rbs by 
what makes a query difficult what makes a query difficult this work tries to answer the question of what makes a query difficult it addresses a novel model that captures the main components of a topic and the relationship between those components and topic difficulty the three components of a topic are the textual expression describing the information need the query or queries the set of documents relevant to the topic the qrels and the entire collection of documents we show experimentally that topic difficulty strongly depends on the distances between these components in the absence of knowledge about one of the model components the model is still useful by approximating the missing component based on the other components we demonstrate the applicability of the difficulty model for several uses such as predicting query difficulty predicting the number of topic aspects expected to be covered by the search results and analyzing the findability of a specific domain 
on ranking the effectiveness of searches on ranking the effectiveness of searches there is a growing interest in estimating the effectiveness of search two approaches are typically considered examining the search queries and examining the retrieved document sets in this paper we take the latter approach we use four measures to characterize the retrieved document sets and estimate the quality of search these measures are i the clustering tendency as measured by the coxlewis statistic ii the sensitivity to document perturbation iii the sensitivity to query perturbation and iv the local intrinsic dimensionality we present experimental results for the task of ranking queries according to the search effectiveness over the trec discs and dataset our ranking of queries is compared with the ranking based on the average precision using the kendall t statistic the best individual estimator is the sensitivity to document perturbation and yields kendall t of when combined with the clustering tendency based on the coxlewis statistic and the query perturbation measure it results in kendall t of which to our knowledge is the highest correlation with the average precision reported to date 
document clustering with prior knowledge document clustering with prior knowledge document clustering is an important tool for text analysis and is used in many different applications we propose to incorporate prior knowledge of cluster membership for document cluster analysis and develop a novel semisupervised document clustering model the method models a set of documents with weighted graph in which each document is represented as a vertex and each edge connecting a pair of vertices is weighted with the similarity value of the two corresponding documents the prior knowledge indicates pairs of documents that known to belong to the same cluster then the prior knowledge is transformed into a set of constraints the document clustering task is accomplished by finding the best cuts of the graph under the constraints we apply the model to the normalized cut method to demonstrate the idea and concept our experimental evaluations show that the proposed document clustering model reveals remarkable performance improvements with very limited training samples and hence is a very effective semisupervised classification tool 
text clustering with extended user feedback text clustering with extended user feedback text clustering is most commonly treated as a fully automated task without user feedback however a variety of researchers have explored mixedinitiative clustering methods which allow a user to interact with and advise the clustering algorithm this mixedinitiative approach is especially attractive for text clustering tasks where the user is trying to organize a corpus of documents into clusters for some particular purpose eg clustering their email into folders that reflect various activities in which they are involved this paper introduces a new approach to mixedinitiative clustering that handles several natural types of user feedback we first introduce a new probabilistic generative model for text clustering the speclustering model and show that it outperforms the commonly used mixture of multinomials clustering model even when used in fully autonomous mode with no user input we then describe how to incorporate four distinct types of user feedback into the clustering algorithm and provide experimental evidence showing substantial improvements in text clustering when this user feedback is incorporated 
nearduplicate detection by instancelevel constrained clustering nearduplicate detection by instancelevel constrained clustering for the task of nearduplicated document detection both traditional fingerprinting techniques used in database community and bagofword comparison approaches used in information retrieval community are not sufficiently accurate this is due to the fact that the characteristics of nearduplicated documents are different from that of both almostidentical documents in the data cleaning task and relevant documents in the search task this paper presents an instancelevel constrained clustering approach for nearduplicate detection the framework incorporates information such as document attributes and content structure into the clustering process to form nearduplicate clusters gathered from several collections of public comments sent to us government agencies on proposed new regulations the experimental results demonstrate that our approach outperforms other nearduplicate detection algorithms and as about as effective as human assessors 
less is more less is more traditionally information retrieval systems aim to maximize thenumber of relevant documents returned to a user within some windowof the top for that goal the probability ranking principle whichranks documents in decreasing order of probability of relevance isprovably optimal however there are many scenarios in which thatranking does not optimize for the users information need oneexample is when the user would be satisfied with some limitednumber of relevant documents rather than needing all relevantdocuments we show that in such a scenario an attempt to returnmany relevant documents can actually reduce the chances of findingany relevant documents
high accuracy retrieval with multiple nested ranker high accuracy retrieval with multiple nested ranker high precision at the top ranks has become a new focus of research in information retrieval this paper presents the multiple nested ranker approach that improves the accuracy at the top ranks by iteratively reranking the top scoring documents at each iteration this approach uses the ranknet learning algorithm to rerank a subset of the results this splits the problem into smaller and easier tasks and generates a new distribution of the results to be learned by the algorithm we evaluate this approach using different settings on a data set labeled with several degrees of relevance we use the normalized discounted cumulative gain ndcg to measure the performance because it depends not only on the position but also on the relevance score of the document in the ranked list our experiments show that making the learning algorithm concentrate on the top scoring results improves precision at the top ten documents in terms of the ndcg score 
semantic search via xml fragments semantic search via xml fragments in some ir applications it is desirable to adopt a high precision search strategy to return a small set of documents that are highly focused and relevant to the users information need with these applications in mind we investigate semantic search using the xml fragments query language on text corpora automatically preprocessed to encode semantic information useful for retrieval we identify three xml fragment operations that can be applied to a query to conceptualize restrict or relate terms in the query we demonstrate how these operations can be used to address four different querytime semantic needs to specify target information type to disambiguate keywords to specify search term context or to relate select terms in the query we demonstrate the effectiveness of our semantic search technology through a series of experiments using the two applications in which we embed this technology and show that it yields significant improvement in precision in the search results 
elicitation of term relevance feedback elicitation of term relevance feedback term relevance feedback has had a long history in information retrieval however research on interactive term relevance feedback has yielded mixed results in this paper we investigate several aspects related to the elicitation of term relevance feedback the display of document surrogates the technique for identifying or selecting terms and sources of expansion terms we conduct a between subjects experiment n of three term relevance feedback interfaces using the trec hard collection and evaluate each interface with respect to query length and retrieval performance results demonstrate that queries created with each experimental interface significantly outperformed corresponding baseline queries even though there were no differences in performance between interface conditions results also demonstrate that pseudorelevance feedback runs outperformed both baseline and experimental runs as assessed by recalloriented measures but that usergenerated terms improved precision 
findsimilar findsimilar search systems have for some time provided users with the ability to request documents similar to a given document interfaces provide this feature via a link or button for each document in the search results we call this feature findsimilar or similarity browsing we examined findsimilar as a search tool like relevance feedback for improving retrieval performance our investigation focused on findsimilars documenttodocument similarity the reexamination of documents during a search and the users browsing pattern findsimilar with a querybiased similarity avoiding the reexamination of documents and a breadthlike browsing pattern achieved a increase in the arithmetic mean average precision and a increase in the geometric mean average precision over our baseline retrieval this performance matched that of a more traditionally styled iterative relevance feedback technique 
exploring the limits of singleiteration clarification dialogs exploring the limits of singleiteration clarification dialogs singleiteration clarification dialogs as implemented in the trec hard track represent an attempt to introduce interaction into ad hoc retrieval while preserving the many benefits of largescale evaluations although previous experiments have not conclusively demonstrated performance gains resulting from such interactions it is unclear whether these findings speak to the nature of clarification dialogs or simply the limitations of current systems to probe the limits of such interactions we employed a human intermediary to formulate clarification questions and exploit user responses in addition to establishing a plausible upper bound on performance we were also able to induce an ontology of clarifications to characterize human behavior this ontology in turn serves as the input to a regression model that attempts to determine which types of clarification questions are most helpful our work can serve to inform the design of interactive systems that initiate user dialogs 
large scale semisupervised linear svms large scale semisupervised linear svms large scale learning is often realistic only in a semisupervised setting where a small set of labeled examples is available together with a large collection of unlabeled data in many information retrieval and data mining applications linear classifiers are strongly preferred because of their ease of implementation interpretability and empirical performance in this work we present a family of semisupervised linear support vector classifiers that are designed to handle partiallylabeled sparse datasets with possibly very large number of examples and features at their core our algorithms employ recently developed modified finite newton techniques our contributions in this paper are as follows a we provide an implementation of transductive svm tsvm that is significantly more efficient and scalable than currently used dual techniques for linear classification problems involving large sparse datasets b we propose a variant of tsvm that involves multiple switching of labels experimental results show that this variant provides an order of magnitude further improvement in training efficiency c we present a new algorithm for semisupervised learning based on a deterministic annealing da approach this algorithm alleviates the problem of local minimum in the tsvm optimization procedure while also being computationally attractive we conduct an empirical study on several document classification tasks which confirms the value of our methods in large scale semisupervised settings 
graphbased text classification graphbased text classification automatic classification of data items based on training samples can be boosted by considering the neighborhood of data items in a graph structure eg neighboring documents in a hyperlink environment or coauthors and their publications for bibliographic data entries this paper presents a new method for graphbased classification with particular emphasis on hyperlinked text documents but broader applicability our approach is based on iterative relaxation labeling and can be combined with either bayesian or svm classifiers on the feature spaces of the given data items the graph neighborhood is taken into consideration to exploit locality patterns while at the same time avoiding overfitting in contrast to prior work along these lines our approach employs a number of novel techniques dynamically inferring the linkclass pattern in the graph in the run of the iterative relaxation labeling judicious pruning of edges from the neighborhood graph based on node dissimilarities and node degrees weighting the influence of edges based on a distance metric between the classification labels of interest and weighting edges by content similarity measures our techniques considerably improve the robustness and accuracy of the classification outcome as shown in systematic experimental comparisons with previously published methods on three different realworld datasets 
constructing informative prior distributions from domain knowledge in text classification constructing informative prior distributions from domain knowledge in text classification supervised learning approaches to text classification are in practice often required to work with small and unsystematically collected training sets the alternative to supervised learning is usually viewed to be building classifiers by hand using a domain experts understanding of which features of the text are related to the class of interest this is expensive requires a degree of sophistication about linguistics and classification and makes it difficult to use combinations of weak predictors we propose instead combining domain knowledge with training examples in a bayesian framework domain knowledge is used to specify a prior distribution for the parameters of a logistic regression model and labeled training data is used to produce a posterior distribution whose mode we take as the final classifier we show on three text categorization data sets that this approach can rescue what would otherwise be disastrously bad training situations producing much more effective classifiers 
unifying userbased and itembased collaborative filtering approaches by similarity fusion unifying userbased and itembased collaborative filtering approaches by similarity fusion memorybased methods for collaborative filtering predict new ratings by averaging weighted ratings between respectively pairs of similar users or items in practice a large number of ratings from similar users or similar items are not available due to the sparsity inherent to rating data consequently prediction quality can be poor this paper reformulates the memorybased collaborative filtering problem in a generative probabilistic framework treating individual useritem ratings as predictors of missing ratings the final rating is estimated by fusing predictions from three sources predictions based on ratings of the same item by other users predictions based on different item ratings made by the same user and third ratings predicted based on data from other but similar users rating other but similar items existing userbased and itembased approaches correspond to the two simple cases of our framework the complete model is however more robust to data sparsity because the different types of ratings are used in concert while additional ratings from similar users towards similar items are employed as a background model to smooth the predictions experiments demonstrate that the proposed methods are indeed more robust against data sparsity and give better recommendations 
personalized recommendation driven by information flow personalized recommendation driven by information flow we propose that the information access behavior of a group of people can be modeled as an information flow issue in which people intentionally or unintentionally influence and inspire each other thus creating an interest in retrieving or getting a specific kind of information or product information flow models how information is propagated in a social network it can be a real social network where interactions between people reside it can be moreover a virtual social network in that people only influence each other unintentionally for instance through collaborative filtering we leverage users access patterns to model information flow and generate effective personalized recommendations first an early adoption based information flow eabif network describes the influential relationships between people second based on the fact that adoption is typically category specific we propose a topicsensitive eabif teabif network in which access patterns are clustered with respect to the categories once an item has been accessed by early adopters personalized recommendations are achieved by estimating whom the information will be propagated to with high probabilities in our experiments with an online document recommendation system the results demonstrate that the eabif and the teabif can respectively achieve an improved precision recall of and compared to traditional collaborative filtering given an early adopter exists 
analysis of a lowdimensional linear model under recommendation attacks analysis of a lowdimensional linear model under recommendation attacks collaborative filtering techniques have become popular in the past decade as an effective way to help people deal with information overload recent research has identified significant vulnerabilities in collaborative filtering techniques shilling attacks in which attackers introduce biased ratings to influence recommendation systems have been shown to be effective against memorybased collaborative filtering algorithms we examine the effectiveness of two popular shilling attacks the random attack and the average attack on a modelbased algorithm that uses singular value decomposition svd to learn a lowdimensional linear model our results show that the svdbased algorithm is much more resistant to shilling attacks than memorybased algorithms furthermore we develop an attack detection method directly built on the svdbased algorithm and show that this method detects random shilling attacks with high detection rates and very low false alarm rates 
evaluating evaluation metrics based on the bootstrap evaluating evaluation metrics based on the bootstrap this paper describes how the bootstrap approach to statistics can be applied to the evaluation of ir effectiveness metrics first we argue that bootstrap hypothesis tests deserve more attention from the ir community as they are based on fewer assumptions than traditional statistical significance tests we then describe straightforward methods for comparing the sensitivity of ir metrics based on bootstrap hypothesis tests unlike the heuristicsbased swap method proposed by voorhees and buckley our method estimates the performance difference required to achieve a given significance level directly from bootstrap hypothesis test results in addition we describe a simple way of examining the accuracy of rank correlation between two metrics based on the bootstrap estimate of standard error we demonstrate the usefulness of our methods using test collections and runs from the ntcir clir track for comparing seven ir metrics including those that can handle graded relevance and those based on the geometric mean 
statistical precision of information retrieval evaluation statistical precision of information retrieval evaluation we introduce and validate bootstrap techniques to compute confidence intervals that quantify the effect of testcollection variability on average precision ap and mean average precision map ir effectiveness measures we consider the test collection in ir evaluation to be a representative of a population of materially similar collections whose documents are drawn from an infinite pool with similar characteristics our model accurately predicts the degree of concordance between system results on randomly selected halves of the trec ad hoc corpus we advance a framework for statistical evaluation that uses the same general framework to model other sources of chance variation as a source of input for metaanalysis techniques 
a statistical method for system evaluation using incomplete judgments a statistical method for system evaluation using incomplete judgments we consider the problem of largescale retrieval evaluation and we propose a statistical method for evaluating retrieval systems using incomplete judgments unlike existing techniques that rely on effectively complete and thus prohibitively expensive relevance judgment sets produce biased estimates of standard performance measures or produce estimates of nonstandard measures thought to be correlated with these standard measures our proposed statistical technique produces unbiased estimates of the standard measures themselvesour proposed technique is based on random sampling while our estimates are unbiased by statistical design their variance is dependent on the sampling distribution employed as such we derive a sampling distribution likely to yield low variance estimates we test our proposed technique using benchmark trec data demonstrating that a sampling pool derived from a set of runs can be used to efficiently and effectively evaluate those runs we further show that these sampling pools generalize well to unseen runs our experiments indicate that highly accurate estimates of standard performance measures can be obtained using a number of relevance judgments as small as of the typical trecstyle judgment pool 
learning to advertise learning to advertise contenttargeted advertising the task of automatically associating ads to a web page constitutes a key web monetization strategy nowadays further it introduces new challenging technical problems and raises interesting questions for instance how to design ranking functions able to satisfy conflicting goals such as selecting advertisements ads that are relevant to the users and suitable and profitable to the publishers and advertisers in this paper we propose a new framework for associating ads with web pages based on genetic programming gp our gp method aims at learning functions that select the most appropriate ads given the contents of a web page these ranking functions are designed to optimize overall precision and minimize the number of misplacements by using a real ad collection and web pages from a newspaper we obtained a gain over a stateoftheart baseline method of in average precision further by evolving individuals to provide good ranking estimations gp was able to discover ranking functions that are very effective in placing ads in web pages while avoiding irrelevant ones 
getting work done on the web getting work done on the web many searches on the web have a transactional intent we argue that pages satisfying transactional needs can be distinguished from the more common pages that have some information and links but cannot be used to execute a transaction based on this hypothesis we provide a recipe for constructing a transaction annotator by constructing an annotator with one corpus and then demonstrating its classification performance on anotherwe establish its robustness finally we show experimentally that a search procedure that exploits such preannotation greatly outperforms traditional search for retrieving transactional pages 
you are what you say you are what you say in todays datarich networked world people express many aspects of their lives online it is common to segregate different aspects in different places you might write opinionated rants about movies in your blog under a pseudonym while participating in a forum or web site for scholarly discussion of medical ethics under your real name however it may be possible to link these separate identities because the movies journal articles or authors you mention are from a sparse relation space whose properties eg many items related to by only a few users allow reidentification this reidentification violates peoples intentions to separate aspects of their life and can have negative consequences it also may allow other privacy violations such as obtaining a stronger identifier like name and addressthis paper examines this general problem in a specific setting reidentification of users from a public web movie forum in a private movie ratings dataset we present three major results first we develop algorithms that can reidentify a large proportion of public users in a sparse relation space second we evaluate whether private dataset owners can protect user privacy by hiding data we show that this requires extensive and undesirable changes to the dataset making it impractical third we evaluate two methods for users in a public forum to protect their own privacy suppression and misdirection suppression doesnt work here either however we show that a simple misdirection strategy works well mention a few popular items that you havent rated 
a compositional context sensitive multidocument summarizer a compositional context sensitive multidocument summarizer the usual approach for automatic summarization is sentence extraction where key sentences from the input documents are selected based on a suite of features while word frequency often is used as a feature in summarization its impact on system performance has not been isolated in this paper we study the contribution to summarization of three factors related to frequency content word frequency composition functions for estimating sentence importance from word frequency and adjustment of frequency weights based on context we carry out our analysis using datasets from the document understanding conferences studying not only the impact of these features on automatic summarizers but also their role in human summarization our research shows that a frequency based summarizer can achieve performance comparable to that of stateoftheart systems but only with a good composition function context sensitivity improves performance and significantly reduces repetition 
information graphics information graphics information graphics are nonpictorial graphics such as bar charts and line graphs that depict attributes of entities and relations among entities most information graphics appearing in popular media have a communicative goal or intended message consequently information graphics constitute a form of language this paper argues that information graphics are a valuable knowledge resource that should be retrievable from a digital library and that such graphics should be taken into account when summarizing a multimodal document for subsequent indexing and retrieval but to accomplish this the information graphic must be understood and its message recognized the paper presents our bayesian system for recognizing the primary message of one kind of information graphic simple bar charts and discusses the potential role of an information graphics message in indexing graphics and summarizing multimodal documents 
news to go news to go we present an evaluation of a novel hierarchical text summarization method that allows users to view summaries of web documents from small mobile devices unlike previous approaches ours does not require the documents to be in html since it infers a hierarchical structure automatically currently the method is used to summarize news articles sent to a web mail account in plain text format subjects used a webenabled mobile phone emulator to access the accounts inbox and view the summarized news articles they then used the summaries to complete several informationseeking tasks which involved answering factual questions about the stories in comparing the hierarchical text summary setting to that in which subjects were given the full text articles there was no significant difference in task accuracy or the time taken to complete the task however in the hierarchical summarization setting the number of bytes transferred per user request is less than half that of the full text case finally in comparing the new method to three other summarization methods subjects achieved significantly better accuracy on the tasks when using hierarchical summaries 
clustering of search results using temporal attributes clustering of search results using temporal attributes clustering of search results is an important feature in many of todays information retrieval applications the notion of hit list clustering appears in web search engines and enterprise search engines as a mechanism that allows users to further explore the coverage of a query however there has been little work on exposing temporal attributes for constructing and presentation of clusters these attributes appear in documents as part of the textual content eg as a date and time token or as a temporal reference in a sentence in this paper we outline a model and describe a prototype that shows the main ideas 
a complex document information processing prototype a complex document information processing prototype we developed a prototype for integrated retrieval and aggregation of diverse information contained in scanned paper documents such complex document information processing combines several forms of image processing together with textuallinguistic processing to enable effective analysis of complex document collections a necessity for a wide range of applications this is the first system to attempt integrated retrieval from complex documents we report its current capabilities 
inferring document relevance via average precision inferring document relevance via average precision we consider the problem of evaluating retrieval systems using a limited number of relevance judgments recent work has demonstrated that one can accurately estimate average precision via a judged pool corresponding to a relatively small random sample of documents in this work we demonstrate that given values or estimates of average precision one can accurately infer the relevances of unjudged documents combined we thus show how one can efficiently and accurately infer a large judged pool from a relatively small number of judged documents thus permitting accurate and efficient retrieval evaluation on a large scale 
automatic construction of knownitem finding test beds automatic construction of knownitem finding test beds note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
adaptive querybased sampling for distributed ir adaptive querybased sampling for distributed ir note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
peng peng note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
examining assessor attributes at hard examining assessor attributes at hard note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
user expectations from xml element retrieval user expectations from xml element retrieval the primary aim of xml element retrieval is to return to users xml elements rather than whole documents this poster describes a small study in which we elicited users expectations ie their anticipated experience when interacting with an xml retrieval system as compared to a traditional flat document retrieval system 
theoretical benchmarks of xml retrieval theoretical benchmarks of xml retrieval this poster investigates the use of theoretical benchmarks to describe the matching functions of xml retrieval systems and the properties of specificity and exhaustivity in xml retrieval theoretical benchmarks concern the formal representation of qualitative properties of ir models to this end situation theory framework for the metaevaluation of xml retrieval is presented 
question classification with loglinear models question classification with loglinear models question classification has become a crucial step in modern question answering systems previous work has demonstrated the effectiveness of statistical machine learning approaches to this problem this paper presents a new approach to building a question classifier using loglinear models evidence from a rich and diverse set of syntactic and semantic features is evaluated as well as approaches which exploit the hierarchical structure of the question classes 
communitybased snippetindexes for pseudoanonymous personalization in web search communitybased snippetindexes for pseudoanonymous personalization in web search we describe and evaluate an approach to personalizing web search that involves postprocessing the results returned by some underlying search engine so that they re ect the interests of a community of likeminded searchersto do this we leverage the search experiences of the community by mining the title and snippet texts of results that have been selected by community members in response to their queries our approach seeks to build a communitybased snippet index that re ects the evolving interests of a group of searchers this index is then sed to rerank the results returned by the underlying search engine by boosting the ranking of key results that have been freq ently selected for similar q eries by community members in the past 
bias and the limits of pooling bias and the limits of pooling modern retrieval test collections are built through a process called pooling in which only a sample of the entire document set is judged for each topic the idea behind pooling is to find enough relevant documents such that when unjudged documents are assumed to be nonrelevant the resulting judgment set is sufficiently complete and unbiased as document sets grow larger a constantsize pool represents an increasingly small percentage of the document set and at some point the assumption of approximately complete judgments must become invalidthis paper demonstrates that the aquaint test collection exhibits bias caused by pools that were too shallow for the document set size despite having many diverse runs contribute to the pools the existing judgment set favors relevant documents that contain topic title words even though relevant documents containing few topic title words are known to exist in the document set the paper concludes with suggested modifications to traditional pooling and evaluation methodology that may allow very large reusable test collections to be built 
term proximity scoring for adhoc retrieval on very large text collections term proximity scoring for adhoc retrieval on very large text collections we propose an integration of term proximity scoring into okapi bm the relative retrieval effectiveness of our retrieval method compared to pure bm varies from collection to collectionwe present an experimental evaluation of our method and show that the gains achieved over bm as the size of the underlying text collection increases we also show that for stemmed queries the impact of term proximity scoring is larger than for unstemmed queries 
an exploratory web log study of multitasking an exploratory web log study of multitasking the web search multitasking study based on automatic task session detection procedure is described the results of the study multitasking is very rare it usually covers only task sessions it is frequently formed into a temporal inclusion of an interrupting task session into the interrupted session the quantitative characteristics of multitasking greatly differ from the characteristics of sequential execution of one and several tasks a searcher minimizes task switching costs he avoids multitasking and while multitasking he uses cheapest manner of task switching 
tensor space model for document analysis tensor space model for document analysis vector space model vsm has been at the core of information retrieval for the past decades vsm considers the documents as vectors in high dimensional spacein such a vector space techniques like latent semantic indexing lsi support vector machines svm naive bayes etc can be then applied for indexing and classification however in some cases the dimensionality of the document space might be extremely large which makes these techniques infeasible due to the curse of dimensionality in this paper we propose a novel tensor space model for document analysis we represent documents as the second order tensors or matrices correspondingly a novel indexing algorithm called tensor latent semantic indexing tensorlsi is developed in the tensor space our theoretical analysis shows that tensorlsi is much more computationally efficient than the conventional latent semantic indexing which makes it applicable for extremely large scale data set several experimental results on standard document data sets demonstrate the efficiency and effectiveness of our algorithm 
first largescale information retrieval experiments on turkish texts first largescale information retrieval experiments on turkish texts we present the results of the first largescale turkish information retrieval experiments performed on a treclike test collection the test bed which has been created for this study contains million words documents ad hoc queries and has a size of about mb all documents come from the turkish newspaper milliyet we implement and apply simple to sophisticated stemmers and various querydocument matching functions and show that truncating words at a prefix length of creates an effective retrieval environment in turkish however a lemmatizerbased stemmer provides significantly better effectiveness over a variety of matching functions 
learning a ranking from pairwise preferences learning a ranking from pairwise preferences we introduce a novel approach to combining rankings from multiple retrieval systems we use a logistic regression model or an svm to learn a ranking from pairwise document preferences our approach requires no training data or relevance scores and outperforms a popular voting algorithm 
automated performance assessment in interactive qa automated performance assessment in interactive qa in interactive question answering qa users and systems take turns to ask questions and provide answers in such an interactive setting user questions largely depend on the answers provided by the system one question is whether user followup questions can provide feedback for the system to automatically assess its performance eg assess whether a correct answer is delivered this selfawareness can make qa systems more intelligent for information seeking for example by adapting better strategies to cope with problematic situations therefore this paper describes our initial investigation in addressing this problem our results indicate that interaction context can provide useful cues for automated performance assessment in interactive qa 
stylistic text segmentation stylistic text segmentation this paper focuses on a method for the stylistic segmentation of text documents our technique involves mapping the change in a feature throughout a text we use the linguistic features of conjunction and modality through taxonomies from systemic functional linguistics this segmentation has applications in automated summarization particularly of large documents 
on hierarchical web catalog integration with conceptual relationships in thesaurus on hierarchical web catalog integration with conceptual relationships in thesaurus web catalog integration is an interesting problem in current digital content management past studies have shown that using a flattened structure with auxiliary information extracted from the source catalog can improve the integration results however the nature of a flattened structure ignores the hierarchical relationships and thus the performance improvement of catalog integration may be reduced in this paper we propose an enhanced hierarchical catalog integration ehci approach with conceptual thesauri extracted from the source catalog the results show that our enhanced hierarchical integration approach effectively boosts the accuracy of hierarchical catalog integration 
rpref rpref we present rpref our generalization of the bpref evaluation metric for assessing the quality of search engine results given graded rather than binary user relevance judgments 
a new web page summarization method a new web page summarization method in this paper we present a novel multiwebpage summarization algorithm it adds the graph based ranking algorithm into the framework of maximum marginal relevance mmr method to not only capture the main topic of the web pages but also eliminate the redundancy existing in the sentences of the summary result the experiment result indicates that the new approach has the better performance than the previous methods 
nmf and plsi nmf and plsi in this paper we show that plsi and nmf optimize the same objective function although plsi and nmf are different algorithms as verified by experiments in addition we also propose a new hybrid method that runs plsi and nmf alternatively to achieve better solutions 
using historical data to enhance rank aggregation using historical data to enhance rank aggregation rank aggregation is a pervading operation in ir technology we hypothesize that the performance of scorebased aggregation may be affected by artificial usually meaningless deviations consistently occurring in the input score distributions which distort the combined result when the individual biases differ from each other we propose a scorebased rank aggregation model where the source scores are normalized to a common distribution before being combined early experiments on available data from several trec collections are shown to support our proposal 
enterprise search behaviour of software engineers enterprise search behaviour of software engineers technical professionals spend of their time at work searching for information and have specialized information needs that are not wellserved by generic enterprise search tools in this study we investigated how a group of software engineers use a workplace search system we identify patterns of search behaviour specific to this group and distinct from general web and intranet search patterns and make design recommendations for search systems that will better serve the needs of this group 
evaluating sources of query expansion terms evaluating sources of query expansion terms this study investigates the effectiveness of retrieval systems and human users in generating terms for query expansion we compare three sources of terms system generated terms terms users select from topranked sentences and user generated terms results demonstrate that overall the system generated more effective expansion terms than users but that users selection of terms improved precision at the top of the retrieved document list 
comparing two blind relevance feedback techniques comparing two blind relevance feedback techniques note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
information retrieval with commonsense knowledge information retrieval with commonsense knowledge this paper employs conceptnet which covers a rich set of commonsense concepts to retrieve images with text descriptions by focusing on spatial relationships evaluation on test data of the imageclef shows that integrating commonsense knowledge in information retrieval is feasible 
refining hierarchical taxonomy structure via semisupervised learning refining hierarchical taxonomy structure via semisupervised learning note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
quantative analysis of the impact of judging inconsistency on the performance of relevance feedback quantative analysis of the impact of judging inconsistency on the performance of relevance feedback practical constrains of user interfaces make the users judgment during the feedback loop deviate from real thoughts when the full document is readthis is often overlooked in evaluation of relevance feedbackthis paper quantitatively analyze the impact of judging inconsistency on the performance of relevance feedback 
swordfish swordfish extracting morphemes from words is a nontrivial task rule based stemming approaches such as porters algorithm have encountered some success however they are restricted by their ability to identify a limited number of affixes and are language dependent when dealing with languages with many affixes rule based approaches generally require many more rules to deal with all the possible word forms deriving these rules requires a larger effort on the part of linguists and in some instances can be simply impractical we propose an unsupervised ngram based approach named swordfish using ngram probabilities in the corpus possible morphemes are identified we look at two possible methods for identifying candidate morphemes one using joint probabilities between two ngrams and the second based on log odds between prefix probabilities initial results indicate the joint probability approach to be better for english while the prefix ratio approach is better for finnish and turkish 
authorship attribution with thousands of candidate authors authorship attribution with thousands of candidate authors in this paper we use a blog corpus to demonstrate that we can often identify the author of an anonymous text even where there are many thousands of candidate authors our approach combines standard information retrieval methods with a text categorization metalearning scheme that determines when to even venture a guess 
simple questions to improve pseudorelevance feedback results simple questions to improve pseudorelevance feedback results we explore interactive methods to further improve the performance of pseudorelevance feedback studies citeria suggest that new methods for tackling difficult queries are required our approach is to gather more information about the query from the user by asking her simple questions the equally simple responses are used to modify the original query our experiments using the trec robust track queries show that we can obtain a significant improvement in mean average precision averaging around over pseudorelevance feedback this improvement is also spread across more queries compared to ordinary pseudorelevance feedback as suggested by geometric mean average precision 
is xml retrieval meaningful to users is xml retrieval meaningful to users the aim of this study is to investigate whether element retrieval as opposed to fulltext retrieval is meaningful and useful for searchers when carrying out informationseeking tasks our results suggest that searchers find the structural breakdown of documents useful when browsing within retrieved documents and provide support for the usefulness of element retrieval in interactive settings 
building a test collection for complex document information processing building a test collection for complex document information processing research and development of information access technology for scanned paper documents has been hampered by the lack of public test collections of realistic scope and complexity as part of a project to create a prototype system for search and mining of masses of document images we are assembling a terabyte dataset to support evaluation of both endtoend complex document information processing cdip tasks eg text retrieval and data mining as well as component technologies such as optical character recognition ocr document structure analysis signature matching and authorship attribution 
enhancing topic tracking with temporal information enhancing topic tracking with temporal information in this paper we propose a new strategy with time granularity reasoning for utilizing temporal information in topic tracking compared with previous ones our work has four distinguished characteristics firstly we try to determine a set of topic times for a target topic from the given ontopic stories it helps to avoid the negative influence from other irrelevant times secondly we take into account time granularity variance when deciding whether a coreference relationship exists between two times thirdly both publication time and times presented in texts are considered finally as time is only one attribute of a topic we increase the similarity between a story and a target topic only when they are related not only temporally but also semantically experiments on two tdt corpora show that our method makes good use of temporal information in news stories 
a comparative study of the effect of search feature design on user experience in digital libraries dls a comparative study of the effect of search feature design on user experience in digital libraries dls this study investigates the impact of different search feature designs in dls on user search experience the results indicate that the impact is significant in terms of the number of queries issued search steps zerohits pages returned and search errors 
representing clusters for retrieval representing clusters for retrieval note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
onesided measures for evaluating ranked retrieval effectiveness with spontaneous conversational speech onesided measures for evaluating ranked retrieval effectiveness with spontaneous conversational speech early speech retrieval experiments focused on news broadcasts for which adequate automatic speech recognition asr accuracy could be obtained like newspapers news broadcasts are a manually selected and arranged set of stories evaluation designs reflected that using known story boundaries as a basis for evaluation substantial advances in asr accuracy now make it possible to build search systems for some types of spontaneous conversational speech but present evaluation designs continue to rely on known topic boundaries that are no longer well matched to the nature of the materials we propose a new class of measures for speech retrieval based on manual annotation of points at which a user with specific topical interests would wish replay to begin 
combining fields in knownitem email search combining fields in knownitem email search emails are examples of structured documents with various fields these fields can be exploited to enhance the retrieval effectiveness of an information retrieval ir system that mailing list archives in recent experiments of the trec enterprise track various fields were applied to varying degrees of success by the participants in his work using a fieldbased weighting model we investigate the retrieval performance attainable by each field and examine when fields evidence should be combined or not 
improving qa retrieval using document priors improving qa retrieval using document priors we present a simple way to improve document retrieval for question answering systems the method biases the retrieval system toward documents that contain words that have appeared in other documents containing answers to the same type of question the method works with virtually any retrieval system and exhibits a statistically significant performance improvement over a strong baseline 
contentbased video retrieval contentbased video retrieval a new shot level video browsing method based on semantic visual features eg car mountain and fire is proposed to facilitate contentbased retrieval the videos binary semantic feature vector is utilized to calculate the score of similarity between two shot keyframes the score is then used to browse the similar keyframes in terms of semantic visual features a pilot user study was conducted to better understand users behaviors in video retrieval context three video retrieval and browsing systems are compared temporal neighbor semantic visual feature and fused browsing system the initial results indicated that the semantic visual feature browsing was effective and efficient for visual centric tasks but not for nonvisual centric tasks 
action modeling action modeling we present a novel language modeling approach to capturing the query reformulation behavior of web search users based on a framework that categorizes eight different types of user moves addingremoving query terms etc we treat search sessions as sequence data and build ngram language models to capture user behavior we evaluated our models in a prediction task the results suggest that useful patterns of activity can be extracted from user histories furthermore by examining prediction performance under different order ngram models we gained insight into the amount of historycontext that is associated with different types of user actions our work serves as the basis for more refined user models 
a method of rating the credibility of news documents on the web a method of rating the credibility of news documents on the web we propose a method to rate the credibility of news articles using three clues commonality of the contents of articles among different news publishers numerical agreement versus contradiction of numerical values reported in the articles and objectivity based on subjective speculative phrases and news sources we tested this method on news stories taken from seven different news sites on the web the average agreement between the systemproduced credibility and the manual judgments of three human assessors on the sample articles was the limitations of the current approach and future directions are discussed 
an analysis of the coupling between training set and neighborhood sizes for the knn classifier an analysis of the coupling between training set and neighborhood sizes for the knn classifier we consider the relationship between training set size and the parameter k for the knearest neighbors knn classifier when few examples are available we observe that accuracy is sensitive to k and that best k tends to increase with training size we explore the subsequent risk that k tuned on partitions will be suboptimal after aggregation and retraining this risk is found to be most severe when little data is available for larger training sizes accuracy becomes increasingly stable with respect to k and the risk decreases 
factfocused novelty detection factfocused novelty detection methods for detecting sentences in an input document set which are both relevant and novel with respect to an information need would be of direct benefit to many systems such as extractive text summarizers however satisfactory levels of agreement between judges performing this task manually have yet to demonstrated leaving researchers to conclude that the task is too subjective in previous experiments judges were asked to first identify sentences that are relevant to a general topic and then to eliminate sentences from the list that do not contain new information currently a new task is proposed in which annotators perform the same procedure but within the context of a specific factual information need in the experiment satisfactory levels of agreement between independent annotators were achieved on the first step of identifying sentences containing relevant information relevant however the results indicate that judges do not agree on which sentences contain novel information 
unity unity the exponential growth of the web and the increasing ability of web search engines to index data have led to a problem of plenty the number of results returned per query is typically in the order of millions of documents for many common queries although there is the benefit of added coverage for every query the problem of ranking these documents and giving the best results gets worse the problem is even more difficult in case of temporal and ambiguous queries we try to address this problem using feedback from user query logs we leverage a technology called units for generating query refinements which are shown as also try queries on yahoo search we consider these refinements as subconcepts which help define user intent and use them to improve search relevance the results obtained via live testing on yahoo search are encouraging 
improving personalized web search using result diversification improving personalized web search using result diversification we present and evaluate methods for diversifying search results to improve personalized web search a common personalization approach involves reranking the top n search results such that documents likely to be preferred by the user are presented higher the usefulness of reranking is limited in part by the number and diversity of results considered we propose three methods to increase the diversity of the top results and evaluate the effectiveness of these methods 
using small xml elements to support relevance using small xml elements to support relevance small xml elements are often estimated relevant by the retrieval model but they are not desirable retrieval units this paper presents a generic model that exploits the information obtained from small elements we identify relationships between small and relevant elements and use this linking information to reinforce the relevance of other elements before removing the small ones our experiments using the inex testbed show the effectiveness of our approach 
give me just one highly relevant document give me just one highly relevant document we introduce an evaluation metric called pmeasure for the task of retrieving ltione highly relevant document it models user behaviour in practical tasks such as knownitem search and is more stable and sensitive than reciprocal rank which cannot handle graded relevance 
feature diversity in cluster ensembles for robust document clustering feature diversity in cluster ensembles for robust document clustering the performance of document clustering systems depends on employing optimal text representations which are not only difficult to determine beforehand but also may vary from one clustering problem to another as a first step towards building robust document clusterers a strategy based on feature diversity and cluster ensembles is presented in this work experiments conducted on a binary clustering problem show that our method is robust to nearoptimal model order selection and able to detect constructive interactions between different document representations in the test bed 
lightening the load of document smoothing for better language modeling retrieval lightening the load of document smoothing for better language modeling retrieval we hypothesized that language modeling retrieval would improve if we reduced the need for document smoothing to provide an inverse document frequency idf like effect we created inverse collection frequency icf weighted query models as a tool to partially separate the idflike role from document smoothing compared to maximum likelihood estimated mle queries the icf weighted queries achieved a improvement in mean average precision on description queries the icf weighted queries performed better with less document smoothing than that required by mle queries language modeling retrieval may benefit from a means to separately incorporate an idflike behavior outside of document smoothing 
the effect of ocr errors on stylistic text classification the effect of ocr errors on stylistic text classification recently interest is growing in nontopical text classification tasks such as genre classification sentiment analysis and authorship profiling we study to what extent ocr errors affect stylistic text classification from scanned documents we find that even a relatively high level of errors in the ocred documents does not substantially affect stylistic classification accuracy 
history repeats itself history repeats itself thanks to the ubiquity of the internet search engine search box users have come to depend on search engines both to find and refind information however refinding behavior has not been significantly addressed here we look at refinding queries issued to the yahoo search engine by users over a year 
early precision measures early precision measures we report the statistically significant mean impacts of blind feedback as implemented by participants for the reliable information access ria workshop on retrieval measures including several primary recall measures not originally reported we find that blind feedback was detrimental to measures focused on the first relevant item even when it boosted early precision measures such as mean precision implying that the conventional reporting of ad hoc precision needs enhancement 
an experimental study on automatically labeling hierarchical clusters using statistical features an experimental study on automatically labeling hierarchical clusters using statistical features note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
strict and vague interpretation of xmlretrieval queries strict and vague interpretation of xmlretrieval queries structural hints in xmlretrieval queries can be used to specify both the granularity of the search result the target element and where in a document to search support elements these hints might be interpreted either strictly or vaguely but does it matter if an xml search engine interprets these in one way and the user in another the performance of all runs submitted to inex content and structure cas tasks were measured for each of four different interpretations of cas runs that perform well for one interpretation of target elements do so regardless of the interpretation of support elements but how to interpret the target element does matter this suggests that to perform well on all cas queries it is necessary to know how the target structure specification should be interpreted we extend the nexi query language to include this and hypothesize that using this will increase the overall performance of search engines 
why structural hints in queries do not help xmlretrieval why structural hints in queries do not help xmlretrieval for many years it has been commonly held that a user who adds structural hints to a query will improve precision in an element retrieval search at inex we conducted an experiment to test this assumption we present the unexpected result that structural hints in queries do not improve precision an analysis of the topics and the judgments suggests that this is because users are particularly bad at giving structural hints 
searching the web using composed pages searching the web using composed pages note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
a study of realtime query expansion effectiveness a study of realtime query expansion effectiveness in this poster we describe the study of an interface technique that provides a list of suggested additional query terms as a searcher types a search query in effect offering interactive query expansion iqe options while the query is formulated analysis of the results shows that offering iqe during query formulation leads to better quality initial queries and an increased uptake of query expansion these findings have implications for how iqe should be offered in retrieval interfaces 
a graphbased framework for relation propagation and its application to multilabel learning a graphbased framework for relation propagation and its application to multilabel learning label propagation exploits the structure of the unlabeled documents by propagating the label information of the training documents to the unlabeled documents the limitation with the existing label propagation approaches is that they can only deal with a single type of objects we propose a framework named relation propagation that allows for information propagated among multiple types of objects empirical studies with multilabel text categorization showed that the proposed algorithm is more effective than several semisupervised learning algorithms in that it is capable of exploring the correlation among different categories and the structure of unlabeled documents simultaneously 
measuring similarity of semistructured documents with context weights measuring similarity of semistructured documents with context weights in this work we study similarity measures for textcentric xml documents based on an extended vector space model which considers both document content and structure experimental results based on a benchmark showed superior performance of the proposed measure over the baseline which ignores structural knowledge of xml documents 
incorporating query difference for learning retrieval functions in information retrieval incorporating query difference for learning retrieval functions in information retrieval we discuss information retrieval methods that aim at serving a diverse stream of user queries we propose methods that emphasize the importance of taking into consideration of query difference in learning effective retrieval functions we formulate the problem as a multitask learning problem using a risk minimization framework in particular we show how to calibrate the empirical risk to incorporate query difference in terms of introducing nuisance parameters in the statistical models and we also propose an alternating optimization method to simultaneously learn the retrieval function and the nuisance parameters we illustrate the effectiveness of the proposed methods using modeling data extracted from a commercial search engine 
conceptbased biomedical text retrieval conceptbased biomedical text retrieval one challenging problem for biomedical text retrieval is to find accurate synonyms or name variants for biomedical entities in this paper we propose a new conceptbased approach to tackle this problem in this approach a set of concepts instead of keywords will be extracted from a query first then these concepts will be used for retrieval purpose the experiment results show that the proposed approach can boost the retrieval performance and it generates very good results on trec genomics data sets 
the tijah xml information retrieval system the tijah xml information retrieval system note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
a location annotation system for personal photos a location annotation system for personal photos note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
appraisal navigator appraisal navigator much interesting text n the web consists largely of opinionated or evaluative text as opposed to directly informative text the new field of sentiment analysis seeks to characterize such aspects of natural language text as opposed to just the bare facts we suggest that appraisal expression extraction should be viewed as a fundamental task for sentiment analysis we define an appraisal expression to be a piece of text expressing some evaluative stance towards a particular object the task is to find these elements and characterize the type and orientation positive or negative of the evaluative stance as well as its target and possibly its source potential applications of these methods include new approaches to the nowtraditional tasks of sentiment classification and pinion mining as well as possibly for adversarial textual analysis and intention detection for intelligence applications 
a platform for okapibased contextual information retrieval a platform for okapibased contextual information retrieval we present an extensible javabased platform for contextual retrieval based on the probabilistic information retrieval model modules for dual indexes relevance feedback with blind or machine learning approaches and query expansion with context are integrated into the okapi system to deal with the contextual information this platform allows easy extension to include other types of contextual information 
project contexts to situate personal information project contexts to situate personal information the personal project planner prototype works as an extension to the file manager to provide people with richtext overlays to their information folders files and also email web pages notes richtext documentlike project plans can be created which then provide a context in which to create or reference the email messages electronic documents web pages etc that are needed to complete the plan the user can later locate an information item such as an email message with reference to the plan eg as an alternative to a mostly contextfree search through the inbox or sent mail the planner explores a possibility that an effective organization of projectrelated information can emerge as a natural byproduct of efforts to plan and structure the project 
cheshire cheshire note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
dewild dewild note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
searching for expertise using the terrier platform searching for expertise using the terrier platform note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
dilight dilight note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
supporting semantic visual feature browsing in contentbased video retrieval supporting semantic visual feature browsing in contentbased video retrieval a new shot level video retrieval system that supports semantic visual features eg car mountain and fire browsing is developed to facilitate contentbased retrieval the videos binary semantic feature vector is utilized to calculate the score of similarity between two shot keyframes the score is then used to browse the similar keyframes in terms of semantic visual features 
mathfind mathfind note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
strategy follows technology strategy follows technology in strategic management there has been a debate over many years already in alfred chandler had stated structure follows strategy in the nineteen eighties michael porter modified chandlers dictum about structure following strategy by introducing a second level of structure organizational structure follows strategy which in turn follows structure so the question became what is leading what
personalized query expansion for the web personalized query expansion for the web the inherent ambiguity of short keyword queries demands for enhanced methods for web retrieval in this paper we propose to improve such web queries by expanding them with terms collected from each users personal information repository thus implicitly personalizing the search output we introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels ranging from term and compound level analysis up to global cooccurrence statistics as well as to using external thesauri our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well especially on ambiguous queries producing a very strong increase in the quality of the output rankings subsequently we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query a separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach
using query contexts in information retrieval using query contexts in information retrieval user query is an element that specifies an information need but it is not the only one studies in literature have found many contextual factors that strongly influence the interpretation of a query recent studies have tried to consider the users interests by creating a user profile however a single profile for a user may not be sufficient for a variety of queries of the user in this study we propose to use queryspecific contexts instead of usercentric ones including context around query and context within query the former specifies the environment of a query such as the domain of interest while the latter refers to context words within the query which is particularly useful for the selection of relevant term relations in this paper both types of context are integrated in an ir model based on language modeling our experiments on several trec collections show that each of the context factors brings significant improvements in retrieval effectiveness
towards taskbased personal information management evaluations towards taskbased personal information management evaluations personal information management pim is a rapidly growing area of research concerned with how people store manage and refind information a feature of pim research is that many systems have been designed to assist users manage and refind information but very few have been evaluated this has been noted by several scholars and explained by the difficulties involved in performing pim evaluations the difficulties include that people refind information from within unique personal collections researchers know little about the tasks that cause people to refind information and numerous privacy issues concerning personal information in this paper we aim to facilitate pim evaluations by addressing each of these difficulties in the first part we present a diary study of information refinding tasks the study examines the kind of tasks that require users to refind information and produces a taxonomy of refinding tasks for email messages and web pages in the second part we propose a taskbased evaluation methodology based on our findings and examine the feasibility of the approach using two different methods of task creation
utilitybased information distillation over temporally sequenced documents utilitybased information distillation over temporally sequenced documents this paper examines a new approach to information distillation over temporally ordered documents and proposes a novel evaluation scheme for such a framework it combines the strengths of and extends beyond conventional adaptive filtering novelty detection and nonredundant passage ranking with respect to longlasting information needs tasks with multiple queries our approach supports finegrained user feedback via highlighting of arbitrary spans of text and leverages such information for utility optimization in adaptive settings for our experiments we defined hypothetical tasks based on news events in the tdt corpus with multiple queries per task answer keys nuggets were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses we also propose an extension of the ndcg metric for assessing the utility of ranked passages as a combination of relevance and novelty our results show encouraging utility enhancements using the new approach compared to the baseline systems without incremental learning or the novelty detection components
effective missing data prediction for collaborative filtering effective missing data prediction for collaborative filtering memorybased collaborative filtering algorithms have been widely adopted in many popular recommender systems although these approaches all suffer from data sparsity and poor prediction quality problems usually the useritem matrix is quite sparse which directly leads to inaccurate recommendations this paper focuses the memorybased collaborative filtering problems on two crucial factors similarity computation between users or items and missing data prediction algorithms first we use the enhanced pearson correlation coefficient pcc algorithm by adding one parameter which overcomes the potential decrease of accuracy when computing the similarity of users or items second we propose an effective missing data prediction algorithm in which information of both users and items is taken into account in this algorithm we set the similarity threshold for users and items respectively and the prediction algorithm will determine whether predicting the missing data or not we also address how to predict the missing data by employing a combination of user and item information finally empirical studies on dataset movielens have shown that our newly proposed method outperforms other stateoftheart collaborative filtering algorithms and it is more robust against data sparsity
efficient bayesian hierarchical user modeling for recommendation system efficient bayesian hierarchical user modeling for recommendation system a contentbased personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual users interest a system serving millions of users can learn a better user profile for a new user or a user with little feedback by borrowing information from other users through the use of a bayesian hierarchical model learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive the commonly used em algorithm converges very slowly due to the sparseness of the data in ir applications this paper proposes a new fast learning technique to learn a large number of individual user profiles the efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from netflix and movielens
robust test collections for retrieval evaluation robust test collections for retrieval evaluation lowcost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments while these judgments are very useful for a onetime evaluation it is not clear that they can be trusted when reused to evaluate new systems in this work we formally define what it means for judgments to be reusable the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments we then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort using this method practically guarantees reusability with as few as five judgments per topic taken from only two systems we can reliably evaluate a larger set of ten systems even the smallest sets of judgments can be useful for evaluation of new systems
reliable information retrieval evaluation with incomplete and biased judgements reliable information retrieval evaluation with incomplete and biased judgements information retrieval evaluation based on the pooling method is inherently biased against systems that did not contribute to the pool of judged documents this may distort the results obtained about the relative quality of the systems evaluated and thus lead to incorrect conclusions about the performance of a particular ranking technique
alternatives to bpref alternatives to bpref recently a number of trec tracks have adopted a retrieval effectiveness metric called bpref which has been designed for evaluation environments with incomplete relevance data a gradedrelevance version of this metric called rpref has also been proposed however we show that the application of qmeasure normalised discounted cumulative gain ndcg or average precision avepto condensed lists obtained by ltering out all unjudged documents from the original ranked lists is actually a better solution to the incompleteness problem than bpref furthermore we show that the use of graded relevance boosts the robustness of ir evaluation to incompleteness and therefore that qmeasure and ndcg based on condensed lists are the best choices to this end we use four gradedrelevance test collections from ntcir to compare ten different ir metrics in terms of system ranking stability and pairwise discriminative power
an interactive algorithm for asking and incorporating feature feedback into support vector machines an interactive algorithm for asking and incorporating feature feedback into support vector machines standard machine learning techniques typically require ample training data in the form of labeled instances in many situations it may be too tedious or costly to obtain sufficient labeled data for adequate classifier performance however in text classification humans can easily guess the relevance of features that is words that are indicative of a topic thereby enabling the classifier to focus its feature weights more appropriately in the absence of sufficient labeled data we will describe an algorithm for tandem learning that begins with a couple of labeled instances and then at each iteration recommends features and instances for a human to label tandem learning using an oracle results in much better performance than learning on only features or only instances we find that humans can emulate the oracle to an extent that results in performance accuracy comparable to that of the oracle our unique experimental design helps factor out system error from human error leading to a better understanding of when and why interactive feature selection works
learn from web search logs to organize search results learn from web search logs to organize search results effective organization of search results is critical for improving the utility of any search engine clustering search results is an effective way to organize search results which allows a user to navigate into relevant documents quickly however two deficiencies of this approach make it not always work well the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective and the cluster labels generated are not informative enough to allow a user to identify the right cluster in this paper we propose to address these two deficiencies by learning interesting aspects of a topic from web search logs and organizing search results accordingly and generating more meaningful cluster labels using past query words entered by users we evaluate our proposed method on a commercial search engine log data compared with the traditional methods of clustering search results our method can give better result organization and more meaningful labels
regularized clustering for documents regularized clustering for documents in recent years document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization automatictopic extraction and fast information retrieval or filtering in this paper we propose a novel method for clustering documents using regularization unlike traditional globally regularized clustering methods our method first construct a local regularized linear label predictor for each document vector and then combine all those local regularizers with a global smoothness regularizer so we call our algorithm clustering with local and global regularization clgr we will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix which can be efficiently solved by iterative methods finally our experimental evaluations on several datasets are presented to show the superiorities of clgr over traditional document clustering methods
towards automatic extraction of event and place semantics from flickr tags towards automatic extraction of event and place semantics from flickr tags we describe an approach for extracting semantics of tags unstructured textlabels assigned to resources on the web based on each tags usage patterns in particular we focus on the problem of extracting place and event semantics for tags that are assigned to photos on flickr a popular photo sharing website that supports time and location latitudelongitude metadata we analyze two methods inspired by wellknown burstanalysis techniques and one novel method scalestructure identification we evaluate the methods on a subset of flickr data and show that our scalestructure identification method outperforms the existing techniques the approach and methods described in this work can be used in other domains such as geoannotated web pages where text terms can be extracted and associated with usage patterns
hierarchical classification for automatic image annotation hierarchical classification for automatic image annotation in this paper a hierarchical classification framework has been proposed for bridging the semantic gap effectively and achieving multilevel image annotation automatically first the semantic gap between the lowlevel computable visual features and users real information needs is partitioned into four smaller gaps and multiple approachesallare proposed to bridge these smaller gaps more effectively to learn more reliable contextual relationships between the atomic image concepts and the coappearances of salient objects a multimodal boosting algorithm is proposed to enable hierarchical image classification and avoid interlevel error transmission a hierarchical boosting algorithm is proposed by incorporating concept ontology and multitask learning to achieve hierarchical image classifier training with automatic error recovery to bridge the gap between the computable image concepts and the users real information needs a novel hyperbolic visualization framework is seamlessly incorporated to enable intuitive query specification and evaluation by acquainting the users with a good global view of largescale image collections our experiments on largescale image databases have also obtained very positive results
laplacian optimal design for image retrieval laplacian optimal design for image retrieval relevance feedback is a powerful technique to enhance contentbased image retrieval cbir performance it solicits the users relevance judgments on the retrieved images returned by the cbir systems the users labeling is then used to learn a classifier to distinguish between relevant and irrelevant images however the top returnedimages may not be the most informative ones the challenge is thus to determine which unlabeled images would be the most informative ie improve the classifier the most if they were labeled and used as training samples in this paper we propose a novel active learning algorithm called laplacian optimal design lod for relevance feedback image retrieval our algorithm is based on aregression model which minimizes the least square error on the measured or labeled images and simultaneously preserves the local geometrical structure of the image space specifically we assume that if two images are sufficiently close to each other then their measurements or labels are close as well by constructing a nearest neighbor graph the geometrical structure of the image space can be described by the graph laplacian we discuss how results from the field of optimal experimental design may be used to guide our selection of a subset of images which gives us the most amount of information experimental results on corel database suggest that theproposed approach achieves higher precision in relevance feedback image retrieval
fast generation of result snippets in web search fast generation of result snippets in web search the presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users in this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets we begin by proposing and analysing a document compression method that reduces snippet generation time by over a baseline using the zlib compression library these experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets and so caching documents in ram is essential for a fast snippet generation process using simulation we examine snippet generation performance for different size ram caches finally we propose and analyse document reordering and compaction revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality this scheme effectively doubles the number of documents that can fit in a fixed size cache
the influence of caption features on clickthrough patterns in web search the influence of caption features on clickthrough patterns in web search web search engines present lists of captions comprising title snippet and url to help users decide which search results to visit understanding the influence of features of these captions on web search behavior may help validate algorithms and guidelines for their improved generation in this paper we develop a methodology to use clickthrough logs from a commercial search engine to study user behavior when interacting with search result captions the findings of our study suggest that relatively simple caption features such as the presence of all terms query terms the readability of the snippet and the length of the url shown in the caption can significantly influence users web search behavior
collabsum collabsum almost all existing methods conduct the summarization tasks for single documents separately without interactions for each document under the assumption that the documents are considered independent of each other this paper proposes a novel framework called collabsum for collaborative single document summarizations by making use of mutual influences of multiple documents within a cluster context in this study collabsum is implemented by first employing the clustering algorithm to obtain appropriate document clusters and then exploiting the graphranking based algorithm for collaborative document summarizations within each cluster both the withdocument and crossdocument relationships between sentences are incorporated in the algorithm experiments on the duc and duc datasets demonstrate the encouraging performance of the proposed approach different clustering algorithms have been investigated and we find that the summarization performance relies positively on the quality of document cluster
information reretrieval information reretrieval people often repeat web searches both to find new information on topics they have previously explored and to refind information they have seen in the past the query associated with a repeat search may differ from the initial query but can nonetheless lead to clicks on the same results this paper explores repeat search behavior through the analysis of a oneyear web query log of anonymous users and a separate controlled survey of an additional volunteers our study demonstrates that as many as of all queries are refinding queries refinding appears to be an important behavior for search engines to explicitly support and we explore how this can be done we demonstrate that changes to search engine results can hinder refinding and provide a way to automatically detect repeat searches and predict repeat clicks
studying the use of popular destinations to enhance web search interaction studying the use of popular destinations to enhance web search interaction we present a novel web search interaction feature which for a given query provides links to websites frequently visited by other users with similar information needs these popular destinations complement traditional search results allowing direct navigation to authoritative resources for the query topic destinations are identified using the history of search and browsing behavior of many users over an extended time period whose collective behavior provides a basis for computing source authority we describe a user study which compared the suggestion of destinations with the previously proposed suggestion of related queries as well as with traditional unaided web search results show that search enhanced by destination suggestions outperforms other systems for exploratory tasks with best performance obtained from mining past user behavior at querylevel granularity
neighborhood restrictions in geographic ir neighborhood restrictions in geographic ir geographic information retrieval gir systems allow users to specify a geographic context in addition to a more traditional query enabling the system to pinpoint interesting search results whose relevancy is locationdependent in particular local search services have become a widely used mechanism to find businesses such as hotels restaurants and shops which satisfy a geographical restriction unfortunately many useful types of geographic restrictions are currently not supported in these systems including restrictions that specify the neighborhood in which the business should be located as the boundaries of city neighborhoods are not readily available automated techniques to construct representations of the spatial extent of neighborhoods are required to support this kind of restrictions in this paper we propose such a technique using fuzzy footprints to cope with the inherent vagueness of most neighborhood boundaries and we provide experimental results that demonstrate the potential of our technique in a local search setting
efficient document retrieval in main memory efficient document retrieval in main memory disk access performance is a major bottleneck in traditional information retrieval systems compared to system memory disk bandwidth is poor and seek times are worse
the impact of caching on search engines the impact of caching on search engines in this paper we study the tradeoffs in designing efficient caching systems for web search engines we explore the impact of different approaches such as static vs dynamic caching and caching query results vscaching posting lists using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers we propose a new algorithm for static caching of posting lists which outperforms previous methods we also study the problem of finding the optimal way to split the static cache between answers and posting lists finally we measure how the changes in the query log affect the effectiveness of static caching given our observation that the distribution of the queries changes slowly over time our results and observations are applicable to different levels of the dataaccess hierarchy for instance for a memorydisk layer or a brokerremote server layer
pruning policies for twotiered inverted index with correctness guarantee pruning policies for twotiered inverted index with correctness guarantee the web search engines maintain largescale inverted indexes which are queried thousands of times per second by users eager for information in order to cope with the vast amounts of query loads search engines prune their index to keep documents that are likely to be returned as top results and use this pruned index to compute the first batches of results while this approach can improve performance by reducing the size of the index if we compute the top results only from the pruned index we may notice a significant degradation in the result quality if a document should be in the top results but was not included in the pruned index it will be placed behind the results computed from the pruned index given the fierce competition in the online search market this phenomenon is clearly undesirable
topic segmentation with shared topic detection and alignment of multiple documents topic segmentation with shared topic detection and alignment of multiple documents topic detection and tracking and topic segmentation play an important role in capturing the local and sequential information of documents previous work in this area usually focuses on single documents although similar multiple documents are available in many domains in this paper we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information mi and weighted mutual information wmi that is a combination of mi and term weights the basic idea is that the optimal segmentation maximizes mi or wmi our approach can detect shared topics among documents it can find the optimal boundaries in a document and align segments among documents at the same time it also can handle singledocument segmentation as a special case of the multidocument segmentation and alignment our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents our experimental results show that our algorithm works well for the tasks of singledocument segmentation shared topic detection and multidocument segmentation utilizing information from multiple documents can tremendously improve the performance of topic segmentation and using wmi is even better than using mi for the multidocument segmentation
analyzing feature trajectories for event detection analyzing feature trajectories for event detection we consider the problem of analyzing word trajectories in both time and frequency domains with the specific goal of identifying important and lessreported periodic and aperiodic words a set of words with identical trends can be grouped together to reconstruct an event in a completely unsupervised manner the document frequency of each word across time is treated like a time series where each element is the document frequency inverse document frequency dfidf score at one time point in this paper we first applied spectral analysis to categorize features for different event characteristics important and lessreported periodic and aperiodic modeled aperiodic features with gaussian density and periodic features with gaussian mixture densities and subsequently detected each features burst by the truncated gaussian approach proposed an unsupervised greedy event detection algorithm to detect both aperiodic and periodic events all of the above methods can be applied to time series data in general we extensively evaluated our methods on the year reuters news corpus and showed that they were able to uncover meaningful aperiodic and periodic events
new event detection based on indexingtree and named entity new event detection based on indexingtree and named entity new event detection ned aims at detecting from one or multiple streams of news stories that which one is reported on a new event ie not reported previously with the overwhelming volume of news available today there is an increasing need for a ned system which is able to detect new events more efficiently and accurately in this paper we propose a new ned model to speed up the ned task by using news indexingtree dynamically moreover based on the observation that terms of different types have different effects for ned task two term reweighting approaches are proposed to improve ned accuracy in the first approach we propose to adjust term weights dynamically based on previous story clusters and in the second approach we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories experimental results on two linguistic data consortium ldc datasets tdt and tdt show that the proposed model can improve both efficiency and accuracy of ned task significantly compared to the baseline system and other existing systems
multiplesignal duplicate detection for search evaluation multiplesignal duplicate detection for search evaluation we consider the problem of duplicate document detection for search evaluation given a query and a small number of web results for that query we show how to detect duplicate web documents with precision and recall in contrast charikars algorithm designed for duplicate detection in an indexing pipeline achieves precision but with a recall of our improvement in recall while maintaining high precision comes from combining three ideas first because we are only concerned with duplicate detection among results for the same query the number of pairwise comparisons is small therefore we can afford to compute multiple pairwise signals for each pair of documents a model learned with standard machinelearning techniques improves recall to with precision second most duplicate detection has focused on text analysis of the html contents of a document in some web pages the html is not a good indicator of the final contents of the page we use extended fetching techniques to fill in frames and execute java script including signals based on our richer fetches further improves the recall to and the precision to finally we also explore using signals based on the query comparing contextual snippets based on the richer fetches improves the recall to we show that the overall accuracy of this final model approaches that of human judges
robust classification of rare queries using web knowledge robust classification of rare queries using web knowledge we propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy while dealing in realtime with the query volume of a commercial web search engine we use a blind feedback technique given a query we determine its topic by classifying the web search results retrieved by the query motivated by the needs of search advertising we primarily focus on rare queries which are the hardest from the point of view of machine learning yet in aggregation account for a considerable fraction of search engine traffic empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported we believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience
random walks on the click graph random walks on the click graph search engines can record which documents were clicked for which query and use these querydocument pairs as soft relevance judgments however compared to the true judgments click logs give noisy and sparse relevance information we apply a markov random walk model to a large click log producing a probabilistic ranking of documents for a given query a key advantage of the model is its ability to retrieve relevant documents that have not yet been clicked for that query and rank those effectively we conduct experiments on click logs from image search comparing our backward random walk model to a different forward random walk varying parameters such as walk length and selftransition probability the most effective combination is a long backward walk with high selftransition probability
supporting multiple informationseeking strategies in a single system framework supporting multiple informationseeking strategies in a single system framework this paper reports on an experiment comparing the retrieval effectiveness of an interactive information retrieval iir system which adapts to support different information seeking strategies with that of a standard baseline iir system the experiment with subjects each searching on different topics indicates that using the integrated iir system resulted in significantly better performance including user satisfaction with search results significantly more effective interaction and significantly better usability than using the baseline system
investigating the querying and browsing behavior of advanced search engine users investigating the querying and browsing behavior of advanced search engine users one way to help all users of commercial web search engines be more successful in their searches is to better understand what those users with greater search expertise are doing and use this knowledge to benefit everyone in this paper we study the interaction logs of advanced search engine users and those not so advanced to better understand how these user groups search the results show that there are marked differences in the queries result clicks postquery browsing and search success of users we classify as advanced based on their use of query operators relative to those classified as nonadvanced our findings have implications for how advanced users should be supported during their searches and how their interactions could be used to help searchers of all experience levels find more relevant information and learn improved searching strategies
term feedback for information retrieval with language models term feedback for information retrieval with language models in this paper we study termbased feedback for information retrieval in the language modeling approach with term feedback a user directly judges the relevance of individual terms without interaction with feedback documents taking full control of the query expansion process we propose a clusterbased method for selecting terms to present to the user for judgment as well as effective algorithms for constructing refined query language models from user term feedback our algorithms are shown to bring significant improvement in retrieval accuracy over a nonfeedback baseline and achieve comparable performance to relevance feedback they are helpful even when there are no relevant documents in the top
a support vector method for optimizing average precision a support vector method for optimizing average precision machine learning is commonly used to improve ranked retrieval systems due to computational difficulties few learning techniques have been developed to directly optimize for mean average precision map despite its widespread use in evaluating such systems existing approaches optimizing map either do not find a globally optimal solution or are computationally expensive in contrast we present a general svm learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of map we evaluate our approach using the trec and trec web track corpora wtg comparing against svms optimized for accuracy and rocarea in most cases we show our method to produce statistically significant improvements in map scores
ranking with multiple hyperplanes ranking with multiple hyperplanes the central problem for many applications in information retrieval is ranking and learning to rank is considered as a promising approach for addressing the issue ranking svm for example is a stateoftheart method for learning to rank and has been empirically demonstrated to be effective in this paper we study the issue of learning to rank particularly the approach of using svm techniques to perform the task we point out that although ranking svm is advantageous it still has shortcomings ranking svm employs a single hyperplane in the feature space as the model for ranking which is too simple to tackle complex ranking problems furthermore the training of ranking svm is also computationally costly in this paper we look at an alternative approach to ranking svm which we call multiple hyperplane ranker mhr and make comparisons between the two approaches mhr takes the divideandconquer strategy it employs multiple hyperplanes to rank instances and finally aggregates the ranking results given by the hyperplanes mhr contains ranking svm as a special case and mhr can overcome the shortcomings which ranking svm suffers from experimental results on two information retrieval datasets show that mhr can outperform ranking svm in ranking
a regression framework for learning ranking functions using relative relevance judgments a regression framework for learning ranking functions using relative relevance judgments effective ranking functions are an essential part of commercial search engines we focus on developing a regression framework for learning ranking functions for improving relevance of search engines serving diverse streams of user queries we explore supervised learning methodology from machine learning and we distinguish two types of relevance judgments used as the training data absolute relevance judgments arising from explicit labeling of search results and relative relevance judgments extracted from user click throughs of search results or converted from the absolute relevance judgments we propose a novel optimization framework emphasizing the use of relative relevance judgments the main contribution is the development of an algorithm based on regression that can be applied to objective functions involving preference data ie data indicating that a document is more relevant than another with respect to a query experimental results are carried out using data sets obtained from a commercial search engine our results show significant improvements of our proposed methods over some existing methods
an exploration of proximity measures in information retrieval an exploration of proximity measures in information retrieval in most existing retrieval models documents are scored primarily based on various kinds of term statistics such as withindocument frequencies inverse document frequencies and document lengths intuitively the proximity of matched query terms in a document can also be exploited to promote scores of documents in which the matched query terms are close to each other such a proximity heuristic however has been largely underexplored in the literature it is unclear how we can model proximity and incorporate a proximity measure into an existing retrieval model in this paperwe systematically explore the query term proximity heuristic specifically we propose and study the effectiveness of five different proximity measures each modeling proximity from a different perspective we then design two heuristic constraints and use them to guide us in incorporating the proposed proximity measures into an existing retrieval model experiments on five standard trec test collections show that one of the proposed proximity measures is indeed highly correlated with document relevance and by incorporating it into the kldivergence language model and the okapi bm model we can significantly improve retrieval performance
estimation and use of uncertainty in pseudorelevance feedback estimation and use of uncertainty in pseudorelevance feedback existing pseudorelevance feedback methods typically perform averaging over the topretrieved documents but ignore an important statistical dimension the risk or variance associated with either the individual document models or their combination treating the baseline feedback method as a black box and the output feedback model as a random variable we estimate a posterior distribution for the feedback model by resampling a given querys topretrieved documents using the posterior mean or mode as the enhanced feedback model we then perform model combination over several enhanced models each based on a slightly modified query sampled from the original query we find that resampling documents helps increase individual feedback model precision by removing noise terms while sampling from the query improves robustness worstcase performance by emphasizing terms related to multiple query aspects the result is a metafeedback algorithm that is both more robust and more precise than the original strong baseline method
latent concept expansion using markov random fields latent concept expansion using markov random fields query expansion in the form of pseudorelevance feedback or relevance feedback is a common technique used to improve retrieval effectiveness most previous approaches have ignored important issues such as the role of features and the importance of modeling term dependencies in this paper we propose a robust query expansion technique based onthe markov random field model for information retrieval the technique called latent concept expansion provides a mechanism for modeling term dependencies during expansion furthermore the use of arbitrary features within the model provides a powerful framework for going beyond simple term occurrence features that are implicitly used by most other expansion techniques we evaluate our technique against relevance models a stateoftheart language modeling query expansion technique our model demonstrates consistent and significant improvements in retrieval effectiveness across several trec data sets we also describe how our technique can be used to generate meaningful multiterm concepts for tasks such as query suggestionreformulation
a study of poisson query generation model for information retrieval a study of poisson query generation model for information retrieval many variants of language models have been proposed for information retrieval most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model in this paper we propose and study a new family of query generation models based on poisson distribution we show that while in their simplest forms the new family of models and the existing multinomial models are equivalent however based on different smoothing methods the two families of models behave differently we show that the poisson model has several advantages including naturally accommodating perterm smoothing and modeling accurate background more efficiently we present several variants of the new model corresponding to different smoothing methods and evaluate them on four representative trec test collections the results show that while their basic models perform comparably the poisson model can out perform multinomial model with perterm smoothing the performance can be further improved with twostage smoothing
deconstructing nuggets deconstructing nuggets a methodology based on information nuggets has recently emerged as the de facto standard by which answers to complex questions are evaluated after several implementations in the trec question answering tracks the community has gained a better understanding of its many characteristics this paper focuses on one particular aspect of the evaluation the human assignment of nuggets to answer strings which serves as the basis of the fscore computation as a byproduct of the trec ciqa task identical answer strings were independently evaluated twice which allowed us to assess the consistency of human judgments based on these results we explored simulations of assessor behavior that provide a method to quantify scoring variations understanding these variations in turn lets researchers be more confident in their comparisons of systems
interesting nuggets and their impact on definitional question answering interesting nuggets and their impact on definitional question answering current approaches to identifying definitional sentences in the context of question answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets this is insufficient as they do not address the novelty factor that a definitional nugget must also possess this paper proposes to address the deficiency by building a human interest model from external knowledge it is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic we compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering
a probabilistic graphical model for joint answer ranking in question answering a probabilistic graphical model for joint answer ranking in question answering graphical models have been applied to various information retrieval and natural language processing tasks in the recent literature in this paper we apply a probabilistic graphical model for answer ranking in question answering this model estimates the joint probability of correctness of all answer candidates from which the probability of correctness of an individual candidate can be inferred the joint prediction model can estimate both the correctness of individual answers as well as their correlations which enables a list of accurate and comprehensive answers this model was compared with a logistic regression model which directly estimates the probability of correctness of each individual answer candidate an extensive set of empirical results based on trec questions demonstrates the effectiveness of the joint model for answer ranking furthermore we combine the joint model with the logistic regression model to improve the efficiency and accuracy of answer ranking
structured retrieval for question answering structured retrieval for question answering bagofwords retrieval is popular among question answering qa system developers but it does not support constraint checking and ranking on the linguistic and semantic information of interest to the qa system we present anapproach to retrieval for qa applying structured retrieval techniques to the types of text annotations that qa systems use we demonstrate that the structured approach can retrieve more relevant results more highly ranked compared with bagofwords on a sentence retrieval task we also characterize the extent to which structured retrieval effectiveness depends on the quality of the annotations
on the robustness of relevance measures with incomplete judgments on the robustness of relevance measures with incomplete judgments we investigate the robustness of three widely used ir relevance measures for large data collections with incomplete judgments the relevance measures we consider are the bpref measure introduced by buckley and voorhees the inferred average precision infap introduced by aslam and yilmaz and the normalized discounted cumulative gain ndcg measure introduced by jrvelin and keklinen our main results show that ndcg consistently performs better than both bpref and infap the experiments are performed on standard trec datasets under different levels of incompleteness of judgments and using two different evaluation methods namely the kendall correlation measures order between system rankings and pairwise statistical significance testing the latter may be of independent interest
test theory for assessing ir test collections test theory for assessing ir test collections how good is an ir test collection a series of papers in recent years has addressed the question by empirically enumerating the consistency of performance comparisons using alternate subsets of the collection in this paper we propose using test theory which is based on analysis of variance and is specifically designed to assess test collections using the method we not only can measure test reliability after the fact but we can estimate the test collections reliability before it is even built or used we can also determine an optimal allocation of resources before the fact eg whether to invest in more judges or queries the method which is in widespread use in the field of educational testing complements datadriven approaches to assessing test collections whereas the datadriven method focuses on test results test theory focuses on test designs it offers unique practical results as well as insights about the variety and implications of alternative test designs
strategic system comparisons via targeted relevance judgments strategic system comparisons via targeted relevance judgments relevance judgments are used to compare text retrieval systems given a collection of documents and queries and a set of systems being compared a standard approach to forming judgments is to manually examine all documents that are highly ranked by any of the systems however not all of these relevance judgments provide the same benefit to the final result particularly if the aim is to identify which systems are best rather than to fully order them in this paper we propose new experimental methodologies that can significantly reduce the volume of judgments required in system comparisons using rankbiased precision a recently proposed effectiveness measure we show that judging around documents for each of queries in a trecscale system evaluation containing over runs is sufficient to identify the best systems
frank frank ranking problem is becoming important in many fields especially in information retrieval ir many machine learning techniques have been proposed for ranking problem such as ranksvm rankboost and ranknet among them ranknet which is based on a probabilistic ranking framework is leading to promising results and has been applied to a commercial web search engine in this paper we conduct further study on the probabilistic ranking framework and provide a novel loss function named fidelity loss for measuring loss of ranking the fidelity loss notonly inherits effective properties of the probabilistic ranking framework in ranknet but possesses new properties that are helpful for ranking this includes the fidelity loss obtaining zero for each document pair and having a finite upper bound that is necessary for conducting querylevel normalization we also propose an algorithm named frank based on a generalized additive model for the sake of minimizing the fedelity loss and learning an effective ranking function we evaluated the proposed algorithm for two datasets trec dataset and real web search dataset the experimental results show that the proposed frank algorithm outperforms other learningbased ranking methods on both conventional ir problem and web search
adarank adarank in this paper we address the issue of learning to rank for document retrieval in the task a model is automatically created with some training data and then is utilized for ranking of documents the goodness of a model is usually evaluated with performance measures such as map mean average precision and ndcg normalized discounted cumulative gain ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data existing methods however are only able to train ranking models by minimizing loss functions loosely related to the performance measures for example ranking svm and rankboost train ranking models by minimizing classification errors on instance pairs to deal with the problem we propose a novel learning algorithm within the framework of boosting which can minimize a loss function directly defined on the performance measures our algorithm referred to as adarank repeatedly constructs weak rankers on the basis of reweighted training data and finally linearly combines the weak rankers for making ranking predictions we prove that the training process of adarank is exactly that of enhancing the performance measure used experimental results on four benchmark datasets show that adarank significantly outperforms the baseline methods of bm ranking svm and rankboost
a combined component approach for finding collectionadapted ranking functions based on genetic programming a combined component approach for finding collectionadapted ranking functions based on genetic programming in this paper we propose a new method to discover collectionadapted ranking functions based on genetic programming gp our combined component approach ccais based on the combination of several termweighting components ieterm frequency collection frequency normalization extracted from wellknown ranking functions in contrast to related work the gp terminals in our cca are not based on simple statistical information of a document collection but on meaningful effective and proven components experimental results show that our approach was able to outper form standard tfidf bm and another gpbased approach in two different collections cca obtained improvements in mean average precision up to for the trec collection and for the wbr collection a large brazilian web collection over the baseline functions the cca evolution process also was able to reduce the overtraining commonly found in machine learning methods especially genetic programming and to converge faster than the other gpbased approach used for comparison
feature selection for ranking feature selection for ranking ranking is a very important topic in information retrieval while algorithms for learning ranking models have been intensively studied this is not the case for feature selection despite of its importance the reality is that many feature selection methods used in classification are directly applied to ranking we argue that because of the striking differences between ranking and classification it is better to develop different feature selection methods for ranking to this end we propose a new feature selection method in this paper specifically for each feature we use its value to rank the training instances and define the ranking accuracy in terms of a performance measure or a loss function as the importance of the feature we also define the correlation between the ranking results of two features as the similarity between them based on the definitions we formulate the feature selection issue as an optimization problem for which it is to find the features with maximum total importance scores and minimum total similarity scores we also demonstrate how to solve the optimization problem in an efficient way we have tested the effectiveness of our feature selection method on two information retrieval datasets and with two ranking models experimental results show that our method can outperform traditional feature selection methods for the ranking task
relaxed online svms for spam filtering relaxed online svms for spam filtering spam is a key problem in electronic communication including largescale email systems and the growing number of blogs contentbased filtering is one reliable method of combating this threat in its various forms but some academic researchers and industrial practitioners disagree on how best to filter spam the former have advocated the use of support vector machines svms for contentbased filtering as this machine learning methodology gives stateoftheart performance for text classification however similar performance gains have yet to be demonstrated for online spam filtering additionally practitioners cite the high cost of svms as reason to prefer faster if less statistically robust bayesian methods in this paper we offer a resolution to this controversy first we show that online svms indeed give stateoftheart classification performance on online spam filtering on large benchmark data sets second we show that nearly equivalent performance may be achieved by a relaxed online svm rosvm at greatly reduced computational cost our results are experimentally verified on email spam blog spam and splog detection tasks
know your neighbors know your neighbors web spam can significantly deteriorate the quality of search engine results thus there is a large incentive for commercial search engines to detect spam pages efficiently and accurately in this paper we present a spam detection system that combines linkbased and contentbased features and uses the topology of the web graph by exploiting the link dependencies among the web pages we find that linked hosts tend to belong to the same class either both are spam or both are nonspam we demonstrate three methods of incorporating the web graph topology into the predictions obtained by our base classifier i clustering the host graph and assigning the label of all hosts in the cluster by majority vote ii propagating the predicted labels to neighboring hosts and iii using the predicted labels of neighboring hosts as new features and retraining the classifier the result is an accurate system for detecting web spam tested on a large and public dataset using algorithms that can be applied in practice to largescale web data
diffusionrank diffusionrank while the pagerank algorithm has proven to be very effective for ranking web pages the rank scores of web pages can be manipulated to handle the manipulation problem and to cast a new insight on the web structure we propose a ranking algorithm called diffusionrank diffusionrank is motivated by the heat diffusion phenomena which can be connected to web ranking because the activities flow on the web can be imagined as heat flow the link from a page to another can be treated as the pipe of an airconditioner and heat flow can embody the structure of the underlying web graph theoretically we show that diffusionrank can serve as a generalization of pagerank when the heat diffusion coefficient tends to infinity in such a case diffusionrank pagerank has low ability of antimanipulation when diffusionrank obtains the highest ability of antimanipulation but in such a case the web structure is completely ignored consequently is an interesting factor that can control the balance between the ability of preserving the original web and the ability of reducing the effect of manipulation it is found empirically that when diffusionrank has a penicillinlike effect on the link manipulation moreover diffusionrank can be employed to find grouptogroup relations on the web to divide the web graph into several parts and to find link communities experimental results show that the diffusionrank algorithm achieves the above mentioned advantages as expected
towards musical querybysemanticdescription using the cal data set towards musical querybysemanticdescription using the cal data set querybysemanticdescription qbsdis a natural paradigm for retrieving content from large databases of music a major impediment to the development of good qbsd systems for music information retrieval has been the lack of a cleanlylabeled publiclyavailable heterogeneous data set of songs and associated annotations we have collected the computer audition lab song cal data set by having humans listen to and annotate songs using a survey designed to capture semantic associations between music and words we adapt the supervised multiclass labeling sml model which has shown good performance on the task of image retrieval and use the cal data to learn a model for music retrieval the model parameters are estimated using the weighted mixture hierarchies expectationmaximization algorithm which has been specifically designed to handle realvalued semantic association between words and songs rather than binary class labels the output of the sml model a vector of classconditional probabilities can be interpreted as a semantic multinomial distribution over a vocabulary by also representing a semantic query as a query multinomial distribution we can quickly rank order the songs in a database based on the kullbackleibler divergence between the query multinomial and each songs semantic multinomial qualitative and quantitative results demonstrate that our sml model can both annotate a novel song with meaningful words and retrieve relevant songs given a multiword textbased query
a music search engine built upon audiobased and webbased similarity measures a music search engine built upon audiobased and webbased similarity measures an approach is presented to automatically build a search engine for largescale music collections that can be queried through natural language while existing approaches depend on explicit manual annotations and metadata assigned to the individual audio pieces we automatically derive descriptions by making use of methods from web retrieval and music information retrieval based on the id tags of a collection of mp files we retrieve relevant web pages via google queries and use the contents of these pages to characterize the music pieces and represent them by term vectors by incorporating complementary information about acous tic similarity we are able to both reduce the dimensionality of the vector space and improve the performance of retrieval ie the quality of the results furthermore the usage of audio similarity allows us to also characterize audio pieces when there is no associated information found on the web
building simulated queries for knownitem topics building simulated queries for knownitem topics there has been increased interest in the use of simulated queries for evaluation and estimation purposes in information retrieval however there are still many unaddressed issues regarding their usage and impact on evaluation because their quality in terms of retrieval performance is unlike real queries in this paper wefocus on methods for building simulated knownitem topics and explore their quality against real knownitem topics using existing generation models as our starting point we explore factors which may influence the generation of the knownitem topic informed by this detailed analysis on six european languages we propose a model with improved document and term selection properties showing that simulated knownitem topics can be generated that are comparable to real knownitem topics this is a significant step towards validating the potential usefulness of simulated queries for evaluation purposes and becausebuilding models of querying behavior provides a deeper insight into the querying process so that better retrieval mechanisms can be developed to support the user
crosslingual query suggestion using query logs of different languages crosslingual query suggestion using query logs of different languages query suggestion aims to suggest relevant queries for a given query which help users better specify their information needs previously the suggested terms are mostly in the same language of the input query in this paper we extend it to crosslingual query suggestion clqs for a query in one language we suggest similar or relevant queries in other languages this is very important to scenarios of crosslanguage information retrieval clir and crosslingual keyword bidding for search engine advertisement instead of relying on existing query translation technologies for clqs we present an effective means to map the input query of one language to queries of the other language in the query log important monolingual and crosslingual information such as word translation relations and word cooccurrence statistics etc are used to estimate the crosslingual query similarity with a discriminative model benchmarks show that the resulting clqs system significantly out performs a baseline system based on dictionarybased query translation besides the resulting clqs is tested with french to english clir tasks on trec collections the results demonstrate higher effectiveness than the traditional query translation methods
hits on the web hits on the web this paper describes a largescale evaluation of theeffectiveness of hits in comparison with other linkbased rankingalgorithms when used in combination with a stateoftheart textretrieval algorithm exploiting anchor text we quantified theireffectiveness using three common performance measures the meanreciprocal rank the mean average precision and the normalizeddiscounted cumulative gain measurements the evaluation is based ontwo large data sets a breadthfirst search crawl of millionweb pages containing billion hyperlinks and referencing billion distinct urls and a set of queries sampled from aquery log each query having on average results about ofwhich were labeled by judges we found that hits outperformspagerank but is about as effective as webpage indegree the sameholds true when any of the linkbased features are combined withthe text retrieval algorithm finally we studied the relationshipbetween query specificity and the effectiveness of selectedfeatures and found that linkbased features perform better forgeneral queries whereas bmf performs better for specificqueries
hits hits trec hits hits trec we propose a novel method of analysing data gathered fromtrec or similar information retrieval evaluation experiments we define two normalized versions of average precision that we use to construct a weighted bipartite graph of trec systems and topics we analyze the meaning of well known and somewhat generalized indicators fromsocial network analysis on the systemstopics graph we apply this method to an analysis of trec data amongthe results we find that authority measures systems performance that hubness of topics reveals that some topics are better than others at distinguishing more or less effective systems that with current measures a system that wants to be effective in trec needs to be effective on easy topics and that by using different effectiveness measures this is no longer the case
combining content and link for classification using matrix factorization combining content and link for classification using matrix factorization the world wide web contains rich textual contents that areinterconnected via complex hyperlinks this huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample it is thus difficult to apply traditional mining or learning methods for solving web mining problems eg web page classification by exploiting both the content and the link structure the research in this direction has recently received considerable attention but are still in an early stage though a few methods exploit both the link structure or the content information some of them combine the only authority information with the content information and the others first decompose the link structure into hub and authority features then apply them as additional document features being practically attractive for its great simplicity this paper aims to design an algorithm that exploits both the content and linkage information by carrying out a joint factorization on both the linkage adjacency matrix and the documentterm matrix and derives a new representation for web pages in a lowdimensional factor space without explicitly separating them as content hub or authority factors further analysis can be performed based on the compact representation of web pages in the experiments the proposed method is compared with stateoftheart methods and demonstrates an excellent accuracy in hypertext classification on the webkb and cora benchmarks
federated text retrieval from uncooperative overlapped collections federated text retrieval from uncooperative overlapped collections in federated text retrieval systems the query is sent to multiple collections at the same time the results returned by collections are gathered and ranked by a central broker that presents them to the user it is usually assumed that the collections have little overlap however in practice collections may share many common documents as either exact or near duplicates potentially leading to high numbers of duplicates in the final results considering the natural band width restrictions and efficiency issues of federated search sendingqueries to redundant collections leads to unnecessary costs we propose a novel method for estimating the rate of overlap among collections based on sampling then using theestimated overlap statistics we propose two collection selection methods that aim to maximize the number of unique relevant documents in the final results we show experimentally that although our estimates of overlap are not in exact our suggested techniques can significantly improve the search effectiveness when collections overlap
evaluating sampling methods for uncooperative collections evaluating sampling methods for uncooperative collections many server selection methods suitable for distributed information retrieval applications rely in the absence of cooperation on the availability of unbiased samples of documents from the constituent collections we describe a number of sampling methods which depend only on the normal queryresponse mechanism of the applicable search facilities we evaluate these methods on a number of collections typical of a personal metasearch application results demonstrate that biases exist for all methods particularly toward longer documents and that in some cases these biases can be reduced but not eliminated by choice of parameterswe also introduce a new sampling technique multiple queries which produces samples of similar quality to the best current techniques but with significantly reduced cost
updating collection representations for federated search updating collection representations for federated search to facilitate the search for relevant information across a setof online distributed collections a federated information retrieval system typically represents each collection centrally by a set of vocabularies or sampled documents accurate retrieval is therefore related to how precise each representation reflects the underlying content stored in that collection as collections evolve over time collection representations should also be updated to reflect any change however a current solution has not yet been proposed in this study we examine both the implications of outofdate representation sets on retrieval accuracy as well as proposing three different policies for managing necessary updates each policyis evaluated on a testbed of fortyfour dynamic collections over an eightweek period our findings show that outofdate representations significantly degrade performance overtime however adopting a suitable update policy can minimise this problem
a time machine for text search a time machine for text search text search over temporally versioned document collections such as web archives has received little attention as a research problem as a consequence there is no scalable and principled solution to search such a collection as of a specified time in this work we address this shortcoming and propose an efficient solution for timetravel text search by extending the inverted file index to make it ready for temporal search we introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results in order to further improve the performance of timetravel queries we introduce two principled techniques to trade off index size for its performance these techniques can be formulated as optimization problems that can be solved to nearoptimality finally our approach is evaluated in a comprehensive series of experiments on two largescale realworld datasets results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections
principles of hashbased text retrieval principles of hashbased text retrieval hashbased similarity search reduces a continuous similarity relation to the binary concept similar or not similar two feature vectors are considered as similar if they are mapped on the same hash key from its runtime performance this principle is unequaledwhile being unaffected by dimensionality concerns at the same time similarity hashing is applied with great success for near similarity search in large document collections and it is considered as a key technology for nearduplicate detection and plagiarism analysis this papers reveals the design principles behind hashbased search methods and presents them in a unified way we introduce new stress statistics that are suited to analyze the performance of hashbased search methods and we explain the rationale of their effectiveness based on these insights we show how optimum hash functions for similarity search can be derived we also present new results of a comparative study between different hashbased search methods
compressed permuterm index compressed permuterm index recently manning et al resorted the permuterm indexof garfield as a timeefficient and elegant solution to the string dictionary problem in which pattern queries may possibly include one wildcard symbol called tolerant retrieval problem unfortunately the permuterm index is space inefficient because its quadruples the dictionary size in this paper we propose the compressed permuterm index which solves the tolerant retrieval problem in optimal query time ie time proportional to the length of the searched pattern and space close to the kth order empirical entropy of the indexed dictionary our index can be used to solve also more sophisticated queries which involve several wildcard symbols or require to prefixmatch multiple fields in a database of recordsthe result is based on an elegant variant of the burrowswheeler transform defined on a dictionary of strings of variable length which allows to easily adapt known compressed indexes makinennavarro to solve the tolerant retrieval problem experiments show that our index supports fast queries within a space occupancy that is close to the one achievable by compressing the string dictionary via gzip bzip or ppmdi this improves known approaches based on frontcoding by more than in absolute space occupancy still guaranteeing comparable query time
query performance prediction in web search environments query performance prediction in web search environments current prediction techniques which are generally designed for contentbased queries and are typically evaluated on relatively homogenous test collections of small sizes face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist in this paper we present three techniques to address these challenges we focus on performance prediction for two types of queries in web search environments contentbased and namedpage finding our evaluation is mainly performed on the gov collection in addition to evaluating our models for the two types of queries separately we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types to assist prediction under the mixedquery situation a novel query classifier is adopted results show that our prediction of web query performance is substantially more accurate than the current stateoftheart prediction techniques consequently our paper provides a practical approach to performance prediction in realworld web settings
broad expertise retrieval in sparse data environments broad expertise retrieval in sparse data environments expertise retrieval has been largely unexplored on data other than the wc collection at the same time many intranets of universities and other knowledgeintensive organisations offer examples of relatively small but clean multilingual expertise data covering broad ranges of expertise areas we first present two main expertise retrieval tasks along with a set of baseline approaches based on generative language modeling aimed at finding expertise relations between topics and people for our experimental evaluation we introduce and release a new test set based on a crawl of a university site using this test set we conduct two series of experiments the first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set the second is aimed at assessing refined models that exploit characteristic features of the new test set such as the organizational structure of the university and the hierarchical structure of the topics in the test set expertise retrieval models are shown to be robust with respect to environments smaller than the wc collection and current techniques appear to be generalizable to other settings
a semantic approach to contextual advertising a semantic approach to contextual advertising contextual advertising or context match cm refers to the placement of commercial textual advertisements within the content of a generic web page while sponsored search ss advertising consists in placing ads on result pages from a web search engine with ads driven by the originating query in cm there is usually an intermediary commercial adnetwork entity in charge of optimizing the ad selection with the twin goal of increasing revenue shared between the publisher and the adnetwork and improving the user experience with these goals in mind it is preferable to have ads relevant to the page content rather than generic ads the ss market developed quicker than the cm market and most textual ads are still characterized by bid phrases representing those queries where the advertisers would like to have their ad displayed hence the first technologies for cm have relied on previous solutions for ss by simply extracting one or more phrases from the given page content and displaying ads corresponding to searches on these phrases in a purely syntactic approach however due to the vagaries of phrase extraction and the lack of context this approach leads to many irrelevant ads to overcome this problem we propose a system for contextual ad matching based on a combination of semantic and syntactic features
how well does result relevance predict session satisfaction how well does result relevance predict session satisfaction perquery relevance measures provide standardized repeatable measurements of search result quality but they ignore much of what users actually experience in a full search session this paper examines how well we can approximate a users ultimate sessionlevel satisfaction using a simple relevance metric we find that thisrelationship is surprisingly strong by incorporating additional properties of the query itself we construct a model which predicts user satisfaction even more accurately than relevance alone
a new approach for evaluating query expansion a new approach for evaluating query expansion the effectiveness of information retrieval ir systems is influenced by the degree of term overlap between user queries and relevant documents querydocument term mismatch whether partial or total is a fact that must be dealt with by ir systems query expansion qe is one method for dealing with term mismatch ir systems implementing query expansion are typically evaluated by executing each query twice with and without query expansion and then comparing the two result sets while this measures an overall change in performance it does not directly measure the effectiveness of ir systems in overcoming the inherent issue of term mismatch between the query and relevant documents nor does it provide any insight into how such systems would behave in the presence of querydocument term mismatch in this paper we propose a new approach for evaluating query expansion techniques the proposed approach is attractive because it provides an estimate of system performance under varying degrees of querydocument term mismatch it makes use of readily available test collections and it does not require any additional relevance judgments or any form of manual processing
performance prediction using spatial autocorrelation performance prediction using spatial autocorrelation evaluation of information retrieval systems is one of the core tasks in information retrieval problems include the inability to exhaustively label all documents for a topic generalizability from a small number of topics and incorporating the variability of retrieval systems previous work addresses the evaluation of systems the ranking of queries by difficulty and the ranking of individual retrievals by performance approaches exist for the case of few and even no relevance judgments our focus is on zerojudgment performance prediction of individual retrievals one common shortcoming of previous techniques is the assumption of uncorrelated document scores and judgments if documents are embedded in a highdimensional space as they often are we can apply techniques from spatial data analysis to detect correlations between document scores we find that the low correlation between scores of topically close documents often implies a poor retrieval performance when compared to a state of the art baseline we demonstrate that the spatial analysis of retrieval scores provides significantly better prediction performance these new predictors can also be incorporated with classic predictors to improve performance further we also describe the first largescale experiment to evaluate zerojudgment performance prediction for a massive number of retrieval systems over a variety collections in several languages
an outranking approach for rank aggregation in information retrieval an outranking approach for rank aggregation in information retrieval research in information retrieval usually shows performanceimprovement when many sources of evidence are combined to produce a ranking of documents eg texts pictures sounds etc in this paper we focus on the rank aggregation problem also called data fusion problem where rankings of documents searched into the same collection and provided by multiple methods are combined in order to produce a new ranking in this context we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another we show that the proposed method deals well with the information retrieval distinctive features experimental results are reported showing that the suggested method performs better than the wellknown combsum and combmnz operators
enhancing relevance scoring with chronological term rank enhancing relevance scoring with chronological term rank we introduce a new relevance scoring technique that enhances existing relevance scoring schemes with term position information this technique uses chronological term rank ctr which captures the positions of terms as they occur in the sequence of words in a document ctr is both conceptually and computationally simple when compared to other approaches that use document structure information such as term proximity term order and document features ctr works well when paired with okapi bm we evaluate the performance of various combinations of ctr with okapi bm in order to identify the most effective formula we then compare the performance of the selected approach against the performance of existing methods such as okapi bm pivoted length normalization and language models significant improvements are seen consistently across a variety of trec data and topic sets measured by the major retrieval performance metrics this seems to be the first use of this statistic for relevance scoring there is likely to be greater retrieval improvements possible using chronological term rank enhanced methods in future work
arsa arsa due to its high popularity weblogs or blogs in short present a wealth of information that can be very helpful in assessing the general publics sentiments and opinions in this paper we study the problem of mining sentiment information from blogs and investigate ways to use such information for predicting product sales performance based on an analysis of the complex nature of sentiments we propose sentiment plsa splsa in which a blog entry is viewed as a document generated by a number of hidden sentiment factors training an splsa model on the blog data enables us to obtain a succinct summary of the sentiment information embedded in the blogs we then present arsa an autoregressive sentimentaware model to utilize the sentiment information captured by splsa for predicting product sales performance extensive experiments were conducted on a movie data set we compare arsa with alternative models that do not take into account the sentiment information as well as a model with a different feature selection method experiments confirm the effectiveness and superiority of the proposed approach
vocabulary independent spoken term detection vocabulary independent spoken term detection we are interested in retrieving information from speech data like broadcast news telephone conversations and roundtable meetings today most systems use large vocabulary continuous speech recognition tools to produce word transcripts the transcripts are indexed and query terms are retrieved from the index however query terms that are not part of the recognizers vocabulary cannot be retrieved and the recall of the search is affected in addition to the output word transcript advanced systems provide also phonetic transcripts against which query terms can be matched phonetically such phonetic transcripts suffer from lower accuracy and cannot be an alternative to word transcriptswe present a vocabulary independent system that can handle arbitrary queries exploiting the information provided by having both word transcripts and phonetic transcripts a speech recognizer generates word confusion networks and phonetic lattices the transcripts are indexed for query processing and ranking purposethe value of the proposed method is demonstrated by the relative high performance ofour system which received the highest overall ranking for us english speech data in the recent nist spoken term detection evaluation
improving text classification for oral history archives with temporal domain knowledge improving text classification for oral history archives with temporal domain knowledge this paper describes two new techniques for increasing the accuracy oftopic label assignment to conversational speech from oral history interviews using supervised machine learning in conjunction with automatic speech recognition the first timeshifted classification leverages local sequence information from the order in which the story is told the second temporal label weighting takes the complementary perspective by using the position within an interview to bias label assignment probabilities these methods when used in combination yield between and relative improvements in classification accuracy using a clipped rprecision measure that models the utility of label sets as segment summaries in interactive speech retrieval applications
indexing confusion networks for morphbased spoken document retrieval indexing confusion networks for morphbased spoken document retrieval in this paper we investigate methods for improving the performance of morphbased spoken document retrieval in finnish by extracting relevant index terms from confusion networks our approach uses morphemelike subword units morphs for recognition and indexing this alleviates the problem of outofvocabulary words especially with inflectional languages like finnish confusion networks offer a convenient representation of alternative recognition candidates by aligning mutually exclusive terms and by giving the posterior probability of each term the rank of the competing terms and their posterior probability is used to estimate term frequency for indexing comparing against best recognizer transcripts we show that retrieval effectiveness is significantly improved finally the effect of pruning in recognition is analyzed showing that when recognition speed is increased the reduction in retrieval performance due to the increase in the best error rate can be compensated by using confusion networks
context sensitive stemming for web search context sensitive stemming for web search traditionally stemming has been applied to information retrieval tasks by transforming words in documents to the their root form before indexing and applying a similar transformation to query terms although it increases recall this naive strategy does not work well for web search since it lowers precision and requires a significant amount of additional computation
detecting categorizing and clustering entity mentions in chinese text detecting categorizing and clustering entity mentions in chinese text the work presented in this paper is motivated by the practical need for content extraction and the available data source and evaluation benchmark from the ace program the chinese entity detection and recognition edr task is of particular interest to us this task presents us several languageindependent and languagedependent challenges eg rising from the complication of extraction targets and the problem of word segmentation etc in this paper we propose a novel solution to alleviate the problems special in the task mention detection takes advantages of machine learning approaches and characterbased models it manipulates different types of entities being mentioned and different constitution units ie extents and heads separately mentions referring to the same entity are linked together by integrating mostspecificfirst and closestfirst rule based pairwise clustering algorithms types of mentions and entities are determined by headdriven classification approaches the implemented system achieves ace value of when evaluated on the edr chinese corpus which has been one of the toptier results alternative approaches to mention detection and clustering are also discussed and analyzed
knowledgeintensive conceptual retrieval and passage extraction of biomedical literature knowledgeintensive conceptual retrieval and passage extraction of biomedical literature this paper presents a study of incorporating domainspecific knowledge ie information about concepts and relationships between concepts in a certain domain in an information retrieval ir system to improve its effectiveness in retrieving biomedical literature the effects of different types of domainspecific knowledge in performance contribution are examined based on the trec platform we show that appropriate use of domainspecific knowledge in a proposed conceptual retrieval model yields about improvement over the best reported result in passage retrieval in the genomics track of trec 
heavytailed distributions and multikeyword queries heavytailed distributions and multikeyword queries intersecting inverted indexes is a fundamental operation for many applications in information retrieval and databases efficient indexing for this operation is known to be a hard problem for arbitrary data distributions however text corpora used in information retrieval applications often have convenient powerlaw constraints also known as zipfs law and long tails that allow us to materialize carefully chosen combinations of multikeyword indexes which significantly improve worstcase performance without requiring excessive storage these multikeyword indexes limit the number of postings accessed when computing arbitrary index intersections our evaluation on an ecommerce collection of million products shows that the indexes of up to four arbitrary keywords can be intersected while accessing less than of the postings in the largest singlekeyword index
ester ester we present ester a modular and highly efficient system for combined fulltext and ontology search ester builds on a query engine that supports two basic operations prefix search and join both of these can be implemented very efficiently with a compact index yet in combination provide powerful querying capabilities we show how ester can answer basic sparql graphpattern queries on the ontology by reducing them to a small number of these two basic operations ester further supports a natural blend of such semantic queries with ordinary fulltext queries moreover the prefix search operation allows for a fully interactive and proactive user interface which after every keystroke suggests to the user possible semantic interpretations of his or her query and speculatively executes the most likely of these interpretations as a proof of concept we applied ester to the english wikipedia which contains about million documents combined with the recent yago ontology which contains about million facts for a variety of complex queries ester achieves worstcase query processing times of a fraction of a second on a single machine with an index size of about gb
web text retrieval with a pp querydriven index web text retrieval with a pp querydriven index in this paper we present a querydriven indexingretrieval strategy for efficient full text retrieval from large document collections distributed within a structured pp network our indexing strategy is based on two important properties the generated distributed index stores posting lists for carefully chosen indexing term combinations and the posting lists containing too many document references are truncated to a bounded number of their topranked elements these two properties guarantee acceptable storage and bandwidth requirements essentially because the number of indexing term combinations remains scalable and the transmitted posting lists never exceed a constant size however as the number of generated term combinations can still become quite large we also use term statistics extracted from available query logs to index only such combinations that are frequently present in user queries thus by avoiding the generation of superfluous indexing term combinations we achieve an additional substantial reduction in bandwidth and storage consumption as a result the generated distributed index corresponds to a constantly evolving querydriven indexing structure that efficiently follows current information needs of the users more precisely our theoretical analysis and experimental results indicate that at the price of a marginal loss in retrieval quality for rare queries the generated index size and network traffic remain manageable even for websize document collections furthermore our experiments show that at the same time the achieved retrieval quality is fully comparable to the one obtained with a stateoftheart centralized query engine
using gradient descent to optimize language modeling smoothing parameters using gradient descent to optimize language modeling smoothing parameters note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
locality discriminating indexing for document classification locality discriminating indexing for document classification this paper introduces a locality discriminating indexing ldi algorithm for document classification based on the hypothesis that samples from different classes reside in classspecific manifold structures ldi seeks for a projection which best preserves the withinclass local structures while suppresses the betweenclass overlap comparative experiments show that the proposed method isable to derives compact discriminating document representations for classification
management of keyword variation with frequency based generation of word forms in ir management of keyword variation with frequency based generation of word forms in ir this paper presents a new management method for morphological variation of keywords the method is called fcg frequent case generation it is based on the skewed distributions of word forms in natural languages and is suitable for languages that have either fair amount of morphological variation or are morphologically very rich the proposed method has been evaluated so far with four languages finnish swedish german and russian which show varying degrees of morphological complexity
omes omes existing measures for evaluating clustering results eg fmeasure have the limitation of overestimating cluster quality because they usually adopt the greedy matching between classes reference clusters and clusters system clusters to allow multiple classes to correspond to one same cluster which is in fact a locally optimal solution this paper proposes a new evaluation strategy to overcome the limitation of existing evaluation measures by using optimal matching in graph theory a weighted bipartite graph is built with classes and clusters as two disjoint sets of vertices and the edge weight between any class and any cluster is computed using a basic metric then the total weight of the optimal matching in the graph is acquired and we use it to evaluate the quality of the clusters the optimal matching allows only onetoone matching between classes and clusters and a globally optimal solution can be achieved a preliminary study is performed to demonstrate the effectiveness of the proposed evaluation strategy
revisiting the dependence language model for information retrieval revisiting the dependence language model for information retrieval in this paper we revisit the dependence language modelfor information retrieval proposed in and show that thismodel is deficient from a theoretical point of view we thenpropose a new model well founded theoretically for integratingdependencies between terms in the language modelthis new model is simpler yet more general than the oneproposed in and yields similar results in our experimentson both syntactic and semantic dependencies
quantify query ambiguity using odp metadata quantify query ambiguity using odp metadata query ambiguity prevents existing retrieval systems from returning reasonable results for every query as there is already lots of work done on resolving ambiguity vague queries could be handled using corresponding approaches separately if they can be identified in advance quantification of the degree of lack of ambiguity laysthe groundwork for the identification in this poster we propose such a measure using query topics based on the topic structure selected from the open directory project odp taxonomy we introduce clarity score to quantify the lack of ambiguity with respect to data sets constructed from the trec collections and the rank correlation test results demonstrate a strong positive association between the clarity scores and retrieval precisions for queries
combining errorcorrecting output codes and modelrefinement for text categorization combining errorcorrecting output codes and modelrefinement for text categorization in this work we explore the use of errorcorrecting output codes ecoc to enhance the performance of centroid text classifier the framework is to decompose one multiclass problem into multiple binary problems and then learn the individual binary classification problems by centroid classifier however this kind of decomposition incurs considerable bias for centroid classifier which results in noticeable degradation of performance to address this issue we use modelrefinement to adjust this socalled bias
useroriented text segmentation evaluation measure useroriented text segmentation evaluation measure the paper describes a user oriented performance evaluation measure for text segmentation experiments show that the proposed measure differentiates well between error distributions with varying user impact
story segmentation of broadcast news in arabic chinese and english using multiwindow features story segmentation of broadcast news in arabic chinese and english using multiwindow features the paper describes a maximum entropy based story segmentation system for arabic chinese and english in experiments with broadcast news data from tdt tdt and corpora collected in the darpa gale project we obtain a substantial performance gain using multiple overlapping windows for textbased features
recommending citations for academic papers recommending citations for academic papers we approach the problem of academic literature search by considering an unpublished manuscript as a query to a search system we use the text of previous literature as well as the citation graph that connects it to find relevant related material we evaluate our technique with manual and automatic evaluation methods and find an order of magnitude improvement in mean average precision as compared to a text similarity baseline
exploration of the tradeoff between effectiveness and efficiency for results merging in federated search exploration of the tradeoff between effectiveness and efficiency for results merging in federated search federated search is the task of retrieving relevant documents from different information resources one of the main research problems in federated search is to combine the results from different sources into a single ranked list recent work proposed a regression based method to download some documents from each ranked list of the different sources calculated comparable scores for the documents and estimated mapping functions that transform sourcespecific scores into comparable scores experiments have shown that downloading more documents improves the accuracy of results merging however downloading more documents increases the computation and communication costs
understanding the relationship of information need specificity to search query length understanding the relationship of information need specificity to search query length when searching peoples information needs flowthrough to expressing an information retrieval request posed to asearch engine we hypothesise that the degree of specificity of anir request might correspond to the length of a search query ourresults show a strong correlation between decreasing query lengthand increasing broadness or generality of the ir request we foundan average crossover point of specificity from broad to narrow of words in the query these results have implications for searchengines in responding to queries of differing lengths
an effective snippet generation method using the pseudo relevance feedback technique an effective snippet generation method using the pseudo relevance feedback technique a page or web snippet is document excerpts allowing a user to understand if a document is indeed relevant without accessing it this paper proposes an effective snippet generation method the pseudo relevance feedback technique and text summarization techniques are applied to salient sentences extraction for generating good quality snippets in the experimental results the proposed method showed much better performance than other methods including google and naver
probability ranking principle via optimal expected rank probability ranking principle via optimal expected rank this paper presents a new perspective of the probability ranking principle prp by defining retrieval effectiveness in terms of our novel expected rank measure of a set of documents for a particular query this perspective is based on preserving decision preferences and it imposes weaker conditions on prp than the utilitytheoretic perspective of prp
combining termbased and eventbased matching for question answering combining termbased and eventbased matching for question answering in question answering two main kinds of matching methods for finding answer sentences for a question are termbased approaches which are simple efficient effective and yield high recall and eventbased approaches that take syntactic and semantic information into account the latter often sacrifice recall for increased precision but actually capture the meaning of the events denoted by the textual units of a passage or sentence we propose a robust datadriven method that learns the mapping between questions and answers using logistic regression and show that combining termbased and eventbased approaches significantly outperforms the individual methods
confluence confluence we present confluence an enhancement to a desktop file search tool called confluence which extracts conceptual relationships between files by their temporal access patterns in the file system a limitation of a purely filebased approach is that as file operations are increasingly abstracted by applications their correlation to a users activity weakens and thereby reduces the applicability of their temporal patterns to deal with this problem we augment the file event stream with a stream of window focus events from the ui layer we present algorithms that analyze this new stream extracting the users task information which informs the existing confluence algorithms we present results and conclusions from a preliminary user study on confluence
estimating the value of automatic disambiguation estimating the value of automatic disambiguation a common motivation for personalised search systems is the ability to disambiguate queries based on some knowledge of a users interests an analysis of log files from three search providers covering a range of scenarios suggests that this sort of disambiguation would be of marginal use for more specialised providers but may be of use for wholeofweb search
a generic framework for machine transliteration a generic framework for machine transliteration note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
where to start reading a textual xml document where to start reading a textual xml document in structured information retrieval the aim is to exploit document structure to retrieve relevant components allowing the user to go straight to the relevant material this paper looks at the socalled best entry points beps which are intended to give the user the best starting point to access the relevant information in the document we examine the relationship between beps and relevant components in the inex ad hoc assessments our main findings are the following first although documents are short assessors often choose the best entry point some distance from the start of the document second many of the best entry points coincide with the first relevant character in relevant documents showing a strong relation between the bep and relevant text third we find browsing beps in articles with a single relevant passages and container beps or context beps in articles with more relevant passages
novelty detection using local context analysis novelty detection using local context analysis note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
intraassessor consistency in question answering intraassessor consistency in question answering in this paper we investigate the consistency of answer assessment in a complex question answering task examining features of assessor consistency types of answers and question type
towards robust query expansion towards robust query expansion we propose a languagemodelbased approach for addressing the performance robustness problem with respect to freeparameters values of pseudofeedbackbased queryexpansion methods given a query we create a set of language models representing different forms of its expansion by varying the parameters values of some expansion method then we select a single model using criteria originally proposed for evaluating the performance of using the original query or for deciding whether to employ expansion at all experimental results show that these criteria are highly effective in selecting relevance language models that are not only significantly more effective than poor performing ones but that also yield performance that is almost indistinguishable from that of manually optimized relevance models
automatic classification of web pages into bookmark categories automatic classification of web pages into bookmark categories we describe a technique to automatically classify a web page into an existing bookmark category to help a user to bookmark a page hyperbk compares a bagofwords representation of the page to descriptions of categories in the users bookmark file unlike default web browser dialog boxes in which the user may be presented with the category into which he or she saved the last bookmarked file hyperbk also offers the category most similar to the page being bookmarked the user can also opt to create a new category or save the page elsewhere in an evaluation the users preferred category was offered on average of the time
what emotions do news articles trigger in their readers what emotions do news articles trigger in their readers we study the classification of news articles into emotions they invoke in their readers our work differs from previous studies which focused on the classification of documents into their authors emotions instead of the readers we use various combinations of feature sets to find the best combination for identifying the emotional influences of news articles on readers
evaluating discoursebased answer extraction for whyquestion answering evaluating discoursebased answer extraction for whyquestion answering note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
topic segmentation using weighted lexical links wll topic segmentation using weighted lexical links wll this paper presents two new approaches of lexical chains for topic segmentation using weighted lexical chains wlc or weighted lexical links wll between repeated occurrences of lemmas along the text the main advantage of using these new approaches is the suppression of the empirical parameter called hiatus in lexical chain processing an evaluation according to the windowdiff measure on a large automatically built corpus shows slight improvements in wll compared to stateoftheart methods based on lexical chains
lexical analysis for modeling web query reformulation lexical analysis for modeling web query reformulation modeling web query reformulation processes is still an unsolved problem in this paper we argue that lexical analysis is highly beneficial for this purpose we propose to use the variation in query clarity as well as the partofspeech pattern transitions as indicators of users search actions experiments with a log of million queries showed our techniques to be more flexible than the current approaches while also providing us with interesting insights into users web behavioral patterns
bridging the digital divide bridging the digital divide for digital library and information retrieval technologies to provide solutions for bridging the digital divide in developing countries we need to understand the information access practices of remote and often poor communities in these countries we must understand the information needs of these communities and the best means to provide them access to relevant information to this end we investigated the current information access practices in an indian village
bordaconsensus bordaconsensus consensus clustering is the task of deriving a single labeling by applying a consensus function on a cluster ensemble this work introduces bordaconsensus a new consensus function for soft cluster ensembles based on the borda voting scheme in contrast to classic hard consensus functions that operate on labelings our proposal considers cluster membership information thus being able to tackle multiclass clustering problems initial small scale experiments reveal that compared to stateoftheart consensus functions bordaconsensus constitutes a good performance vs complexity tradeoff
a flexible retrieval system of shapes in binary images a flexible retrieval system of shapes in binary images this poster overviews the main characteristics of a flexible retrieval systems of shapes present in binary images and discusses some evaluation results the system applies multiple indexing criteria of the shapes synthesizing distinct characteristics such as global features of the objects contour fourier coefficients boundary irregularities multifractal spectrum presence of concavities and convexities on the boundary contour scale space distribution the system is flexible since it allows customizing the retrieval function to fit an application need the query is a binary image containing the desired shape and a set of parameters specifying the distinct importance of the shape characteristics that must be taken into account to evaluate the relevance of the retrieved shapes the retrieval function is then defined as a flexible multicriteria fusion function producing ranked results the evaluation experiments showed that this system can be suited to different retrieval purposes and that generally the combination of the distinct shape indexing criteria increases both recall and precision with respect to the application of any single indexing criterion alone
semantic text classification of disease reporting semantic text classification of disease reporting traditional text classification studied in the ir literature is mainly based on topics that is each class or category represents a particular topic eg sports politics or sciences however many realworld text classification problems require more refined classification based on some semantic aspects for example in a set of documents about a particular disease some documents may report the outbreak of the disease some may describe how to cure the disease some may discuss how to prevent the disease and yet some others may include all the above information to classify text at this semantic level the traditional bag of words model is no longer sufficient in this paper we report a text classification study at the semantic level and show that sentence semantic and structure features are very useful for such kind of classification our experimental results based on a disease outbreak dataset demonstrated the effectiveness of the proposed approach
evaluating relevant in context evaluating relevant in context the relevant in context retrieval task is document or article retrieval with a twist where not only the relevant articles should be retrieved but also the relevant information within each article captured by a set of xml elements should be correctly identified our main research question is how to evaluate the relevant in context task we propose a generalized average precision measure that meets two main requirements i the score reflects the ranked list of articles inherent in the result list and at the same time ii the score also reflects how well the retrieved information per article ie the set of elements corresponds to the relevant information the resulting measure was used at inex 
idf revisited idf revisited there have been a number of prior attempts to theoretically justify the effectiveness of the inverse document frequency idf those that take as their starting point robertson and sparck joness probabilistic model are based on strong or complex assumptions we show that a more intuitively plausible assumption suffices moreover the new assumption while conceptually very simple provides a solution to an estimation problem that had been deemed intractable by robertson and walker 
validity and power of ttest for comparing map and gmap validity and power of ttest for comparing map and gmap we examine the validity and power of the ttest wilcoxon test and sign test in determining whether or not the difference in performance between two ir systems is significant empirical tests conducted on subsets of the trec robust retrieval collection indicate that the pvalues computed by these tests for the difference in mean average precision map between two systems are very accurate fora wide range of sample sizes and significance estimates similarly these tests have good power with the ttest proving superior overall the ttest is also valid for comparing geometric mean average precision gmap exhibiting slightly superior accuracy and slightly inferior power than for mapcomparison
modelaveraged latent semantic indexing modelaveraged latent semantic indexing this poster introduces a novel approach to information retrieval that uses statistical model averaging to improve latent semantic indexing lsi instead of choosing a single dimensionality k for lsi we propose using several models of differing dimensionality to inform retrieval to manage this ensemble we weight each models contribution to an extent inversely proportional to its aic akaike information criterion thus each model contributes proportionally to its expected kullbackleibler divergence from the distribution that generated the data we present results on three standard ir test collections demonstrating significant improvement over both the traditional vector space model and singlemodel lsi
characterizing the value of personalizing search characterizing the value of personalizing search we investigate the diverse goals that people have when they issue the same query to a search engine and the ability of current search engines to address such diversity we quantify the potential value of personalizing search results based on this analysis great variance was found in the results that different individuals rated as relevant for the same query even when the same information goal was expressed our analysis suggests that while search engines do a good job of ranking results to maximize global happiness they do not do a very good job for specific individuals
improving retrieval accuracy by weighting document types with clickthrough data improving retrieval accuracy by weighting document types with clickthrough data for enterprise search there exists a relationship between work task and document type that can be used to refine search results in this poster we adapt the popular okapi bm scoring function to weight term frequency based on the relevance of a document type to a work task also we use click frequency for each tasktype pair to estimate a realistic weight using the wc collection from the trec enterprise track for evaluations our approach leads to significant improvements on search precision
protecting source privacy in federated search protecting source privacy in federated search many information sources contain information that can only be accessed through searchspecific search engines federated search provides search solutions of this type of hidden information that cannot be searched by conventional search engines in many scenarios of federated search such as the search among health care providers or among intelligence agencies an individual information source does not want to disclose the source of the search results to users or other sources therefore this paper proposes a twostep federated search protocol that protects the privacy of information sources as far as we know this is the first attempt to address the research problem of protecting source privacy in federated text search
applying ranking svm in query relaxation applying ranking svm in query relaxation we propose an approach qrrs query relaxative ranking svm that divides a ranking function into different relaxation steps so that only cheap features are used in ranking svm of early steps for query efficiency we show search quality in the approach is improved compared to conventional ranking svm
learning to rank collections learning to rank collections collection selection ranking collections according to user query is crucial in distributed search however few features are used to rank collections in the current collection selection methods while hundreds of features are exploited to rank web pages in web search the lack of features affects the efficiency of collection selection in distributed search in this paper we exploit some new features and learn to rank collections with them through svm and rankingsvm respectively experimental results show that our features are beneficial to collection selection and the learned ranking functions outperform the classical cori algorithm
videoreach videoreach this paper presents a novel online video recommendation system called videoreach which alleviates users efforts on finding the most relevant videos according to current viewings without a sufficient collection of user profiles as required in traditional recommenders in this system video recommendation is formulated as finding a list of relevant videos in terms of multimodal relevance ie textual visual and aural relevance and user clickthrough since different videos have different intraweights of relevance within an individual modality and interweights among different modalities we adopt relevance feedback to automatically find optimal weights by user clickthough as well as an attention fusion function to fuse multimodal relevance we use clips as the representative test videos which are searched by top queries from more than k online videos and report superior performance compared with an existing video site
modelling epistemic uncertainty in ir evaluation modelling epistemic uncertainty in ir evaluation modern information retrieval ir test collections violate the completeness assumption of the cranfield paradigm in order to maximise the available resources only a sample of documents ie the pool are judged for relevance by a human assessors the subsequent evaluation protocol does not make any distinctions between assessed or unassesseddocuments as documents that are not in the pool are assumedto be not relevant for the topic this is beneficial from a practical point of view as the relative performance can be compared with confidence if the experimental conditions are fair for all systems however given the incompleteness of relevance assessments two forms of uncertainty emerge during evaluation the first is aleatory uncertainty which refers to variation in system performance across the topic set which is often addressed through the use of statistical significance tests the second form of uncertainty is epistemic which refers to the amount of knowledge or ignorance we have about the estimate of a systems performance epistemic uncertainty is a consequence of incompleteness and is not addressed by the current evaluation protocol in this study we present a first attempt at modelling both aleatory and epistemic uncertainty associatedwith ir evaluation we aim to account for both the variability associated with system performance and the amount of knowledge known about the performance estimate
on the importance of preserving the partorder in shape retrieval on the importance of preserving the partorder in shape retrieval this paper discusses the importance of partorderpreservation in shape matching a part descriptor is introduced that supports both preserving and abandoning the order of parts the evaluation shows that retrieval results are improved by almost if the original ordering is preserved
the relationship between ir effectiveness measures and user satisfaction the relationship between ir effectiveness measures and user satisfaction this paper presents an experimental study of users assessing the quality of google web search results in particular we look at how users satisfaction correlates with the effectiveness of google as quantified by ir measures such as precision and the suite of cumulative gain measures cg dcg ndcg results indicate strong correlation between users satisfaction cg and precision moderate correlation with dcg with perhaps surprisingly negligible correlation with ndcg the reasons for the low correlation with ndcg are examined
a multicriteria contentbased filtering system a multicriteria contentbased filtering system in this paper we present a novel filtering system based on a new model which reshapes the aims of contentbased filtering the filtering system has been developed within the ec project peng aimed at providing news professionals such as journalists with a system supporting both filtering and retrieval capabilities in particular we suggest that in tackling the problem of information overload it is necessary for filtering systems to take into account multiple aspects of incoming documents in order to estimate their relevance to a users profile and in order to help users better understand documents as distinct from solely attempting to either select relevant material from a stream or block inappropriate material aiming to so this a filtering model based on multiple criteria has been defined based on the ideas gleamed in the project requirements stage the filtering model is briefly described in this paper
boosting static pruning of inverted files boosting static pruning of inverted files this paper revisits the static termbased pruning technique presented in carmel et al sigir for adhoc retrieval addressing different issues concerning its algorithmic design not yet taken into account although the original technique is able to retain precision when a considerable part of the inverted file is removed we show that it is possible to improve precision in some scenarios if some key design features are properly selected
resource monitoring in information extraction resource monitoring in information extraction it is often argued that in information extraction ie certain machine learning ml approaches save development time over others or that certain ml methods eg active learning require less training data than others thus saving development cost however such development cost claims are not normally backed up by controlled studies which show that such development cost savings actually occur this situation in language engineering is contrasted with software engineering in general where a lot of studies investigating system development cost have been carried out we argue for the need of controlled studies that measure actual system development time in language engineering to this end we carry out an experiment in resource monitoring for an ie task three named entity taggers for the same surprise domain are developed in parallel using competing methods their human development time is accounted forusing a logging facilitywe report development cost results for parallel implementations of a named entity tagger and present a breakdown of the development time for the three alternative methods we are not aware of detailed previous parallel studies that detail how system development time is spent when creating a named entity tagger
the diligent framework for distributed information retrieval the diligent framework for distributed information retrieval note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
varying approaches to topical web query classification varying approaches to topical web query classification topical classification of web queries has drawn recent interest because of the promise it offers in improving retrieval effectiveness and efficiency however much of this promise depends on whether classification is performed before or after the query is used to retrieve documents we examine two previously unaddressed issues in query classification pre versus postretrieval classification effectiveness and the effect of training explicitly from classified queries versus bridging a classifier trained using a document taxonomy bridging classifiers map the categories of a document taxonomy onto those of a query classification problem to provide sufficient training data we find that training classifiers explicitly from manually classified queries outperforms the bridged classifier by in f score also a preretrieval classifier using only the query terms performs merely worse than the bridged classifier which requires snippets from retrieved documents
a comparison of pooled and sampled relevance judgments a comparison of pooled and sampled relevance judgments test collections are most useful when they are reusable that is when they can be reliably used to rank systems that did not contribute to the pools pooled relevance judgments for very large collections may not be reusable for two easons they will be very sparse and not sufficiently complete and they may be biased in the sense that theywill unfairly rank some class of systems the trec terabyte track judged both a pool and a deep random sample in order to measure the effects of sparseness and bias
clustering short texts using wikipedia clustering short texts using wikipedia subscribers to the popular news or blog feeds rssatom often face the problem of information overload as these feed sources usually deliver large number of items periodically one solution to this problem could be clustering similar items in the feed reader to make the information more manageable for a user clustering items at the feed reader end is a challenging task as usually only a small part of the actual article is received through the feed in this paper we propose a method of improving the accuracy of clustering short texts by enriching their representation with additional features from wikipedia empirical results indicate that this enriched representation of text items can substantially improve the clustering accuracy when compared to the conventional bag of words representation
estimating collection size with logistic regression estimating collection size with logistic regression collection size is an important feature to represent the content summaries of a collection and plays a vital role in collection selection for distributed search in uncooperative environments collection size estimation algorithms are adopted to estimate the sizes of collections with their search interfaces this paper proposes heterogeneous capture hc algorithm in which the capture probabilities of documents are modeled with logistic regression with heterogeneous capture probabilities hc algorithm estimates collection size through conditional maximum likelihood experimental results on real web data show that our hc algorithm outperforms both multiple capturerecapture and capture history algorithms
selection and ranking of text from highly imperfect transcripts for retrieval of video content selection and ranking of text from highly imperfect transcripts for retrieval of video content in the domain of video content retrieval we present an approach for selecting words and phrases from highly imperfect automatically generated transcripts extracted terms are ranked according to their descriptiveness and presented to the user in a multimedia browser interface we use sense querying from the wordnet lexical database for our method of text selection and ranking evaluation of video summarization tasks from users shows that the method of ranking and emphasizing terms according to descriptiveness results in higher accuracy responses in less time compared to the baseline of no ranking
enhancing patent retrieval by citation analysis enhancing patent retrieval by citation analysis this paper proposes a method to combine textbased and citationbased retrieval methods in the invalidity patent search using the ntcir test collection including eight years of uspto patents we show the effectiveness of our method experimentally
mrf based approach for sentence retrieval mrf based approach for sentence retrieval this poster focuses on the study of term context dependence in the application of sentence retrieval based on markov random field mrf three forms of dependence among query terms are considered under different assumptions of term dependence relationship three feature functions are defined with the purpose to utilize association features between query terms in sentence to evaluate the relevance of sentence experimental results have proven the efficiency of the proposed retrieval models in improving the performance of sentence retrieval
improving weak adhoc queries using wikipedia asexternal corpus improving weak adhoc queries using wikipedia asexternal corpus in an adhoc retrieval task the query is usually short and the user expects to find the relevant documents in the first several result pages we explored the possibilities of using wikipedias articles as an external corpus to expand adhoc queries results show promising improvements over measures that emphasize on weak queries
finegrained named entity recognition and relation extraction for question answering finegrained named entity recognition and relation extraction for question answering note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
world knowledge in broadcoverage information filtering world knowledge in broadcoverage information filtering note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
the influence of basic tokenization on biomedical document retrieval the influence of basic tokenization on biomedical document retrieval tokenization is a fundamental preprocessing step in information retrieval systems in which text is turned into index terms this paper quantifies and compares the influence of various simple tokenization techniques on document retrieval effectiveness in two domains biomedicine and news as expected biomedical retrieval is more sensitive to small changes in the tokenization method the tokenization strategy can make the difference between a mediocre and well performing ir system especially in the biomedical domain
using clustering to enhance text classification using clustering to enhance text classification this paper addresses the problem of learning to classify textsby exploiting information derived from clustering both training and testing sets the incorporation of knowledge resulting from clustering into the feature space representation of the texts is expected to boost the performance of a classifier experiments conducted on several widely used datasets demonstrate the effectiveness of the proposed algorithm especially for small training sets
a factopinion classifier for news articles a factopinion classifier for news articles many online newsblog aggregators like google yahoo and msn allow users to browsesearch many hundreds of news sources this results in dozens often hundreds of stories about the same event while the news aggregators cluster these stories allowing the user to efficiently scan the major news items at any given time they do not currently allow alternative browsing mechanisms within the clusters furthermore their intracluster ranking mechanisms are often based on a notion of authoritypopularity of the source in many cases this leads to the classic power law phenomenon the popular storiessources are the ones that are already popularauthoritative thus reinforcing one dominant viewpoint ideally these aggregators would exploit the availability of the tremendous number of sources to identify the various dominant threads or viewpoints about a story and highlight these threads for the users this paper presents an initial limited approach to such an interface it classifies articles into two categories fact and opinion we show that the combination of i a classifier trained on a small k training set of editorialsreports and ii an interactive user interface that ameliorates classification errors by reordering the presentation can be effective in highlighting different underlying viewpoints in a storycluster we briefly discuss the classifier used here the training set and the ui and report on some initial anecdotal user feedback and evaluation
matching resumes and jobs based on relevance models matching resumes and jobs based on relevance models we investigate the difficult problem of matching semistructured resumes and jobs in a large scale realworld collection we compare standard approaches to structured relevance models srm an extensionof relevancebased language model for modeling and retrieving semistructured documents preliminary experiments show that the srm approach achieved promising performance and performed better than typical unstructured relevance models
the utility of linguistic rules in opinion mining the utility of linguistic rules in opinion mining online product reviews are one of the important opinion sources on the web this paper studies the problem of determining the semantic orientations positive or negative of opinions expressed on product features in reviews most existing approaches use a set of opinion words for the purpose however the semantic orientations of many words are context dependent in this paper we propose to use some linguistic rules to deal with the problem together with a new opinion aggregation function extensive experiments show that these rules and the function are highly effective a system called opinion observer has also been built
a comparison of sentence retrieval techniques a comparison of sentence retrieval techniques identifying redundant information in sentences is useful for several applications such as summarization document provenance detecting text reuse and novelty detection the task of identifying redundant information in sentences is defined as follows given a query sentence the task is to retrieve sentences from a given collection that express all or some subset of the information present in the query sentence sentence retrieval techniques rank sentences based on some measure of their similarity to a query the effectiveness of such techniques depends on the similarity measure used to rank sentences an effective retrieval model should be able to handle low word overlap between query and candidate sentences and go beyond just word overlap simple language modeling techniques like query likelihood retrieval have outperformed tfidf and word overlap based methods for ranking sentences in this paper we compare the performance of sentence retrieval using different language modeling techniques for the problem of identifying redundant information
highdimensional visual vocabularies for image retrieval highdimensional visual vocabularies for image retrieval in this paper we formulate image retrieval by text query as a vector space classification problem this is achieved by creating a highdimensional visual vocabulary that represents the image documents in great detail we show how the representation of these image documents enables the application of well known text retrieval techniques such as rocchio tfidf and nave bayes to the semantic image retrieval problem we tested these methods on a corel images subset and achieve stateoftheart retrieval performance using the proposed methods
a web page topic segmentation algorithm based on visual criteria and content layout a web page topic segmentation algorithm based on visual criteria and content layout this paper presents experiments using an algorithm of web page topic segmentation that show significant precision improvement in the retrieval of documents issued from the web track corpus of trec instead of processing the whole document a web page is segmented into different semantic blocks according to visual criteria such as horizontal lines colors and structural tags such as headings lthgtlthgt paragraph ltpgt we conclude that combining visual and content layout criteria gives the best results for increasing the precision the ranking of the page is calculated for relevant segments of pages resulting from the segmentation algorithm
document clustering document clustering clustering algorithms have been widely used in information retrieval applications however it is difficult to define an objective best result this article analyzes some document clustering algorithms and illustrates that they are equivalent to the optimization problem of some global functions experiments show their good performance but there are still counterexamples where they fail to return the optimal solution we argue that montecarlo algorithms in the global optimization framework have the potential to find better solutions than traditional clustering and they are able to handle more complex structures
finding similar experts finding similar experts the task of finding people who are experts on a topic has recently received increased attention we introduce a different expert finding task for which a small number of example experts is given instead of a natural language query and the systems task is to return similar experts we define compare and evaluate a number of ways of representing experts and investigate how the size of theinitial example set affects performance we show that morefinegrained representations of candidates result in higher performance and larger sample sets as input lead to improved precision
active learning for class imbalance problem active learning for class imbalance problem the class imbalance problem has been known to hinder the learning performance of classification algorithms various realworld classification tasks such as text categorization suffer from this phenomenon we demonstrate that active learning is capable of solving the problem
strategies for retrieving plagiarized documents strategies for retrieving plagiarized documents for the identification of plagiarized passages in large document collections we present retrieval strategies which rely on stochastic sampling and chunk indexes using the entire wikipedia corpus we compile ngram indexes and compare them to a new kind of fingerprint index in a plagiarism analysis use case our index provides an analysis speedup by factor and is an order of magnitude smaller while being equivalent in terms of precision and recall
generative modeling of persons and documents for expert search generative modeling of persons and documents for expert search in this paper we address the task of automatically finding an expert within the organization known as the expert search problem we present the theoreticallybased probabilistic algorithm which models retrieved documents as mixtures of expert candidate language models experiments show that our approach outperforms existing theoretically sound solutions
random walk term weighting for information retrieval random walk term weighting for information retrieval we present a way of estimating term weights for information retrieval ir using term cooccurrence as a measure of dependency between termswe use the random walk graphbased ranking algorithm on a graph that encodes terms and cooccurrence dependencies in text from which we derive term weights that represent a quantification of how a term contributes to its context evaluation on two trec collections and topics shows that the random walkbased term weights perform at least comparably to the traditional tfidf term weighting while they outperform it when the distance between cooccurring terms is between and terms
comparing query logs and pseudorelevance feedbackfor websearch query refinement comparing query logs and pseudorelevance feedbackfor websearch query refinement query logs and pseudorelevance feedback prf offer ways in which terms to refine web searchers queries can be selected offered to searchers and used to improve search effectiveness in this poster we present a study of these techniques that aims to characterize the degree of similarity between them across a set of test queries and the same set broken out by query type the results suggest that i similarity increases with the amount of evidence provided to the prf algorithm ii similarity is higherwhen titlessnippets are used for prf than fulltext and iii similarity is higher for navigational than informational queries the findings have implications for the combined usage of query logs and prf in generating query refinement alternatives
automatic extension of nonenglish wordnets automatic extension of nonenglish wordnets note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
first experiments searching spontaneous czech speech first experiments searching spontaneous czech speech note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
power and bias of subset pooling strategies power and bias of subset pooling strategies we define a method to estimate the random and systematic errors resulting from incomplete relevance assessmentsmean average precision map computed over a large number of topics with a shallow assessment pool substantially outperforms for the same adjudication effort map computed over fewer topics with deeper pools and pk computed with pools of the same depth movetofront poolingpreviously reported to yield substantially better rank correlation yields similar power and lower bias compared tofixeddepth pooling
problems with kendalls tau problems with kendalls tau this poster describes a potential problem with a relatively well used measure in information retrieval research kendalls tau rank correlation coefficient the coefficient is best known for its use in determining the similarity of test collections when ranking sets of retrieval runs threshold values for the coefficient have been defined and used in a number of published studies in information retrieval however this poster presents results showing that basing decisions on such thresholds is not as reliableas has been assumed
opinion holder extraction from author and authority viewpoints opinion holder extraction from author and authority viewpoints opinion holder extraction research is important for discriminating between opinions that are viewed from different perspectives in this paper we describe our experience of participation in the ntcir opinion analysis pilot task by focusing on opinion holder extraction results in japanese and english our approach to opinion holder extraction was based on the discrimination between author and authority viewpoints in opinionated sentences and the evaluation results were fair with respect to the japanese documents
incorporating term dependency in the dfr framework incorporating term dependency in the dfr framework term dependency or cooccurrence has been studied in language modelling for instance by metzler croft who showed that retrieval performance could be significantlyenhanced using term dependency information in this work weshow how term dependency can be modelled within the divergence from randomness dfr framework we evaluate our term dependency model on the two adhoc retrieval tasks using the trec gov terabyte collection furthermore we examine the effect of varying the term dependency window size on the retrieval performance of the proposed model our experiments show that term dependency can indeed besuccessfully incorporated within the dfr framework
hits on question answer portals hits on question answer portals questionanswer portals such as naver and yahoo answers are growing in popularity however despite the increased popularity the quality of answers is uneven and while some users usually provide good answers many others often provide bad answers hence estimating the authority or the expected quality of users is a crucial task for this emerging domain with potential applications to answer ranking and to incentive mechanism design we adapt a powerful link analysis methodology from the web domain as a first step towards estimating authority in question answer portals our experimental results over more than million answers from yahoo answers are promising and warrant further exploration along the lines outlined in this poster
heads and tails heads and tails a large fraction of queries submitted to web search enginesoccur very infrequently we describe search log studiesaimed at elucidating behaviors associated with rare andcommon queries we present several analyses and discussresearch directions
dimensionality reduction for dimensionspecific search dimensionality reduction for dimensionspecific search dimensionality reduction plays an important role in efficient similarity search which is often based on knearest neighbor knn queries over a highdimensional feature space in this paper we introduce a novel type of knn query namely conditional knn cknn which considers dimensionspecific constraint in addition to the interpoint distances however existing dimensionality reduction methods are not applicable to this new type of queries we propose a novel meanstd standard deviation guided dimensionality reduction msdr to support a pruning based efficient cknn query processing strategy our preliminary experimental results on d protein structure data demonstrate that the msdr method is promising
an effective method for finding best entry points in semistructured documents an effective method for finding best entry points in semistructured documents focused structured document retrieval employs the concept of best entry point bep which is intended to provide optimal startingpoint from which users can browse to relevant document components in this paper we describe and evaluate a method for finding beps in xml documents experiments conducted within the framework of inex evaluation campaign on the wikipedia xml collection shown the effectiveness of the proposed approach
query rewriting using active learning for sponsored search query rewriting using active learning for sponsored search sponsored search is a major revenue source for search companies web searchers can issue any queries while advertisement keywords are limited query rewriting technique effectively matches user queries with relevant advertisement keywords thus increases the amount of web advertisements available the match relevance is critical for clicks in this study we aim to improve query rewriting relevance for this purpose we use an active learning algorithm called transductive experimental design to select the most informative samples to train the query rewriting relevance model experiments show that this approach significantly improves model accuracy and rewriting relevance
an analysis of peertopeer filesharing system queries an analysis of peertopeer filesharing system queries many studies focus on the web but yet few focus on peertopeer filesharing system queries despite their massive scale in terms of internet traffic we analyzed several million queries collected on the gnutella network and differentiated our findings from those of web queries
investigating the relevance of sponsored results for web ecommerce queries investigating the relevance of sponsored results for web ecommerce queries are sponsored links the primary business model for web search engines providing web consumers with relevant results this research addresses this issue by investigating the relevance of sponsored and nonsponsored links for ecommerce queries from the major search engines the results show that average relevance ratings for sponsored and nonsponsored links are virtually the same although the relevance ratings for sponsored links are statistically higher we used ecommerce queries and retrieved links for these queries from three major web search engines google msn and yahoo we present the implications for web search engines and sponsored search as a longterm business model as well as a mechanism for finding relevant information for searchers
viewing online searching within a learning paradigm viewing online searching within a learning paradigm in this research we investigate whether one can model online searching as a learning paradigm we examined the searching characteristics of participants engaged in searching tasks we classified the searching tasks according to anderson and krathwohls taxonomy an updated version of blooms taxonomy anderson and krathwohl is a six level categorization of cognitive learning research results show that applying takes the most searching effort as measured by queries per session and specific topics searched per sessions the categories of remembering and understanding which are lowerorder learning levels exhibit searching characteristics similar to the higher order categories of evaluating and creating it seems that searchers rely primarily on their internal knowledge and use searching primarily as fact checking and verification when engaged in evaluating and creating implications are that the commonly held notions of web searchers having simple information goals may not be correct we discuss the implications for web searching including designing interfaces to support exploration and alternate views
more efficient parallel computation of pagerank more efficient parallel computation of pagerank note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
using similarity links as shortcuts to relevant web pages using similarity links as shortcuts to relevant web pages successful navigation from a relevant web page to other relevant pages depends on the page linking to other relevant pages we measured the distance to travel from relevant page to relevant page and found a bimodal distribution of distances peaking at and hops in an attempt to make it easier to navigate among relevant pages we added content similarity links to pages with these additional links significantly more relevant documents were close to each other a browser plugin or other tool that provides links to pages similar to a given page should increase the ability of web users to find relevant pages via navigation
fast exact maximum likelihood estimation for mixture of language models fast exact maximum likelihood estimation for mixture of language models a common language modeling approach assumes the data d is generated from a mixture of several language models em algorithm is usually used to find the maximum likelihood estimation of one unknown mixture component given the mixture weights and the other language models in this paper we provide an efficient algorithm of ok complexity to find the exact solution where k is the number of words occurred at least once in d another merit is that the probabilities of many words are exactly zeros which means that the mixture language model also serves as a feature selection technique
timedtextrank timedtextrank graphranking based algorithms eg textrank have been proposed for multidocument summarization in recent years however these algorithms miss an important dimension the temporal dimension for summarizing evolving topics for an evolving topic recent documents are usually more important than earlier documents because recent documents contain much more novel information than earlier documents and a noveltyoriented summary should be more appropriate to reflect the changing topic we propose the timedtextrank algorithm to make use of the temporal information of documents based on the graphranking based algorithm a preliminary study is performed to demonstrate the effectiveness of the proposed timedtextrank algorithm for dynamic multidocument summarization
winnowing wheat from the chaff winnowing wheat from the chaff the web today includes many pages intended to deceive search engines and attain an unwarranted result ranking since the links among web pages are used to calculate authority ranking systems would benefit from knowing which pages contain content to be trusted and which do not we propose and compare various trust propagation methods to estimate the trustworthiness of each page we find that a nontrustpreserving propagation method is able to achieve close to a fifty percent improvement over trustrank in separating spam from nonspam pages
feature engineering for mobile sms spam filtering feature engineering for mobile sms spam filtering mobile spam in an increasing threat that may be addressed using filtering systems like those employed against email spam we believe that email filtering techniques require some adaptation to reach good levels of performance on sms spam especially regarding message representation in order to test this assumption we have performed experiments on sms filtering using top performing email spam filters on mobile spam messages using a suitable feature representation with results supporting our hypothesis
ranking by community relevance ranking by community relevance a web page may be relevant to multiple topics even when nominally on a single topic the page may attract attention and thus links from multiple communities instead of indiscriminately summing the authority provided by all pages we decompose a web page into separate subnodes with respect to each community pointing to it by considering the relevance of these communities we are able to better model the queryspecific reputation for each potential result we apply a total of queries to the trec gov dataset to demonstrate how the use of community relevance can improve ranking performance
query suggestion based on user landing pages query suggestion based on user landing pages this poster investigates a novel query suggestion technique that selects query refinements through a combination of many users postquery navigation patterns and the query logs of a large search engine we compare this technique which uses the queries that retrieve in the topranked search results places where searchers end up after postquery browsing ie the landing pages with an approach based on query refinements from user search sessions extracted from query logs our findings demonstrate the effectiveness of using landing pages for the direct generation of query suggestions as well as the complementary nature of the suggestions it generates with regard to traditional query log based refinement methodologies
making mind and machine meet making mind and machine meet using saracevics relevance types we explore approaches to combining algorithm and cognitive relevance in a term relevance feedback scenario data collected from users who provided relevance feedback about terms suggested by a system for trec hard topics are used the former type of feedback is considered as cognitive relevance and the latter type is considered as algorithm relevance we construct retrieval runs using these two types of relevance feedback and experiment with ways of combining them with simple boolean operators results show minimal differences in performance with respect to the different techniques
using collaborative queries to improve retrieval for difficult topics using collaborative queries to improve retrieval for difficult topics we describe a preliminary analysis of queries created by users for topics from the trec robust track our goal was to explore the potential benefits of using queries created by multiple users on retrieval performance for difficult topics we first examine the overlap in users queries and the overlap in results with respect to different queries for the same topic we then explore the potential benefits of combining users queries in various ways our results provide some evidence that having access to multiple users queries can improve retrieval for individual searchers and for difficult topics
retrieval of discussions from enterprise mailing lists retrieval of discussions from enterprise mailing lists mailing list archives in an enterprise are a valuable source for employees to dig into the past proceedings of the organization that could be relevant to their present task going through the proceedings of discussions about certain topics might be cumbersome and regular search techniques might not work in this context due to the genre that the documents belong to in this paper we propose methods based on theory of subjectivity to retrieve email messages that could contain argumentative discussions about the topic that the user is interested in
effects of highly agreed documents in relevancy prediction effects of highly agreed documents in relevancy prediction finding significant contextual features is a challenging task in the development of interactive information retrieval ir systems this paper investigated a simple method to facilitate such a task by looking at aggregated relevance judgements of retrieved documents our study suggested that the agreement on relevance judgements can indicate the effectiveness of retrieved documents as the source of significant features the effect of highly agreed documents gives us practical implication for the design of adaptive search models in interactive ir systems
detecting word substitutions detecting word substitutions those who want to conceal the content of their communications can do so by replacing words that might trigger attention for example instead of writing the bomb is in position a terrorist may chose to write the flower is in position the substituted sentence would sound a bit odd for a human reader and it has been shown in prior research that such oddity is detectable by text mining approaches however the importance of each component in the suggested oddity detection approach has not been thoroughly investigated also the approach has not been compared with such an obvious candidate for the task as hidden markov models hmm in this work we explore further oddity detection algorithms reported earlier specifically those based on pointwise mutual information pmi and hidden markov models hmm
workload sampling for enterprise search evaluation workload sampling for enterprise search evaluation in real world use of test collection methods it is essential that the query test set be representative of the work load expected in the actual application using a random sample of queries from a media companys query log as a gold standard test set we demonstrate that biases in sitemapderived and top n query sets can lead to significant perturbations in engine rankings and big differences in estimated performance levels
document layout and color driven image retrieval document layout and color driven image retrieval this paper presents a contribution to image indexing applied to the document creation task the presented method ranks a set of photographs based on how well they aesthetically work within a predefined document color harmony document visual balance and image quality are taken into consideration a user study conducted on people with a range of expertise in document creation helped gather the right visual features to consider by the algorithm this shows some benefits for the traditional document creation task as well as for the case of everchanging web page banner colors and layout
largescale clusterbased retrieval experiments on turkish texts largescale clusterbased retrieval experiments on turkish texts we present clusterbased retrieval cbr experiments on the largest available turkish document collection our experiments evaluate retrieval effectiveness and efficiency on both an automatically generated clustering structure and a manual classification of documents in particular we compare cbr effectiveness with fulltext search fs and evaluate several implementation alternatives for cbr our findings reveal that cbr yields comparable effectiveness figures with fs furthermore by using a specifically tailored clusterskipping inverted index we significantly improve inmemory query processing efficiency of cbr in comparison to other traditional cbr techniques and even fs
improving active learning recall via disjunctive boolean constraints improving active learning recall via disjunctive boolean constraints active learning efficiently hones in on the decision boundary between relevant and irrelevant documents but in the process can miss entire clusters of relevant documents yielding classifiers with low recall in this paper we propose a method to increase active learning recall by constraining sampling to a document subset rich in relevant examples
creativity support creativity support we are developing support for creativity in learning through information discovery and exploratory search users engage in creative tasks such as inventing new products and services the system supports evolving information needs it gathers and presents relevant information visually using images and text users are able to search browse and explore results from multiple queries and interact with information elements by manipulating design and expressing interest a field study was conducted to evaluate the system in an undergraduate class the results demonstrated the efficacy of our system for developing creative ideas exposure to diverse information in visual and interactive forms is shown to support students engaged in invention tasks
mqx mqx note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
iskodor iskodor iskodor integrates personal collections peer search and centralized search services user modeling in iskodor fills three roles discovery of sites with suitable information stores contextbased query interpretation and automatic profilebased filtering of new information explanation and control are achieved through graphical depiction of sources explicit feedback regard ingutility and explicit control over peer association behavior and information sharing
babel babel note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
xsite xsite professionals in the workplace need highprecision search tools capable of retrieving information that is useful and appropriate to the task at hand one approach to identifying content which is not only relevant but also useful is to make use of the task context of the search we present xsite an enterprise search engine for the software engineering domain that exploits relationships between users tasks and document genres in the collection to improve retrieval precision
the wild thing goes local the wild thing goes local suppose you are on a mobile device with no keyboard eg a cell phone and you want to perform a near me search where is the nearest pizza how do you enter queries quickly t the wild thing encourages users to enter patterns with implicit and explicit wild cards regular expressions the search engine uses microsoft local live logs to find the most likely queries for a particular location for example is shorthand for the regular expression pqrs mno which matches post office in many places but space needle in seattle some queries are more local than others pizza is likely everywhere whereas boeing company is very likely in seattle and chicago moderately likely nearby and somewhat likely elsewhere smoothing is important not every query is observed everywhere
radio oranje radio oranje the radio oranje demonstrator shows an attractive multimedia user experience in the cultural heritage domain based on a collection of monomedia audio documents it supports online search and browsing of the collection using indexing techniques specialized content visualizations and a related photo database
mobile interface of the memoria project mobile interface of the memoria project this project develops tools to manage personal memories that include a multimedia retrieval system and user interfaces for different devices this paper and demonstration presents the mobile interface which allows browsing retrieving and taking pictures that are automatically annotated with gps data and audio information the multimedia retrieval system uses multimodal information visual content gps metadata and audio information the interface was evaluated in a cultural heritage site
expose expose note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
text categorization for streams text categorization for streams we describe a novel system for evaluating and performing streambased text categorization streambased text categorization considers the text being categorized as a stream of symbols which differs from the traditional featurebased approach which relies on extracting features from the text the system implements characterbased languages modelsspecifically models based on the ppm text compression schemeas well as countbased measures such as rmeasure and cmeasure use of the system demonstrates that all of these techniques outperform svm a featurebased classifier at streamrelated classification tasks such as authorship ascription
search results using timeline visualizations search results using timeline visualizations note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
wikipedia in the pocket wikipedia in the pocket we develop and implement a new indexing technology which allows us to use complete and possibly very large documents as queries while having a retrieval performance comparable to a standard term query our approach aims at retrieval tasks such as near duplicate detection and high similarity search to demonstrate the performance of our technology we have compiled the search index wikipedia in the pocket which contains about million english and german wikipedia articles this indexalong with a search interfacefits on a conventional cd gigabyte the ingredients of our indexing technology are similarity hashing and minimal perfect hashing
nexus nexus note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
geographic ranking for a local search engine geographic ranking for a local search engine traditional ranking schemes of the relevance of a web page to a user query in a search engine are less appropriate when the search term contains geographic information often geographic entities such as addresses city names and location names appear only once or twice in a web page and are typically not in a heading or larger font consequently an alternative ranking approach to the traditional weighted tfidf relevance ranking is need further if a web site contains a geographic entity it is often the case that its in and outneighbours do not refer to the same entity although they may refer to other geographic entities we present a local search engine that applies a novel ranking algorithm suitable for ranking web pages with geographic content we describe its major components geographic ranking focused crawling geographic extractor and the related websites feature
focused ranking in a vertical search engine focused ranking in a vertical search engine since the debut of pagerank and hits hyperlinkinduced web document ranking has come a long way the web has become increasingly vast and topically diverse such vastness has led many into the area of topicsensitive ranking and its variants we address the high dimensionality of the web by providing tools for focused search a focused search engine is one which seeks coverage over a subset of topics of the web and presents users with relevant search results in a known domain this demonstration will introduce readers to the genieknowscom vertical search engine
a doityourself evaluation service for music information retrieval systems a doityourself evaluation service for music information retrieval systems note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
irtoolbox irtoolbox note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
beyond classical measures beyond classical measures this research explores the relationship between information retrieval ir systems effectiveness and users performance accuracy and speed and their satisfaction with the retrieved results precision of the results completeness of the results and overall system success previous studies have concluded that improvements in ir systems based on increase in ir effectiveness measures do not reflect on improvement in users performance this work aims at exploiting factors that can possibly be considered as confounding variables in interactive information retrieval iir evaluation in this research we look at substantive approaches to evaluate iir systems we aim to build an interactive evaluation framework that brings together aspects of systems effectiveness and users performance and satisfaction this research also involves developing methods for capturing users satisfaction with the retrieved results of ir systems as well as examination how users assess their own performance in task completion furthermore we are also interested in identifying evaluation measures which are used in batch mode noninteractive experiment but correlate well in interactive ir system thus by the end of this research we hope to develop a valid and reliable metrics for iir evaluation
people search in the enterprise people search in the enterprise note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
attentionbased information retrieval attentionbased information retrieval in the proposed phd thesis it will be examined how attention data from the user especially generated by an eye tracker can be exploited in order to enhance and personalize information retrieval methods
a summarisation logic for structured documents a summarisation logic for structured documents the logical approach to information retrieval tries to model the relevance of a document given a query as the logical implication between documents and queries in early work van rijsbergen states that the retrieval status value of a document given a query is proportional to the degree of implication between a document and a query based on this several probabilistic logics for information retrieval have been conceived which add an additional layer of abstraction to the information retrieval task probabilistic models for the retrieval of documents are expressed in those logics rather than implemented directly
informationbehaviour modeling with external cues informationbehaviour modeling with external cues much of human activity defines an information context we awaken start work and hold meetings at roughly the same time every day and retrieve the same information items day planners itineraries schedules agendas reports menus web pages etc for many of these activities information retrieval systems in general lack sensitivity to such recurrent context requiring users to remember and reenter search cues for objects regardless of how regularly or consistently the objects are used and to develop adhoc storage strategies
fuzzy temporal and spatial reasoning for intelligent information retrieval fuzzy temporal and spatial reasoning for intelligent information retrieval temporal and spatial information in text documents is often expressed in a qualitative way moreover both are frequently affected by vagueness calling for appropriate extensions of traditional frameworks for qualitative reasoning about time and space our research aims at defining such extensions based on fuzzy set theory and applying the resulting frameworks to two important kinds of intelligent information retrieval viz temporal question answering and geographic information retrieval
paragraph retrieval for whyquestion answering paragraph retrieval for whyquestion answering note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
global resources for peertopeer text retrieval global resources for peertopeer text retrieval the thesis presented in this paper tackles selected issues in unstructured peertopeer information retrieval ppir systems using world knowledge for solving ppir problems a first part uses socalled reference corpora for estimating global term weights such as idf instead of sampling them from the distributed collection a second part of the work will be dedicated to the question of query routing in unstructured ppir systems using peer resource descriptions and world knowledge for query expansion
automatic querytime generation of retrieval expert coefficients for multimedia retrieval automatic querytime generation of retrieval expert coefficients for multimedia retrieval contentbased multimedia information retrieval can be defined as the task of matching a multimodal information need against various components of a multimedia corpus and retrieving relevant elements generally the matching and retrieval takes place across multiple features which can either be visual or audio or can be highlevel or lowlevel and each of which can be seen to be an independent retrieval expert the task of answering a query can thus be formulated as a data fusion problem depending on the query each expert may perform differently and so retrieval coefficients can be used to weight each expert to increase overall performance previous approaches to expert coefficient generation have included queryindependent coefficients identification of queryclasses and machine learning methods to name a few
delighting chinese users delighting chinese users google entered china market as a latecomer in late with no local employees an inadequate product line and small market share this talk will discuss google chinas efforts to build up a team learn about local user needs apply its global innovation model and won over users in the past years
guilt by association as a search principle guilt by association as a search principle the exploitation of fundamental invariants is among the most elegant solutions to many computational problems in a wide variety of domains one of the more powerful approaches to exploit invariants is the principle of guilt by association in particular the principle of guilt by association is the foundation of remote homolog detection protein function prediction disease subtype diagnosis treatment plan prognosis and other challenges in computational biology the principle suggests that two entities are in a specific relationship if they exhibit invariant properties underlying that relationship for example a protein is predicted to have a particular biological function if it exhibits the underlying invariant properties of that functional groupviz guilty by association to other members of that functional group through the shared invariant properties
on iterative intelligent medical search on iterative intelligent medical search searching for medical information on the web has become highly popular but it remains a challenging task because searchers are often uncertain about their exact medical situations and unfamiliar with medical terminology to address this challenge we have built an intelligent medical web search engine called imed which uses medical knowledge and an interactive questionnaire to help searchers form queries this paper focuses on imeds iterative search advisor which integrates medical and linguistic knowledge to help searchers improve search results iteratively such an iterative process is common for general web search and especially crucial for medical web search because searchers often miss desired search results due to their limited medical knowledge and the tasks inherent difficulty imeds iterative search advisor helps the searcher in several ways first relevant symptoms and signs are automatically suggested based on the searchers description of his situation second instead of taking for granted the searchers answers to the questions imed ranks and recommends alternative answers according to their likelihoods of being the correct answers third related mesh medical phrases are suggested to help the searcher refine his situation description we demonstrate the effectiveness of imeds iterative search advisor by evaluating it using real medical case records and usmle medical exam questions
effective and efficient user interaction for long queries effective and efficient user interaction for long queries handling long queries can involve either pruning the query to retain only the important terms reduction or expanding the query to include related concepts expansion while automatic techniques to do so exist roughly performance improvements in terms of map have been realized in past work through interactive variants we show that selectively reducing or expanding a query leads to an average improvement of in map over the baseline for standard trec test collections we demonstrate how user interaction can be used to achieve this improvement most interaction techniques present users with a fixed number of options for all queries we achieve improvements by interacting less with the user ie we present techniques to identify the optimal number of options to present to users resulting in an interface with an average of fewer options to consider previous algorithms supporting interactive reduction and expansion are exponential in nature to extend their utility to operational environments we present techniques to make the complexity of the algorithms polynomial we finally present an analysis of long queries that continue to exhibit poor performance in spite of our new techniques
how do users find things with pubmed how do users find things with pubmed in the context of document retrieval in the biomedical domain this paper explores the complex relationship between the quality of initial query results and the overall utility of an interactive retrieval system we demonstrate that a contentsimilarity browsing tool can compensate for poor retrieval results and that the relationship between retrieval performance and overall utility is nonlinear arguments are advanced with user simulations which characterize the relevance of documents that a user might encounter with different browsing strategies with broader implications to ir this work provides a case study of how user simulations can be exploited as a formative tool for automatic utility evaluation simulationbased studies provide researchers with an additional evaluation tool to complement interactive and cranfieldstyle experiments
towards breaking the quality curse towards breaking the quality curse searching for people on the web is one of the most common query types to the web search engines today however when a person name is queried the returned webpages often contain documents related to several distinct namesakes who have the queried name the task of disambiguating and finding the webpages related to the specific person of interest is left to the user many web people search weps approaches have been developed recently that attempt to automate this disambiguation process nevertheless the disambiguation quality of these techniques leaves a major room for improvement this paper presents a new serverside weps approach it is based on collecting cooccurrence information from theweb and thus it uses theweb as an external data source a skylinebased classification technique is developed for classifying the collected cooccurrence information in order to make clustering decisions the clustering technique is specifically designed to a handle the dominance that exists in data and b to adapt to a given clustering quality measure these properties allow the framework to get a major advantage in terms of result quality over all the latest weps techniques we are aware of including all the methods covered in the recent weps competition 
an unsupervised framework for extracting and normalizing product attributes from multiple web sites an unsupervised framework for extracting and normalizing product attributes from multiple web sites we have developed an unsupervised framework for simultaneously extracting and normalizing attributes of products from multiple web pages originated from different sites our framework is designed based on a probabilistic graphical model that can model the pageindependent content information and the pagedependent layout information of the text fragments in web pages one characteristic of our framework is that previously unseen attributes can be discovered from the clue contained in the layout format of the text fragments our framework tackles both extraction and normalization tasks by jointly considering the relationship between the content and layout information dirichlet process prior is employed leading to another advantage that the number of discovered product attributes is unlimited an unsupervised inference algorithm based on variational method is presented the semantics of the normalized attributes can be visualized by examining the term weights in the model our framework can be applied to a wide range of web mining applications such as product matching and retrieval we have conducted extensive experiments from four different domains consisting of over web pages from over different web sites demonstrating the robustness and effectiveness of our framework
enhancing web search by promoting multiple search engine use enhancing web search by promoting multiple search engine use any given web search engine may provide higher quality results than others for certain queries therefore it is in users best interest to utilize multiple search engines in this paper we propose and evaluate a framework that maximizes users search effectiveness by directing them to the engine that yields the best results for the current query in contrast to prior work on metasearch we do not advocate for replacement of multiple engines with an aggregate one but rather facilitate simultaneous use of individual engines we describe a machine learning approach to supporting switching between search engines and demonstrate its viability at tolerable interruption levels our findings have implications for fluid competition between search engines
score standardization for intercollection comparison of retrieval systems score standardization for intercollection comparison of retrieval systems the goal of system evaluation in information retrieval has always been to determine which of a set of systems is superior on a given collection the tool used to determine system ordering is an evaluation metric such as average precision which computes relative collectionspecific scores we argue that a broader goal is achievable in this paper we demonstrate that by use of standardization scores can be substantially independent of a particular collection allowing systems to be compared even when they have been tested on different collections compared to current methods our techniques provide richer information about system performance improved clarity in outcome reporting and greater simplicity in reviewing results from disparate sources
the good and the bad system the good and the bad system test collections are extensively used in the evaluation of information retrieval systems crucial to their use is the degree to which results from them predict user effectiveness at first past studies did not substantiate a relationship between system and user effectiveness more recently however correlations have begun to emerge the results of this paper strengthen and extend those findings we introduce a novel methodology for investigating the relationship which shows great success in establishing a significant correlation between system and user effectiveness it is shown that users behave differently and discern differences between pairs of systems that have a very small absolute difference in test collection effectiveness our results strengthen the use of test collections in ir evaluation confirming that users effectiveness can be predicted successfully
retrieval sensitivity under training using different measures retrieval sensitivity under training using different measures various measures such as binary preference bpref inferred average precision infap and binary normalised discounted cumulative gain ndcg have been proposed as alternatives to mean average precision map for being less sensitive to the relevance judgements completeness as the primary aim of any system building is to train the system to respond to user queries in a more robust and stable manner in this paper we investigate the importance of the choice of the evaluation measure for training under different levels of evaluation incompleteness we simulate evaluation incompleteness by sampling from the relevance assessments through largescale experiments on two standard trec test collections we examine retrieval sensitivity when training ie if a training process based on any of the four discussed measures has an impact on the final retrieval performance experimental results show that training by bpref infap and ndcg provides significantly better retrieval performance than training by map when relevance judgements completeness is extremely low when relevance judgements completeness increases the measures behave more similarly
attack resistant collaborative filtering attack resistant collaborative filtering the widespread deployment of recommender systems has lead to user feedback of varying quality while some users faithfully express their true opinion many provide noisy ratings which can be detrimental to the quality of the generated recommendations the presence of noise can violate modeling assumptions and may thus lead to instabilities in estimation and prediction even worse malicious users can deliberately insert attack profiles in an attempt to bias the recommender system to their benefit
eigenrank eigenrank a recommender system must be able to suggest items that are likely to be preferred by the user in most systems the degree of preference is represented by a rating score given a database of users past ratings on a set of items traditional collaborative filtering algorithms are based on predicting the potential ratings that a user would assign to the unrated items so that they can be ranked by the predicted ratings to produce a list of recommended items in this paper we propose a collaborative filtering approach that addresses the item ranking problem directly by modeling user preferences derived from the ratings we measure the similarity between users based on the correlation between their rankings of the items rather than the rating values and propose new collaborative filtering algorithms for ranking items based on the preferences of similar users experimental results on real world movie rating data sets show that the proposed approach outperforms traditional collaborative filtering algorithms significantly on the ndcg measure for evaluating ranked results
personalized active learning for collaborative filtering personalized active learning for collaborative filtering collaborative filtering cf requires userrated training examples for statistical inference about the preferences of new users active learning strategies identify the most informative set of training examples through minimum interactions with the users current active learning approaches in cf make an implicit and unrealistic assumption that a user can provide rating for any queried item this paper introduces a new approach to the problem which does not make such an assumption we personalize active learning for the user and query for only those items which the user can provide rating for we propose an extended form of bayesian active learning and use the aspect model for cf to illustrate and examine the idea a comparative evaluation of the new method and a wellestablished baseline method on benchmark datasets shows statistically significant improvements with our method over the performance of the baseline method that is representative for existing approaches which do not take personalization into account
a boosting algorithm for learning bipartite ranking functions with partially labeled data a boosting algorithm for learning bipartite ranking functions with partially labeled data this paper presents a boosting based algorithm for learning a bipartite ranking function brf with partially labeled data until now different attempts had been made to build a brf in a transductive setting in which the test points are given to the methods in advance as unlabeled data the proposed approach is a semisupervised inductive ranking algorithm which as opposed to transductive algorithms is able to infer an ordering on new examples that were not used for its training we evaluate our approach using the trec ohsumed and the reuters data collections comparing against two semisupervised classification algorithms for rocarea auc uninterpolated average precision aup mean precision tp and precisionrecall pr curves in the most interesting cases where there are an unbalanced number of irrelevant examples over relevant ones we show our method to produce statistically significant improvements with respect to these ranking measures
directly optimizing evaluation measures in learning to rank directly optimizing evaluation measures in learning to rank one of the central issues in learning to rank for information retrieval is to develop algorithms that construct ranking models by directly optimizing evaluation measures used in information retrieval such as mean average precision map and normalized discounted cumulative gain ndcg several such algorithms including svmmap and adarank have been proposed and their effectiveness has been verified however the relationships between the algorithms are not clear and furthermore no comparisons have been conducted between them in this paper we conduct a study on the approach of directly optimizing evaluation measures in learning to rank for information retrieval ir we focus on the methods that minimize loss functions upper bounding the basic loss function defined on the ir measures we first provide a general framework for the study and analyze the existing algorithms of svmmap and adarank within the framework the framework is based on upper bound analysis and two types of upper bounds are discussed moreover we show that we can derive new algorithms on the basis of this analysis and create one example algorithm called permurank we have also conducted comparisons between svmmap adarank permurank and conventional methods of ranking svm and rankboost using benchmark datasets experimental results show that the methods based on direct optimization of evaluation measures can always outperform conventional methods of ranking svm and rankboost however no significant difference exists among the performances of the direct optimization methods themselves
query dependent ranking using knearest neighbor query dependent ranking using knearest neighbor many ranking models have been proposed in information retrieval and recently machine learning techniques have also been applied to ranking model construction most of the existing methods do not take into consideration the fact that significant differences exist between queries and only resort to a single function in ranking of documents in this paper we argue that it is necessary to employ different ranking models for different queries and onduct what we call querydependent ranking as the first such attempt we propose a knearest neighbor knn method for querydependent ranking we first consider an online method which creates a ranking model for a given query by using the labeled neighbors of the query in the query feature space and then rank the documents with respect to the query using the created model next we give two offline approximations of the method which create the ranking models in advance to enhance the efficiency of ranking and we prove a theory which indicates that the approximations are accurate in terms of difference in loss of prediction if the learning algorithm used is stable with respect to minor changes in training examples our experimental results show that the proposed online and offline methods both outperform the baseline method of using a single ranking function
asymmetric distance estimation with sketches for similarity search in highdimensional spaces asymmetric distance estimation with sketches for similarity search in highdimensional spaces efficient similarity search in highdimensional spaces is important to contentbased retrieval systems recent studies have shown that sketches can effectively approximate l distance in highdimensional spaces and that filtering with sketches can speed up similarity search by an order of magnitude it is a challenge to further reduce the size of sketches which are already compact without compromising accuracy of distance estimation
resin resin results caching is an efficient technique for reducing the query processing load hence it is commonly used in real search engines this technique however bounds the maximum hit rate due to the large fraction of singleton queries which is an important limitation in this paper we propose resin an architecture that uses a combination of results caching and index pruning to overcome this limitation
reorganizing compressed text reorganizing compressed text recent research has demonstrated beyond doubts the benefits of compressing natural language texts using wordbased statistical semistatic compression not only it achieves extremely competitive compression rates but also direct search on the compressed text can be carried out faster than on the original text indexing based on inverted lists benefits from compression as well
user adaptation user adaptation several recent studies have found only a weak relationship between the performance of a retrieval system and the success achievable by human searchers we hypothesize that searchers are successful precisely because they alter their behavior to explore the possible causal relation between system performance and search behavior we control system performance hoping to elicit adaptive search behaviors subjects each completed searches using either a standard system or one of two degraded systems using a general linear model we isolate the main effect of system performance by measuring and removing main effects due to searcher variation topic difficulty and the position of each search in the time series we find that searchers using our degraded systems are as successful as those using the standard system but that in achieving this success they alter their behavior in ways that could be measured in real time by a suitably instrumented system our findings suggest quite generally that some aspects of behavioral dynamics may provide unobtrusive indicators of system performance
exploring folksonomy for personalized search exploring folksonomy for personalized search as a social service in web folksonomy provides the users the ability to save and organize their bookmarks online with social annotations or tags social annotations are high quality descriptors of the web pages topics as well as good indicators of web users interests we propose a personalized search framework to utilize folksonomy for personalized search specifically three properties of folksonomy namely the categorization keyword and structure property are explored in the framework the rank of a web page is decided not only by the term matching between the query and the web pages content but also by the topic matching between the users interests and the web pages topics in the evaluation we propose an automatic evaluation framework based on folksonomy data which is able to help lighten the common high cost in personalized search evaluations a series of experiments are conducted using two heterogeneous data sets one crawled from delicious and the other from dogear extensive experimental results show that our personalized search approach can significantly improve the search quality
to personalize or not to personalize to personalize or not to personalize in most previous work on personalized search algorithms the results for all queries are personalized in the same manner however as we show in this paper there is a lot of variation across queries in the benefits that can be achieved through personalization for some queries everyone who issues the query is looking for the same thing for other queries different people want very different results even though they express their need in the same way we examine variability in user intent using both explicit relevance judgments and largescale log analysis of user behavior patterns while variation in user behavior is correlated with variation in explicit relevance judgments the same query there are many other factors such as result entropy result quality and task that can also affect the variation in behavior we characterize queries using a variety of features of the query the results returned for the query and peoples interaction history with the query using these features we build predictive models to identify queries that can benefit from personalization
the opposite of smoothing the opposite of smoothing exploiting information induced from queryspecific clustering of topretrieved documents has long been proposed as means for improving precision at the very top ranks of the returned results we present a novel language model approach to ranking queryspecific clusters by the presumed percentage of relevant documents that they contain while most previous cluster ranking approaches focus on the cluster as a whole our model also exploits information induced from documents associated with the cluster our model substantially outperforms previous approaches for identifying clusters containing a high relevantdocument percentage furthermore using the model to produce document ranking yields precisionattopranks performance that is consistently better than that of the initial ranking upon which clustering is performed the performance also favorably compares with that of a stateoftheart pseudofeedback retrieval method
enhancing text clustering by leveraging wikipedia semantics enhancing text clustering by leveraging wikipedia semantics most traditional text clustering methods are based on bag of words bow representation based on frequency statistics in a set of documents bow however ignores the important information on the semantic relationships between key terms to overcome this problem several methods have been proposed to enrich text representation with external resource in the past such as wordnet however many of these approaches suffer from some limitations wordnet has limited coverage and has a lack of effective wordsense disambiguation ability most of the text representation enrichment strategies which append or replace document terms with their hypernym and synonym are overly simple in this paper to overcome these deficiencies we first propose a way to build a concept thesaurus based on the semantic relations synonym hypernym and associative relation extracted from wikipedia then we develop a unified framework to leverage these semantic relations in order to enhance traditional content similarity measure for text clustering the experimental results on reuters and ohsumed datasets show that with the help of wikipedia thesaurus the clustering performance of our method is improved as compared to previous methods in addition with the optimized weights for hypernym synonym and associative concepts that are tuned with the help of a few labeled data users provided the clustering performance can be further improved
knowledge transformation from word space to document space knowledge transformation from word space to document space in most ir clustering problems we directly cluster the documents working in the document space using cosine similarity between documents as the similarity measure in many realworld applications however we usually have knowledge on the word side and wish to transform this knowledge to the document concept side in this paper we provide a mechanism for this knowledge transformation to the best of our knowledge this is the first model for such type of knowledge transformation this model uses a nonnegative matrix factorization model x fsgt where x is the word document semantic matrix f is the posterior probability of a word belonging to a word cluster and represents knowledge in the word space g is the posterior probability of a document belonging to a document cluster and represents knowledge in the document space and s is a scaled matrix factor which provides a condensed view of x we show how knowledge on words can improve document clustering ie knowledge in the word space is transformed into the document space we perform extensive experiments to validate our approach
a study of learning a merge model for multilingual information retrieval a study of learning a merge model for multilingual information retrieval this paper proposes a learning approach for the merging process in multilingual information retrieval mlir to conduct the learning approach we also present a large number of features that may influence the mlir merging process these features are mainly extracted from three levels query document and translation after the feature extraction we then use the frank ranking algorithm to construct a merge model to our knowledge this practice is the first attempt to use a learningbased ranking algorithm to construct a merge model for mlir merging in our experiments three test collections for the task of crosslingual information retrieval clir in ntcir and are employed to assess the performance of our proposed method moreover several merging methods are also carried out for a comparison including traditional merging methods the step merging strategy and the merging method based on logistic regression the experimental results show that our method can significantly improve merging quality on two different types of datasets in addition to the effectiveness through the merge model generated by frank our method can further identify key factors that influence the merging process this information might provide us more insight and understanding into mlir merging
bilingual topic aspect classification with a few training examples bilingual topic aspect classification with a few training examples this paper explores topic aspect ie subtopic or facet classification for english and chinese collections the evaluation model assumes a bilingual user who has found documents on a topic and identified a few passages in each language on aspects of that topic additional passages are then automatically labeled using a knearestneighbor classifier and local ie result set latent semantic analysis experiments show that when few training examples are available in either language classification using training examples from both languages can often achieve higher effectiveness than using training examples from just one language when the total number of training examples is held constant classification effectiveness correlates positively with the fraction of samelanguage training examples in the training set these results suggest that supervised classification can benefit from handannotating a few samelanguage examples and that when performing classification in bilingual collections it is useful to label some examples in each language
crosslingual location search crosslingual location search address geocoding the process of finding the map location for a structured postal address is a relatively wellstudied problem in this paper we consider the more general problem of crosslingual location search where the queries are not limited to postal addresses and the language and script used in the search query is different from the one in which the underlying data is stored to the best of our knowledge our system is the first crosslingual location search system that is able to geocode complex addresses we use a statistical machine transliteration system to convert location names from the script of the query to that of the stored data however we show that it is not sufficient to simply feed the resulting transliterations into a monolingual geocoding system as the ambiguity inherent in the conversion drastically expands the location search space and significantly lowers the quality of results the strength of our approach lies in its integrated endtoend nature we use abstraction and fuzzy search in the text domain to achieve maximum coverage despite transliteration ambiguities while applying spatial constraints in the geographic domain to focus only on viable interpretations of the query our experiments with structured and unstructured queries in a set of diverse languages and scripts arabic english hindi and japanese searching for locations in different regions of the world show full crosslingual location search accuracy at levels comparable to that of commercial monolingual systems we achieve these levels of performance using techniques that may be applied to crosslingual searches in any languagescript and over arbitrary spatial data
a study of methods for negative relevance feedback a study of methods for negative relevance feedback negative relevance feedback is a special case of relevance feedback where we do not have any positive example this often happens when the topic is difficult and the search results are poor although in principle any standard relevance feedback technique can be applied to negative relevance feedback it may not perform well due to the lack of positive examples in this paper we conduct a systematic study of methods for negative relevance feedback we compare a set of representative negative feedback methods covering vectorspace models and language models as well as several special heuristics for negative feedback evaluating negative feedback methods requires a test set with sufficient difficult topics but there are not many naturally difficult topics in the existing test collections we use two sampling strategies to adapt a test collection with easy topics to evaluate negative feedback experiment results on several trec collections show that language model based negative feedback methods are generally more effective than those based on vectorspace models and using multiple negative models is an effective heuristic for negative feedback our results also show that it is feasible to adapt test collections with easy topics for evaluating negative feedback methods through sampling
a bayesian logistic regression model for active relevance feedback a bayesian logistic regression model for active relevance feedback relevance feedback which traditionally uses the terms in the relevant documents to enrich the users initial query is an effective method for improving retrieval performance the traditional relevance feedback algorithms lead to overfitting because of the limited amount of training data and large term space this paper introduces an online bayesian logistic regression algorithm to incorporate relevance feedback information the new approach addresses the overfitting problem by projecting the original feature space onto a more compact set which retains the necessary information the new set of features consist of the original retrieval score the distance to the relevant documents and the distance to nonrelevant documents to reduce the human evaluation effort in ascertaining relevance we introduce a new active learning algorithm based on variance reduction to actively select documents for user evaluation the new active learning algorithm aims to select feedback documents to reduce the model variance the variance reduction approach leads to capturing relevance diversity and uncertainty of the unlabeled documents in a principled manner these are the critical factors of active learning indicated in previous literature experiments with several trec datasets demonstrate the effectiveness of the proposed approach
a clusterbased resampling method for pseudorelevance feedback a clusterbased resampling method for pseudorelevance feedback typical pseudorelevance feedback methods assume the topretrieved documents are relevant and use these pseudorelevant documents to expand terms the initial retrieval set can however contain a great deal of noise in this paper we present a clusterbased resampling method to select better pseudorelevant documents based on the relevance model the main idea is to use document clusters to find dominant documents for the initial retrieval set and to repeatedly feed the documents to emphasize the core topics of a query experimental results on largescale web trec collections show significant improvements over the relevance model for justification of the resampling approach we examine relevance density of feedback documents a higher relevance density will result in greater retrieval accuracy ultimately approaching true relevance feedback the resampling approach shows higher relevance density than the baseline relevance model on all collections resulting in better retrieval accuracy in pseudorelevance feedback this result indicates that the proposed method is effective for pseudorelevance feedback
selecting good expansion terms for pseudorelevance feedback selecting good expansion terms for pseudorelevance feedback pseudorelevance feedback assumes that most frequent terms in the pseudofeedback documents are useful for the retrieval in this study we reexamine this assumption and show that it does not hold in reality many expansion terms identified in traditional approaches are indeed unrelated to the query and harmful to the retrieval we also show that good expansion terms cannot be distinguished from bad ones merely on their distributions in the feedback documents and in the whole collection we then propose to integrate a term classification process to predict the usefulness of expansion terms multiple additional features can be integrated in this process our experiments on three trec collections show that retrieval effectiveness can be much improved when term classification is used in addition we also demonstrate that good terms should be identified directly according to their possible impact on the retrieval effectiveness ie using supervised learning instead of unsupervised learning
learning to rank with partiallylabeled data learning to rank with partiallylabeled data ranking algorithms whose goal is to appropriately order a set of objectsdocuments are an important component of information retrieval systems previous work on ranking algorithms has focused on cases where only labeled data is available for training ie supervised learning in this paper we consider the question whether unlabeled test data can be exploited to improve ranking performance we present a framework for transductive learning of ranking functions and show that the answer is affirmative our framework is based on generating better features from the test data via kernelpca and incorporating such features via boosting thus learning different ranking functions adapted to the individual test queries we evaluate this method on the letor trec ohsumed dataset and demonstrate significant improvements
learning to rank with softrank and gaussian processes learning to rank with softrank and gaussian processes in this paper we address the issue of learning to rank for document retrieval using thurstonian models based on sparse gaussian processes thurstonian models represent each document for a given query as a probability distribution in a score space these distributions over scores naturally give rise to distributions over document rankings however in general we do not have observed rankings with which to train the model instead each document in the training set is judged to have a particular relevance level for example bad fair good or excellent the performance of the model is then evaluated using information retrieval ir metrics such as normalised discounted cumulative gain ndcg recently taylor et al presented a method called softrank which allows the direct gradient optimisation of a smoothed version of ndcg using a thurstonian model in this approach document scores are represented by the outputs of a neural network and score distributions are created artificially by adding random noise to the scores the softrank mechanism is a general one it can be applied to different ir metrics and make use of different underlying models in this paper we extend the softrank framework to make use of the score uncertainties which are naturally provided by a gaussian process gp which is a probabilistic nonlinear regression model we further develop the model by using sparse gaussian process techniques which give improved performance and efficiency and show competitive results against baseline methods when tested on the publicly available letor ohsumed data set we also explore how the available uncertainty information can be used in prediction and how it affects model performance
learning to rank at querytime using association rules learning to rank at querytime using association rules some applications have to present their results in the form of ranked lists this is the case of many information retrieval applications in which documents must be sorted according to their relevance to a given query this has led the interest of the information retrieval community in methods that automatically learn effective ranking functions in this paper we propose a novel method which uncovers patterns or rules in the training data associating features of the document with its relevance to the query and then uses the discovered rules to rank documents to address typical problems that are inherent to the utilization of association rules such as missing rules and rule explosion the proposed method generates rules on a demanddriven basis at querytime the result is an extremely fast and effective ranking method we conducted a systematic evaluation of the proposed method using the letor benchmark collections we show that generating rules on a demanddriven basis can boost ranking performance providing gains ranging from to outperforming the stateoftheart methods that learn to rank with no need of timeconsuming and laborious preprocessing as a highlight we also show that additional information such as query terms can make the generated rules more discriminative further improving ranking performance
learning to rank with ties learning to rank with ties designing effective ranking functions is a core problem for information retrieval and web search since the ranking functions directly impact the relevance of the search results the problem has been the focus of much of the research at the intersection of web search and machine learning and learning ranking functions from preference data in particular has recently attracted much interest the objective of this paper is to empirically examine several objective functions that can be used for learning ranking functions from preference data specifically we investigate the roles of ties in the learning process by ties we mean preference judgments that two documents have equal degree of relevance with respect to a query this type of data has largely been ignored or not properly modeled in the past in this paper we analyze the properties of ties and develop novel learning frameworks which combine ties and preference data using statistical paired comparison models to improve the performance of learned ranking functions the resulting optimization problems explicitly incorporating ties and preference data are solved using gradient boosting methods experimental studies are conducted using three publicly available data sets which demonstrate the effectiveness of the proposed new methods
querysensitive mutual reinforcement chain and its application in queryoriented multidocument summarization querysensitive mutual reinforcement chain and its application in queryoriented multidocument summarization sentence ranking is the issue of most concern in document summarization early researchers have presented the mutual reinforcement principle mr between sentence and term for simultaneous key phrase and salient sentence extraction in generic singledocument summarization in this work we extend the mr to the mutual reinforcement chain mrc of three different text granularities ie document sentence and terms the aim is to provide a general reinforcement framework and a formal mathematical modeling for the mrc going one step further we incorporate the query influence into the mrc to cope with the need for queryoriented multidocument summarization while the previous summarization approaches often calculate the similarity regardless of the query we develop a querysensitive similarity to measure the affinity between the pair of texts when evaluated on the duc dataset the experimental results suggest that the proposed querysensitive mrc qsmrc is a promising approach for summarization
commentsoriented document summarization commentsoriented document summarization comments left by readers on web documents contain valuable information that can be utilized in different information retrieval tasks including document search visualization and summarization in this paper we study the problem of commentsoriented document summarization and aim to summarize a web document eg a blog post by considering not only its content but also the comments left by its readers we identify three relations namely topic quotation and mention by which comments can be linked to one another and model the relations in three graphs the importance of each comment is then scored by i graphbased method where the three graphs are merged into a multirelation graph ii tensorbased method where the three graphs are used to construct a rdorder tensor to generate a commentsoriented summary we extract sentences from the given web document using either featurebiased approach or uniformdocument approach the former scores sentences to bias keywords derived from comments while the latter scores sentences uniformly with comments in our experiments using a set of blog posts with manually labeled sentences our proposed summarization methods utilizing comments showed significant improvement over those not using comments the methods using featurebiased sentence extraction approach were observed to outperform that using uniformdocument approach
multidocument summarization using clusterbased link analysis multidocument summarization using clusterbased link analysis the markov random walk model has been recently exploited for multidocument summarization by making use of the link relationships between sentences in the document set under the assumption that all the sentences are indistinguishable from each other however a given document set usually covers a few topic themes with each theme represented by a cluster of sentences the topic themes are usually not equally important and the sentences in an important theme cluster are deemed more salient than the sentences in a trivial theme cluster this paper proposes the clusterbased conditional markov random walk model clustercmrw and the clusterbased hits model clusterhits to fully leverage the clusterlevel information experimental results on the duc and duc datasets demonstrate the good effectiveness of our proposed summarization models the results also demonstrate that the clustercmrw model is more robust than the clusterhits model with respect to different cluster numbers
multidocument summarization via sentencelevel semantic analysis and symmetric matrix factorization multidocument summarization via sentencelevel semantic analysis and symmetric matrix factorization multidocument summarization aims to create a compressed summary while retaining the main characteristics of the original set of documents many approaches use statistics and machine learning techniques to extract sentences from documents in this paper we propose a new multidocument summarization framework based on sentencelevel semantic analysis and symmetric nonnegative matrix factorization we first calculate sentencesentence similarities using semantic analysis and construct the similarity matrix then symmetric matrix factorization which has been shown to be equivalent to normalized spectral clustering is used to group sentences into clusters finally the most informative sentences are selected from each group to form the summary experimental results on duc and duc data sets demonstrate the improvement of our proposed framework over the implemented existing summarization systems a further study on the factors that benefit the high performance is also conducted
algorithmic mediation for collaborative exploratory search algorithmic mediation for collaborative exploratory search we describe a new approach to information retrieval algorithmic mediation for intentional synchronous collaborative exploratory search using our system two or more users with a common information need search together simultaneously the collaborative system provides tools user interfaces and most importantly algorithmicallymediated retrieval to focus enhance and augment the teams search and communication activities collaborative search outperformed post hoc merging of similarly instrumented single user runs algorithmic mediation improved both collaborative search allowing a team of searchers to find relevant information more efficiently and effectively and exploratory search allowing the searchers to find relevant information that cannot be found while working individually
exploiting correlated keywords to improve approximate information filtering exploiting correlated keywords to improve approximate information filtering information filtering also referred to as publishsubscribe complements onetime searching since users are able to subscribe to information sources and be notified whenever new documents of interest are published in approximate information filtering only selected information sources that are likely to publish documents relevant to the user interests in the future are monitored to achieve this functionality a subscriber exploits statistical metadata to identify promising publishers and index its continuous query only in those publishers the statistics are maintained in a directory usually on a perkeyword basis thus disregarding possible correlations among keywords using this coarse information poor publisher selection may lead to poor filtering performance and thus loss of interesting documents
a user browsing model to predict search engine click data from past observations a user browsing model to predict search engine click data from past observations search engine click logs provide an invaluable source of relevance information but this information is biased because we ignore which documents from the result list the users have actually seen before and after they clicked otherwise we could estimate document relevance by simple counting in this paper we propose a set of assumptions on user browsing behavior that allows the estimation of the probability that a document is seen thereby providing an unbiased estimate of document relevance to train test and compare our model to the best alternatives described in the literature we gather a large set of real data and proceed to an extensive crossvalidation experiment our solution outperforms very significantly all previous models as a side effect we gain insight into the browsing behavior of users and we can compare it to the conclusions of an eyetracking experiments by joachims et al in particular our findings confirm that a user almost always see the document directly after a clicked document they also explain why documents situated just after a very relevant document are clicked more often
learning query intent from regularized click graphs learning query intent from regularized click graphs this work presents the use of click graphs in improving query intent classifiers which are critical if vertical search and generalpurpose search services are to be offered in a unified user interface previous works on query classification have primarily focused on improving feature representation of queries eg by augmenting queries with search engine results in this work we investigate a completely orthogonal approach instead of enriching feature representation we aim at drastically increasing the amounts of training data by semisupervised learning with click graphs specifically we infer class memberships of unlabeled queries from those of labeled ones according to their proximities in a click graph moreover we regularize the learning with click graphs by contentbased classification to avoid propagating erroneous labels we demonstrate the effectiveness of our algorithms in two different applications product intent and job intent classification in both cases we expand the training data with automatically labeled queries by over two orders of magnitude leading to significant improvements in classification performance an additional finding is that with a large amount of training data obtained in this fashion classifiers using only query wordsphrases as features can work remarkably well
retrieval and feedback models for blog feed search retrieval and feedback models for blog feed search blog feed search poses different and interesting challenges from traditional ad hoc document retrieval the units of retrieval the blogs are collections of documents the blog posts in this work we adapt a stateoftheart federated search model to the feed retrieval task showing a significant improvement over algorithms based on the best performing submissions in the trec blog distillation task we also show that typical query expansion techniques such as pseudorelevance feedback using the blog corpus do not provide any significant performance improvement and in many cases dramatically hurt performance we perform an indepth analysis of the behavior of pseudorelevance feedback for this task and develop a novel query expansion technique using the link structure in wikipedia this query expansion technique provides significant and consistent performance improvements for this task yielding a and improvement in map over the unexpanded query for our baseline and federated algorithms respectively
learning to reduce the semantic gap in web image retrieval and annotation learning to reduce the semantic gap in web image retrieval and annotation we study in this paper the problem of bridging the semantic gap between lowlevel image features and highlevel semantic concepts which is the key hindrance in contentbased image retrieval piloted by the rich textual information of web images the proposed framework tries to learn a new distance measure in the visual space which can be used to retrieve more semantically relevant images for any unseen query image the framework differentiates with traditional distance metric learning methods in the following ways a rankingbased distance metric learning method is proposed for image retrieval problem by optimizing the leaveoneout retrieval performance on the training data to be scalable millions of images together with rich textual information have been crawled from the web to learn the similarity measure and the learning framework particularly considers the indexing problem to ensure the retrieval efficiency to alleviate the noises in the unbalanced labels of images and fully utilize the textual information a latent dirichlet allocation based topiclevel text model is introduced to define pairwise semantic similarity between any two images the learnt distance measure can be directly applied to applications such as contentbased image retrieval and searchbased image annotation experimental results on the two applications in a two million web image database show both the effectiveness and efficiency of the proposed framework
a latticebased approach to querybyexample spoken document retrieval a latticebased approach to querybyexample spoken document retrieval recent efforts on the task of spoken document retrieval sdr have made use of speech lattices speech lattices contain information about alternative speech transcription hypotheses other than the best transcripts and this information can improve retrieval accuracy by overcoming recognition errors present in the best transcription in this paper we look at using lattices for the querybyexample spoken document retrieval task retrieving documents from a speech corpus where the queries are themselves in the form of complete spoken documents query exemplars we extend a previously proposed method for sdr with short queries to the querybyexample task specifically we use a retrieval method based on statistical modeling we compute expected word counts from document and query lattices estimate statistical models from these counts and compute relevance scores as divergences between these models experimental results on a speech corpus of conversational english show that the use of statistics from lattices for both documents and query exemplars results in better retrieval accuracy than using only best transcripts for either documents or queries or both in addition we investigate the effect of stop word removal which further improves retrieval accuracy to our knowledge our work is the first to have used a latticebased approach to querybyexample spoken document retrieval
a few examples go a long way a few examples go a long way we address a specific enterprise document search scenario where the information need is expressed in an elaborate manner in our scenario information needs are expressed using a short query of a few keywords together with examples of key reference pages given this setup we investigate how the examples can be utilized to improve the endtoend performance on the document retrieval task our approach is based on a language modeling framework where the query model is modified to resemble the example pages we compare several methods for sampling expansion terms from the example pages to support querydependent and queryindependent query expansion the latter is motivated by the wish to increase aspect recall and attempts to uncover aspects of the information need not captured by the query
a unified and discriminative model for query refinement a unified and discriminative model for query refinement this paper addresses the issue of query refinement which involves reformulating illformed search queries in order to enhance relevance of search results query refinement typically includes a number of tasks such as spelling error correction word splitting word merging phrase segmentation word stemming and acronym expansion in previous research such tasks were addressed separately or through employing generative models this paper proposes employing a unified and discriminative model for query refinement specifically it proposes a conditional random field crf model suitable for the problem referred to as conditional random field for query refinement crfqr given a sequence of query words crfqr predicts a sequence of refined query words as well as corresponding refinement operations in that sense crfqr differs greatly from conventional crf models two types of crfqr models namely a basic model and an extended model are introduced one merit of employing crfqr is that different refinement tasks can be performed simultaneously and thus the accuracy of refinement can be enhanced furthermore the advantages of discriminative models over generative models can be fully leveraged experimental results demonstrate that crfqr can significantly outperform baseline methods furthermore when crfqr is used in web search a significant improvement of relevance can be obtained
query expansion using gazebased feedback on the subdocument level query expansion using gazebased feedback on the subdocument level we examine the effect of incorporating gazebased attention feedback from the user on personalizing the search process employing eye tracking data we keep track of document parts the user read in some way we use this information on the subdocument level as implicit feedback for query expansion and reranking
affective feedback affective feedback user feedback is considered to be a critical element in the information seeking process especially in relation to relevance assessment current feedback techniques determine content relevance with respect to the cognitive and situational levels of interaction that occurs between the user and the retrieval system however apart from reallife problems and information objects users interact with intentions motivations and feelings which can be seen as critical aspects of cognition and decisionmaking the study presented in this paper serves as a starting point to the exploration of the role of emotions in the information seeking process results show that the latter not only interweave with different physiological psychological and cognitive processes but also form distinctive patterns according to specific task and according to specific user
optimizing relevance and revenue in ad search optimizing relevance and revenue in ad search the primary business model behind web search is based on textual advertising where contextually relevant ads are displayed alongside search results we address the problem of selecting these ads so that they are both relevant to the queries and profitable to the search engine showing that optimizing ad relevance and revenue is not equivalent selecting the best ads that satisfy these constraints also naturally incurs high computational costs and time constraints can lead to reduced relevance and profitability we propose a novel twostage approach which conducts most of the analysis ahead of time an offine preprocessing phase leverages additional knowledge that is impractical to use in real time and rewrites frequent queries in a way that subsequently facilitates fast and accurate online matching empirical evaluation shows that our method optimized for relevance matches a stateoftheart method while improving expected revenue when optimizing for revenue we see even more substantial improvements in expected revenue
a generation model to unify topic relevance and lexiconbased sentiment for opinion retrieval a generation model to unify topic relevance and lexiconbased sentiment for opinion retrieval opinion retrieval is a task of growing interest in social life and academic research which is to find relevant and opinionate documents according to a users query one of the key issues is how to combine a documents opinionate score the ranking score of to what extent it is subjective or objective and topic relevance score current solutions to document ranking in opinion retrieval are generally adhoc linear combination which is short of theoretical foundation and careful analysis in this paper we focus on lexiconbased opinion retrieval a novel generation model that unifies topicrelevance and opinion generation by a quadratic combination is proposed in this paper with this model the relevancebased ranking serves as the weighting factor of the lexiconbased sentiment ranking function which is essentially different from the popular heuristic linear combination approaches the effect of different sentiment dictionaries is also discussed experimental results on trec blog datasets show the significant effectiveness of the proposed unified model improvements of and have been obtained in terms of map and p respectively the conclusion is not limited to blog environment besides the unified generation model another contribution is that our work demonstrates that in the opinion retrieval task a bayesian approach to combining multiple ranking functions is superior to using a linear combination it is also applicable to other result reranking applications in similar scenario
discriminative probabilistic models for passage based retrieval discriminative probabilistic models for passage based retrieval the approach of using passagelevel evidence for document retrieval has shown mixed results when it is applied to a variety of test beds with different characteristics one main reason of the inconsistent performance is that there exists no unified framework to model the evidence of individual passages within a document this paper proposes two probabilistic models to formally model the evidence of a set of top ranked passages in a document the first probabilistic model follows the retrieval criterion that a document is relevant if any passage in the document is relevant and models each passage independently the second probabilistic model goes a step further and incorporates the similarity correlations among the passages both models are trained in a discriminative manner furthermore we present a combination approach to combine the ranked lists of document retrieval and passagebased retrieval
a new probabilistic retrieval model based on the dirichlet compound multinomial distribution a new probabilistic retrieval model based on the dirichlet compound multinomial distribution the classical probabilistic models attempt to capture the ad hoc information retrieval problem within a rigorous probabilistic framework it has long been recognized that the primary obstacle to effective performance of the probabilistic models is the need to estimate a relevance model the dirichlet compound multinomial dcm distribution which relies on hierarchical bayesian modeling techniques or the polya urn scheme is a more appropriate generative model than the traditional multinomial distribution for text documents we explore a new probabilistic model based on the dcm distribution which enables efficient retrieval and accurate ranking because the dcm distribution captures the dependency of repetitive word occurrences the new probabilistic model is able to model the concavity of the score function more effectively to avoid the empirical tuning of retrieval parameters we design several parameter estimation algorithms to automatically set model parameters additionally we propose a pseudorelevance feedback algorithm based on the latent mixture modeling of the dirichlet compound multinomial distribution to further improve retrieval accuracy finally our experiments show that both the baseline probabilistic retrieval algorithm based on the dcm distribution and the corresponding pseudorelevance feedback algorithm outperform the existing language modeling systems on several trec retrieval tasks
tfidf uncovered tfidf uncovered interpretations of tfidf are based on binary independence retrieval poisson information theory and language modelling this paper contributes a review of existing interpretations and then tfidf is systematically related to the probabilities pqd and pdq two approaches are explored a space of independent and a space of disjoint terms for independent terms an extreme querynonquery term assumption uncovers tfidf and an analogy of pdq and the probabilistic odds ord q mirrors relevance feedback for disjoint terms a relationship between probability theory and tfidf is established through the integral x dx log x this study uncovers components such as divergence from randomness and pivoted document length to be inherent parts of a documentquery independence dqi measure and interestingly an integral of the dqi over the term occurrence probability leads to tfidf
separate and inequal separate and inequal web pages like people are often known by others in a variety of contexts when those contexts are sufficiently distinct a pages importance may be better represented by multiple domains of authority rather than by one that indiscriminately mixes reputations in this work we determine domains of authority by examining the contexts in which a page is cited however we find that it is not enough to determine separate domains of authority our model additionally determines the local flow of authority based upon the relative similarity of the source and target authority domains in this way we differentiate both incoming and outgoing hyperlinks by topicality and importance rather than treating them indiscriminately we find that this approach compares favorably to other topical ranking methods on two realworld datasets and produces an approximately improvement in precision and quality of the top ten results over pagerank
browserank browserank this paper proposes a new method for computing page importance referred to as browserank the conventional approach to compute page importance is to exploit the link graph of the web and to build a model based on that graph for instance pagerank is such an algorithm which employs a discretetime markov process as the model unfortunately the link graph might be incomplete and inaccurate with respect to data for determining page importance because links can be easily added and deleted by web content creators in this paper we propose computing page importance by using a user browsing graph created from user behavior data in this graph vertices represent pages and directed edges represent transitions between pages in the users web browsing history furthermore the lengths of staying time spent on the pages by users are also included the user browsing graph is more reliable than the link graph for inferring page importance this paper further proposes using the continuoustime markov process on the user browsing graph as a model and computing the stationary probability distribution of the process as page importance an efficient algorithm for this computation has also been devised in this way we can leverage hundreds of millions of users implicit voting on page importance experimental results show that browserank indeed outperforms the baseline methods such as pagerank and trustrank in several tasks
exploring traversal strategy for web forum crawling exploring traversal strategy for web forum crawling in this paper we study the problem of web forum crawling web forum has now become an important data source of many web applications while forum crawling is still a challenging task due to complex insite link structures and login controls of most forum sites without carefully selecting the traversal path a generic crawler usually downloads many duplicate and invalid pages from forums and thus wastes both the precious bandwidth and the limited storage space to crawl forum data more effectively and efficiently in this paper we propose an automatic approach to exploring an appropriate traversal strategy to direct the crawling of a given target forum in detail the traversal strategy consists of the identification of the skeleton links and the detection of the pageflipping links the skeleton links instruct the crawler to only crawl valuable pages and meanwhile avoid duplicate and uninformative ones and the pageflipping links tell the crawler how to completely download a long discussion thread which is usually shown in multiple pages in web forums the extensive experimental results on several forums show encouraging performance of our approach following the discovered traversal strategy our forum crawler can archive more informative pages in comparison with previous related work and a commercial generic crawler
finding questionanswer pairs from online forums finding questionanswer pairs from online forums online forums contain a huge amount of valuable user generated content in this paper we address the problem of extracting questionanswer pairs from forums questionanswer pairs extracted from forums can be used to help question answering services eg yahoo answers among other applications we propose a sequential patterns based classification method to detect questions in a forum thread and a graph based propagation method to detect answers for questions in the same thread experimental results show that our techniques are very promising
retrieval models for question and answer archives retrieval models for question and answer archives retrieval in a question and answer archive involves finding good answers for a users question in contrast to typical document retrieval a retrieval model for this task can exploit question similarity as well as ranking the associated answers in this paper we propose a retrieval model that combines a translationbased language model for the question part with a query likelihood approach for the answer part the proposed model incorporates wordtoword translation probabilities learned through exploiting different sources of information experiments show that the proposed translation based language model for the question part outperforms baseline methods significantly by combining with the query likelihood language model for the answer part substantial additional effectiveness improvements are obtained
predicting information seeker satisfaction in community question answering predicting information seeker satisfaction in community question answering question answering communities such as naver and yahoo answers have emerged as popular and often effective means of information seeking on the web by posting questions for other participants to answer information seekers can obtain specific answers to their questions users of popular portals such as yahoo answers already have submitted millions of questions and received hundreds of millions of answers from other participants however it may also take hours and sometime days until a satisfactory answer is posted in this paper we introduce the problem of predicting information seeker satisfaction in collaborative question answering communities where we attempt to predict whether a question author will be satisfied with the answers submitted by the community participants we present a general prediction model and develop a variety of content structure and communityfocused features for this task our experimental results obtained from a largescale evaluation over thousands of real questions and user ratings demonstrate the feasibility of modeling and predicting asker satisfaction we complement our results with a thorough investigation of the interactions and information seeking patterns in question answering communities that correlate with information seeker satisfaction our models and predictions could be useful for a variety of applications such as user intent inference answer ranking interface design and query suggestion and routing
discovering key concepts in verbose queries discovering key concepts in verbose queries current search engines do not in general perform well with longer more verbose queries one of the main issues in processing these queries is identifying the key concepts that will have the most impact on effectiveness in this paper we develop and evaluate a technique that uses querydependent corpusdependent and corpusindependent features for automatic extraction of key concepts from verbose queries we show that our method achieves higher accuracy in the identification of key concepts than standard weighting methods such as inverse document frequency finally we propose a probabilistic model for integrating the weighted key concepts identified by our method into a query and demonstrate that this integration significantly improves retrieval effectiveness for a large set of natural language description queries derived from trec topics on several newswire and web collections
ambiguous queries ambiguous queries although there are many papers examining ambiguity in information retrieval this paper shows that there is a whole class of ambiguous word that past research has barely explored it is shown that the class is more ambiguous than other word types and is commonly used in queries the lack of test collections containing ambiguous queries is highlighted and a method for creating collections from existing resources is described tests using the new collection show the impact of query ambiguity on an ir system it is shown that conventional systems are incapable of dealing effectively with such queries and that current assumptions about how to improve search effectiveness do not hold when searching on this common query type
automatically identifying localizable queries automatically identifying localizable queries personalization of web search results as a technique for improving user satisfaction has received notable attention in the research community over the past decade much of this work focuses on modeling and establishing a profile for each user to aid in personalization our work takes a more querycentric approach in this paper we present a method for efficient automatic identification of a class of queries we define as localizable from a web search engine query log we determine a set of relevant features and use conventional machine learning techniques to classify queries our experiments find that our technique is able to identify localizable queries with accuracy
realtime automatic tag recommendation realtime automatic tag recommendation tags are usergenerated labels for entities existing research on tag recommendation either focuses on improving its accuracy or on automating the process while ignoring the efficiency issue we propose a highlyautomated novel framework for realtime tag recommendation the tagged training documents are treated as triplets of words docs tags and represented in two bipartite graphs which are partitioned into clusters by spectral recursive embedding sre tags in each topical cluster are ranked by our novel ranking algorithm a twoway poisson mixture model pmm is proposed to model the document distribution into mixture components within each cluster and aggregate words into word clusters simultaneously a new document is classified by the mixture model based on its posterior probabilities so that tags are recommended according to their ranks experiments on largescale tagging datasets of scientific documents citeulike and web pages delicious indicate that our framework is capable of making tag recommendation efficiently and effectively the average tagging time for testing a document is around second with over test documents correctly labeled with the top nine tags we suggested
efficient topk querying over socialtagging networks efficient topk querying over socialtagging networks online communities have become popular for publishing and searching content as well as for finding and connecting to other users usergenerated content includes for example personal blogs bookmarks and digital photos these items can be annotated and rated by different users and these social tags and derived userspecific scores can be leveraged for searching relevant content and discovering subjectively interesting items moreover the relationships among users can also be taken into consideration for ranking search results the intuition being that you trust the recommendations of your close friends more than those of your casual acquaintances
social tag prediction social tag prediction in this paper we look at the social tag prediction problem given a set of objects and a set of tags applied to those objects by users can we predict whether a given tag couldshould be applied to a particular object we investigated this question using one of the largest crawls of the social bookmarking system delicious gathered to date for urls in delicious we predicted tags based on page text anchor text surrounding hosts and other tags applied to the url we found an entropybased metric which captures the generality of a particular tag and informs an analysis of how well that tag can be predicted we also found that tagbased association rules can produce very highprecision predictions as well as giving deeper understanding into the relationships between tags our results have implications for both the study of tagging systems as potential information retrieval tools and for the design of such systems
spectral geometry for simultaneously clustering and ranking query search results spectral geometry for simultaneously clustering and ranking query search results how best to present query search results is an important problem in search engines and information retrieval systems when a single query retrieves many results simply showing them as a long list will provide users with poor overview nowadays ranking and clustering query search results have been two useful separate postprocessing techniques to organize retrieved documents in this paper we proposed a spectral analysis method based on the content similarity networks to integrate the clustering and ranking techniques for improving literature search the new approach organizes all these search results into categories intelligently and simultaneously rank the results in each category a variety of theoretical and empirical studies have demonstrated that the presented method performs well in real applications especially in biomedical literature retrieval moreover any free text information can be analyzed with the new method ie the proposed approach can be applied to various information systems such as web search engines and literature search service
a rankaggregation approach to searching for optimal queryspecific clusters a rankaggregation approach to searching for optimal queryspecific clusters to improve the precision at the very top ranks of a document list presented in response to a query researchers suggested to exploit information induced from clustering of documents highly ranked by some initial search we propose a novel model for ranking such queryspecific clusters by the presumed percentage of relevant documents that they contain the model is based on i proposing a palette of witness cluster properties that purportedly correlate with this percentage ii devising concrete quantitative measures for these properties and iii ordering the clusters via aggregation of rankings induced by these individual measures empirical evaluation shows that our model is consistently more effective than previously suggested methods in detecting clusters containing a high relevantdocument percentage furthermore the precisionattopranks performance of this model transcends that of standard documentbased retrieval and competes with that of a stateoftheart documentbased retrieval approach
a comparative evaluation of different link types on enhancing document clustering a comparative evaluation of different link types on enhancing document clustering with a growing number of works utilizing link information in enhancing document clustering it becomes necessary to make a comparative evaluation of the impacts of different link types on document clustering various types of links between text documents including explicit links such as citation links and hyperlinks implicit links such as coauthorship links and pseudo links such as content similarity links convey topic similarity or topic transferring patterns which is very useful for document clustering in this study we adopt a relaxation labeling rlbased clustering algorithm which employs both content and linkage information to evaluate the effectiveness of the aforementioned types of links for document clustering on eight datasets the experimental results show that linkage is quite effective in improving contentbased document clustering furthermore a series of interesting findings regarding the impacts of different link types on document clustering are discovered through our experiments
spotsigs spotsigs motivated by our work with political scientists who need to manually analyze large web archives of news sites we present spotsigs a new algorithm for extracting and matching signatures for near duplicate detection in large web crawls our spot signatures are designed to favor naturallanguage portions of web pages over advertisements and navigational bars
local text reuse detection local text reuse detection text reuse occurs in many different types of documents and for many different reasons one form of reuse duplicate or nearduplicate documents has been a focus of researchers because of its importance in web search local text reuse occurs when sentences facts or passages rather than whole documents are reused and modified detecting this type of reuse can be the basis of new tools for text analysis in this paper we introduce a new approach to detecting local text reuse and compare it to other approaches this comparison involves a study of the amount and type of reuse that occurs in real documents including trec newswire and blog collections
tscan tscan a topic is defined as a seminal event or activity along with all directly related events and activities it is represented as a chronological sequence of documents by different authors published on the internet in this paper we define a task called topic anatomy which summarizes and associates core parts of a topic graphically so that readers can understand the content easily the proposed topic anatomy model called tscan derives the major themes of a topic from the eigenvectors of a temporal block association matrix then the significant events of the themes and their summaries are extracted by examining the constitution of the eigenvectors finally the extracted events are associated through their temporal closeness and context similarity to form the evolution graph of the topic experiments based on the official tdt corpus demonstrate that the generated evolution graphs comprehensibly describe the storylines of topics moreover in terms of content coverage and consistency the produced summaries are superior to those of other summarization methods based on human composed reference summaries
a new rank correlation coefficient for information retrieval a new rank correlation coefficient for information retrieval in the field of information retrieval one is often faced with the problem of computing the correlation between two ranked lists the most commonly used statistic that quantifies this correlation is kendalls often times in the information retrieval community discrepancies among those items having high rankings are more important than those among items having low rankings the kendalls statistic however does not make such distinctions and equally penalizes errors both at high and low rankings
learning from labeled features using generalized expectation criteria learning from labeled features using generalized expectation criteria it is difficult to apply machine learning to new domains because often we lack labeled problem instances in this paper we provide a solution to this problem that leverages domain knowledge in the form of affinities between input features and classes for example in a baseball vs hockey text classification problem even without any labeled data we know that the presence of the word puck is a strong indicator of hockey we refer to this type of domain knowledge as a labeled feature in this paper we propose a method for training discriminative probabilistic models with labeled features and unlabeled instances unlike previous approaches that use labeled features to create labeled pseudoinstances we use labeled features directly to constrain the models predictions on unlabeled instances we express these soft constraints using generalized expectation ge criteria terms in a parameter estimation objective function that express preferences on values of a model expectation in this paper we train multinomial logistic regression models using ge criteria but the method we develop is applicable to other discriminative probabilistic models the complete objective function also includes a gaussian prior on parameters which encourages generalization by spreading parameter weight to unlabeled features experimental results on text classification data sets show that this method outperforms heuristic approaches to training classifiers with labeled features experiments with human annotators show that it is more beneficial to spend limited annotation time labeling features rather than labeling instances for example after only one minute of labeling features we can achieve accuracy on the ibm vs mac text classification problem using gefl whereas ten minutes labeling documents results in an accuracy of only 
a simple and efficient sampling method for estimating ap and ndcg a simple and efficient sampling method for estimating ap and ndcg we consider the problem of large scale retrieval evaluation recently two methods based on random sampling were proposed as a solution to the extensive effort required to judge tens of thousands of documents while the first method proposed by aslam et al is quite accurate and efficient it is overly complex making it difficult to be used by the community and while the second method proposed by yilmaz et al infap is relatively simple it is less efficient than the former since it employs uniform random sampling from the set of complete judgments further none of these methods provide confidence intervals on the estimated values
a general optimization framework for smoothing language models on graph structures a general optimization framework for smoothing language models on graph structures recent work on language models for information retrieval has shown that smoothing language models is crucial for achieving good retrieval performance many different effective smoothing methods have been proposed which mostly implement various heuristics to exploit corpus structures in this paper we propose a general and unified optimization framework for smoothing language models on graph structures this framework not only provides a unified formulation of the existing smoothing heuristics but also serves as a road map for systematically exploring smoothing methods for language models we follow this road map and derive several different instantiations of the framework some of the instantiations lead to novel smoothing methods empirical results show that all such instantiations are effective with some outperforming the state of the art smoothing methods
deep classification in largescale text hierarchies deep classification in largescale text hierarchies most classification algorithms are best at categorizing the web documents into a few categories such as the top two levels in the open directory project such a classification method does not give very detailed topicrelated class information for the user because the first two levels are often too coarse however classification on a largescale hierarchy is known to be intractable for many target categories with crosslink relationships among them in this paper we propose a novel deepclassification approach to categorize web documents into categories in a largescale taxonomy the approach consists of two stages a search stage and a classification stage in the first stage a categorysearch algorithm is used to acquire the category candidates for a given document based on the category candidates we prune the largescale hierarchy to focus our classification effort on a small subset of the original hierarchy as a result the classification model is trained on the small subset before being applied to assign the category for a new document since the category candidates are sufficiently close to each other in the hierarchy a statisticallanguagemodel based classifier using ngram features is exploited furthermore the structure of the taxonomy can be utilized in this stage to improve the performance of classification we demonstrate the performance of our proposed algorithms on the open directory project with over categories experimental results show that our proposed approach can reach on the measure of mif at the th level which is improvement over topdown based svm classification algorithms
topicbridged plsa for crossdomain text classification topicbridged plsa for crossdomain text classification in many web applications such as blog classification and newsgroup classification labeled data are in short supply it often happens that obtaining labeled data in a new domain is expensive and time consuming while there may be plenty of labeled data in a related but different domain traditional text classification approaches are not able to cope well with learning across different domains in this paper we propose a novel crossdomain text classification algorithm which extends the traditional probabilistic latent semantic analysis plsa algorithm to integrate labeled and unlabeled data which come from different but related domains into a unified probabilistic model we call this new model topicbridged plsa or tplsa by exploiting the common topics between two domains we transfer knowledge across different domains through a topicbridge to help the text classification in the target domain a unique advantage of our method is its ability to maximally mine knowledge that can be transferred between domains resulting in superior performance when compared to other stateoftheart text classification approaches experimental evaluation on different kinds of datasets shows that our proposed algorithm can improve the performance of crossdomain text classification significantly
trnongreedy active learning for text categorization using convex ansductive experimental design trnongreedy active learning for text categorization using convex ansductive experimental design in this paper we propose a nongreedy active learning method for text categorization using leastsquares support vector machines lssvm our work is based on transductive experimental design ted an active learning formulation that effectively explores the information of unlabeled data despite its appealing properties the optimization problem is however nphard and thuslike most of other active learning methodsa greedy sequential strategy to select one data example after another was suggested to find a suboptimum in this paper we formulate the problem into a continuous optimization problem and prove its convexity meaning that a set of data examples can be selected with a guarantee of global optimum we also develop an iterative algorithm to efficiently solve the optimization problem which turns out to be very easytoimplement our text categorization experiments on two text corpora empirically demonstrated that the new active learning algorithm outperforms the sequential greedy algorithm and is promising for active text categorization applications
classifiers without borders classifiers without borders accurate web page classification often depends crucially on information gained from neighboring pages in the local web graph prior work has exploited the class labels of nearby pages to improve performance in contrast in this work we utilize a weighted combination of the contents of neighbors to generate a better virtual document for classification in addition we break pages into fields finding that a weighted combination of text from the target and fields of neighboring pages is able to reduce classification error by more than a third we demonstrate performance on a large dataset of pages from the open directory project and validate the approach using pages from a crawl from the stanford webbase interestingly we find no value in anchor text and unexpected value in page titles and especially titles of parent pages in the virtual document
evaluation over thousands of queries evaluation over thousands of queries information retrieval evaluation has typically been performed over several dozen queries each judged to nearcompleteness there has been a great deal of recent work on evaluation over much smaller judgment sets how to select the best set of documents to judge and how to estimate evaluation measures when few judgments are available in light of this it should be possible to evaluate over many more queries without much more total judging effort the million query track at trec used two document selection algorithms to acquire relevance judgments for more than queries we present results of the track along with deeper analysis investigating tradeoffs between the number of queries and number of judgments shows that up to a point evaluation over more queries with fewer judgments is more costeffective and as reliable as fewer queries with more judgments total assessor effort can be reduced by with no appreciable increase in evaluation errors
novelty and diversity in information retrieval evaluation novelty and diversity in information retrieval evaluation evaluation measures act as objective functions to be optimized by information retrieval systems such objective functions must accurately reflect user requirements particularly when tuning ir systems and learning ranking functions ambiguity in queries and redundancy in retrieved documents are poorly reflected by current evaluation measures in this paper we present a framework for evaluation that systematically rewards novelty and diversity we develop this framework into a specific evaluation measure based on cumulative gain we demonstrate the feasibility of our approach using a test collection based on the trec question answering track
relevance assessment relevance assessment we investigate to what extent people making relevance judgements for a reusable ir test collection are exchangeable we consider three classes of judge gold standard judges who are topic originators and are experts in a particular information seeking task silver standard judges who are task experts but did not create topics and bronze standard judges who are those who did not define topics and are not experts in the task
intuitionsupporting visualization of users performance based on explicit negative higherorder relevance intuitionsupporting visualization of users performance based on explicit negative higherorder relevance modeling the beyondtopical aspects of relevance are currently gaining popularity in ir evaluation for example the discounted cumulated gain dcg measure implicitly models some aspects of higherorder relevance via diminishing the value of relevant documents seen later during retrieval eg due to information cumulated redundancy and effort in this paper we focus on the concept of negative higherorder relevance nhor made explicit via negative gain values in ir evaluation we extend the computation of dcg to allow negative gain values perform an experiment in a laboratory setting and demonstrate the characteristics of nhor in evaluation the approach leads to intuitively reasonable performance curves emphasizing from the users point of view the progression of retrieval towards success or failure we discuss normalization issues when both positive and negative gain values are allowed and conclude by discussing the usage of nhor to characterize test collections
relevance judgments between trec and nontrec assessors relevance judgments between trec and nontrec assessors this paper investigates the agreement of relevance assessments between official trec judgments and those generated from an interactive ir experiment results show that of documents judged relevant by our users matched official trec judgments several factors contributed to differences in the agreements the number of retrieved relevant documents the number of relevant documents judged system effectiveness per topic and the ranking of relevant documents
evaluation measures for preference judgments evaluation measures for preference judgments there has been recent interest in collecting user or assessor preferences rather than absolute judgments of relevance for the evaluation or learning of ranking algorithms since measures like precision recall and dcg are defined over absolute judgments evaluation over preferences will require new evaluation measures that explicitly model them we describe a class of such measures and compare absolute and preference measures over a large trec collection
exploring evaluation metrics exploring evaluation metrics in retrieval experiments an effectiveness metrics is used to generate a score for each systemtopic pair being tested it is then usual to average the systemtopic scores to obtain a system score which is used for the purpose of system comparison in this paper we explore the ramifications of using the geometric mean gmap rather than the arithmetic mean map when computing an aggregate system score from a set of systemtopic scores we find that gmap does indeed handle variability in topic difficulty more consistently than does the usual map aggregation method
a new interpretation of average precision a new interpretation of average precision we consider the question of whether average precision as a measure of retrieval effectiveness can be regarded as deriving from a model of user searching behaviour it turns out that indeed it can be so regarded under a very simple stochastic model of user behaviour
comparing metrics across trec and ntcir comparing metrics across trec and ntcir note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
relevance thresholds in system evaluations relevance thresholds in system evaluations we introduce and explore the concept of an individuals relevance threshold as a way of reconciling differences in outcomes between batch and user experiments
precisionatten considered redundant precisionatten considered redundant information retrieval systems are compared using evaluation metrics with researchers commonly reporting results for simple metrics such as precisionat or reciprocal rank together with more complex ones such as average precision or discounted cumulative gain in this paper we demonstrate that complex metrics are as good as or better than simple metrics at predicting the performance of the simple metrics on other topics therefore reporting of results from simple metrics alongside complex ones is redundant
structuring collections with scattergather extensions structuring collections with scattergather extensions a major component of sensemaking is organizinggrouping labeling and summarizingthe data at hand in order to form a useful mental model a necessary precursor to identifying missing information and to reasoning about the data previous work has shown the scattergather model to be useful in exploratory activities that occur when users encounter unknown document collections however the topic structure communicated by scattergather is closely tied to the behavior of the underlying clustering algorithm this structure may not reflect the mental model most applicable to the information need in this paper we describe the initial design of a mixedinitiative information structuring tool that leverages aspects of the wellstudied scattergather model but permits the user to impose their own desired structure when necessary
text collections for fire text collections for fire the aim of the forum for information retrieval evaluation fire is to create a cranfieldlike evaluation framework in the spirit of trec clef and ntcir for indian language information retrieval for the first year six indian languages have been selected bengali hindi marathi punjabi tamil and telugu this poster describes the tasks as well as the document and topic collections that are to be used at the fire workshop
a longitudinal study of realtime search assistance adoption a longitudinal study of realtime search assistance adoption we present findings from a log based study designed to track the adoption of features of a new realtime query refinement interface deployed on the yahoo search engine several trends from the first four months are noted and discussed
topicrank topicrank note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
talking the talk vs walking the walk talking the talk vs walking the walk traditional information retrieval models assume that users express their information needs via text queries ie their talk in this poster we consider web browsing behavior outside of interactions with retrieval systems ie users walk as an alternative source of signal describing users information needs and compare it to the queryexpressed information needs on a large dataset our findings demonstrate that information needs expressed in different behavior modalities are largely nonoverlapping and that past behavior in each modality is the most accurate predictor of future behavior in that modality results also show that browsing data provides a stronger source of signal than search queries due to its greater volume which explains previous work that has found implicit behavioral data to be a valuable source of information for user modeling and personalization
exploring mouse movements for inferring query intent exploring mouse movements for inferring query intent clickthrough on search results have been successfully used to infer user interest and preferences but are often noisy and potentially ambiguous we explore the potential of a complementary more sensitive signal mouse movements in providing insights into the intent behind a web search query we report preliminary results of studying user mouse movements on search result pages with the goal of inferring user intent in particular to explore whether we can automatically distinguish the different query classes such as navigational vs informational our preliminary exploration confirms the value of studying mouse movements for user intent inference and suggests interesting avenues for future exploration
emulating querybiased summaries using document titles emulating querybiased summaries using document titles generating querybiased summaries can take up a large part of the response time of interactive information retrieval iir systems this paper proposes to use document titles as an alternative to queries in the generation of summaries the use of document titles allows us to pregenerate summaries statically and thus improve the response speed of iir systems our experiments suggest that titlebiased summaries are a promising alternative to querybiased summaries
hierarchical naive bayes models for representing user profiles hierarchical naive bayes models for representing user profiles in this paper we show how a user profile can be enhanced when a more detailed description of the products is included two main assumptions have been considered the first implies that the set of features used to describe an item can be organized into a welldefined set of components or categories and the second is that the users rating for a given item is obtained by combining user opinions of the relevance of each component
a topical pagerank based algorithm for recommender systems a topical pagerank based algorithm for recommender systems in this paper we propose a topical pagerank based algorithm for recommender systems which aim to rank products by analyzing previous useritem relationships and recommend toprank items to potentially interested users we evaluate our algorithm on movielens dataset and empirical experiments demonstrate that it outperforms other stateoftheart recommending algorithms
the impact of history length on personalized search the impact of history length on personalized search personalized search is a promising way to better serve different users information needs search history is one of the major information sources for search personalization we investigated the impact of history length on the effectiveness of personalized ranking we carried out taskbased user study for web search and obtained ranked relevance judgments for all queries query contexts derived from previous queries in the same task are used to rerank results for the current query experimental results show that the performance of personalization generally improves as more queries are accumulated but most of the benefits come from a few immediately preceding queries
user preference choices for complex question answering user preference choices for complex question answering question answering systems increasingly need to deal with complex information needs that require more than simple factoid answers the evaluation of such systems is usually carried out using precision or recallbased system performance metrics previous work has demonstrated that when users are shown two search result lists sidebyside they can reliably differentiate between the qualities of the lists we investigate the consistency between this userbased approach and systemoriented metrics in the question answering environment our initial results indicate that the two methodologies show a high level of disagreement
towards personalized distributed information retrieval towards personalized distributed information retrieval our aim is to investigate if and how the performance of distributed information retrieval dir systems can be improved through personalization toward this aim we are building a testbed of document collections and corresponding personalized relevance judgments in this paper we discuss our intended approach for personalizing the three different phases of the dir process we also describe the test collection we are building and discuss our methodology for evaluating personalized dir using relevance information taken from social bookmarking data
taskaware search personalization taskaware search personalization search personalization has been pursued in many ways in order to provide better result rankings and better overall search experience to individual users however blindly applying personalization to all user queries for example by a background model derived from the users longterm queryandclick history is not always appropriate for aiding the user in accomplishing her actual task user interests change over time a user sometimes works on very different categories of tasks within a short timespan and historybased personalization may impede a users desire of discovering new topics in this paper we propose a personalization framework that is selective in a twofold sense first it selectively employs personalization techniques for queries that are expected to benefit from prior history information while refraining from undue actions otherwise second we introduce the notion of tasks representing different granularity levels of a user profile ranging from very specific search goals to broad topics and base our reasoning selectively on queryrelevant user tasks these considerations are cast into a statistical language model for tasks queries and documents supporting both judicious query expansion and result reranking the effectiveness of our method is demonstrated by an empirical user study
personal vs nonpersonal blogs personal vs nonpersonal blogs we address the task of separating personal from nonpersonal blogs and report on a set of baseline experiments where we compare the performance on a small set of features across a set of five classifiers we show that with a limited set of features a performance of up to can be obtained
exploiting subjectivity analysis in blogs to improve political leaning categorization exploiting subjectivity analysis in blogs to improve political leaning categorization in this paper we address a relatively new and interesting text categorization problem classify a political blog as either liberal or conservative based on its political leaning our subjectivity analysis based method is twofold we identify subjective sentences that contain at least two strong subjective clues based on the general inquirer dictionary from subjective sentences identified we extract opinion expressions and other features to build political leaning classifiers experimental results with a political blog corpus we built show that by using features from subjective sentences can significantly improve the classification performance in addition by extracting opinion expressions from subjective sentences we are able to reveal opinions that are characteristic of a specific political leaning to some extent
ranking opinionated blog posts using opinionfinder ranking opinionated blog posts using opinionfinder the aim of an opinion finding system is not just to retrieve relevant documents but to also retrieve documents that express an opinion towards the query target entity in this work we propose a way to use and integrate an opinionidentification toolkit opinionfinder into the retrieval process of an information retrieval ir system such that opinionated relevant documents are retrieved in response to a query in our experiments we vary the number of topranked documents that must be parsed in response to a query and investigate the effect on opinion retrieval performance and required parsing time we find that opinion finding retrieval performance is improved by integrating opinionfinder into the retrieval system and that retrieval performance grows as more posts are parsed by opinionfinder however the benefit eventually tails off at a deep rank suggesting that an optimal setting for the system has been achieved
searching blogs and news searching blogs and news blognews search engines are very important channels to reach information about the realtime happenings in this paper we study the popular queries collected over one year period and compare their search results returned by a blog search engine ie technorati and a news search engine ie google news we observed that the numbers of hits returned by the two search engines for the same set of queries were highly correlated suggesting that blogs often provide commentary to current events reported in news as many popular queries are related to some events we further observed a high cohesiveness among the returned search results for these queries
aggregated clickthrough data in a homogeneous user community aggregated clickthrough data in a homogeneous user community there are many proposed methods for using clickthrough data for common queries to improve the quality of search results returned for that query in this study we examine the search behaviour of users in a closeknit community on such queries we argue that the benefit of using aggregated clickthrough data varies from task to task it may improve document rankings for navigational or specific informational queries but is less likely to be of value to users issuing a broad informational query
to tag or not to tag to tag or not to tag we present hamlet a suite of principles scoring models and algorithms to automatically propagate metadata along edges in a document neighborhood as a showcase scenario we consider tag prediction in communitybased web tagging applications experiments using realworld data demonstrate the viability of our approach in largescale environments where tags are scarce to the best of our knowledge hamlet is the first system to promote an efficient and precise reuse of shared metadata in highly dynamic largescale web tagging systems
exploring question subjectivity prediction in community qa exploring question subjectivity prediction in community qa in this paper we begin to investigate how to automatically determine the subjectivity orientation of questions posted by real users in community question answering cqa portals subjective questions seek answers containing private states such as personal opinion and experience in contrast objective questions request objective verifiable information often with support from reliable sources knowing the question orientation would be helpful not only for evaluating answers provided by users but also for guiding the cqa engine to process questions more intelligently our experiments on yahoo answers data show that our method exhibits promising performance
on the evolution of the yahoo answers qa community on the evolution of the yahoo answers qa community while question answering communities have been gaining popularity for several years we wonder if the increased popularity actually improves or degrades the user experience in addition automatic qa systems which utilize different sources such as search engines and social media are emerging rapidly qa communities have already created abundant resources of millions of questions and hundreds of millions of answers the question whether they will continue to serve as an effective source is of information for web search and question answering is of vital importance in this poster we investigate the temporal evolution of a popular qa community yahoo answers with respect to its effectiveness in answering three basic types of questions factoid opinion and complex questions our experiments show that yahoo answers keeps growing rapidly while its overall quality as an information source for factoid questionanswering degrades however instead of answering factoid questions it might be more effective to answer opinion and complex questions
detecting synonyms in social tagging systems to improve content retrieval detecting synonyms in social tagging systems to improve content retrieval collaborative tagging used in online social content systems is naturally characterized by many synonyms causing low precision retrieval we propose a mechanism based on user preference profiles to identify synonyms that can be used to retrieve more relevant documents by expanding the users query using a popular online book catalog we discuss the effectiveness of our method over usual similarity based expansion methods
soping soping with the booming development of the web popular chinese forums enable people to find experienced customers reviews for products in order to get an allaround opinion about one product users need to go through plenty of web pages which is timeconsuming and inefficient consequently automatic review mining and summarization has become a hot research topic recently however previous approaches are not applicable for mining chinese customer reviews in this paper we introduce soping a chinese customer review mining system that mines reviews from forums specifically we propose a novel searchbased approach to extract product features and a featureoriented sentence orientation determination method our experimental results show that our proposed techniques are highly effective
combining learnbased and lexiconbased techniques for sentiment detection without using labeled examples combining learnbased and lexiconbased techniques for sentiment detection without using labeled examples in this work we propose a novel scheme for sentiment classification without labeled examples which combines the strengths of both learnbased and lexiconbased approaches as follows we first use a lexiconbased technique to label a portion of informative examples from given task or domain then learn a new supervised classifier based on these labeled ones finally apply this classifier to the task the experimental results indicate that proposed scheme could dramatically outperform learnbased and lexiconbased techniques
semisupervised spam filtering semisupervised spam filtering the results of the ecmlpkdd discovery challenge suggest that semisupervised learning methods work well for spam filtering when the source of available labeled examples differs from those to be classified we have attempted to reproduce these results using data from the and trec spam track and have found the opposite effect methods like selftraining and transductive support vector machines yield inferior classifiers to those constructed using supervised learning on the labeled data alone we investigate differences between the ecmlpkdd and trec data sets and methodologies that may account for the opposite results
limits of opinionfinding baseline systems limits of opinionfinding baseline systems in opinionfinding the retrieval system is tasked with retrieving not just relevant documents but which also express an opinion towards the query target entity most opinionfinding systems are based on a twostage approach where initially the system aims to retrieve relevant documents which are then reranked according to the extent to which they are detected to be of an opinionated nature in this work we investigate how the underlying baseline retrieval system performance affects the overall opinionfinding performance we apply two effective opinionfinding techniques to all the baseline runs submitted to the trec blog track and draw new insights and conclusions
web query translation via web log mining web query translation via web log mining this paper describes a method to automatically acquire query translation pairs by mining web clickthrough data the extraction requires no crawling or chinese words segmentation and can capture popular translations experimental results on a real clickthrough data show that only of the extracted queries are in the dictionary and our method can achieve in top to in top precision in translating web queries moreover the extracted translations are semantically relevant to the source query which is particularly useful for crosslingual information retrieval clir
analyzing web text association to disambiguate abbreviation in queries analyzing web text association to disambiguate abbreviation in queries we introduce a statistical model for abbreviation disambiguation in web search based on analysis of web data resources including anchor text click log and query log by combining evidence from multiple sources we are able to accurately disambiguate the abbreviation in queries experiments on real web search queries show promising results
bloggers as experts bloggers as experts we address the task of blog feed distillation to find blogs that are principally devoted to a given topic the task may be viewed as an association finding task between topics and bloggers under this view it resembles the expert finding task for which a range of models have been proposed we adopt two language modelingbased approaches to expert finding and determine their effectiveness as feed distillation strategies the two models capture the idea that a human will often search for key blogs by spotting highly relevant posts the posting model or by taking global aspects of the blog into account the blogger model results show the blogger model outperforms the posting model and delivers stateofthe art performance outofthebox
search effectiveness with a breadthfirst crawl search effectiveness with a breadthfirst crawl previous scalability experiments found that early precision improves as collection size increases however that was under the assumption that a collections documents are all sampled with uniform probability from the same population we contrast this to a large breadthfirst web crawl an important scenario in realworld web search where the early documents have quite different characteristics from the later documents
guide focused crawler efficiently and effectively using online topical importance estimation guide focused crawler efficiently and effectively using online topical importance estimation focused crawling is a critical technique for topical resource discovery on the web we propose a new frontier prioritizing algorithm namely the otie online topical importance estimation algorithm which efficiently and effectively combines linkbased and contentbased analysis to evaluate the priority of an uncrawled url in the frontier we then demonstrate oties advantages over traditional prioritizing algorithms by real crawling experiments
web page retrieval in ubiquitous sensor environments web page retrieval in ubiquitous sensor environments this paper proposes new concept of query free web search for daily living we ordinarily benefit from additional information about our daily activities that we are currently engaged in when washing a coffee maker for example we receive the benefit if we obtain such information as cleaning a coffee maker with vinegar removes its stain well our proposed method automatically searches for a web page including such information relates to an activity of daily living when the activity is performed we assume that wireless sensor nodes are attached to daily objects to detect object use our method makes a query from the names of objects which are used then the method retrieves a web page relates to the activity of daily living by using the query
automatic document prior feature selection for web retrieval automatic document prior feature selection for web retrieval document prior features such as pagerank and url depth can improve the retrieval effectiveness of web information retrieval ir systems however not all queries equally benefit from the application of a document prior feature this paper aims to investigate whether the retrieval performance can be further enhanced by selecting the best document prior feature on a perquery basis we present a novel method for selecting the best document prior feature on a perquery basis we evaluate our technique on the trec gov web test collection and its associated trec web search tasks our experiments demonstrate the effectiveness and robustness of our proposed selection method
using parsimonious language models on web data using parsimonious language models on web data in this paper we explore the use of parsimonious language models for web retrieval these models are smaller thus more efficient than the standard language models and are therefore well suited for largescale web retrieval we have conducted experiments on four trec topic sets and found that the parsimonious language model results in improvement of retrieval effectiveness over the standard language model for all datasets and measures in all cases the improvement is significant and more substantial than in earlier experiments on newspapernewswire data
query preprocessing query preprocessing in this poster paper we propose a novel approach to improve web search relevancy by tokenizing a vietnamese query text prior submitting it to a search engine evaluations demonstrate its effectiveness and practical value
adimage adimage with the prevalence of recording devices and the ease of media sharing consumers are embracing huge amounts of internet videos there arise the needs for effective video advertisement systems following their phenomenal success in text we propose a novel advertising system adimage which automatically associates relevant ads by matching characteristic images referred to as adimages analogous to adwords here the proposed image matching method is invariant to certain distortions commonly observed in shared videos adimage also avoids the pitfalls of poor tagging qualities in shared videos and provides a brandnew venue to specify ad targets by image objects moreover we formulate the image matching scores and the parameterized bidding information as a nonlinear optimization problem for maximizing the system revenues and user perception
bagofvisualwords expansion using visual relatedness for video indexing bagofvisualwords expansion using visual relatedness for video indexing bagofvisualwords bow has been popular for visual classification in recent years in this paper we propose a novel bow expansion method to alleviate the effect of visual word correlation problem we achieve this by diffusing the weights of visual words in bow based on visual word relatedness which is rigorously defined within a visual ontology the proposed method is tested in video indexing experiment on trecvid video retrieval benchmark and an improvement of over the traditional bow is reported
a word shape coding method for camerabased document images a word shape coding method for camerabased document images this paper reports a word shape coding method to facilitate retrieval of camerabased document images without ocr due to perspective distortion many reported word shape coding methods fail on camerabased images in this paper the problem is addressed by approximating the perspective transformation with an affine transformation and employing an affine invariant namely length ratio to represent the connected components components in a document image are classified into a few clusters each of which is assigned with a representative symbol retrieval are based on words comprising of symbols the experiment results showed that the proposed method achieved an average retrieval precision of and recall of 
term clouds as surrogates for user generated speech term clouds as surrogates for user generated speech user generated spoken audio remains a challenge for automatic speech recognition asr technology and contentbased audio surrogates derived from asrtranscripts must be error robust an investigation of the use of term clouds as surrogates for podcasts demonstrates that asr term clouds closely approximate term clouds derived from humangenerated transcripts across a range of cloud sizes a user study confirms the conclusion that asrclouds are viable surrogates for depicting the content of podcasts
a faceted interface for multimedia search a faceted interface for multimedia search with the rapid increase in online video services video retrieval systems are becoming increasingly important search tools to many users in many different fields in this poster we present a novel video retrieval interface which supports the creation of multiple search facets to aid users carrying out complex multifaceted search tasks the interface allows multiple searches to be executed and viewed simultaneously and allows material to be reorganized between the facets an experiment is presented which compares the faceted interface to a tabbed interface similar to that on modern web browsers and some preliminary results are given
wisa wisa we present a novel web image semantic analysis wisa system which explores the problem of adaptively modeling the distributions of the semantic labels of the web image on its surrounding text to deal with this problem we employ a new piecewise penalty weighted regression model to learn the weights of the contributions of the different parts of the surrounding text to the semantic labels of images experimental results on a real web image data set show that it can improve the performance of web image semantic annotation significantly
onebutton search extracts wider interests onebutton search extracts wider interests this poster presents an overview of the characteristics of a onebutton information retrieval interface with closed captions from tv watching activities which is intended to lighten the burden of remembering and entering query terms while watching tv we investigated this interface with an experimental system named video bookmarking search which estimates query terms from closed captions with namedentity recognition and sentence labeling techniques according to an empirical evaluation for search queries from bookmarks using seven actual tv shows on city life travel health and cuisine we found wider queries and search results are acceptable through the queryinputfree interface despite the fact that the number of queries and search results that are directly relevant to the users original intentions is not high the main reason is a watching users interest is wider than what is expressed with query terms
product retrieval for grocery stores product retrieval for grocery stores we introduce a grocery retrieval system that maps shopping lists written in natural language into actual products in a grocery store we have developed the system using nine months of shopping basket data from a large finnish supermarket to evaluate the system we used real shopping lists gathered from customers of the supermarket our system achieves over precision for products at rank one and the precision is around for products at rank 
a reranking model for genomics aspect search a reranking model for genomics aspect search in this paper we propose a reranking model to improve the aspectlevel performance in the biomedical domain this model iteratively computes the maximum hidden aspect for every retrieved passage and then reranks these passages from aspect subsets the experimental results show the improvements of the aspectlevel performance up to for genomics topics and for genomics topics
improving biomedical document retrieval using domain knowledge improving biomedical document retrieval using domain knowledge research articles typically introduce new results or findings and relate them to knowledge entities of immediate relevance however a large body of context knowledge related to the results is often not explicitly mentioned in the article to overcome this limitation the stateoftheart information retrieval approaches rely on the latent semantic analysis in which terms in articles are projected to a lower dimensional latent space and best possible matches in this space are identified however this approach may not perform well enough if the number of explicit knowledge entities in the articles is too small compared to the amount of knowledge in the domain we address the problem by exploiting a domain knowledge layer a rich network of relations among knowledge entities in the domain extracted from a large corpus of documents the knowledge layer supplies the context knowledge that lets us relate different knowledge entities and hence improve the information retrieval performance we develop and study a new framework for i learning and aggregating the relations in the knowledge layer from the literature corpus ii and for exploiting these relations to improve the informationretrieval of relevant documents
kleio kleio kleio is an advanced information retrieval ir system developed at the uk national centre for text mining nactem the system offers textual and metadata searches across medline and provides enhanced searching functionality by leveraging terminology management technologies
enhancing keywordbased botanical information retrieval with information extraction enhancing keywordbased botanical information retrieval with information extraction keywordbased retrieval matches search terms and documents via term cooccurrence such an approach does not allow matching based on the specific plant characteristic descriptions that are often used in botanical text retrieval this study applies information extraction techniques to automatically extract plant characteristic information from text and allows users to search using such information in combination with keywords an evaluation experiment was conducted using actual users the results indicate that this approach enhances taskbased retrieval performance
how medical expertise influences web search interaction how medical expertise influences web search interaction domain expertise can have an important influence on how people search in this poster we present findings from a logbased study into how medical domain experts search the web for information related to their expertise as compared with nonexperts we find differences in sites visited query vocabulary and search behavior the findings have implications for the automatic identification of domain experts from interaction logs and the use of domain knowledge in applications such as query suggestion or page recommendation to support nonexperts
generating diverse katakana variants based on phonemic mapping generating diverse katakana variants based on phonemic mapping in japanese it is quite common for the same word to be written in several different ways this is especially true for katakana words which are typically used for transliterating foreign languages this ambiguity becomes critical for automatic processing such as information retrieval ir to tackle this problem we propose a simple but effective approach to generating katakana variants by considering phonemic representation of the original language for a given word the proposed approach is evaluated through an assessment of the variants it generates also the impact of the generated variants on ir is studied in comparison to an existing approach using katakana rewriting rules
exploiting sequential dependencies for expert finding exploiting sequential dependencies for expert finding we propose an expert finding method based on assumption of sequential dependence between a candidate expert and the query terms in the scope of a document we assume that the strength of relation of a candidate to the documents content depends on its position in this document with respect to the positions of the query terms the experiments on the official enterprise trec data demonstrate the advantage of our method over the method based on independence of query terms and persons in a document
modeling expert finding as an absorbing random walk modeling expert finding as an absorbing random walk we introduce a novel approach to expert finding based on multistep relevance propagation from documents to related candidates relevance propagation is modeled with an absorbing random walk the evaluation on the two official enterprise trec data sets demonstrates the advantage of our method over the stateoftheart method based on onestep propagation
a scalable assistant librarian a scalable assistant librarian in this paper we discuss our work in progress towards a scalable hierarchical classification system for books using the library of congress subject hierarchy we examine the characteristics of this domain which make the problem very challenging and we look at several appropriate performance measurements we show that both hieron and hierarchical support vector machines perform moderately well
information retrieval on bug locations by learning colocated bug report clusters information retrieval on bug locations by learning colocated bug report clusters bug locating usually involves intensive search activities and incurs unpredictable cost of labor and time an issue of information retrieval on bug locations is particularly addressed to facilitate identifying bugs from software code in this paper a novel bug retrieval approach with colocation shrinkage cs is proposed the proposed approach has been implemented in opensource software projects collected from realworld repositories and consistently improves the retrieval accuracy of a stateoftheart support vector machine svm model
summarization of compressed text images summarization of compressed text images automatic summarization of jbig coded textual images is discussed compressed images are partially decompressed to compute relevant features the feature extraction method is free from using any character recognition module summary sentences are ranked experiment considers documents in indic scripts that lack in having any efficient ocr systems script independent aspect of the approach is highlighted through use of two most popular indic scripts sentence selection efficiency of about is achieved when judged against manmade summarization a nonparametric distributionfree rank statistic shows a correlation coefficient of as a measure of the minimum strength of the associations between sentence ranking by machine and human
a method for transferring retrieval scores between collections with nonoverlapping vocabularies a method for transferring retrieval scores between collections with nonoverlapping vocabularies we present a method for projecting retrieval scores across two corpora with a shared parallel corpus
improving relevance feedback in language modeling with score regularization improving relevance feedback in language modeling with score regularization we demonstrate that regularization can improve feedback in a language modeling framework
theoretical bounds on and empirical robustness of score regularization to different similarity measures theoretical bounds on and empirical robustness of score regularization to different similarity measures we present theoretical bounds and empirical robustness of score regularization given changes in the similarity measure
a study of query length a study of query length we analyse query length and fit powerlaw and poisson distributions to four different query sets we provide a practical model for query length based on the truncation of a poisson distribution for short queries and a powerlaw distribution for longer queries that better fits real query length distributions than earlier proposals
dont have a stemmer dont have a stemmer the choice of indexing terms used to represent documents crucially determines how e ective subsequent retrieval will be ir systems commonly use rulebased stemmers to normalize surface word forms to combat the problem of not finding documents that contain words related to query terms by inflectional or derivational morphology but such stemmers are not available in all languages in this paper we explore the effectiveness of unsupervised morphological segmentation as an alternative to stemming using test sets in thirteen european languages we find that unsupervised segmentation is significantly better than unnormalized words in several cases by more than however rulebased stemming if available is better in low complexity languages we also compare these methods to the use of character ngrams finding that on average ngrams yield the best performance
parsimonious concept modeling parsimonious concept modeling note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
parsimonious relevance models parsimonious relevance models we describe a method for applying parsimonious language models to reestimate the term probabilities assigned by relevance models we apply our method to six topic sets from test collections in five different genres our parsimonious relevance models i improve retrieval effectiveness in terms of map on all collections ii significantly outperform their nonparsimonious counterparts on most measures and iii have a precision enhancing effect unlike other blind relevance feedback methods
authortopic evolution analysis using threeway nonnegative paratucker authortopic evolution analysis using threeway nonnegative paratucker analyzing threeway data has attracted a lot of attention recently due to the intrinsic rich structures in realworld datasets the paratucker model has been proposed to combine the axis capabilities of the parafac model and the structural generality of the tucker model however no algorithms have been developed for fitting the paratucker model in this paper we propose tanpt algorithm to solve the paratucker model we apply the algorithm for temporal relation coclustering on authortopic evolution experiments on dblp datasets demonstrate its effectiveness
exploiting proximity feature in bigram language model for information retrieval exploiting proximity feature in bigram language model for information retrieval language modeling approaches have been effectively dealing with the dependency among query terms based on ngram such as bigram or trigram models however bigram language models suffer from adjacencysparseness problem which means that dependent terms are not always adjacent in documents but can be far from each other sometimes with distance of a few sentences in a document to resolve the adjacencysparseness problem this paper proposes a new type of bigram language model by explicitly incorporating the proximity feature between two adjacent terms in a query experimental results on three test collections show that the proposed bigram language model significantly improves previous bigram model as well as taos approach the stateofart method for proximitybased method
measuring concept relatedness using language models measuring concept relatedness using language models over the years the notion of concept relatedness has attracted considerable attention a variety of approaches based on ontology structure information content association or context have been proposed to indicate the relatedness of abstract ideas we propose a method based on the cross entropy reduction between language models of concepts which are estimated based on documentconcept assignments the approach shows improved or competitive results compared to stateoftheart methods on two test sets in the biomedical domain
querydrift prevention for robust query expansion querydrift prevention for robust query expansion pseudofeedbackbased automatic query expansion yields effective retrieval performance on average but results in performance inferior to that of using the original query for many information needs we address an important cause of this robustness issue namely the query drift problem by fusing the results retrieved in response to the original query and to its expanded form our approach posts performance that is significantly better than that of retrieval based only on the original query and more robust than that of retrieval using the expanded query
adaptive labeldriven scaling for latent semantic indexing adaptive labeldriven scaling for latent semantic indexing this paper targets on enhancing latent semantic indexing lsi by exploiting category labels specifically in the termdocument matrix the vector for each term either appearing in labels or semantically close to labels is scaled before performing singular value decomposition svd to boost its impact on the generated left singular vectors as a result the similarities among documents in the same category are increased furthermore an adaptive scaling strategy is designed to better utilize the hierarchical structure of categories experimental results show that the proposed approach is able to significantly improve the performance of hierarchical text categorization
fixedthreshold smo for joint constraint learning algorithm of structural svm fixedthreshold smo for joint constraint learning algorithm of structural svm in this paper we describe a fixedthreshold sequential minimal optimization fsmo for a joint constraint learning algorithm of structural classification svm problems because fsmo uses the fact that the joint constraint formulation of structural svm has b fsmo breaks down the quadratic programming qp problems of structural svm into a series of smallest qp problems each involving only one variable by using only one variable fsmo is advantageous in that each qp subproblem does not need subset selection
posterior probabilistic clustering using nmf posterior probabilistic clustering using nmf we introduce the posterior probabilistic clustering ppc which provides a rigorous posterior probability interpretation for nonnegative matrix factorization nmf and removes the uncertainty in clustering assignment furthermore ppc is closely related to probabilistic latent semantic indexing plsi
on document splitting in passage detection on document splitting in passage detection passages can be hidden within a text to circumvent their disallowed transfer such release of compartmentalized information is of concern to all corporate and governmental organization we explore the methodology to detect such hidden passages within a document a document is divided into passages using various document splitting techniques and a text classifier is used to categorize such passages we present a novel document splitting technique called dynamic windowing which significantly improves precision recall and f measure
learning with support vector machines for querybymultipleexamples learning with support vector machines for querybymultipleexamples we explore an alternative information retrieval paradigm called querybymultipleexamples qbme where the information need is described not by a set of terms but by a set of documents intuitive ideas for qbme include using the centroid of these documents or the wellknown rocchio algorithm to construct the query vector we consider this problem from the perspective of text classification and find that a better query vector can be obtained through learning with support vector machines svms for online queries we show how svms can be learned from oneclass examples in linear time for offline queries we show how svms can be learned from positive and unlabeled examples together in linear or polynomial time the effectiveness and efficiency of the proposed approaches have been confirmed by our experiments on four realworld datasets
question classification with semantic tree kernel question classification with semantic tree kernel question classification plays an important role in most question answering systems in this paper we exploit semantic features in support vector machines svms for question classification we propose a semantic tree kernel to incorporate semantic similarity information a diverse set of semantic features is evaluated experimental results show that svms with semantic features especially semantic classes can significantly outperform the stateoftheart systems
generalising multiple capturerecapture to nonuniform sample sizes generalising multiple capturerecapture to nonuniform sample sizes algorithms in distributed information retrieval often rely on accurate knowledge of the size of a collection the multiple capturerecapture method of shokouhi et al is one of the more reliable algorithms for determining collection size but it relies on samples with a uniform number of documents such uniform samples are often hard to obtain in a working system
predicting when browsing context is relevant to search predicting when browsing context is relevant to search we investigate a representative case of sudden information need change of web users by analyzing search engine query logs we show that the majority of queries submitted by users after browsing documents in the news domain are related to the most recently browsed document we investigate ways of identifying whether a query is a good candidate for contextualization conditioned on the most recently browsed document by a user we build a successful classifier for this task which achieves precision at recall
xmlaided phrase indexing for hypertext documents xmlaided phrase indexing for hypertext documents we combine techniques of xml mining and text mining for the benefit of information retrieval by manipulating the word sequence according to the xml structure of the markedup text we strengthen phrase boundaries so that they are more obvious to the algorithms that extract multiword sequences from text consequently the quality of the indexed phrases improves which has a positive effect on the average precision measured by the inex standards
proximityaware scoring for xml retrieval proximityaware scoring for xml retrieval proximityaware scoring functions lead to significant effectiveness improvements for text retrieval for xml ir we can sometimes enhance the retrieval quality by exploiting knowledge about the document structure combined with established text ir methods this paper introduces modified proximity scores that take the document structure into account and demonstrates the effect for the inex benchmark
locating relevant text within xml documents locating relevant text within xml documents traditional document retrieval has shown to be a competitive approach in xml element retrieval which is counterintuitive since the element retrieval task requests all and only relevant document parts to be retrieved this paper conducts a comparative analysis of document and element retrieval highlights the relative strengths and weaknesses of both approaches and explains the relative effectiveness of document retrieval approaches at element retrieval tasks
a flexible extension of xpath to improve xml querying a flexible extension of xpath to improve xml querying this work presents a flexible xml selection language flexpath which allows the formulation of flexible constraints on both structure and content of xml documents some experimental results obtained with a preliminary prototype are described in order to show that the idea promises good results
combining document and paragraphbased entity ranking combining document and paragraphbased entity ranking we study entity ranking on the inex entity track and propose a simple graphbased ranking approach that enables to combine scores on document and paragraph level the combined approach improves the retrieval results not only on the inex testset but similarly on trecs expert finding task
reranking search results using documentpassage graphs reranking search results using documentpassage graphs we present a novel passagebased approach to reranking documents in an initially retrieved list so as to improve precision at top ranks while most work on passagebased document retrieval ranks a document based on the query similarity of its constituent passages our approach leverages information about the centrality of the document passages with respect to the initial document list passage centrality is induced over a bipartite documentpassage graph wherein edge weights represent documentpassage similarities empirical evaluation shows that our approach yields effective reranking performance furthermore the performance is superior to that of previously proposed passagebased document ranking methods
utilizing phrase based semantic information for term dependency utilizing phrase based semantic information for term dependency previous work on term dependency has not taken into account semantic information underlying query phrases in this work we study the impact of utilizing phrase based concepts for term dependency we use wikipedia to separate important and less important term dependencies and treat them accordingly as features in a linear featurebased retrieval model we compare our method with a markov random field mrf model on four trec document collections our experimental results show that utilizing phrase based concepts improves the retrieval effectiveness of term dependency and reduces the size of the feature set to large extent
inferring the most important types of a query inferring the most important types of a query in this paper we present a technique for ranking the most important types or categories for a given query rather than trying to find the category of the query known as query categorization our approach seeks to find the most important types related to the query results not necessarily the query category falls into this ranking of types and therefore our approach can be complementary
on multiword entity ranking in peertopeer search on multiword entity ranking in peertopeer search previously we postulated the advantage of using entity extraction to implement a new peertopeer pp search framework for reducing network traffic and providing a trade off between precision and recall we now propose an entity ranking method designed for the short documents characteristic of pp which significantly improves both precision and recall in top results pp search we construct a dynamic entity corpus using ngrams statistics and metadata study its reliability and use it to identify correlations between user query terms
sitebased dynamic pruning for query processing in search engines sitebased dynamic pruning for query processing in search engines web search engines typically index and retrieve at the page level in this study we investigate a dynamic pruning strategy that allows the query processor to first determine the most promising websites and then proceed with the similarity computations for those pages only within these sites
exploiting mds projections for crosslanguage ir exploiting mds projections for crosslanguage ir in this paper we describe some preliminary work on using monolingual projections of document collections for performing crosslanguage information retrieval tasks the proposed methodology uses multidimensional scaling for projecting the vectorspace representations of a given multilingual document collection into spaces of lower dimensionality an independent projection is computed for each different language and the structural similarities of the resulting projections are exploited for information retrieval tasks
local approximation of pagerank and reverse pagerank local approximation of pagerank and reverse pagerank we consider the problem of approximating the pagerank of a target node using only local information provided by a link server we prove that local approximation of pagerank is feasible if and only if the graph has low indegree and admits fast pagerank convergence while natural graphs such as the web graph are abundant with high indegree nodes making local pagerank approximation too costly we show that reverse natural graphs tend to have low indegree while maintaining fast pagerank convergence it follows that calculating reverse pagerank locally is frequently more feasible than computing pagerank locally finally we demonstrate the usefulness of reverse pagerank in five different applications
improving text classification accuracy using topic modeling over an additional corpus improving text classification accuracy using topic modeling over an additional corpus the world wide web has many document repositories that can act as valuable sources of additional data for various machine learning tasks in this paper we propose a method of improving text classification accuracy by using such an additional corpus that can easily be obtained from the web this additional corpus can be unlabeled and independent of the given classification task the method proposed here uses topic modeling to extract a set of topics from the additional corpus those extracted topics then act as additional features of the data of the given classification task an evaluation on the rcv dataset shows significant improvement over a baseline method
an algorithm for text categorization an algorithm for text categorization a novel and efficient learning algorithm is proposed for the binary linear classification problem the algorithm is trained using the rocchios relevance feedback technique and builds a classifier by the intermediate hyperplane of two common tangent hyperplanes for the given category and its complement experimental results presented are very encouraging and justify the need for further research
hypergraph partitioning for document clustering hypergraph partitioning for document clustering hypergraph partitioning has been considered as a promising method to address the challenges of high dimensionality in document clustering with documents modeled as vertices and the relationship among documents captured by the hyperedges the goal of graph partitioning is to minimize the edge cut therefore the definition of hyperedges is vital to the clustering performance while several definitions of hyperedges have been proposed a systematic understanding of desired characteristics of hyperedges is still missing to that end in this paper we first provide a unified clique perspective of the definition of hyperedges which serves as a guide to define hyperedges with this perspective based on the concepts of hypercliques and shared reverse nearest neighbors we propose three new types of clique hyperedges and analyze their properties regarding purity and size issues finally we present an extensive evaluation using realworld document datasets the experimental results show that with shared reverse nearest neighbor based hyperedges the clustering performance can be improved significantly in terms of various external validation measures without the need for fine tuning of parameters
pagerank based clustering of hypertext document collections pagerank based clustering of hypertext document collections clustering hypertext document collection is an important task in information retrieval most clustering methods are based on document content and do not take into account the hypertext links here we propose a novel pagerank based clustering prc algorithm which uses the hypertext structure the prc algorithm produces graph partitioning with high modularity and coverage the comparison of the prc algorithm with two content based clustering algorithms shows that there is a good match between prc clustering and content based clustering
an alignmentbased pattern representation model for information extraction an alignmentbased pattern representation model for information extraction note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
relational distancebased collaborative filtering relational distancebased collaborative filtering in this paper we present a novel hybrid recommender system called relationalcf which integrate content and demographic information into a collaborative filtering framework by using relational distance computation approaches without the effort of form transformation and feature construction our experiments suggest that the effective combination of various kinds of information based on relational distance approaches provides improved accurate recommendations than other approaches
clustering search results for mobile terminals clustering search results for mobile terminals mobile terminals such as cell phones are much more restricted in terms of inputoutput functionality and therefore some special techniques must be incorporated to enable them to be easily used for web searching further searching for a location name is related to a dazzling variety of topics we relate these two factors to each other to yield a new search system for map and text information presenting search results as clusters is helpful for users especially in a mobile environment the system makes mobile web searching easier and more efficient
refining search results with facet landscapes refining search results with facet landscapes note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
icetea icetea note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
crosslingual search over european languages crosslingual search over european languages in this paper we present a system for crosslingual information retrieval which can handle tens of languages and millions of documents functioning of the system is demonstrated on corpus of european legislation languages more than documents per language the system uses an interactive webinterface which can take advantage of a predefined thesaurus allowing the user to dynamically rerank the retrieval results based on the mapping onto a predefined thesaurus
social recommendations at work social recommendations at work online communities have become popular for publishing and searching content and also for connecting to other users usergenerated content includes for example personal blogs bookmarks and digital photos items can be annotated and rated by different users and users can connect to others that are usually friends andor share common interests
bilkent news portal bilkent news portal note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
geographic ir and visualization in time and space geographic ir and visualization in time and space this demonstration will show how graphical geospatial query specifications can be used to obtain sets of georeferenced data ranked by probability of relevance and displayed geographically and temporally in a geospatial browser with temporal support
finegrained relevance feedback for xml retrieval finegrained relevance feedback for xml retrieval this demonstration presents an xml ir system that allows users to give feedback of different granularities and types using dempstershafer theory of evidence to compute expanded and reweighted queries
dynamic visualization of music classification systems dynamic visualization of music classification systems note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
from concepts to implementation and visualization from concepts to implementation and visualization researchers have been studying and developing teaching materials for information retrieval ir such as toolkits also have been built that provide handson experience to students for example irtoolbox is an effort to close the gap between the students understanding of ir concepts and reallife indexing and search systems such tools might be good for helping students in nontechnical areas such as in the library and information science field to develop their conceptual model of search engines however they do not cover emerging topics and skills such as contentbased image retrieval cbir and fusion search although there is open source software such as those in httpwwwsearchtoolscomtoolstoolsopensourcehtml that can be used to teach basic and advanced ir topics they require a student to have highlevel technical knowledge and to spend a long time to gain a practical understanding of these topics
exploiting xml structure to improve information retrieval in peertopeer systems exploiting xml structure to improve information retrieval in peertopeer systems with the advent of xml as a standard for representation and exchange of structured documents a growing amount of xmldocuments are being stored in peertopeer pp networks current research on pp search engines proposes the use of information retrieval ir techniques to perform contentbased search but does not take into account structural features of documents
affective feedback affective feedback user feedback is considered to be a critical element in the information seeking process an important aspect of the feedback cycle is relevance assessment that has progressively become a popular practice in web searching activities and interactive information retrieval ir the value of relevance assessment lies in the disambiguation of the users information need which is achieved by applying various feedback techniques such techniques vary from explicit to implicit and help determine the relevance of the retrieved documents
exploring and measuring dependency trees for informationretrieval exploring and measuring dependency trees for informationretrieval natural language processing techniques are believed to hold a tremendous potential to supplement the purely quantitative methods of text information retrieval this has led to the emergence of a large number of nlpbased ir research projects over the last few years even though the empirical evidence to support this has often been inadequate most contributions of nlp to ir mainly concentrate on document representation and compound term matching strategies researchers have noted that the simple termbased representation of document content such as vector representation is usually inadequate for accurate discrimination the bag of words representation does not invoke linguistic considerations and allow modelling of relationships between subsets of words however even though a variety of content indicator such as syntactic phrase have been tried and investigated for representing documents rather than single terms in ir systems the matching strategy over those representation still cannot go beyond traditional statistical techniques that measure term cooccurrence characteristics and proximity in analyzing text structure
the search for expertise the search for expertise note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
task detection for activitybased desktop search task detection for activitybased desktop search the desktop search tools provide powerful query capabilities and result presentation techniques however they do not take the user context into account we propose to exploit collected information about user activities with desktop files and applications for activitybased desktop search when i prepare for a project review and type in a search box the name of a colleague i expect to find her last deliverable draft but not her email with a paper review or our joint conference presentation ideally the desktop search system should be able to infer my current task from the logs of my previous activities and present taskspecific search results
using a mediated query approach for matching unstructured query with structured resources using a mediated query approach for matching unstructured query with structured resources note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
understanding system implementation and user behavior in a collaborative information seeking environment understanding system implementation and user behavior in a collaborative information seeking environment note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
biomedical crosslanguage information retrieval biomedical crosslanguage information retrieval note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
towards a combined model for search and navigation of annotated documents towards a combined model for search and navigation of annotated documents note ocr errors may be found in this reference list extracted from the full text article acm has opted to expose the complete list rather than only correct and linked references
context and linking in retrieval from personal digital archives context and linking in retrieval from personal digital archives advances in digital capture and storage technologies mean that it is now possible to capture and store ones entire life experiences in personal digital archives these vast personal archives or human digital memories hdms pose new challenges and opportunities for the research community not the least of which is developing effective means of retrieval from hdms personal archive retrieval research is still in its infancy and there is much scope for novel research my phd proposes to develop effective hdm retrieval algorithms by combining rich sources of context associated with items such as location and people present data with information obtained by linking hdm items in novel ways
extending language modeling techniques to models of search and browsing activity in a digital library extending language modeling techniques to models of search and browsing activity in a digital library users searching for information in a digital library or on the www can be modeled as individuals moving through a semantic space by issuing queries and clicking on hyperlinks as they go they emit a stream of interaction data most of it is linguistic data lots of it is captured in logs some of it is used to guess what the user is searching for but to most information retrieval systems each user interaction is a stateless point in this space there is a timeline connecting each of these points but systems seldom make use of this as sequence data in part because there is no clear way to systematically characterize the meaningful relations within a sequence of user activity it is a problem of pragmatics as much as it is of semanticsthe fact that a user clicked on a particular link or added a particular term to their query has meaning primarily in relation to the preceding actions a remaining challenge in ir is to extract features of the user interaction data that will give meaning to those relations
homepage live automatic block tracing for web personalization homepage live automatic block tracing for web personalization the emergence of personalized homepage providers eg google homepage and microsoft windows live has enabled web users to select interesting web contents and to aggregate them in a single web page the interesting web contents are usually some specific blocks of web pages rather than a whole web page to satisfy the users requirements personalized homepage providers predefine a lot of candidate content blocks for users to composite their own homepages however it requires tremendous manual efforts to define the content blocks and the coverage is still very limited in this paper we propose a novel personalized homepage system called homepage live to allow users to use draganddrop action to collect their favorite web content blocks and organize them in a single page moreover homepage live can also automatically trace the changes of blocks with the evolvement of the containing pages by measuring the tree edit distance of the selected blocks besides by exploiting the immutable elements of web pages the tracing algorithm performance is significant improved experimental results demonstrate the effectiveness and efficiency of our proposed algorithm
open user profiles for adaptive news systems help or harm open user profiles for adaptive news systems help or harm over the last five years a range of projects focused progressively more elaborated techniques for adaptive news delivery however the adaptation process in these systems has become more complicated and thus less transparent to the users in this paper we concentrate on the application of open user models in adding transparency and controllability to adaptive news systems we present a personalized news system yournews that allowed their user to view and edit their interest profiles and report a study of the system contrary to our expectations the study demonstrated that this ability to edit user profiles can harm the system and user performance and has to be used with caution
investigating behavioral variability in web search investigating behavioral variability in web search in this paper we describe a longitudinal logbased study that investigated variability in peoples interaction behavior when engaged in searchrelated activities on the world wide web we analyze the consistency of interaction patterns for more than two thousand volunteer users over a period of five months findings of our analysis suggest that there are dramatic differences in variability in key aspects of the interaction within and between queries and within and between users our findings also indicate the existence of at least two distinct classes of user navigators and explorers who exhibit large differences in their search behavior these findings have implications for the design of tools to support more effective searchrelated interactions on the web
csurf a contextdriven nonvisual webbrowser csurf a contextdriven nonvisual webbrowser web sites are designed for graphical mode of interactionsighted users can cut to the chase and quickly identifyrelevant information in web pages on the contrary individuals with visual disabilities have to use screenreaders tobrowse the web as screenreaders process pages sequentially and read through everything web browsing can become strenuous and timeconsuming although the use ofshortcuts and searching offers some improvements the problem still remains in this paper we address the problemof information overload in nonvisual web access using thenotion of context our prototype system csurf embodyingour approach provides the usual features of a screenreaderhowever when a user follows a link csurf captures thecontext of the link using a simple topicboundary detectiontechnique and uses it to identify relevant information onthe next page with the help of a support vector machine astatistical machinelearning model then csurf reads theweb page starting from the most relevant section identifiedby the model we conducted a series experiments to evaluate the performance of csurf against the stateoftheartscreenreader jaws our results show that the use of context can potentially save browsing time and substantiallyimprove browsing experience of visually disabled people
geotracker geospatial and temporal rss navigation geotracker geospatial and temporal rss navigation the web is rapidly moving towards a platform for mass collaboration in content production and consumption fresh content on a variety of topics people and places is being created and made available on the web at breathtaking speed navigating the content effectively not only requires techniques such as aggregating various rssenabled feeds but it also demands a new browsing paradigm in this paper we present novel geospatial and temporal browsing techniques that provide users with the capability of aggregating and navigating rss enabled content in a timely personalized and automatic manner in particular we describe a system called geotracker that utilizes both a geospatial representation and a temporal chronological presentation to help users spot the most relevant updates quickly within the context of this work we provide a middleware engine that supports intelligent aggregation and dissemination of rss feeds with personalization to desktops and mobile devices we study the navigation capabilities of this system on two kinds of data sets namely world cup soccer data collected over two months and breaking news items that occur every day we also demonstrate that the application of such technologies to the video search results returned by youtube and google greatly enhances a users ability in locating and browsing videos based on his or her geographical interests finally we demonstrate that the location inference performance of geotracker compares well against machine learning techniques used in the natural language processinginformation retrieval community despite its algorithm simplicity it preserves high recall percentages
learning information intent via observation learning information intent via observation users in an organization frequently request help by sending request messages to assistants that express an information intent an intention to update an information system assistants spend a significant amount of time and effort processing these messages for example human resource assistants process requests to update personnel records and executive assistants process requests to schedule conference rooms or to make travel reservations to process the intent of a message assistants read the message and then locate complete and submit a form that corresponds to the expressed intent automatically or semiautomatically processing the intent of a message on behalf of an assistant would ease the mundane and repetitive nature of this kind of work for a wellunderstood domain a straightforward application of natural language processing techniques can be used to build an intelligent form interface to semiautomatically process information intents however high performance parsers are based on machine learning algorithms that require a large collection of messages that have been labeled by an expert the generation of a labeled corpus of messages is a major barrier to the construction of a parser in this paper we investigate the construction of a natural language processing system and an intelligent form system that observes an assistant processing a request the intelligent form system then generates a labeled training corpus by interpreting the observations this paper reports on the measurement of the performance of the machine learning algorithms based on real data the combination of observations machine learning and interaction design produces an effective intelligent form interface based on natural language processing
pagelevel template detection via isotonic smoothing pagelevel template detection via isotonic smoothing we develop a novel framework for the pagelevel template detectionproblem our framework is built on two main ideas the first is theautomatic generation of training data for a classifier that given apage assigns a templateness score to every dom node of the page thesecond is the global smoothing of these pernode classifier scores bysolving a regularized isotonic regression problem the latter followsfrom a simple yet powerful abstraction of templateness on a page ourextensive experiments on humanlabeled test data show that our approachdetects templates effectively
towards domainindependent information extraction from web tables towards domainindependent information extraction from web tables traditionally information extraction from web tables hasfocused on small more or less homogeneous corpora oftenbased on assumptions about the use of table tagsa multitude of differenthtml implementations of web tables make these approaches difficultto scale in this paper we approach the problem ofdomainindependent information extraction from web tables byshifting our attention from the treebased representation of webpages to a variation of the twodimensional visual box model used byweb browsers to display the information on the screen the therebyobtained topological and style information allows us to fill the gapcreated by missing domainspecific knowledge about content and tabletemplates we believe that in a future step this approach canbecome the basis for a new way of largescale knowledge acquisitionfrom the current visual web
web object retrieval web object retrieval the primary function of current web search engines is essentially relevance ranking at the document level however myriad structured information about realworld objects embedded in static web pages and online web databases documentlevel information retrieval can unfortunately lead to highly inaccurate relevance ranking in answering objectoriented queries in this paper we propose a paradigm shift to enable searching at the object level in traditional information retrieval models documents are taken as the retrieval units and the content of a document is considered reliable however this reliability assumption is no longer valid in the object retrieval context when multiple copies of information about the same object typically exist these copies may be inconsistent because of diversity of web site qualities and the limited performance of current information extraction techniques if we simply combine the noisy and inaccurate attribute information extracted from different sources we may not be able to achieve satisfactory retrieval performance in this paper we propose several language models for web object retrieval namely an unstructured object retrieval model a structured object retrieval model and a hybrid model with both structured and unstructured retrieval features we test these models on a paper search engine and compare their performances we conclude that the hybrid model is the superior by taking into account the extraction errors at varying levels
summarizing email conversations with clue words summarizing email conversations with clue words with the ever increasing popularity of emails email overloadbecomes a major problem for email users email summarization is oneway not only to solve this problem but also to make use of onesemail corpus in this paper we propose a new framework for emailsummarization one novelty is to use a fragment quotation graph to try tocapture an email conversation the second novelty is to use clue words tomeasure the importance of sentences in conversation summarization based on clue words and their scores we propose a method called cws which is capableof producing a summary of any length as requested by the user we provide acomprehensive comparison of cws with various existing methods on the enrondata set preliminary results suggest that cws provides better summariesthan existing methods
organizing and searching the world wide web of facts step two harnessing the wisdom of the crowds organizing and searching the world wide web of facts step two harnessing the wisdom of the crowds as part of a large effort to acquire large repositories of facts from unstructured text on the web a seedbased framework for textual information extraction allows for weakly supervised extraction of class attributes eg side effects and generic equivalent for drugs from anonymized query logs the extraction is guided by a small set of seed attributes without any need for handcrafted extraction patterns or further domainspecific knowledge the attributes of classes pertaining to various domains of interest to web search users have accuracy levels significantly exceeding current state of the art inherently noisy search queries are shown to be a highly valuable albeit unexplored resource for webbased information extraction for the task of class attribute extraction as well as for named entity discovery
do not crawl in the dust different urls with similar text do not crawl in the dust different urls with similar text we consider the problem of dust different urls with similar text such duplicate urls are prevalent in web sites as web server software often uses aliases and redirections and dynamically generates the same page from various different url requests we present a novel algorithm dustbuster for uncovering dust that is for discovering rules that transform a given url to others that are likely to have similar content dustbuster mines dust effectively from previous crawl logs or web server logs without examining page contents verifying these rules via sampling requires fetching few actual web pages search engines can benefit from information about dust to increase the effectiveness of crawling reduce indexing overhead and improve the quality of popularity statistics such as pagerank
a new suffix tree similarity measure for document clustering a new suffix tree similarity measure for document clustering in this paper we propose a new similarity measure to compute the pairwisesimilarity of textbased documents based on suffix tree document model byapplying the new suffix tree similarity measure in groupaverage agglomerativehierarchical clustering gahc algorithm we developed a newsuffix tree document clustering algorithm nstc our experimental results ontwo standard document clustering benchmark corpus ohsumed and rcv indicatethat the new clustering algorithm is a very effective document clusteringalgorithm comparing with the results of traditional keyword tfidf similarity measurein the same ghac algorithm nstc achieved an improvement of on theaverage of fmeasure score furthermore we apply the new clusteringalgorithm in analyzing the web documents in online forum communities a topicoriented clustering algorithm is developed to help people in assessingclassifying and searching the the web documents in a large forum community
scaling up all pairs similarity search scaling up all pairs similarity search given a large collection of sparse vector data in a high dimensional space we investigate the problem of finding all pairs of vectors whose similarity score as determined by a function such as cosine distance is above a given threshold we propose novel optimization and indexing techniques for this problem resulting in an algorithm that is both faster and simpler than the previous stateoftheart approaches we demonstrate the effectiveness of our algorithm on the public dblp dataset and on two realworld web applications generating recommendations for the orkut social network and computing pairs of similar queries from search snippet data among the million most frequently issued google queries our algorithm is between times to times faster than previous algorithms on these datasets
detecting nearduplicates for web crawling detecting nearduplicates for web crawling nearduplicate documents are commonly found on the web a pair of nearduplicate web pages differ from each other in a very small portion the differences commonly consist of advertisements and timestamps such differences are irrelevant for web search during web crawling it is useful to quickly ascertain whether a newly crawled web page is a nearduplicate of a previously crawled web page or not
demographic prediction based on users browsing behavior demographic prediction based on users browsing behavior demographic information plays an important role in personalized web applications however it is usually not easy to obtain this kind of personal data such as age and gender in this paper we made a first approach to predict users gender and age from their web browsing behaviors in which the webpage view information is treated as a hidden variable to propagate demographic information between different users there are three main steps in our approach first learning from the webpage clickthough data web pages are associated with users known age and gender tendency through a discriminative model second users unknown age and gender are predicted from the demographic information of the associated web pages through a bayesian framework third based on the fact that web pages visited by similar users may be associated with similar demographic tendency and users with similar demographic information would visit similar web pages a smoothing component is employed to overcome the data sparseness of web clickthough log experiments are conducted on a real web clickthrough log to demonstrate the effectiveness of the proposed approach the experimental results show that the proposed algorithm can achieve up to improvements on gender prediction and on age prediction in terms of macro f comparing with baseline algorithms
why we search visualizing and predicting user behavior why we search visualizing and predicting user behavior the aggregation and comparison of behavioral patterns on the www represent a tremendous opportunity for understanding past behaviors and predicting future behaviors in this paper we take a first step at achieving this goal we present a large scale study correlating the behaviors of internet users on multiple systems ranging in size from million queries to million blog posts to news articles we formalize a model for events in these timevarying datasets and study their correlation we have created an interface for analyzing the datasets which includes a novel visual artifact the dtwradar for summarizing differences between time series using our tool we identify a number of behavioral properties that allow us to understand the predictive power of patterns of use
topic sentiment mixture modeling facets and opinions in weblogs topic sentiment mixture modeling facets and opinions in weblogs in this paper we define the problem of topicsentiment analysis on weblogs and propose a novel probabilistic model to capture the mixture of topics and sentiments simultaneouslythe proposed topicsentiment mixture tsm model can reveal the latent topical facets in a weblog collection the subtopics in the results of an ad hoc query and their associated sentiments it could also provide general sentiment models that are applicable to any ad hoc topics with a specifically designed hmm structure the sentiment models and topic models estimated with tsm can be utilized to extract topic life cycles and sentiment dynamics empirical experiments on different weblog datasets show that this approach is effective for modeling the topic facets and sentiments and extracting their dynamics from weblog collections the tsm model is quite general it can be applied to any text collections with a mixture of topics and sentiments thus has many potential applications such as search result summarization opinion tracking and user behavior prediction
wherefore art thou rxanonymized social networks hidden patterns and structural steganography wherefore art thou rxanonymized social networks hidden patterns and structural steganography in a social network nodes correspond to people or othersocial entities and edges correspond to social linksbetween them in an effort to preserve privacy thepractice of anonymization replaces names with meaninglessunique identifiers we describe a family of schemes suchthat even from a single anonymized copy of a social network it is possible for an adversary to learn whether edges existor not between specific targeted pairs of nodes
information flow modeling based on diffusion rate for prediction and ranking information flow modeling based on diffusion rate for prediction and ranking information flows in a network where individuals influence each other the diffusion rate captures how efficiently the information can diffuse among the users in the network we propose an information flow model that leverages diffusion rates for prediction identify where information should flow to and ranking identify who will most quickly receive the information for prediction we measure how likely information will propagate from a specific sender to a specific receiver during a certain time period accordingly a ratebased recommendation algorithm is proposed that predicts who will most likely receive the information during a limited time period for ranking we estimate the expected time for information diffusion to reach a specific user in a network subsequently a diffusionrank algorithm is proposed that ranks users based on how quickly information will flow to them experiments on two datasets demonstrate the effectiveness of the proposed algorithms to both improve the recommendation performance and rank users by the efficiency of information flow
netprobe a fast and scalable system for fraud detection in online auction networks netprobe a fast and scalable system for fraud detection in online auction networks given a large online network of online auction uesrs and their histories of transactions how can we spot anomalies or even auction fraud we describe the algorithms and system design decisions behind our proposed netprobe system for uncovering auction fraud we show that it is possible to do fast and scalable fraud detection in large auction networks the main idea is to use the machinery of markov random fields mrf and try to guess the hidden state fraudhonest of each participant we describe the algorithms behind our system that are based on belief propagation we provide our own incremental but accurate approximations to it and we list and justify our design decisions for efficient crawling of real auction networks we report experiments on synthetic graphs containing as many as nodes and edges where netprobe was able to spot fraudulent nodes with over precision and recall with execution times in the order of seconds we also report experiments on a real graph consisting of about transactions between more than ebay users where netprobe was highly effective at unearthing hidden networks of fraudsters within a realistic response time of about minutes
the complex dynamics of collaborative tagging the complex dynamics of collaborative tagging the debate within the web community over the optimal means by which to organize information often pits formalized classifications against distributed collaborative tagging systems a number of questions remain unanswered however regarding the nature of collaborative tagging systems including whether coherent categorization schemes can emerge from unsupervised tagging by users this paper uses data from tagged sites on the social bookmarking site delicious to examine the dynamics of collaborative tagging systems in particular we examine whether the distribution of the frequency of use of tags for popular sites with a long history many tags and many users can be described by a power law distribution often characteristic of what are considered complex systems we produce a generative model of collaborative tagging in order to understand the basic dynamics behind tagging including how a power law distribution of tags could arise we empirically examine the tagging history of sites in order to determine how this distribution arises over time and patterns prior to a stable distribution lastly by focusing on the highfrequency tags of a site where the distribution of tags is a stabilized power law we show how tag cooccurrence networks for a sample domain of tags can be used analyze the meaning of particular tags given their relationship to other tags
expertise networks in online communities structure and algorithms expertise networks in online communities structure and algorithms webbased communities have become an important place for people to seek and share expertise we find that networks in these communities typically differ in their topology from other online networks such as the world wide web systems targeted to augment webbased communities by automatically identifying users with expertise for example need to adapt to the underlying interaction dynamics in this study we analyze the java forum a large online helpseeking community using social network analysis methods we test a set of networkbased ranking algorithms including pagerank and hits on this large size social network in order to identify users with high expertise we then use simulations to identify a small number of simple rules governing the questionanswer dynamic in the network these simple rules not only replicate the structural characteristics and algorithm performance on the empirically observed java forum but also allow us to evaluate how other algorithms may perform in communities with different characteristics we believe this approach will be fruitful for practical algorithm design and implementation for online expertisesharing communities
internetscale collection of humanreviewed data internetscale collection of humanreviewed data enterprise data processing and content aggregation systems often require extensive use of human reviewed data eg for training and monitoring machine learningbased applications today these needs are often met by inhouse efforts or offshore contracting emerging applications attempt to provide automation for human reviewed data collection at internetscale we conduct extensive experiments to study the effectiveness of one such application we also study the feasibility of using yahoo answers a general questionanswering forum for human review data collection
detectives detecting coalition hit inflation attacks in advertising networks streams detectives detecting coalition hit inflation attacks in advertising networks streams click fraud is jeopardizing the industry of internet advertising internet advertising is crucial for the thriving of the entire internet since it allows producers to advertise their products and hence contributes to the well being of ecommerce moreover advertising supports the intellectual value of the internet by covering the running expenses of the content publishers sites some publishers are dishonest and use automation to generate traffic to defraud the advertisers similarly some advertisers automate clicks on the advertisements of their competitors to deplete their competitors advertising budgets this paper describes the advertising network model and focuses on the most sophisticated type of fraud which involves coalitions among fraudsters we build on several published theoretical results to devise the similarityseeker algorithm that discovers coalitions made by pairs of fraudsters we then generalize the solution to coalitions of arbitrary sizes before deploying our system on a real network we conducted comprehensive experiments on data samples for proof of concept we detected numerous coalitions that span numerous sites interestingly of the discovered sites were real fraudsters
extraction and search of chemical formulae in text documents on the web extraction and search of chemical formulae in text documents on the web often scientists seek to search for articles on the web related to a particular chemical when a scientist searches for a chemical formula using a search engine today she gets back articles where the exact keyword string expressing the chemical formula is found searching for the exact occurrence of keywords while searching results in two problems for this domain a if the author searches for ch and the article has hc the article is not returned and b ambiguous searches like he return all documents where helium is mentioned as well as documents where the pronoun he occurs to remedy these deficiencies we propose a chemical formula search engine to build a chemical formula search engine we must solve the following problems extract chemical formulae from text documents index chemical formulae and design a ranking function for articles where the chemical formulae occur furthermore query models are introduced for formula search and for each a scoring scheme based on features of partial formulae is proposed to measure the relevance of chemical formulae and queries we evaluate algorithms for identifying chemical formulae in documents using a classification method based on support vector machines svm and a probabilistic model based on conditional random fields crf different methods for svm and crf to tune the tradeoff between recall and precision forimbalanced data are proposed to improve the overall performancea feature selection method based on frequency and discrimination is used to remove uninformative and redundant features experiments show that our approaches of chemical formula extraction work well especially after tradeoff tuning the results also demonstrate that feature selection can reduce the index size without changing the ranked query results much
a contentdriven reputation system for the wikipedia a contentdriven reputation system for the wikipedia online forums for the collaborative creation of bodies of informationare a phenomenon of rising importance the wikipedia is one of thebestknown examples the open nature of such forums could benefitfrom a notion of reputation for its authors author reputation couldbe used to flag new contributions from lowreputation authors and itcould be used to allow only authors with good reputation to contributeto controversial or critical pages a reputation system for thewikipedia would also provide an incentive to give highqualitycontributions
google news personalization scalable online collaborative filtering google news personalization scalable online collaborative filtering several approaches to collaborative filtering have been studied butseldom have the studies been reported for large several millions ofusers and items and dynamic the underlying item set is continuallychanging settings in this paper we describe our approach tocollaborative filtering for generating personalized recommendations forusers of google news we generate recommendations using threeapproaches collaborative filtering using minhashclustering probabilisticlatent semantic indexing plsi and covisitation counts we combine recommendations fromdifferent algorithms using a linear modelour approach is content agnostic and consequently domain independent makingit easily adaptible for other applications and languages withminimal effort this paper will describe our algorithms and system setup indetail and report results of running the recommendations engine on google news
exploring in the weblog space by detecting informative and affective articles exploring in the weblog space by detecting informative and affective articles weblogs have become a prevalent source of information for people to express themselves in general there are two genres of contents in weblogs the first kind is about the webloggers personal feelingsthoughts or emotions we call this kind of weblogs affective articlesa second kind of weblogs is about technologies and different kinds of informative news in this paper we present a machine learning method for classifying informative and affective articles among weblogs we consider this problem as a binary classification problem by using machine learning approaches we achieve on informationretrieval performance measures including precision recall and f we set up three studies on the applications of above classificationapproach in both research and industrial fields we use the above classification approach to improve the performance of classificationof emotions from weblog articles we also develop an intentdriven weblogsearch engine based on the classification techniques to improvethe satisfaction of web users finally we use above classificationapproach to search for weblogs with a great deal of informative articles
spam doublefunnel connecting web spammers with advertisers spam doublefunnel connecting web spammers with advertisers spammers use questionable search engine optimization seo techniques topromote their spam links into top search results in this paper wefocus on one prevalent type of spam redirection spam where one canidentify spam pages by the thirdparty domains that these pages redirecttraffic to we propose a fivelayer doublefunnel model for describingendtoend redirection spam present a methodology for analyzing thelayers and identify prominent domains on each layer using two sets ofcommercial keywords one targeting spammers and the other targetingadvertisers the methodology and findings are useful for search enginesto strengthen their ranking algorithms against spam for legitimatewebsite owners to locate and remove spam doorway pages and forlegitimate advertisers to identify unscrupulous syndicators who serveads on spam pages
globetp templatebased database replication for scalable web applications globetp templatebased database replication for scalable web applications generic database replication algorithms do not scale linearly inthroughput as they require to apply all update deletion andinsertion udi queries to every database replica the throughput istherefore limited to the point where the number of udi queries aloneis sufficient to overload one server in such scenarios partialreplication of a database can help as update queries are executedonly by a subset of all servers in this paper we propose globetp asystem that employs partial replication to improve databasethroughput globetp exploits the fact that a web applications queryworkload is composed of a small set of read and write templates usingknowledge of these templates and their respective execution costsglobetp provides database table placements that produce significantimprovements in database throughput we demonstrate the efficiencyof this technique using two different industry standard benchmarksin our experiments globetp increases the throughput by to compared to full replication while using identical hardwareconfiguration furthermore adding a single query cache improves thethroughput by another to 
consistencypreserving caching of dynamic database content consistencypreserving caching of dynamic database content with the growing use of dynamic web content generated from relationaldatabases traditional caching solutions for throughput and latencyimprovements are ineffective we describe a middleware layer calledganesh that reduces the volume of data transmitted withoutsemantic interpretation of queries or results it achieves thisreduction through the use of cryptographic hashing to detectsimilarities with previous results these benefits do not require anycompromise of the strict consistency semantics provided by thebackend database further ganesh does not require modifications toapplications web servers or database servers and works withclosedsource applications and databases using two benchmarksrepresentative of dynamic web sites measurements of our prototypeshow that it can increase endtoend throughput by as much as twofoldfor nondata intensive applications and by as much as tenfold for dataintensive ones
optimized query planning of continuous aggregation queries in dynamic data dissemination networks optimized query planning of continuous aggregation queries in dynamic data dissemination networks continuous queries are used to monitor changes to time varying data and to provide results useful for online decision making typically a user desires to obtain the value of some aggregation function over distributed data items for example to know a the average of temperatures sensed by a set of sensors b the value of index of midcap stocks in these queries a client specifies a coherency or accuracy requirement as part of the query in this paper we present a lowcost scalable technique to answer continuous aggregation queries using a content distribution network of dynamic data items in such a network of data aggregators each data aggregator serves a set of data items at specific coherencies just as various fragments of a dynamic webpage are served by one or more nodes of a cdn our technique involves decomposing a client query into subqueries and executing subqueries on judiciously chosen data aggregators for executing an incoherency bounded continuous query a query plan is required which includes the set of subqueries their individual incoherency bounds and data aggregators which can execute these subqueries an optimal query execution plan should satisfy client querys coherency requirement with least cost measured in terms of the number of refresh messages sent from aggregators to the client for estimating query execution cost we build a continuous query cost model which can be used to estimate the number of messages required to satisfy the client specified incoherency bound performance results using realworld traces show that our cost based query planning leads to queries being executed using less than one third the number of messages required by existing schemes
a scalable application placement controller for enterprise data centers a scalable application placement controller for enterprise data centers given a set of machines and a set ofweb applications with dynamicallychanging demands an application placement controller decideshow many instances to run for each application and where toput them while observing all kinds of resource constraints thisproblem is np hard in this paper we propose an online algorithmthat uses heuristics to efficiently solve this problem it allowsmultiple applications to share a single machine and strivesto maximize the total satisfied application demand to minimizethe number of application starts and stops and to balance the loadacross machines it can produce within seconds highquality solutionsfor hard placement problems with thousands of machinesand thousands of applications this scalability is crucial for dynamicresource provisioning in largescale enterprise data centersour algorithm significantly and consistently outperforms the existingstateoftheart algorithm under a wide variety of workloads
a unified platform for data driven web applications with automatic clientserver partitioning a unified platform for data driven web applications with automatic clientserver partitioning datadriven web applications are structured into three tiers with different programming models at each tier this division forces developers to manually partition application functionality across the tiers resulting in complex logic suboptimal partitioning and expensive repartitioning of applications
myxdns a request routing dns server with decoupled server selection myxdns a request routing dns server with decoupled server selection this paper presents the architecture and the preliminary evaluation ofa request routing dns server that decouples server selectionfrom the rest of dns functionality our dns server which we refer toas myxdns exposes welldefined apis for uploading an externallycomputed server selection policy and for interacting with an external networkproximity service with myxdns researchers can explore their ownnetwork proximity metrics and request routing algorithms withouthaving to worry about dns internals furthermore myxdns is based onopensource mydns and is available to public stresstesting of myxdnsindicated that it achieves its flexibility at an acceptable cost asingle myxdns running on a lowlevel server can process reqsecwith submillisecond response even in the presence of continuousupdates to server selection policy
robust web page segmentation for mobile terminal using contentdistances and page layout information robust web page segmentation for mobile terminal using contentdistances and page layout information the demand of browsing information from general web pages using a mobile phone is increasing however since the majority of web pages on the internet are optimized for browsing from pcs it is difficult for mobile phone users to obtain sufficient information from the web therefore a method to reconstruct pcoptimized web pages for mobile phone users is essential an example approach is to segment the web page based on its structure and utilize the hierarchy of the content element to regenerate a page suitable for mobile phone browsing in our previous work we have examined a robust automatic web page segmentation scheme which uses the distance between content elements based on the relative html tag hierarchy ie the number and depth of html tags in web pages however this scheme has a problem that the contentdistance based on the order of html tags does not always correspond to the intuitional distance between content elements on the actual layout of a web page in this paper we propose a hybrid segmentation method which segments web pages based on both the contentdistance calculated by the previous scheme and a novel approach which utilizes web page layout information experiments conducted to evaluate the accuracy of web page segmentation results prove that the proposed method can segment web pages more accurately than conventional methods furthermore implementation and evaluation of our system on the mobile phone prove that our method can realize superior usability compared to commercial web browsers
prive anonymous locationbased queries in distributed mobile systems prive anonymous locationbased queries in distributed mobile systems nowadays mobile users with positioning devices can access locationbased services lbs and query about points of interest in theirproximity for such applications to succeed privacy andconfidentiality are essential encryption alone is not adequatealthough it safeguards the system against eavesdroppers the queriesthemselves may disclose the location and identity of the userrecently there have been proposed centralized architectures basedon kanonymity which utilize an intermediate anonymizer between themobile users and the lbs however the anonymizer must be updatedcontinuously with the current locations of all users moreover thecomplete knowledge of the entire system poses a security threat ifthe anonymizer is compromised
a mobile application framework for the geospatial web a mobile application framework for the geospatial web in this paper we present an application framework that leverages geospatial content on the world wide web by enabling innovative modes of interaction and novel types of user interfaces on advanced mobile phones and pdas we discuss the current development steps involved in building mobile geospatial web applications and derive three technological prerequisites for our framework spatial query operations based on visibility and field of view a d environment model and a presentationindependent data exchange format for geospatial query results we propose the local visibility model as a suitable xmlbased candidate and present a prototype implementation
ationaided retrieval ationaided retrieval users searching for information in hypermedia environments often perform querying followed by manual navigation yet the conventional texthypertext retrieval paradigm does not explicity take postquery navigation into account this paper proposes a new retrieval paradigm called navigationaided retrieval nar which treats both querying and navigation as firstclass activities in the nar paradigm querying is seen as a means to identify starting points for navigation and navigation is guided based on information supplied in the query nar is a generalization of the conventional probabilistic information retrieval paradigm which implicitly assumes no navigation takes place 
ient search engine measurements ient search engine measurements we address the problem of measuring relevance neutral search quality metrics like corpus size index freshness and density of duplicates in the index the recently proposed estimators for such metrics suffer from significant bias andor poor performance due to inaccurate approximation of the so called document degrees 
efficient search in large textual collections with redundancy efficient search in large textual collections with redundancy current web search engines focus on searching only the most recent snapshotof the web in some cases however it would be desirable to search overcollections that include many different crawls and versions of each pageone important example of such a collection is the internet archive thoughthere are many others since the data size of such an archive is multipletimes that of a single snapshot this presents us with significant performancechallenges current engines use various techniques for index compressionand optimized query execution but these techniques do not exploit thesignificant similarities between different versions of a page or betweendifferent pages
the discoverability of the web the discoverability of the web previous studies have highlighted the rapidity with which new contentarrives on the web we study the extent to which this new content canbe efficiently discovered in the crawling model our study has twoparts first we employ a maximum cover formulation to study the inherentdifficulty of the problem in a setting in which we have perfectestimates of likely sources of links to new content secondwe relax the requirement of perfect estimates into a more realisticsetting in which algorithms must discover new content using historicalstatistics to estimate which pages are most likely to yield links tonew content
combining classifiers to identify online databases combining classifiers to identify online databases we address the problem of identifying the domain of onlinedatabases more precisely given a set f of web forms automaticallygathered web by a focused crawler and an onlinedatabase domain d our goal is to select from f only theforms that are entry points to databases in d having aset of web forms that serve as entry points to similar onlinedatabases is a requirement for many applications and techniquesthat aim to extract and integrate hiddenweb informationincluding metasearchers database selection toolshiddenweb crawlers formschema matching and mergingand in the construction of online database directorieswe propose a new strategy that automatically and accuratelyclassifies online databases based on features thatcan be easily extracted from web forms by judiciouslypartitioning the space of form features this strategy allowsthe use of simpler classifiers that can be constructed usinglearning techniques that are better suited for each partitionexperiments using real web data in a representativeset of domains show that the use of different classifiers leadsto high accuracy precision and recall this indicates thatour modular classifier composition provides an effective andscalable solution for classifying online databases
an adaptive crawler for locating hiddenweb entry points an adaptive crawler for locating hiddenweb entry points in this paper we describe new adaptive crawling strategiesto efficiently locate the entry points to hiddenweb sourcesthe fact that hiddenweb sources are very sparsely distributedmakes the problem of locating them especially challengingwe deal with this problem by using the contents ofpages to focus the crawl on a topic by prioritizing promisinglinks within the topic and by also following links that maynot lead to immediate benefit we propose a new frameworkwhereby crawlers automatically learn patterns of promisinglinks and adapt their focus as the crawl progresses thusgreatly reducing the amount of required manual setup andtuning our experiments over real web pages in a representativeset of domains indicate that online learning leadsto significant gains in harvest rates the adaptive crawlersretrieve up to three times as many forms as crawlers thatuse a fixed focus strategy
random web crawls random web crawls this paper proposes a random web crawl model a web crawl is a biased and partial image of the web this paper deals with the hyperlink structure ie a web crawl is a graph whose vertices are the pages and whose edges are the hypertextual links of course a web crawl has a very particular structure we recall some known results about it we then propose a model generating similar structures our model simply simulates a crawling ie builds and crawls the graph at the same time the graphs generated have lot of known properties of web crawls our model is simpler than most random web graph models but captures the sames properties notice that it modelizes the crawling process instead of the page writting process of web graph models
extraction and classification of dense communities in the web extraction and classification of dense communities in the web the world wide web www is rapidly becoming important for society as a medium for sharing data information and services andthere is a growing interest in tools for understanding collectivebehaviors and emerging phenomena in the www in this paper wefocus on the problem of searching and classifying communities in the web loosely speaking a community is a groupof pages related to a common interest more formally communitieshave been associated in the computer science literature with theexistence of a locally dense subgraph of the webgraph where webpages are nodes and hyperlinks are arcs of the webgraph thecore of our contribution is a new scalable algorithm for findingrelatively dense subgraphs in massive graphs we apply ouralgorithm on webgraphs built on three publicly available largecrawls of the web with raw sizes up to m nodes and g arcsthe effectiveness of our algorithm in finding dense subgraphs isdemonstrated experimentally by embedding artificial communities inthe webgraph and counting how many of these are blindly foundeffectiveness increases with the size and density of thecommunities it is close to for communities of a thirtynodes or more even at low density it is still about evenfor communities of twenty nodes with density over of thearcs present at the lower extremes the algorithm catches ofdense communities made of ten nodes we complete our community watch system by clustering the communities found in thewebgraph into homogeneous groups by topic and labelling eachgroup by representative keywords
web projections learning from contextual subgraphs of the web web projections learning from contextual subgraphs of the web research on web search has demonstrated the value of using information about the graphical structure of the web in ranking search results to date specific graphical properties have been used in these analyses we introduce a web projection method that generalizes prior efforts of graphical relationships of the web in several ways with the approach we create subgraphs by projecting sets of pages and domains onto the larger web graph and then use machine learning to construct predictive models that operate on graphical properties we describe the method and then present experiments that illustrate the construction of predictive models of search result quality and user query reformulation
supervised rank aggregation supervised rank aggregation this paper is concerned with rank aggregation the task of combining results of individual ranking functions in metasearch previously rank aggregation was performed mainly by using unsupervised methods it is hard for the unsupervised approach to improve ranking performances by leveraging the use of labeled data when such data is available we propose employing a supervised learning approach to perform the task which we refer to as supervised rank aggregation we set up a general framework for conducting rank aggregation with supervised learning in which learning for rank aggregation is formalized as an optimization issue that minimizes disagreements with the labeled ground truth data as case study we focus on markov chain based rank aggregation in this paper the optimization problem is not a convex optimization problem for markov chain based methods however and thus is hard to solve we transform the optimization problem into semidefinite programming and give proofs on the correctness experimental results on metasearches show that supervised rank aggregation can significantly outperform existing unsupervised methods
navigating the intranet with high precision navigating the intranet with high precision despite the success of web search engines search over large enterprise intranets still suffers from poor result quality earlier work that compared intranets and the internet from the view point of keyword search has pointed to several reasons why the search problem is quite different in these two domains in this paper we address the problem of providing high quality answers to navigational queries in the intranet eg queries intended to find product or personal home pages service pages etc our approach is based on offline identification of navigational pages intelligent generation of termvariants to associate with each page and the construction of separate indices exclusively devoted to answering navigational queries using a testbed of m pages from the ibm intranet we present evaluation results that demonstrate that for navigational queries our approach of using custom indices produces results of significantly higher precision than those produced by a general purpose search algorithm
optimizing web search using social annotations optimizing web search using social annotations this paper explores the use of social annotations to improve web search nowadays many services eg delicious have been developed for web users to organize and share their favorite web pages on line by using social annotations we observed that the social annotations can benefit the web search in two aspects the annotations are usually good summaries of corresponding web pages the count of annotations indicates the popularity of web pages two novel algorithms are proposed to incorporate these information into page ranking socialsimrank ssr calculates the similarity between social annotations and web queries socialpagerank spr captures the popularity of web pages preliminary experimental results show that ssr can find the latent semantic association between queries and annotations while spr successfully measures the quality popularity of a web page from the web users perspective we further empirically evaluate the proposed methods with manually annotated queries and autogenerated queries on a dataset consisting of web pages with different annotations experiments show that both ssr and spr benefit the web search significantly by incorporating both the spr and ssr features the quality of search results can be improved by as much as and compared with the original performance in map on two query sets respectively
robust methodologies for modeling web click distributions robust methodologies for modeling web click distributions metrics such as click counts are vital to online businessesbut their measurement has been problematic due to inclusion ofhigh variance robot trafficwe posit that by applying statistical methods more rigorous than havebeen employed to date that we can build a robust model of thedistribution of clicks following which we can set probabilistically soundthresholds to address outliers and robotsprior research in this domain has used inappropriatestatistical methodology to model distributions and current industrialpractice eschews this research for conservative adhoc clicklevel thresholdsprevailing belief is that such distributions are scalefree power lawdistributions but using more rigorous statistical methods we find thebest description of the data is instead provided by a scalesensitive zipfmandelbrot mixture distributionour results are based on ten datasets from various verticals in theyahoo domainsince mixture models can overfit the datawe take care to use the bic loglikelihood method which penalizes overly complex modelsusing a mixture model in the web activity domain makes sense because there are likely multiple classes of usersin particular we have noticed that there is a significantly large set of users that visit theyahoo portal exactly once a day we surmise these may be robotstesting internet connectivity by pinging the yahoo main website
predicting clicks estimating the clickthrough rate for new ads predicting clicks estimating the clickthrough rate for new ads search engine advertising has become a significant aspect of the web browsing experience the order in which a search engine displays ads greatly affects the probability that a user will see and click on each ad consequently the ranking has a strong impact on the revenue the search engine receives from the ads further showing the user an ad that they prefer to click on also improves user satisfaction for these reasons it is crucially important to be able to estimate the clickthrough rate of ads in the system for ads that have been repeatedly displayed this is empirically measurable but when ads initially appear other means must be used we show that we can use features of ads keywords and advertisers to learn a model that accurately predicts the clickthough rate for an ad we also show that using our model improves the convergence and performance of an advertising system as a result our model would improve both revenues and user satisfaction
dynamics of bid optimization in online advertisement auctions dynamics of bid optimization in online advertisement auctions we consider the problem of online keyword advertising auctions amongmultiple bidders with limited budgets and study a natural biddingheuristic in which advertisers attempt to optimize their utility byequalizing their returnoninvestment across all keywords we showthat existing auction mechanisms combined with this heuristic canexperience cycling as has been observed in many current systemsand therefore propose a modified class of mechanisms with smallrandom perturbations this perturbation is reminiscent of the smalltimedependent perturbations employed in the dynamical systemsliterature to convert many types of chaos into attracting motionswe show that the perturbed mechanism provably converges in the caseof firstprice auctions and experimentally converges in the case ofsecondprice auctions moreover the point of convergence has anatural economic interpretation as the unique market equilibrium inthe case of firstprice mechanisms in the case of secondpriceauctions we conjecture that it converges to the supplyawaremarket equilibrium thus our results can be alternatively describedas a tatonnement process for convergence to market equilibriumin which prices are adjusted on the side of the buyers rather thanthe sellers we also observe that perturbation in mechanism designis useful in a broader context in general it can allow bidders toshare a particular item leading to stable allocations andpricing for the bidders and improved revenue for the auctioneer
compareampcontrast using the web to discover comparable cases for news stories compareampcontrast using the web to discover comparable cases for news stories comparing and contrasting is an important strategy people employ tounderstand new situations and create solutions for new problemssimilar events can provide hints for problem solving as well as largercontexts for understanding the specific circumstances of an eventlessons can be learned from past experience insights can be gainedabout the new situation from familiar examples and trends can be discovered among similar events as the largest knowledge base forhuman beings the web provides both an opportunity and a challengeto discover comparable cases in order to facilitate situation analysisand problem solving in this paper we present compareampcontrasta system that uses the web to discover comparable cases for newsstories documents about similar situations but involving distinctentities the system analyzes a news story given by the user and builds a model of the story with the story model the system dynamically discovers entities comparable to the mainentity in the original story and uses these comparable entities asseeds to retrieve web pages about comparable cases the system is domainindependent does not require any domainspecific knowledgeengineering efforts and deals with the complexity of unstructuredtext and noise on the web in a robust waywe evaluated the system with an experiment on acollection of news articles and a user study
answering bounded continuous search queries in the world wide web answering bounded continuous search queries in the world wide web search queries applied to extract relevant information from the world wide web over a period of time may be denoted as continuous search queries the improvement of continuous search queries may concern not only the quality of retrieved results but also the freshness of results ie the time between the availability of a respective data object on the web and the notification of a user by the search engine in some cases a user should be notified immediately since the value of the respective information decreases quickly as eg news about companies that affect the value of respective stocks or sales offers for products that may no longer be available after a short period of time in the document filtering literature the optimization of such queries is usually based on threshold classification documents above a quality threshold are returned to a user the threshold is tuned in order to optimize the quality of retrieved results the disadvantage of such approaches is that the amount of information returned to a user may hardly be controlled without further userinteraction in this paper we consider the optimization of bounded continuous search queries where only the estimated best k elements are returned to a user we present a new optimization method for bounded continuous search queries based on the optimal stopping theory and compare the new method to methods currently applied by web search systems the new method provides results of significantly higher quality for the cases where very fresh results have to be delivered
answering relationship queries on the web answering relationship queries on the web finding relationships between entities on the web eg the connections between different places or the commonalities of people is a novel and challenging problem existing web search engines excel in keyword matching and document ranking but they cannot well handle many relationship queries this paper proposes a new method for answering relationship queries on two entities our method first respectively retrieves the top web pages for either entity from a web search engine it then matches these web pages and generates an ordered list of web page pairs each web page pair consists of one web page for either entity the top ranked web page pairs are likely to contain the relationships between the two entities one main challenge in the ranking process is to effectively filter out the large amount of noise in the web pages without losing much useful information to achieve this our method assigns appropriate weights to terms in web pages and intelligently identifies the potential connecting terms that capture the relationships between the two entities only those top potential connecting terms with large weights are used to rank web page pairs finally the top ranked web page pairs are presented to the searcher for each such pair the query terms and the top potential connecting terms are properly highlighted so that the relationships between the two entities can be easily identified we implemented a prototype on top of the google search engine and evaluated it under a wide variety of query scenarios the experimental results show that our method is effective at finding important relationships with low overhead
dynamic personalized pagerank in entityrelation graphs dynamic personalized pagerank in entityrelation graphs extractors and taggers turn unstructured text into entityrelation er graphs where nodes are entities email paper person conference company and edges are relations wrote cited worksfor typed proximity search of the form typeperson near companyibm paperxml is an increasingly useful search paradigm in er graphs proximity search implementations either perform a pageranklike computation at query time which is slow or precompute store and combine perword pageranks which can be very expensive in terms of preprocessing time and space we present hubrank a new system for fast dynamic spaceefficient proximity searches in er graphs during preprocessing hubrank computes and indexes certain sketchy random walk fingerprints for a small fraction of nodes carefully chosen using query log statistics at query time a small active subgraph is identified bordered by nodes with indexed fingerprints these fingerprints are adaptively loaded to various resolutions to form approximate personalized pagerank vectors ppvs ppvs at remaining active nodes are now computed iteratively we report on experiments with citeseers er graph and millions of real citeseer queries some representative numbers follow on our testbed hubrank preprocesses and indexes times faster than wholevocabulary ppv computation a text index is mb wholevocabulary ppvs would consume gb if ppvs are truncated to mb precision compared to true pagerank drops to in contrast hubrank has precision at mb hubranks average query time is milliseconds querytime pagerank computation takes seconds on average
a largescale evaluation and analysis of personalized search strategies a largescale evaluation and analysis of personalized search strategies although personalized search has been proposed for many years and many personalization strategies have been investigated it is still unclear whether personalization is consistently effective on different queries for different users and under different search contexts in this paper we study the problem and get some preliminary conclusions we present a largescale personalized search evaluation framework based on search logs and then evaluate five personalized search strategies including two clickbased and three profilebased ones using day msn search logs by analyzing the results we reveal that personalized search has significant improvement over common web search on some queries but it also has little effect on other queries eg queries with small click entropy and even harms search accuracy under some situations furthermore we show that clickbased personalization strategies perform consistently and considerablely well while profilebased ones are unstable in our experiments we also reveal that both longterm and shortterm contexts are very important in improving search performance for profilebased personalized search strategies
privacyenhancing personalized web search privacyenhancing personalized web search personalized web search is a promising way to improve search quality by customizing search results for people with individual information goals however users are uncomfortable with exposing private preference information to search engines on the other hand privacy is not absolute and often can be compromised if there is a gain in service or profitability to the user thus a balance must be struck between search quality and privacy protection this paper presents a scalable way for users to automatically build rich user profiles these profiles summarize a users interests into a hierarchical organization according to specific interests two parameters for specifying privacy requirements are proposed to help the user to choose the content and degree of detail of the profile information that is exposed to the search engine experiments showed that the user profile improved search quality when compared to standard msn rankings more importantly results verified our hypothesis that a significant improvement on search quality can be achieved by only sharing some higherlevel user profile information which is potentially less sensitive than detailed personal information
defeating script injection attacks with browserenforced embedded policies defeating script injection attacks with browserenforced embedded policies web sites that accept and display content such as wiki articles orcomments typically filter the content to prevent injected script codefrom running in browsers that view the site the diversity of browserrendering algorithms and the desire to allow rich content makesfiltering quite difficult however and attacks such as the samy andyamanner worms have exploited filtering weaknesses to solve thisproblem this paper proposes a simple mechanism calledbrowserenforced embedded policies beep the idea is that a website can embed a policy inside its pages that specifies which scriptsare allowed to run the browser which knows exactly when it will runa script can enforce this policy perfectly we have added beepsupport to several browsers and built tools to simplify addingpolicies to web applications we found that supporting beep inbrowsers requires only small and localized modifications modifyingweb applications requires minimal effort and enforcing policies isgenerally lightweight
subspace secure crossdomain communication for web mashups subspace secure crossdomain communication for web mashups combining data and code from thirdparty sources has enableda new wave of web mashups that add creativity andfunctionality to web applications however browsers arepoorly designed to pass data between domains often forcingweb developers to abandon security in the name of functionalityto address this deficiency we developed subspace anovel crossdomain communication mechanism that allowsefficient communication across domains without sacrificingsecurity our prototype requires only a small javascriptlibrary and works across all major browsers we believesubspace can serve as a new secure communication primitivefor web mashups
exposing private information by timing web applications exposing private information by timing web applications we show that the time web sites take to respond to http requests can leak private information using two different types of attacks the first directly measures response times from a web site to expose private information such as validity of an username at a secured site or the number of private photos in a publicly viewable gallery the second called crosssite timing enables a malicious web site to obtain information from the users perspective at another site for example a malicious site can learn if the user is currently logged in at a victim site and in some cases the number of objects in the users shopping cart our experiments suggest that these timing vulnerabilities are widespread we explain in detail how and why these attacks work and discuss methods for writing web application code that resists these attacks
on anonymizing query logs via tokenbased hashing on anonymizing query logs via tokenbased hashing in this paper we study the privacy preservation properties of aspecific technique for query log anonymization tokenbased hashingin this approach each query is tokenized and then a secure hashfunction is applied to each token we show that statisticallinguistic techniques may be applied to partially compromise theanonymization we then analyze the specific risks that arise fromthese partial compromises focused on revelation of identity fromunambiguous names addresses and so forth and the revelation offacts associated with an identity that are deemed to be highlysensitive our goal in this work is twofold to show that tokenbasedhashing is unsuitable for anonymization and to present a concreterisk analysis framework for evaluating other proposals
cantina a contentbased approach to detecting phishing web sites cantina a contentbased approach to detecting phishing web sites phishing is a significant problem involving fraudulent email and web sites that trick unsuspecting users into revealing private information in this paper we present the design implementation and evaluation of cantina a novel contentbased approach to detecting phishing web sites based on the wellknown tfidf algorithm used in information retrieval we also discuss the design and evaluation of several heuristics we developed to reduce our false positive rates our experiments show that cantina is good at detecting phishing sites correctly labeling approximately of phishing sites
learning to detect phishing emails learning to detect phishing emails each month more attacks are launched with the aim of making web users believe that they are communicating with a trusted entity for the purpose of stealing account information logon credentials and identity information in general this attack method commonly known as phishing is most commonly initiated by sending out emails with links to spoofed websites that harvest information we present a method for detecting these attacks which in its most general form is an application of machine learning on a feature set designed to highlight usertargeted deception in electronic communication this method is applicable with slight modification to detection of phishing websites or the emails used to direct victims to these sites we evaluate this method on a set of approximately such phishing emails and nonphishing emails and correctly identify over of the phishing emails while only misclassifying on the order of of the legitimate emails we conclude with thoughts on the future for such techniques to specifically identify deception specifically with respect to the evolutionary nature of the attacks and information available
a largescale study of web password habits a largescale study of web password habits we report the results of a large scale study of password use andpassword reuse habits the study involved half a million users over athree month period a client component on users machines recorded avariety of password strength usage and frequency metrics this allows usto measure or estimate such quantities as the average number of passwordsand average number of accounts each user has how many passwords she typesper day how often passwords are shared among sites and how often they areforgotten we get extremely detailed data on password strength the typesand lengths of passwords chosen and how they vary by site the data is thefirst large scale study of its kind and yields numerous other insightsinto the role the passwords play in users online experience
a fault model and mutation testing of access control policies a fault model and mutation testing of access control policies to increase confidence in the correctness of specified policies policy developers can conduct policy testing by supplying typical test inputs requests and subsequently checking test outputs responses against expected ones unfortunately manual testing is tedious and few tools exist for automated testing of xacml policies
analyzing web access control policies analyzing web access control policies xacml has emerged as a popular access control language on the web but because of its rich expressiveness it has proved difficult to analyze in an automated fashion previous attempts to analyze xacml policies either use propositional logic or full firstorder logic in this paper we present a formalization of xacml using description logics dl this formalization allows us to extend the subset of xacml supported by propositional logicbased analysis tools we also provide a new analysis service policy redundancy mapping xacml to description logics allows us to use offtheshelf dl reasoners for analysis tasks such as policy comparison policy verification and querying we provide empirical evaluation of a policy analysis tool that was implemented on top of open source reasoner pellet
compiling cryptographic protocols for deployment on the web compiling cryptographic protocols for deployment on the web cryptographic protocols are useful for trust engineering in web transactions the cryptographic protocol programming language cppl provides a model wherein trust management annotations are attached to protocol actions and are used to constrain the behavior of a protocol participant to be compatible with its own trust policy
toward expressive syndication on the web toward expressive syndication on the web syndication systems on the web have attracted vast amounts of attention in recent years as technologies have emerged and matured there has been a transition to more expressive syndication approaches that is subscribers and publishers are provided with more expressive means of describing their interests and published content enabling more accurate information filtering in this paper we formalize a syndication architecture that utilizes expressive web ontologies and logicbased reasoning for selective content dissemination this provides finer grained control for filtering and automated reasoning for discovering implicit subscription matches both of which are not achievable in less expressive approaches we then address one of the main limitations with such a syndication approach namely matching newly published information with subscription requests in an efficient and practical manner to this end we investigate continuous query answering for a large subset of the web ontology language owl specifically we formally define continuous queries ie subscriptions for owl knowledge bases and present a novel algorithm for continuous query answering in a large subset of this language lastly an evaluation of the query approach is shown demonstrating its effectiveness for syndication purposes
exhibit lightweight structured data publishing exhibit lightweight structured data publishing it is no surprise that semantic web researchers and enthusiasts are excited to publish and accumulate semistructured data on the web but looking beyond our community we recognize that many many other people also have structured data and want to publish it in rich browsing interfaces these smalltime authors fall into the same category as those early enthusiasts of the web who were simply excited by the opportunity of using the new medium to share information that they cared about with this insight we create a lightweight structured data publishing framework called exhibit that duplicates many factors we believe have contributed to the original growth of the web we argue that appealing to this segment of the web populationaddressing their publishing needs and desires at very low cost in many aspectslets us leverage their labor to structureize existing content on the web that has previously been authored in html by hand and is remaining hard to harvest automatically
sparql towards support for subgraph extraction queries in rdf databases sparql towards support for subgraph extraction queries in rdf databases many applications in analytical domains often have the need to connect the dots ie query about the structure of data in bioinformatics for example it is typical to want to query about interactions between proteins the aim of such queries is to extract relationships between entities ie paths from a data graph often such queries will specify certain constraints that qualifying results must satisfy eg paths involving a set of mandatory nodes unfortunately most present day semantic web query languages including the current draft of the anticipated recommendation sparql lack the ability to express queries about arbitrary path structures in data in addition many systems that support some limited form of path queries rely on main memory graph algorithms limiting their applicability to very large scale graphs in this paper we present an approach for supporting path extraction queries our proposal comprises i a query language sparql which extends sparql with path variables and path variable constraint expressions and ii a novel query evaluation framework based on efficient algebraic techniques for solving path problems which allows for path queries to be efficiently evaluated on disk resident rdf graphs the effectiveness of our proposal is demonstrated by a performance evaluation of our approach on both real world based and synthetic datasets
measuring semantic similarity between words using web search engines measuring semantic similarity between words using web search engines semantic similarity measures play important roles in information retrieval andnatural language processing in information retrieval semantic similarity measures are usedin automatic query suggestion and expansionprevious work in semantic webrelated applications such as community mining relation extractionautomatic meta data extraction have used various semantic similarity measures despite the usefulness of semantic similarity measures in these applicationsrobustly measuring semantic similarity between two words or entities remains a challenging tasksemantic similarity is a dynamic phenomenon that changes over time and across domains in this paper we propose a robust semantic similarity measure that uses the information available on the web to measure similarity between words or entities we propose a method that exploits page counts and text snippets returned by a web search engine to measure semantic similarity between words we define various similarity scores for two given words p and q using the page counts for the queries p q and p and qmoreover we propose a novel approach to compute semantic similarity usingautomatically extracted lexicosyntactic patterns from text snippets these different similarity scores are integrated using support vector machines toleverage a robust semantic similarity measureexperimental results on millercharles benchmark dataset show that the proposed measure outperforms all the existing webbased semantic similarity measures by a wide margin achieving a correlation coefficient of moreover the proposed semantic similarity measure significantly improves the accuracy fmeasure of in a community mining task and improves accuracy in a entity disambiguation task thereby verifying the capability of the proposed measure to capture semantic similarity using web content
using google distance to weight approximate ontology matches using google distance to weight approximate ontology matches discovering mappings between concept hierarchies is widely regarded asone of the hardest and most urgent problems facing the semantic web theproblem is even harder in domains where concepts are inherently vagueand illdefined and cannot be given a crisp definition a notion ofapproximate concept mapping is required in such domains butuntil now no such notion is available
hierarchical perceptronlike learning for ontologybased information extraction hierarchical perceptronlike learning for ontologybased information extraction recent work on ontologybased information extraction ie has tried to make an increased use of the knowledge from the target ontology in order to improve the semantic annotation results however only very few approaches are able to benefit from the ontology structure and one of them is not a learning system thus is not easy to adapt to new domains whereas the other one does not perform semantic annotation of documents but only ontology population
from sparql to rules and back from sparql to rules and back as the data and ontology layers of the semantic web stack have achieved a certain level of maturity in standard recommendations such as rdf and owl the current focus lies on two related aspects on the one hand the definition of a suitable query language for rdf sparql seems to be close to candidate recommendation status within the wc the establishment of the rules layer on top of the existing stack on the other hand marks the next step to be tackled where especially languages with their roots in logic programming and deductive databases are receiving considerable attention the purpose of this paper is threefold first we discuss the formal semantics of sparql extending recent results in several ways second we provide translations from sparql to datalog with stratified negation as failure third we propose some useful and easy to implement extensions of sparql based on this translation as it turns out the combination serves for direct implementations of sparql on top of existing rules engines as well as a basis for more general rules and query languages on top of rdf
bridging the gap between owl and relational databases bridging the gap between owl and relational databases schema statements in owl are interpreted in a different way fromsimilar statements in a relational database setting this can leadto problems in datacentric applications where owls interpretationof the statements intended as constraints may be confusing andorinappropriate we propose an extension of owl that attempts to mimicthe intuition behind integrity constraints in relational databaseswe discuss the algorithms for checking constraint satisfaction fordifferent types of knowledge bases and show that provided theconstraints are satisfied we can disregard them while answering abroad range of positive queries
activerdf objectoriented semantic web programming activerdf objectoriented semantic web programming objectoriented programming is the current mainstream programming paradigm but existing rdf apis are mostly tripleoriented traditional techniques for bridging a similar gap between relational databases and objectoriented programs cannot be applied directly given the different nature of semantic web data as can for example be seen in the semantics of class membership inheritance relations and object conformance to schemas
explorations in the use of semantic web technologies for product information management explorations in the use of semantic web technologies for product information management master data refers to core business entities a company uses repeatedlyacross many business processes and systems such as lists orhierarchies of customers suppliers accounts products or organizationalunits product information is the most important kind of master dataand product information management pim is becoming critical formodern enterprises because it provides a rich business context for variousapplications existing pim systems are less flexible and scalable forondemand business as well as too weak to completely capture and usethe semantics of master data this paper explores how to use semanticweb technologies to enhance a collaborative pim system by simplifyingmodeling and representation while preserving enough dynamicflexibility furthermore we build a semantic pim system using one ofthe stateofart ontology repositories and summarize the challenges weencountered based on our experimental results especially on performanceand scalability we believe that our study and experiences are valuablefor both semantic web community and master data management community
yago a core of semantic knowledge unifying wordnet and wikipedia yago a core of semantic knowledge unifying wordnet and wikipedia we present yago a lightweight and extensible ontology with high coverage and qualityyago builds on entities and relations and currently contains roughly entities and factsthis includes the isa hierarchy as well as nontaxonomic relations between entities such as haswonprizethe facts have been automatically extracted from the unification of wikipedia and wordnetusing a carefully designed combination of rulebased and heuristic methods described in this paperthe resulting knowledge base is a major step beyond wordnet in quality by adding knowledge aboutindividuals like persons organizations products etc with their semantic relationships and in quantity by increasing the number of facts by more than an order of magnitudeour empirical evaluation of fact correctness shows an accuracy of about yago is based on a logically clean model which is decidable extensible and compatible with rdfsfinally we show how yago can be further extended by stateoftheart information extraction techniques
ontology summarization based on rdf sentence graph ontology summarization based on rdf sentence graph ontology summarization is very important to quick understanding and selection of ontologies in this paper we study extractive summarization of ontology which generates an indicative summary of ontology automatically by extracting a salient part of ontology we use a notion of rdf sentence as the basic unit of ontology an rdf sentence graph is proposed to characterize the linkage between rdf sentences derived from ontology from the viewpoint of a surfer the salience of an rdf sentence is assessed in terms of its centrality in the graph we propose to summarize an ontology as a set of salient rdf sentences extracted from the ontology according to a reranking strategy we compare several methods in assessing the salience of rdf sentences and give an overall evaluation of experimented results experiments show that the rdf sentence graph approach to ontology summarization is feasible
just the right amount extracting modules from ontologies just the right amount extracting modules from ontologies the ability to extract meaningful fragments from an ontology is key for ontology reuse we propose a definition of a module that guarantees to completely capture the meaning of a given set ofterms ie to include all axioms relevant to the meaning of theseterms and study the problem of extracting minimally sized moduleswe show that the problem of deciding if a module is minimal isundecidable even for rather restricted sublanguages of owl dlhence we propose two approximations ie alternativedefinitions of modules for a vocabulary that still provide the aboveguarantee but that are possibly too strict and that may thusresult in larger modules the first approximation is semantic andcan be checked using existing dl reasoners the second is syntacticand can be computed in polynomial time finally we report on anempirical evaluation of our syntactic approximation thatdemonstrates that the modules we extract are surprisingly small
the two cultures mashing up web and the semantic web position paper the two cultures mashing up web and the semantic web position paper there is a common perception that there are two competing visions about the future evolution of the web the semantic web and web visions we believe that the technologies and core strengths of these visions are complementary rather than in competition in fact both technologies need each other in order to scale beyond their own strongholds the semantic web can learn from web focus on community and interactivity while web can draw from semantic web infrastructure to facilitate things like mashups
analysis of topological characteristics of huge online social networking services analysis of topological characteristics of huge online social networking services social networking services are a fastgrowing business in the internet however it is unknown if online relationships and their growth patterns are the same as in reallife social networks in this paper we compare the structures of three online social networking services cyworld myspace and orkut each with more than million users respectively we have access to complete data of cyworlds ilchon friend relationships and analyze its degree distribution clusteringproperty degree correlation and evolution over time we also use cyworld data to evaluate the validity of snowball sampling method which we use to crawl and obtain partial network topologies of myspace and orkut cyworld the oldest of the three demonstrates a changing scaling behavior over time in degree distribution the latest cyworld datas degree distribution exhibits a multiscaling behaviorwhile those of myspace and orkut have simple scaling behaviors with different exponents very interestingly each of the two exponents corresponds to the different segments in cyworlds degree distribution certain online social networking services encourage online activities that cannot be easily copied in real life we show that they deviate from closeknit online social networks which show a similar degree correlation pattern to reallife social networks
ptag large scale automatic generation of personalized annotation tags for the web ptag large scale automatic generation of personalized annotation tags for the web the success of the semantic web depends on the availability of web pages annotated with metadata free form metadata or tags as used in social bookmarking and folksonomies based systems have become more and more popular and successful such tags are relevant keywords associated with or assigned to a piece of information eg a web page thus describing the item and enabling keywordbased classification in this paper we propose ptag a method which automatically generates personalized tags for web pages keywords are generated based on the content of the web page but also based on the content of the users desktop thus expressing a personalized viewpoint very relevant for personal tags we implemented and tested several algorithms for this approach and evaluated the relevance of the resulting keywords these evaluations showed very promising results and we are therefore very confident that such a user oriented automatic tagging approach can provide large scale personalized metadata annotation as an important step towards realizing the semantic web
connecting the bottom of the pyramid an exploratory case study of indias rural communication environment connecting the bottom of the pyramid an exploratory case study of indias rural communication environment this paper is based on our exploratory study of a south indian village in chamrajanagar district of karnataka the study was to understand the rural communication environment and villagers communication preferences we examined peoples lifestyle working conditions and their communication ecosystem our study revealed that villagers unlike urban inhabitants interacted with people outside the village only for specific rather than casual purposes another interesting aspect of rural communication was the marginal use of the postal system and the ubiquitous use of pay phone apart from word of mouth and facetoface interactions in fact personal facetoface interaction was usually preferred among villages in this region over other kinds of communication despite infrastructural constraints like poor transport services
communication as informationseeking the case for mobile social software for developing regions communication as informationseeking the case for mobile social software for developing regions in this paper we describe several findings from a multiyear multimethod study of how information and communication technologies have been adopted and adapted in central asia we have found that mobile phone usage is outpacing that of internet adoption that access to the internet is primarily through public access sites carrying with it issues regarding privacy and surveillance that people rely on their social networks as information sources that public institutions tend to be fairly weak as citizen resources and that information seeking and communication are conflated in peoples usage patterns with different technologies in addition in the developed world social networking software has grown rapidly and shown itself to have significant potential for mobilizing a population based on the collection of findings from central asia and observing patterns of technology usage in other parts of the world our research leads to the conclusion that exploring mobile social software holds significant potential as an ict that meshes well with preexisting patterns of communication and information seeking and also leverages the most predominant pattern of technology adoption many of the findings from this research echo results from studies in other geographic areas and so we anticipate that much of this research will be relevant to developing regions generally
optimal audiovisual representations for illiterate users of computers optimal audiovisual representations for illiterate users of computers we present research leading toward an understanding of the optimal audiovisual representation for illustrating concepts for illiterate and semiliterate users of computers in our user study which to our knowledge is the first of its kind we presented each of different health symptoms to illiterate subjects in one representation randomly selected among the following ten text static drawings static photographs handdrawn animations and video each with and without voice annotation the goal was to see how comprehensible these representation types were for an illiterate audience we used a methodology for generating each of the representations tested in a way that fairly stacks one representational type against the others our main results are that richer information is not necessarily better understood overall voice annotation generally helps in speed of comprehension but bimodal audiovisual information can be confusing for the target population the relative value of dynamic imagery versus static imagery depends on other factors analysis of these statistically significant results and additional detailed results are also provided
identifying and discriminating between web and peertopeer trafficin the network core identifying and discriminating between web and peertopeer trafficin the network core traffic classification is the ability to identify and categorizenetwork traffic by application type in this paper we consider theproblem of traffic classification in the network coreclassification at the core is challenging because only partialinformation of the flows and their contributors is available weaddress this problem by developing and evaluating a classificationframework that can classify a flow using only unidirectional flowinformation we validated this approach using recent fullpayloadpacket traces that we collected and preclassified to establish abase truth from our evaluation we find that flow statisticsalong the servertoclient path of a tcp connection provides higherclassification accuracy than flow statistics along theclienttoserver path because collection of the servertoclientflow statistics may not always be feasible we developed andverified an algorithm that can estimate the missing statistics froma unidirectional packet trace
long distance wireless mesh network planning problem formulation and solution long distance wireless mesh network planning problem formulation and solution several research efforts as well as deployments have chosen ieee as a lowcost longdistance access technology to bridge thedigital divide in this paper we consider the important issue ofplanning such networks to the minimize system cost this is a nontrivialtask since it involves several sets of variables the networktopology tower heights antenna types to be used and theirorientations and radio transmit powers the task is further complicated due to the presence of network performance constraints and theinterdependence among the variables our first contribution inthis paper is the formulation of this problem in terms of the variables constraints and the optimization criterion our second contributionis in identifying the dependencies among the variables and breakingdown the problem into four tractable subparts in this process we extensively use domain knowledge to strike a balance between tractability and practicality
is highquality vod feasible using pp swarming is highquality vod feasible using pp swarming digital media companies have recently started embracing pp networks as an alternative content distribution channel however the drawback of the current em pp swarming systems is that users need to download the full video and hence wait a long time before they can start watching it while a lot of effort has gone into optimizing the distribution of large files little research has been done on how to enable highquality videoondemand vod functionality with pp swarming systems the main challenges reside in ensuring that users can start watching a movie at any point in time while providing small startup times sustainable playback rates and high swarming efficiencies
turning portlets into services the consumer profile turning portlets into services the consumer profile portlets strive to play at the front end the same role that web services currently enjoy at the back end namely enablers of application assembly through reusable services however it is wellknown in the component community that the larger the component the more reduced the reuse hence the coarsegrained nature of portlets they encapsulate also the presentation layer can jeopardize this vision of portlets as reusable services to avoid this situation this work proposes a perspective shift in portlet development by introducing the notion of organization profile while the user profile characterises the end user eg age name etc the organization profile captures the idiosyncrasies of the organization through which the portlet is being delivered eg the portal owner as far as the portlet functionality is concerned the user profile is dynamic and hence requires the portlet to be customised at run time by contrast the organization profile is known at registration time and it is not always appropriatepossible to consider it at run time rather it is better to customize the code at development time and produce an organizationspecific portlet which builtin custom functionality in this scenario we no longer have a portlet but a family of portlets and the portlet provider becomes the assembly line of this family this work promotes this vision by introduces an organizationaware wsrpcompliant architecture that let portlet consumers registry and handle family portlets in the same way that traditional portlets in so doing portlets are nearer to become truly reusable services
a framework for rapid integration of presentation components a framework for rapid integration of presentation components the development of user interfaces uis is one of the most timeconsuming aspects in software development in this context the lack of proper reuse mechanisms for uis is increasingly becoming manifest especially as software development is more and more moving toward composite applications in this paper we propose a framework for the integration of standalone modules or applications where integration occurs at the presentation layer hence the final goal is to reduce the effort required for ui development by maximizing reuse
integrating valuebased requirement engineering models to webml using vip business modeling framework integrating valuebased requirement engineering models to webml using vip business modeling framework requirement engineering is emerging as an increasingly important discipline for supporting web application development as these are designed to satisfy diverse stakeholder needs additional functional information multimedia and usability requirements as compared to traditional software applications moreover when considering innovative ecommerce applications valuebased requirements engineering is an extremely relevant methodology which exploits the concept of economic value during the requirements engineering activity in contrast most of the methodologies proposed for the development of web applications primarily focus on the system design and paying less attention to the requirements engineering and specifically to valuebased requirement engineering focusing this aspect the paper presents integration of valuebased requirement engineering models to webml models using our recently proposed vip business modeling framework the integration process is demonstrated using a wellknown ecommerce application example by first presenting example vip business models and then deriving webml process structural and other models from these business models
towards effective browsing of large scale social annotations towards effective browsing of large scale social annotations this paper is concerned with the problem of browsing social annotations today a lot of services eg delicious flickr have been provided based on social annotations these services help users to manage and share their favorite url photos etc however due to exponential increasing of the social annotations more and more users are facing the problem of finding desired resources among a large annotation data existing methods such as tag cloud annotation matching work well only when the annotation scale is relatively small thus an effective approach for browsing the large scale annotations and the associated resources is on great demand of both users and service providers in this paper we propose a novel algorithm namely effective large scale annotation browser elsaber to browse the largescale social annotations with the help of elsaber the users could browse the largesale annotations in a semantic hierarchical and efficient way more specifically the semantic relations among annotations are explored for similar resource browsing the hierarchical relations among annotations are also constructed for topdown browsing the powerlaw distribution of social annotations is studied for efficient browsing by incorporating the personal and time information the elsaber can be further extended for personalized and timerelated browsing a prototype system is also implemented based on elsaber and shows promising results
supporting endusers in the creation of dependable web clips supporting endusers in the creation of dependable web clips web authoring environments enable endusers to create applications that integrate information from other web sources users can create web sites that include builtin components to dynamically incorporate for example weather information stockquotes or the latest news from different web sources recent surveys conducted among endusers have indicated an increasing interest in creating such applications unfortunately web authoring environments do not provide support beyond a limited set of builtin components this work addresses this limitation by providing enduser support for clipping information from a web site to incorporate it into the enduser site the support consists of a mechanism to identify the clipping target with multiple markers to increase robustness and a dynamic assessment of the retrieved information to quantify its reliability the clipping approach has been integrated as a feature into a popular web authoring tool on which we present the results of two preliminary studies
effort estimation how valuable is it for a web company to use a crosscompany data set compared to using its own singlecompany data set effort estimation how valuable is it for a web company to use a crosscompany data set compared to using its own singlecompany data set previous studies comparing the prediction accuracy of effort models built using web cross and singlecompany data sets have been inconclusive and as such replicated studies are necessary to determine under what circumstances a company can place reliance on a crosscompany effort modelthis paper therefore replicates a previous study by investigating how successful a crosscompany effort model is i to estimate effort for web projects that belong to a single company and were not used to build the crosscompany model ii compared to a singlecompany effort model our singlecompany data set had data on web projects from a single company and our crosscompany data set had data on web projects from different companies the effort estimates used in our analysis were obtained by means of two effort estimation techniques namely forward stepwise regression and casebased reasoningour results were similar to those from the replicated study showing that predictions based on the singlecompany model were significantly more accurate than those based on the crosscompany model
towards the theoretical foundation of choreography towards the theoretical foundation of choreography with the growth of interest on the web services people pay more andmore attention to choreography that is to describe collaborationsof participants from a global viewpoint in accomplishing a commonbusiness goal in this paper based on a simple choreographylanguages and a roleoriented process languages we study somefundamental issues related to choreography especially related toimplementation including semantics projection and naturalprojection dominant role in choices and iterations etc we developthe concept of emphdominant role and propose some novel languagesstructures related to it the study reveals some clues about thelanguage semantics specification and implementation ofchoreography
introduction and evaluation of martlet a scientific workflow language for abstracted parallelisation introduction and evaluation of martlet a scientific workflow language for abstracted parallelisation the workflow language martlet described in this paper implements a new programming model that allows users to write parallel programs and analyse distributed data without having to be aware of the details of the parallelisation martlet abstracts the parallelisation of the computation and the splitting of the data through the inclusion of constructs inspired by functional programming these allow programs to be written as an abstract description that can be adjusted automatically at runtime to match the data set and available resources using this model it is possible to write programs to perform complex calculations across a distributed data set such as singular value decomposition or least squares problems as well as creating an intuitive way of working with distributed systems
semiautomated adaptation of service interactions semiautomated adaptation of service interactions in todays web many functionalitywise similar web services are offered through heterogeneous interfaces operation definitions and business protocols ordering constraints defined on legal operation invocation sequences the typical approach to enable interoperation in such a heterogeneous setting is through developing adapters there have been approaches for classifying possible mismatches between service interfaces and business protocols to facilitate adapter development however the hard job is that of identifying given two service specifications the actual mismatches between their interfaces and business protocols 
reliable qos monitoring based on client feedback reliable qos monitoring based on client feedback servicelevel agreements slas establish a contract between service providersand clients concerning quality of service qos parameters without properpenalties service providers have strong incentives to deviate from theadvertised qos causing losses to the clients reliable qos monitoring andproper penalties computed on the basis of delivered qos are thereforeessential for the trustworthiness of a serviceoriented environment in thispaper we present a novel qos monitoring mechanism based on quality ratings from theclients a reputation mechanism collects the ratings and computes theactual quality delivered to the clients the mechanism provides incentives forthe clients to report honestly and pays special attention to minimizing costand overhead
preferencebased selection of highly configurable web services preferencebased selection of highly configurable web services a key challenge for dynamic web service selection is that web services are typically highly configurable and service requesters often have dynamic preferences on service configurations current approaches such as wsagreement describe web services by enumerating the various possible service configurations an inefficient approach when dealing with numerous service attributes with large value spaces we model web service configurations and associated prices and preferences more compactly using utility function policies which also allows us to draw from multiattribute decision theory methods to develop an algorithm for optimal service selection in this paper we present an owl ontology for the specification of configurable web service offers and requests and a flexible and extensible framework for optimal service selection that combines declarative logicbased matching rules with optimization methods such as linear programming assuming additive pricepreference functions experimental results indicate that our algorithm introduces an overhead of only around sec compared to a random service selection while giving optimal results the overhead as percentage of total time decreases as the number of offers and configurations increase 
speeding up adaptation of web service compositions using expiration times speeding up adaptation of web service compositions using expiration times web processes must often operate in volatile environments where the parameters of the participating service providers change during the life cycle of the process optimally adapting to these changes therefore becomes an important challenge adaptation requires the knowledge from each of the service providers as to how much each of their service parameters have changed and using this knowledge to determine whether the web process should make a different and subsequently more optimal decision monitoring this information change requires additional computations and may become a timeconsuming process in this paper we use service expiration times obtained from predefined service level agreements to reduce the computational overhead of adaptation we use the intuition that services whose parameters have not expired need not be considered for querying for revised information using two realistic scenarios we illustrate our approach and demonstrate the associated computational savings
diane an integrated approach to automated service discovery matchmaking and composition diane an integrated approach to automated service discovery matchmaking and composition automated matching of semantic service descriptions is the key to automatic service discovery and binding but when trying to find a match for a certain request it may often happen that the request cannot be serviced by a single offer but could be handled by combining existing offers in this case automatic service composition is needed although automatic composition is an active field of research it is mainly viewed as a planning problem and treated separatedly from service discovery
multiway slcabased keyword search in xml data multiway slcabased keyword search in xml data keyword search for smallest lowest common ancestors slcasin xml data has recently beenproposed as a meaningful way to identify interesting data nodes inxml data whose subtrees contain an input set of keywords in thispaper we generalize this useful search paradigm to support keywordsearch beyond the traditional and semantics to include bothand and or boolean operators as wellwe first analyze properties of thelca computation and propose more efficient algorithms to solve thetraditional keyword search problem with only and semantics wethen extend our approach to handle general keyword search involvingcombinations of and and or boolean operators theeffectiveness of our new algorithms is demonstrated with acomprehensive experimental performance study
visibly pushdown automata for streaming xml visibly pushdown automata for streaming xml we propose the study of visibly pushdown automata vpa for processing xml documents vpas are pushdown automata where the input determines the stack operation and xml documents are naturally visibly pushdown with the vpa pushing onto the stack on opentags and popping the stack on closetags in this paper we demonstrate the power and ease visibly pushdown automata give in the design of streaming algorithms for xml documents
mappingdriven xml transformation mappingdriven xml transformation clio is an existing schemamapping tool that provides userfriendly means tomanage and facilitate the complex task of transformation and integration ofheterogeneous data such as xml over the web or in xml databases by means ofmappings from source to target schemas clio can help users convenientlyestablish the precise semantics of data transformation and integration in thispaper we study the problem of how to efficiently implement such datatransformation ie generating target data from the source data based on schemamappings we present a threephase framework for highperformance xmltoxmltransformation based on schema mappings and discuss methodologies and algorithmsfor implementing these phases in particular we elaborate on novel techniquessuch as streamed extraction of mapped source values and scalable diskbasedmerging of overlapping data including duplicate elimination we compare ourtransformation framework with alternative methods such as using xquery or sqlxmlprovided by current commercial databases the results demonstrate that thethreephase framework although as simple as it is is highly scalable andoutperforms the alternative methods by orders of magnitude
querying and maintaining a compact xml storage querying and maintaining a compact xml storage as xml database sizes grow the amount of space used for storing the data andauxiliary supporting data structures becomes a major factor in query and updateperformance this paper presents a new storage scheme for xml datathat supports all navigational operations in near constant timein addition to supporting efficient queries the space requirement of theproposed scheme is within a constant factor of the information theoreticminimum while insertions and deletions can be performed in near constant timeas well as a result the proposed structure features a small memory footprintthat increases cache locality whilst still supporting standard apis such asdom and necessary database operations such as queries and updatesefficiently analysis and experiments showthat the proposed structure is space and time efficient
xml design for relational storage xml design for relational storage design principles for xml schemas that eliminate redundancies andavoid update anomalies have been studied recently several normalforms generalizing those for relational databases have beenproposed all of them however are based on the assumption of anative xml storage while in practice most of xml data is stored inrelational databases
a highperformance interpretive approach to schemadirected parsing a highperformance interpretive approach to schemadirected parsing xml delivers key advantages in interoperability due to itsflexibility expressiveness and platformneutrality as xml hasbecome a performancecritical aspect of the next generation ofbusiness computing infrastructure however it has become increasinglyclear that xml parsing often carries a heavy performance penalty andthat current widelyused parsing technologies are unable to meet theperformance demands of an xmlbased computing infrastructure severalefforts have been made to address this performance gap through the useof grammarbased parser generation while the performance ofgenerated parsers has been significantly improved adoption of thetechnology has been hindered by the complexity of compiling anddeploying the generated parsers through careful analysis of theoperations required for parsing and validation we have devised a setof specialized bytecodes designed for the task of xml parsing andvalidation these bytecodes are designed to engender the benefits offinegrained composition of parsing and validation that make existingcompiled parsers fast while being coarsegrained enough to minimizeinterpreter overhead this technique of using an interpretivevalidating parser balances the need for performance against therequirements of simple tooling and robust scalable infrastructureour approach is demonstrated with a specialized schema compiler usedto generate bytecodes which in turn drive an interpretive parserwith almost as little tooling and deployment complexity as atraditional interpretive parser the bytecodedriven parser usuallydemonstrates performance within of the fastest fully compiledsolutions
a nofrills architecture for lightweight answer retrieval a nofrills architecture for lightweight answer retrieval in a new model for answer retrieval document collections are distilled offline into large repositories of facts each fact constitutes a potential direct answer to questions seeking a particular kind of entity or relation such as questions asking about the date of particular events question answering becomes equivalent to online fact retrieval which greatly simplifies the defacto system architecture
webce accessing webbased applications on consumer devices webce accessing webbased applications on consumer devices in a world where all devices will be interconnected the boundaries between the different devices will start to disappear devices will be able to access each others applications sessions can be suspended on one device and resumed on another device devices can serve as each others input and output device and alldevices will be able to connect to the internet this will give true mobility to the user as heshe will not be restricted to the time and location where heshe accesses an application 
collaborative ict for indian business clusters collaborative ict for indian business clusters indian business clusters have contributed immensely to the countrys industrial output poverty alleviation and employment generation however with recent globalization these clusters can loose out to international competitors if they do not continuously innovate and take advantage of the new opportunities that are available through economic liberalization in this paper we discuss how information and communication technologies ict can help in improving the productivity and growth of these clusters
delay tolerant applications for low bandwidth and intermittently connected users the aaqua experience delay tolerant applications for low bandwidth and intermittently connected users the aaqua experience with the explosive growth and spread of internet web access from mobile and rural users has become significant but these users face problems of low bandwidth and intermittent internet connectivity to make the benefits of the internet reach the common man in developing countries accessibility and availability of the information has to be improved aaqua is an online multilingual multimedia agricultural portal for disseminating information from and to rural communities considering resource constrained rural environments we have designed and implemented an offline solution which provides an online experience to users in disconnected mode our solution is based on heterogeneous database synchronization which involves only a small synchronization payload ensuring an efficient use of available bandwidth offline aaqua has been deployed in the field and systematic studies of our solution show that user experience has improved tremendously not only in disconnected mode but also in connected mode
a browser for a publicdomain speechweb a browser for a publicdomain speechweb a speechweb is a collection of hyperlinked applications which are accessed remotely by speech browsers running on enduser devices links are activated through spoken commands despite the fact that protocols and technologies for creating and deploying speech applications have been readily available for several years we have not seen the development of a publicdomain speechweb in this paper we show how freelyavailable software and commonlyused communication protocols can be used to change this situation
a novel clusteringbased rss aggregator a novel clusteringbased rss aggregator in recent years different commercial weblog subscribing systemshave been proposed to return stories from users subscribed feedsin this paper we propose a novel clusteringbased rss aggregatorcalled as rss clusgator system rcs for weblog reading notethat an rss feed may have several different topics a user mayonly be interested in a subset of these topics in addition therecould be many different stories from multiple rss feeds whichdiscuss similar topic from different perspectives a user may beinterested in this topic but do not know how to collect all feedsrelated to this topic in contrast to many previous works wecluster all stories in rss feeds into hierarchical structure to betterserve the readers through this way users can easily find all theirinterested stories to make the system current we propose aflexible time window for incremental clustering rcs utilizesboth link information and content information for efficientclustering experiments show the effectiveness of rcs
adaptive faceted browser for navigation in open information spaces adaptive faceted browser for navigation in open information spaces open information spaces have several unique characteristics such as their changeability large size complexity and diverse user base these result in novel challenges during user navigation information retrieval and data visualization in open information spaces we propose a method of navigation in open information spaces based on an enhanced faceted browser with support for dynamic facet generation and adaptation based on user characteristics
an assessment of tag presentation techniques an assessment of tag presentation techniques with the growth of social bookmarking a new approach for metadata creation called tagging has emerged in this paper we evaluate the use of such tags the main goal of our evaluation is to investigate the effect of some of the different properties that can be utilized in presenting tags eg alphabetization using larger fonts etc we show that a number of these factors can affect the ease with which users can find tags and use the tools for presenting tags to users
an information statebased dialogue manager for making voice web smarter an information statebased dialogue manager for making voice web smarter in this paper we propose the integration of intelligent components technologies natural language and discourse management in voice web interfaces to make them smarter we describe how we have integrated reusable components of dialogue management and language processing in a multilingual voice system to improve its friendliness and portability the dialogue management component deals with complex dialogue phenomena such as userinitiative dialogues and follows the information statebased theory the resulting dialogue system supports friendly communication through the telephone and the web in several languages english spanish catalan and italian the dialogue system has been adapted to guide the users to access online public administration services
behavior based web page evaluation behavior based web page evaluation this paper describes our efforts to investigate factors in users browsing behavior to automatically evaluate web pages that the user shows interest in to evaluate web pages automatically we developed a clientside logginganalyzing tool the ginis framework this work focuses primarily on clientside user behavior using a customized web browser and ajax technologies first ginis unobtrusively gathers logs of user behavior through the users natural interaction with the web browser then it analyses the logs and extracts effective rules to evaluate web pages using c machine learning system eventually ginis becomes able to automatically evaluate web pages using these learned rules
generating efficient labels to facilitate web accessibility generating efficient labels to facilitate web accessibility for many users with a disability it can be difficult or impossible to use a computer mouse to navigate the web an alternative way to select elements on a web page is the label typing approach in which users select elements by typing part of the label in most cases these labels are specified by the page authors but some selectable elements do not have an obvious textual description thus requiring that a label be generated the set of element labels on a web page must be both efficient to select by text input and meaningful to the user this paper discusses our approach to this problem using page structural analysis and user history to determine important elements of a page and then matching this information with the efficiency model of the input device
generation documentation and presentation of mathematical equations and symbolic scientific expressions using pure html and css generation documentation and presentation of mathematical equations and symbolic scientific expressions using pure html and css this paper describes a comprehensive method for presenting mathematical equations and expressions using only pure html and css this method renders the equations portable and editable and contrasts with previous procedures that represent equations as whole graphic objects methods for generating and documenting the equations using html and javascript are also described such that the equations can be interpreted and converted to or from other formats such as latex mathml or linear representation
geotv navigating geocoded rss to create an iptv experience geotv navigating geocoded rss to create an iptv experience the web is rapidly moving towards a platform for mass collaboration in content production and consumption from three screens computers mobile phones and tvs while there has been a surge of interests in making web content accessible from mobile devices there is a significant lack of progress when it comes to making the web experience suitable for viewing on a television towards this end we describe a novel concept namely geotv where we explore a framework by which web content can be presented or pushed in a meaningful manner to create an entertainment experience for the tv audience fresh content on a variety of topics people and places is being created and made available on the web at breathtaking speed navigating fresh content effectively on tv demands a new browsing paradigm that requires few mouse clicks or user interactions from the remote control novel geospatial and temporal browsing techniques are provided in geotv that allow users the capability of aggregating and navigating rssenabled content in a timely personalized and automatic manner for viewing in an iptv environment this poster is an extension of our previous work on geotracker that utilizes both a geospatial representation and a temporal chronological presentation to help users spot the most relevant updates quickly within the context of a webenabled environment we demonstrate the usability of such a tool that greatly enhances a users ability in locating and browsing videos based on his or her geographical interests and various innovative interface designs of showing rssenabled information for an iptv environment
summarization of online image collections via implicit feedback summarization of online image collections via implicit feedback the availability of map interfaces and locationaware devices makes a growing amount of unstructured georeferenced information available on the web in particular ten million tagged georeferenced photos are now available on flickr a photosharing website we show how we analyze flickr images to generate aggregate knowledge in the form of representative tags for arbitrary areas in the world we display these tags on a map interface in an interactive web application along with images associated with each tag we then use the aggregate user interactions with the tags and images to learn which images best describe the area shown on the map
system for reminding a user of information obtained through a web browsing experience system for reminding a user of information obtained through a web browsing experience dont we forget and waste varied information obtained through our own web browsing in the past we propose a system for reminding a user of information obtained through web browsing experience the system extracts keywords from a content of a web page viewed presently and retrieves a context in past web browsing related to the keyword we defined the context as a sequence of web browsing when many web pages related to the keyword were viewed intensively because we assumed that a lot of information connected to the current content was obtained in the sequence the information is not only what page did heshe views but also how did heshe find the web pages out and what knowledge was heshe acquired from a web page concretely when a user browses web pages the proposed system shows a list of the contexts supposed as important one related to current web page automatically if user selects the context details in the context are showed graphically with marks indicating characteristic activities
the scratchpad sensemaking support for the web the scratchpad sensemaking support for the web the world wide web is a powerful platform for a wide range of information tasks dramatic advances in technology such as improved search capabilities and the ajax application model have enabled entirely new webbased applications and usage patterns making many tasks easier to perform than ever before however few tools have been developed to assist with sensemaking tasks complex research behaviors in which users gather and comprehend information from many sources to answer potentially vague nonprocedural questions sensemaking tasks are common and include for example researching vacation destinations or deciding how to invest this paper presents the scratchpad an extension to the standard browser interface that is designed to capture organize and exploit the information discovered while performing a sensemaking task
towards multigranularity multifacet ebook retrieval towards multigranularity multifacet ebook retrieval generally speaking digital libraries have multiple granularities of semantic units book chapter page paragraph and word however there are two limitations of current ebook retrieval systems the granularity of retrievable units is either too big or too small scales such as chapters paragraphs are ignored the retrieval results should be grouped by facets to facilitate users browsing and exploration to overcome these limitations we propose a multigranularity multifacet ebook retrieval approach
visualizing structural patterns in web collections visualizing structural patterns in web collections we present a tool describex suitable for exploring andvisualizing the structural patterns present in collections ofxml documents describex can be employed by developersto interactively discover for example those xpath expressionsthat will actually return elements known to occur inthe collection
adaptive record extraction from web pages adaptive record extraction from web pages we describe an adaptive method for extracting records fromweb pages our algorithm combines a weighted tree matchingmetric with clustering for obtaining data extraction patternswe compare our method experimentally to the stateoftheartand show that our approach is very competitivefor rigidlystructured records such as product descriptionsand far superior for looselystructured records such as entrieson blogs
exploit sequencing views in semantic cache to accelerate xpath query evaluation exploit sequencing views in semantic cache to accelerate xpath query evaluation in xml databases materializing queries and their results into views in a semantic cache can improve the performance of query evaluation by reducing computational complexity and io cost although there are a number of proposals of semantic cache for xml queries the issues of fast cache lookup and compensation query construction could be further studied in this paper based on sequential xpath queries we propose fastclu a fast cache lookup algorithm and efficq an efficient compensation query constructing algorithm to solve these two problems reported experimental results show that our algorithms outperform previous algorithms and can achieve good performance of query evaluation
extensible schema documentation with xslt extensible schema documentation with xslt xml schema documents are defined using an xml syntax which means that the idea of generating schema documentation through standard xml technologies is intriguing we present xdoc a framework for generating schemadocumentation solely through xslt the framework uses scx an xml syntax for xml schema components as intermediate format and produces xmlbased output formats using a modular set of xslt stylesheets xdoc is highly configurable and carefully crafted towards extensibility this proves especially useful for composite schemas where additional schema information like schematron rules are embedded into xml schemas
preserving xml queries during schema evolution preserving xml queries during schema evolution in xml databases new schema versions may be released as frequently as once every two weeks this poster describes a taxonomy of changes for xml schema evolution it examines the impact of those changes on the schema validation and query evaluation based on that study it proposes guidelines for xml schema evolution and for writing queries in such a way that they continue to operate as expected across evolving schemas
spath a path language for xml schema spath a path language for xml schema xml is increasingly being used as a typed data format and therefore it becomes more important to gain access to the type system very often this is an xml schema the xml schema path language spath presented in this paper provides access to xml schema components by extending the wellknown xpath language to also include the domain of xml schemas using spath xml developers gain access to xml schemas and thus can more easily develop software which is type or schemaaware and thus more robust
the use of xml to express a historical knowledge base the use of xml to express a historical knowledge base since conventional historical records have been written assuming human readers they are not wellsuited for computers to collect and process automatically if computers could understand descriptions in historical records and process them automatically it would be easy to analyze them from different perspectives in this paper we review a number of existing frameworks used to describe historical events and make a comparative assessment of these frameworks in terms of usability based on deep cases of fillmores core grammar based on this assessment we propose a new description framework and have created a microformat vocabulary set suitable for that framework
urest an unsupervised record extraction system urest an unsupervised record extraction system we demonstrate a system that extracts record sets from recordlist web pages with no direct human supervision our system urest reframes the problem of unsupervised record extraction as a twophase machine learning problem with a clustering phase where structurally similar regions are discovered and a record cluster detection phase where discovered grouping of regions are ranked by their likelihood of being records this framework simplifies the record extraction task and allows for independent analysis of the algorithms and the underlying features in our work we survey a large set of features under this simplified framework we conclude with an preliminary comparison of urest against similar systems and show improvements in the extraction accuracy
xmlbased multimodal interaction framework for contact center applications xmlbased multimodal interaction framework for contact center applications in this paper we consider a way to represent contact center applications as a set of multiple xml documents written in different markups including voicexml and ccxml applications can comprise a dialog with ivr call routing and agent scripting functionalities we also consider ways how such applications can be executed in runtime contact center environment
xmlbased xml schema access xmlbased xml schema access xml schemas abstract data model are components which are the structures that eventually define a schema as a whole xml schemas xml syntax on the other hand is not a direct representation of the schema components and it proves to be surprisingly hard to derive a schemas components from the xml syntax the schema component xml syntax scx is a representation which attempts to map schema components as faithfully as possible to xml structures scx serves as the starting point for applications which need access to schema components and want to do so using standardized and widely available xml technologies
academic web search engine generating a survey automatically academic web search engine generating a survey automatically given a document repository search engine is very helpful to retrieveinformation currently vertical search is a hot topic and googlescholar is an example for academic search however most verticalsearch engines only return the flat ranked list without an efficientresult exhibition for given users we study this problem and designeda vertical search engine prototype dolphin where the flexibleuseroriented templates can be defined and the surveylike results arepresented according to the template
a cautious surfer for pagerank a cautious surfer for pagerank this work proposes a novel cautious surfer to incorporate trust into the process of calculating authority for web pages we evaluate a total of sixty queries over two large realworld datasets to demonstrate that incorporating trust can improve pageranks performance
a clustering method for web data with multitype interrelated components a clustering method for web data with multitype interrelated components traditional clustering algorithms work on flat data making the assumption that the data instances can only be represented by a set of homogeneous and uniform features many real world data however is heterogeneous in nature comprising of multiple types of interrelated components we present a clustering algorithm ksvmeans that integrates the well known kmeans clustering with the highly popular support vector machinessvm in order to utilize the richness of data our experimental results on authorship analysis of two real world datasets show that ksvmeans achieves better clustering performance than homogeneous data clustering
a largescale study of robotstxt a largescale study of robotstxt search engines largely rely on web robots to collect information from the web due to the unregulated openaccess nature of the web robot activities are extremely diverse such crawling activities can be regulated from the server side by deploying the robots exclusion protocol in a file called robotstxt although it is not an enforcement standard ethical robots and many commercial will follow the rules specified in robotstxt with our focused crawler we investigate websites from education government news and business domains five crawls have been conducted in succession to study the temporal changes through statistical analysis of the data we present a survey of the usage of web robots rules at the web scale the results also show that the usage of robotstxt has increased over time
a linkbased ranking scheme for focused search a linkbased ranking scheme for focused search this paper introduces a novel link based ranking algorithm based on a model of focused web surfers focusedrank is described and compared to implementations of pagerank and topicsensitive pagerank and a user study is conducted to measure the relevance and precision of each our results are shown to be statistically significant warranting further research into linkbased ranking schemes for focused search
a link classification based approach to website topic hierarchy generation a link classification based approach to website topic hierarchy generation hierarchical models are commonly used to organize a websites content a websites content structure can be represented by a topic hierarchy a directed tree rooted at a websites homepage in which the vertices and edges correspond to web pages and hyperlinks in this work we propose a new method for constructing the topic hierarchy of a website we model the websites link structure using weighted directed graph in which the edge weights are computed using a classifier that predicts if an edge connects a pair of nodes representing a topic and a subtopic we then pose the problem of building the topic hierarchy as finding the shortestpath tree and directed minimum spanning tree in the weighted graph weve done extensive experiments using real websites and obtained very promising results
a searchbased chinese word segmentation method a searchbased chinese word segmentation method in this paper we propose a novel chinese word segmentation method which leverages the huge deposit of web documents and search technology it simultaneously solves ambiguous phrase boundary resolution and unknown word identification problems evaluations prove its effectiveness
anchorbased proximity measures anchorbased proximity measures we present a family of measures of proximity of an arbitrary vertex ina directed graph to a prespecified subset of vertices called theanchor our measures are based on three different propagation schemesand two different uses of the connectivity structure of the graphwe consider a webspecific application of the above measureswith two disjoint anchors good and bad web pages and study the accuracy of these measures in this context
automatic search engine performance evaluation with clickthrough data analysis automatic search engine performance evaluation with clickthrough data analysis performance evaluation is an important issue in web search engine researches traditional evaluation methods rely on lots of human efforts and are therefore quite timeconsuming with click through data analysis we proposed an automatic search engine performance evaluation method this method is a cranfieldlike one and it generates navigational type query topics and answers automatically based on search users query and click behavior experimental results based on a commercial chinese search engines user logs show that the automatically method gets a similar evaluation result with traditional assessorbased ones
automatic searching of tables in digital libraries automatic searching of tables in digital libraries tables are ubiquitous unfortunately no search engine supportstable search in this paper we propose a novel table specificsearching engine tableseer to facilitate the tableextracting indexing searching and sharing in addition wepropose an extensive set of mediumindependent metadata to preciselypresent tables given a query tableseer ranks the returnedresults using an innovative ranking algorithm tablerankwith a tailored vector space model and a novel term weightingscheme experimental results show that tableseer outperformsexisting search engines on table search in addition incorporatingmultiple weighting factors can significantly improve the rankingresults
bayesian network based sentence retrieval model bayesian network based sentence retrieval model this paper makes an intensive investigation of the application of bayesian network in sentence retrieval and introduces three bayesian network based sentence retrieval models with or without consideration of term relationships term relationships in this paper are considered from two perspectives the fist one observes relationships between pairs of terms and the second one focuses on relationships between terms and term sets experiments have proven the efficiency of bayesian network in the application of sentence retrieval particularly and retrieval result with consideration of the second kind of term relationship performs best in improving retrieval precision
brand awareness and the evaluation of search results brand awareness and the evaluation of search results we investigate the effect of search engine brand ie identifying name or logo that distinguishes a product from its competitors on evaluation of system performance our research is motivated by the large amount of search traffic directed to a handful of web search engines even though most are of equal technical quality with similar interfaces we conducted a laboratory study with participants measuring the effect of four search engine brands while controlling for the quality of search engine results using average relevance ratings there was a difference between the most highly rated search engine and the lowest even though search engine results were identical in both content and presentation we discuss implications for search engine marketing and the design of search engine quality studies
causal relation of queries from temporal logs causal relation of queries from temporal logs in this paper we study a new problem of mining causal relation ofqueries in search engine query logs causal relation between twoqueries means event on one query is the causation of some eventon the other query we first detect events in query logs byefficient statistical frequency threshold then the causal relationof queries is mined by the geometric features of the events finallythe granger causality test gct is utilized to further rerank thecausal relation of queries according to their gct coefficients inaddition we develop a dimensional visualization tool to displaythe detected relationship of events in a more intuitive way theexperimental results on the msn search engine query logsdemonstrate that our approach can accurately detect the events intemporal query logs and the causal relation of queries is detectedeffectively
classifying web sites classifying web sites in this paper we present a novel method for the classification of web sites this method exploits both structure and content of web sites in order to discern their functionality it allows for distinguishing between eight of the most relevant functional classes of web sites we show that a preclassification of web sites utilizing structural properties considerably improves a subsequent textual classification with standard techniques we evaluate this approach on a dataset comprising more than web sites with about million crawled and million known web pages our approach achieves an accuracy of for the coarsegrained classification of these web sites
comparing apples and oranges normalized pagerank for evolving graphs comparing apples and oranges normalized pagerank for evolving graphs pagerank is the best known technique for linkbased importance ranking the computed importance scores however are not directly comparable across different snapshots of an evolving graph we present an efficiently computable normalization for pagerank scores that makes them comparable across graphs furthermore we show that the normalized pagerank scores are robust to nonlocal changes in the graph unlike the standard pagerank measure
designing efficient sampling techniques to detect webpage updates designing efficient sampling techniques to detect webpage updates due to resource constraints web archiving systems and searchengines usually have difficulties keeping the entire localrepository synchronized with the web we advance the stateofartof the samplingbased synchronization techniques by answering achallenging question given a sampled webpage and itschange status which other webpages and how many of them are alsolikely to change we present a study of various downloadinggranularities and policies and propose an adaptive model based onthe update history and the popularity of the webpages we runextensive experiments on a large dataset of approximately webpages to demonstrate that it is most likely to find more updated webpages in the current or upper directories of the changed samples moreover the adaptive strategies outperform the nonadaptive one in terms of detecting important changes
determining the user intent of web search engine queries determining the user intent of web search engine queries determining the user intent of web searches is a difficult problem due to the sparse data available concerning the searcher in this paper we examine a method to determine the user intent underlying web search engine queries we qualitatively analyze samples of queries from seven transaction logs from three different web search engines containing more than five million queries from this analysis we identified characteristics of user queries based on three broad classifications of user intent the classifications of informational navigational and transactional represent the type of content destination the searcher desired as expressed by their query we implemented our classification algorithm and automatically classified a separate web search engine transaction log of over a million queries submitted by several hundred thousand users our findings show that more than of web queries are informational in nature with about each being navigational and transactional in order to validate the accuracy of our algorithm we manually coded queries and compared the classification to the results from our algorithm this comparison showed that our automatic classification has an accuracy of of the remaining of the queries the user intent is generally vague or multifaceted pointing to the need to for probabilistic classification we illustrate how knowledge of searcher intent might be used to enhance future web search engines
epci extracting potentially copyright infringement texts from the web epci extracting potentially copyright infringement texts from the web in this paper we propose a new system extracting potentially copyright infringement texts from the web called epci epci extracts them in the following way generating a set of queries based on a given copyright reserved seedtext putting every query to search engine apis gathering the search result web pages from high ranking until the similarity between the given seedtext and the search result pages becomes less than a given threshold value and merging all the gathered pages then reranking them in the order of their similarity our experimental result using seedtexts shows that epci is able to extract potentially copyright infringement web pages per a given copyright reserved seedtext with precision in average
efficient training on biased minimax probability machine for imbalanced text classification efficient training on biased minimax probability machine for imbalanced text classification the biased minimax probability machine constructs a classifier which deals with the imbalanced learning tasks in this paper we propose a second order cone programming based algorithm to train the model we outline the theoretical derivatives of the biased classification model and address the text classification tasks where negative training documents significantly outnumber the positive ones using the proposed strategy we evaluated the learning scheme in comparison with traditional solutions on three different datasets empirical results have shown that our method is more effective and robust to handle imbalanced text classification problems
electoral search using the verkiezingskijker an experience report electoral search using the verkiezingskijker an experience report the netherlands had parliamentary elections on november we built a system which helped voters to make an informed choiceamong the many participating parties one of the most importantpieces of information in the dutch election and subsequentcoalition government formation is the party program atext document with an average length of pages our systemprovides the voter with focused access to party programsenabling her to make a topicwise comparison of partiesviewpoints we complemented this type of access what do theparties promise with access to news what happens aroundthese topics and blogs what do people say about themwe describe the system including design technical details anduser statistics
exploration of query context for information retrieval exploration of query context for information retrieval a number of existing information retrieval systems propose the notion of query context to combine the knowledge of query and user into retrieval to reveal the most exact description of users information needs in this paper we interpret query context as a document consisting of sentences related to the current query this kind of query context is used to reestimate the relevance probabilities of topranked documents and then rerank topranked documents the experiments show that the proposed contextbased approach for information retrieval can greatly improved relevance of search results
firstorder focused crawling firstorder focused crawling this paper reports a new general framework of focused webcrawling based on relational subgroup discovery predicatesare used explicitly to represent the relevance clues ofthose unvisited pages in the crawl frontier and then firstorderclassification rules are induced using subgroup discoverytechnique the learned relational rules with sufficientsupport and confidence will guide the crawling process afterwardswe present the many interesting features of ourproposed firstorder focused crawler together with preliminarypromising experimental results
generative models for name disambiguation generative models for name disambiguation name ambiguity is a special case of identity uncertainty where one person can be referenced by multiple name variations in different situations or even share the same name with other people in this paper we present an efficient framework by using two novel topicbased models extended from probabilistic latent semantic analysis plsa and latent dirichlet allocation lda our models explicitly introduce a new variable for persons and learn the distribution of topics with regard to persons and words experiments indicate that our approach consistently outperforms other unsupervised methods including spectral and dbscan clustering scalability is addressed by disambiguating authors in over papers from the entire citeseer dataset
gigahash scalable minimal perfect hashing for billions of urls gigahash scalable minimal perfect hashing for billions of urls a minimal perfect function maps a static set of n keys on to the range of integers n we present a scalable high performance algorithm based on random graphs for constructing minimal perfect hash functions mphfs for a set of n keys our algorithm outputs a description of h in expected time on the evaluation of hx requires three memory accesses for any key x and the description of h takes up n bytes n bits this is the best most space efficient known result to date our approach reduces the space requirement to of the well known previous minimal perfect hashing mph scheme due to czech havas and majewski and to of a more recent algorithm due to botelho kohoyakawa and ziviani using a simple heuristic and huffman coding the space requirement is further reduced to n bytes n bits we present a high performance architecture that is easy to parallelize and scales well to very large data sets encountered in internet search applications experimental results on a one billion url url dataset obtained from live search crawl data show that the proposed algorithm a finds an mphf for one billion url urls in less than minutes and b requires only bitskey for the description of h
how naga uncoils searching with entities and relations how naga uncoils searching with entities and relations current keywordoriented search engines for the world wide web do not allow specifying the semantics of queries we address this limitation with naga a new semantic search engine naga builds on a large semantic knowledge base of binary relationships facts derived from the web naga provides a simple yet expressive query language to query this knowledge base the results are then ranked with an intuitive scoring mechanism we show the effectiveness and utility of naga by comparing its output with that of google on some interesting queries
identifying ambiguous queries in web search identifying ambiguous queries in web search it is widely believed that some queries submitted to search engines are by nature ambiguous eg java apple however few studies have investigated the questions of how many queries are ambiguous and how can we automatically identify an ambiguous query this paper deals with these issues first we construct the taxonomy of query ambiguity and ask human annotators to manually classify queries based upon it from manually labeled results we found that query ambiguity is to some extent predictable we then use a supervised learning approach to automatically classify queries as being ambiguous or not experimental results show that we can correctly identify of labeled queries with a machine learning approach finally we estimate that about of queries in a real search log are ambiguous
web page classification with heterogeneous data fusion web page classification with heterogeneous data fusion web pages are more than text and they contain much contextual andstructural information eg the title meta data the anchor textetc each of which can be seen as a data source or arepresentation due to the different dimensionality and differentrepresenting forms of these heterogeneous data sources simplyputting them together would not greatly enhance the classificationperformance we observe that via a kernel function differentdimensions and types of data sources can be represented into acommon format of kernel matrix which can be seen as a generalizedsimilarity measure between web pages in this sense a kernellearning approach is employed to fuse these heterogeneous datasources the experimental results on a collection of the odpdatabase validate the advantages of the proposed method over anysingle data source and the uniformly weighted combination ofheterogeneous data sources
learning information diffusion process on the web learning information diffusion process on the web many text documents on the web are not originally created but forwarded or copied from other source documents the phenomenon of document forwarding or transmission between various web sites is denoted as web information diffusion this paper focuses on mining information diffusion processes for specific topics on the web a novel system called lidpw is proposed to address this problem using matching learning techniques the source site and source document of each document are identified and the diffusion process composed of a series of diffusion relationships is visually presented to users the effectiveness of lidpw is validated on a real data set a preliminary user study is performed and the results show that lidpw does benefit users to monitor the information diffusion process of a specific topic and aid them to discover the diffusion start and diffusion center of the topic
medsearch a specialized search engine for medical information medsearch a specialized search engine for medical information people are thirsty for medical information existing web search engines cannot handle medical search well because they do not consider its special requirements often a medical information searcher is uncertain about his exact questions and unfamiliar with medical terminology therefore he prefers to pose long queries describing his symptoms in plain english and receive comprehensive relevant information from search results this paper presents medsearch a specialized medical web search engine to address these challenges medsearch can assist ordinary internet users to search for medical information by accepting queries of extended length providing diversified search results and suggesting related medical phrases
mining contiguous sequential patterns from web logs mining contiguous sequential patterns from web logs finding contiguous sequential patterns csp is an important problem in web usage mining in this paper we propose a new data structure updown tree for csp mining an updown tree combines suffix tree and prefix tree for efficient storage of all the sequences that contain a given item the special structure of updown tree ensures efficient detection of csps experiments show that updown tree improves csp mining in terms of both time and memory usage comparing to previous approaches
monitoring the evolution of cached content in google and msn monitoring the evolution of cached content in google and msn in this paper we describe a capturerecapture experiment conducted on googles and msns cache directories the anticipated outcome of this work was to monitor evolution rates in this these web search services as well as measure their ability to index and maintain fresh and uptodate results in their cache directories in our intentions was to also employ yahoo in our experiments however we could not estimate the refresh rate of yahoo since this search service does not publish the information when the cache version was taken while google and msn do
review spam detection review spam detection it is now a common practice for ecommerce web sites to enable their customers to write reviews of products that they have purchased such reviews provide valuable sources of information on these products they are used by potential customers to find opinions of existing users before deciding to purchase a product they are also used by product manufacturers to identify problems of their products and to find competitive intelligence information about their competitors unfortunately this importance of reviews also gives good incentive for spam which contains false positive or malicious negative opinions in this paper we make an attempt to study review spam and spam detection to the best of our knowledge there is still no reported study on this problem
multifactor clustering for a marketplace search interface multifactor clustering for a marketplace search interface search engines provide a small window to the vast repository of data they index and against which they search they try their best to return the documents that are of relevance to the user however depending on how specific the user is or how clear the user is on what heshe is searching a large number of results may be returned users struggle to manage this vast result set looking for the items of interest clustering search results is one way of alleviating this navigation pain in this paper we describe a clustering system that enables clustering search results in an online marketplace search system the online marketplace we focus on poses unique challenges in indexing and findability in this paper we describe the design and implementation of a clustering system to help cluster search results in such an environment
on ranking techniques for desktop search on ranking techniques for desktop search this paper addresses the desktop search problem by considering varioustechniques for ranking results of a search query over thefile system first basic ranking techniques which are based ona single file feature eg file name file content access date etcare considered next two learningbased ranking schemes are presented and are shown to besignificantly more effective than the basic ranking methods finallya novel ranking technique based on query selectiveness is consideredfor use during the coldstart period of the system this method isalso shown to be empirically effective even though it does notinvolve any learning
querydriven indexing for peertopeer text retrieval querydriven indexing for peertopeer text retrieval we describe a querydriven indexing framework for scalable text retrieval over structured pp networks to cope with the bandwidth consumption problem that has been identified as the major obstacle for fulltext retrieval in pp networks we truncate posting lists associated with indexing features to a constant size storing only topk ranked document references to compensate for the loss of information caused by the truncation we extend the set of indexing features with carefully chosen term sets indexing term sets are selected based on the query statistics extracted from query logs to index only such combinations that are a frequently present in user queries and b nonredundant wrt the rest of the index the distributed index is compact and efficient as it constantly evolves adapting to the current query popularity distribution moreover it is possible to tradeoff the storagebandwidth requirements with the query answering quality by tuning the indexing parameters our theoretical analysis and experimental results indicate that we can indeed achieve scalable pp text retrieval for very large document collections and deliver good retrieval performance
query topic detection for reformulation query topic detection for reformulation in this paper we show that most multiple term queries includemore than one topic and users usually reformulate their queries bytopics instead of terms in order to provide empirical evidence onusers reformulation behavior and to help search engines betterhandle the query reformulation problem we focus on detectinginternal topics in the original query and analyzing usersreformulation to those topics in this paper we utilize theinteraction information ii to measure the degree of one subquerybeing a topic based on the local search results theexperimental results on query log show that most usersreformulate query at the topical level and our proposed iibasedalgorithm is a good method to detect topics from original queries
scan a smallworld structured pp overlay for multidimensional queries scan a smallworld structured pp overlay for multidimensional queries this paper presents a structured pp overlay scan that augments can overlay with long links based on kleinbergs smallworld model in a ddimensional cartesian space the construction of long links does not require estimate of network size queries in multidimensional data space can achieve ologdnd hops by equipping each node with ologdnd long links and od short links
sring a structured non dht pp overlay supporting string range queries sring a structured non dht pp overlay supporting string range queries this paper presents sring a structured non dht pp overlay that efficiently supports exact and range queries on multiple attribute values in sring all attribute values are interpreted as strings formed by a base alphabet and are published in lexicographical order two virtual rings are built nring is built in a skiplist way for range partition and queries dring is built in a smallworld way for the construction of nring a leaveandjoin based load balancing method is used to balance range overload in the network with heterogeneous nodes
search engine retrieval of changing information search engine retrieval of changing information in this paper we analyze the web coverage of three search engines google yahoo and msn we conducted a month study collecting web content or information pages linked from australian federal and local government web pages the key feature of this domain is that new information pages are constantly added but the web pages tend to provide links only to the more recently added information pages search engines list only some of the information pages and their coverage varies from month to month metasearch engines do little to improve coverage of information pages because the problem is not the size of web coverage but the frequency with which information is updated we conclude that organizations such as governments which post important information on the web cannot rely on all relevant pages being found with conventional search engines and need to consider other strategies to ensure important information can be found
search engines and their public interfaces which apis are the most synchronized search engines and their public interfaces which apis are the most synchronized researchers of commercial search engines often collect datausing the application programming interface api or byscraping results from the web user interface wui butanecdotal evidence suggests the interfaces produce differentresults we provide the first in depth quantitative analysisof the results produced by the google msn and yahoo apiand wui interfaces after submitting a variety of queriesto the interfaces for months we found significant discrepanciesin several categories our findings suggest that theapi indexes are not older but they are probably smallerfor google and yahoo researchers may use our findings tobetter understand the differences between the interfaces andchoose the best api for their particular type of queries
spam and popularity ratings for combating link spam spam and popularity ratings for combating link spam we present a new approach for propagating spam scores in web graphs in order to combat link spam the resulting spam rating is then used for propagating popularity scores like pagerank both propagations work even in presence of censure links that represent distrust initial testing using a c prototype on small examples show more reasonable results than other published approaches
summary attributes and perceived search quality summary attributes and perceived search quality we conducted a series of experiments in which surveyed web search users answered questions about the quality of search results on the basis of the result summaries summaries shown to different groups of users were editorially constructed so that they differed in only one attribute such as length some attributes had no effect on users quality judgments while in other cases changing an attribute had a halo effect which caused seemingly unrelated dimensions of result quality to be rated higher by users
tag clouds for summarizing web search results tag clouds for summarizing web search results in this paper we describe an application pubcloud that uses tag clouds for the summarization of results from queries over the pubmed database of biomedical literature pubcloud responds to queries of this database with tag clouds generated from words extracted from the abstracts returned by the query the results of a user study comparing the pubcloud tagcloud summarization of query results with the standard result list provided by pubmed indicated that the tag cloud interface is advantageous in presenting descriptive information and in reducing user frustration but that it is less effective at the task of enabling users to discover relations between concepts
towards efficient dominant relationship exploration of the product items on the web towards efficient dominant relationship exploration of the product items on the web in recent years there has been a prevalence of search engines being employed to find useful information in the web as they efficiently explore hyperlinks between web pages which define a natural graph structure that yields a good ranking unfortunately current search engines cannot effectively rank those relational data which exists on dynamic websites supported by online databases in this study to rank such structured data ie find the best items we propose an integrated online system consisting of compressed data structure to encode the dominant relationship of the relational data efficient querying strategies and updating scheme are devised to facilitate the ranking process extensive experiments illustrate the effectiveness and efficiency of our methods as such we believe the work in this paper can be complementary to traditional search engines
understanding web search via a learning paradigm understanding web search via a learning paradigm investigating whether one can view web searching as a learning process we examined the searching characteristics of participants engaged in searching tasks we classified the searching tasks according an updated version of bloom taxonomy a six level categorization of cognitive learning results show that applying takes the most searching effort as measured by queries per session and specific topics searched per sessions the lower level categories of remembering and understanding exhibit searching characteristics similar to the higher order learning of evaluating and creating it appears that searchers rely primarily on their internal knowledge for evaluating and creating using searching primarily as fact checking and verification implications are that the commonly held notion that web searchers have simple information needs may not be correct we discuss the implications for web searching including designing interfaces to support exploration
using dgap patterns for index compression using dgap patterns for index compression sequential patterns of dgaps exist pervasively in inverted lists of web document collection indices due to the cluster property in this paper the information of dgap sequential patterns is used as a new dimension for improving inverted index compression we first detect dgap sequential patterns using a novel data structure updown tree based on the detected patterns we further substitute each pattern with its pattern id in the inverted lists that contain it the resulted inverted lists are then coded with an existing coding scheme experiments show that this approach can effectively improve the compression ratio of existing codes
utility analysis for topically biased pagerank utility analysis for topically biased pagerank pagerank is known to be an efficient metric for computing general document importance in the web while commonly used as a onesizefitsall measure the ability to produce topically biased ranks has not yet been fully explored in detail in particular it was still unclear to what granularity of topic the computation of biased page ranks makes sense in this paper we present the results of a thorough quantitative and qualitative analysis of biasing pagerank on open directory categories we show that the map quality of biased pagerank generally increases with the odp level up to a certain point thus sustaining the usage of more specialized categories to bias pagerank on in order to improve topic specific search
sliding window technique for the web log analysis sliding window technique for the web log analysis the results of the web query log analysis may be significantly shifted depending on the fraction of agents nonhuman clients which were not excluded from the log to detect and exclude agents the web log studies use threshold values for a number of requests submitted by a client during the observation period however different studies use different observation periods and a threshold assigned to one period usually incomparable with the threshold assigned to the other period we propose the uniform method equally working on the different observation periods the method bases on the sliding window technique a threshold is assigned to the sliding window rather than to the whole observation period besides we estimate the suboptimal values of the parameters of the method a window size and a threshold
a management and performance framework for semantic web servers a management and performance framework for semantic web servers the unification of semantic web query languages under the sparql standard and the development of commercialquality implementations are encouraging industries to use semantic technologies for managing information current implementations however lack the performance monitoring and management services that the industry expects in this paper we present a performance and management framework interface to a generic sparql web server we leverage existing standards for instrumentation to make the system readytomanage through existing monitoring applications and we provide a performance framework which has the distinct feature of providing measurement results through the same sparql interface used to query data eliminating the need for special interfaces
a probabilistic semantic approach for discovering web services a probabilistic semantic approach for discovering web services service discovery is one of challenging issues in serviceoriented computing currently most of the existing service discovering and matching approaches are based on keywordsbased strategy however this method is inefficient and timeconsuming in this paper we present a novel approach for discovering web services based on the current dominating mechanisms of discovering and describing web services with uddi and wsdl the proposed approach utilizes probabilistic latent semantic analysis plsa to capture semantic concepts hidden behind words in the query and advertisements in services so that services matching is expected to carry out at concept level we also present related algorithms and preliminary experiments to evaluate the effectiveness of our approach
acquiring ontological knowledge from query logs acquiring ontological knowledge from query logs we present a method for acquiring ontological knowledge using search query logs we first use query logs to identify important contexts associated with terms belonging to a semantic category we then use these contexts to harvest new words belonging to this category our evaluation on selected categories indicates that the method works very well to help harvesting terms achieving to accuracy in categorizing newly acquired terms
altering document term vectors for classification ontologies as expectations of cooccurrence altering document term vectors for classification ontologies as expectations of cooccurrence in this paper we extend the stateoftheart in utilizing background knowledge for supervised classification by exploiting the semantic relationships between terms explicated in ontologies preliminary evaluations indicate that the new approach generally improves precision and recall and reveals patterns indicating the usefulness of such background knowledge
building and managing personalized semantic portals building and managing personalized semantic portals in this paper we present a generic semantic portal semport which provides better user support with personalized views semantic navigation ontologybased search and three different kinds of semantic hyperlinks semport also supplies distributed content editingprovision in real time as a case study semport is tested on the course modules web page cmwp of the school of electronics and computer science ecs
deriving knowledge from figures for digital libraries deriving knowledge from figures for digital libraries figures in digital documents contain important information current digital libraries do not summarize and index information available within figures for document retrieval we present our system on automatic categorization of figures and extraction of data from d plots a machinelearning based method is used to categorize figures into a set of predefined types based on image features an automated algorithm is designed to extract data values from solid line curves in d plots the semantic type of figures and extracted data values from d plots can be integrated with textual information within documents to provide more effective document retrieval services for digital library users experimental evaluation has demonstrated that our system can produce results suitable for real world use
development of a semantic web based mobile local search system development of a semantic web based mobile local search system this paper described the development of a semantic web based mobile local search system it illustrates the first application of semantic web technology in mobile communication the semantic web based mobile local search system described in this paper established the ontology for fields which represents user inquiries the ontology for these fields consists of classes and individuals finally it provides a movement service for reaching the point of interest in the shortest time by downloading geographic information through the navigation service based on a local search service this system becomes very attractive and provides the fastest most optimal movement path to the user because the downloaded geographic information includes realtime traffic information
estimating the cardinality of rdf graph patterns estimating the cardinality of rdf graph patterns most rdf query languages allow for graph structure search through a conjunction of triples which is typically processed using join operations a key factor in optimizing joins is determining the join order which depends on the expected cardinality of intermediate results this work proposes a patternbased summarization framework for estimating the cardinality of rdf graph patterns we present experiments on real world and synthetic datasets which confirm the feasibility of our approach
extending webml towards semantic web extending webml towards semantic web available methodologies for developing sematic web applications do not fully exploit the whole potential deriving from interaction with ontological data sources here we introduce an extension of the webml modeling framework to fulfill most of the design requirements emerging for the new area of semantic web we generalize the development process to support semantic web applications and we introduce a set of new primitives for ontology importing and querying
image annotation by hierarchical mapping of features image annotation by hierarchical mapping of features in this paper we propose a novel approach of image annotation byconstructing a hierarchical mapping between lowlevel visualfeatures and text features utilizing the relations within and acrossboth visual features and text features moreover we propose a novelannotation strategy that maximizes both the accuracy and thediversity of the generated annotation by generalizing or specifyingthe annotation in the corresponding annotation hierarchyexperiments with scientific images from royal society ofchemistry journals show that the proposed annotation approachproduces satisfactory results at different levels of annotations
integrating web directories by learning their structures integrating web directories by learning their structures documents in the web are often organized using category trees by information providers eg cnn bbc or search engines eg google yahoo such category trees are commonly known as web directories the category tree structures from different internet content providers may be similar to some extent but are usually not exactly the same as a result it is desirable to integrate these category trees together so that web users only need to browse through a unified category tree to extract information from multiple providers in this paper we address this problem by capturing structural information of multiple category trees which are embedded with the knowledge of professional in organizing the documents our experiments with real web data show that the proposed technique is promising
learning ontologies to improve the quality of automatic web service matching learning ontologies to improve the quality of automatic web service matching this paper presents a novel technique that significantly improves the quality of semantic web service matching by automatically generating ontologies based on web service descriptions and using these ontologies to guide the mapping between web services the experimental results indicate that with our unsupervised approach we can eliminate up to of incorrect matches that are made by dictionarybased approaches
ontology engineering using volunteer labor ontology engineering using volunteer labor we describe an approach designed to reduce the costs of ontology development through the use of untrained volunteer knowledge engineers results are provided from an experiment in which volunteers were asked to judge the correctness of automatically inferred subsumption relationships in the biomedical domain the experiment indicated that volunteers can be recruited fairly easily but that their attention is difficult to hold that most do not understand the subsumption relationship without training and that incorporating learned estimates of trust into voting systems is beneficial to aggregate performance
semantic personalization of web portal contents semantic personalization of web portal contents enriching web applications with personalized data is of major interest for facilitating the user access to the published contents and therefore for guaranteeing successful user navigation we propose a conceptual model for extracting personalized recommendations based on user profiling ontological domain models and semantic reasoning the approach offers a highlevel representation of the designed application based on a domainspecific metamodel for web applications called webml
the largest scholarly semantic networkever the largest scholarly semantic networkever scholarly entities such as articles journals authors and institutions are now mostly ranked according to expert opinion and citation data the andrew w mellon foundation funded mesur project at the los alamos national laboratory is developing metrics of scholarly impact that can rank a wide range of scholarly entities on the basis of their usage the mesur project starts with the creation of a semantic network model of the scholarly community that integrates bibliographic citation and usage data collected from publishers and repositories worldwide it is estimated that this scholarly semantic network will include approximately million articles million authors journals and conferences million citations and billion usagerelated events the largest scholarly semantic network ever created the developed scholarly semantic network will then serve as a standardized platform for the definition and validation of new metrics of scholarly impact this poster describes the mesur projects data aggregation and processing techniques including the owl scholarly ontology that was developed to model the scholarly communication process
blogscope spatiotemporal analysis of the blogosphere blogscope spatiotemporal analysis of the blogosphere we present blogscope wwwblogscopenet a system for analyzing the blogosphere blogscope is an information discovery and text analysis system that offers a set of unique features such features include spatiotemporal analysis of blogs flexible navigation of the blogosphere through information bursts keyword correlations and burst synopsis as well as enhanced ranking functions for improved query answer relevance we describe the system its design and the features of the current version of blogscope
eos expertise oriented search using social networks eos expertise oriented search using social networks in this paper we present the design and implementation of our expertise oriented search system eos httpwwwarnetminernet eos is a researcher social network system it has gathered information about a halfmillion computer science researchers from the web and constructed a social network among the researchers through their coauthorship in particular the relationship in the social network information is used in both ranking experts for a given topic and searching for associations between researchers our experimental results demonstrate that the proposed methods for expert finding and association search in a social network are both more effective and efficient than the baseline methods
exploring social dynamics in online media sharing exploring social dynamics in online media sharing it is now feasible to view media at home as easily as textbased pages were viewed when the web first appeared this development has led to the emergence of media sharing and search services providing hosting indexing and access to large online media repositories many of these sharing services also have a social aspect to them this paper provides an initial analysis of the social interactions on a video sharing and search service initial results show that many users do not form social networks in the community and a very small number do not appear to contribute to the wider community however it does seem that people who use the tools available to form social connections do so often this shows some hope for the future and the possibility of leveraging off of these social networks to aid users of these services eg in searching for new media
finding community structure in megascale social networks finding community structure in megascale social networks community analysis algorithm proposed by clauset newman and moore cnm algorithm finds community structure in social networks unfortunately cnm algorithm does not scale well and its use is practically limited to networks whose sizes are up to nodes we show that this inefficiency is caused from merging communities in unbalanced manner and that a simple heuristics that attempts to merge community structures in a balanced manner can dramatically improve community structure analysis the proposed techniques are tested using data sets obtained from existing social networking service that hosts million users we have tested three three variations of the heuristics the fastest method processes a sns friendship network with million users in minutes times faster than cnm and another friendship network with million users in minutes respectively another one processes a network with nodes in minutes times faster than cnm finds community structures that has improved modularity and scales to a network with million
life is sharable mechanisms to support and sustain blogging life experience life is sharable mechanisms to support and sustain blogging life experience recent trend in the development of mobile devices wireless communications sensor technologies weblogs and peertopeer communications have prompted a new design opportunity for enhancing social interactions this paper introduces our preliminary experiences in designing a prototype utilizing the aforementioned technologies to share life experience users equipped with camera phones coupled with shortrange communication technology such as rfid can capture life experience and share it as weblogs to other people however in reality this is easier said than done the success of weblogs relies on the active participation and willingness of people to contribute to encourage active participations a ranking system agreerank is specifically developed to get them motivated
measuring credibility of users in an elearning environment measuring credibility of users in an elearning environment learning villages lv is an elearning platform for peoples online discussions and frequently citing postings of one another in this paper we propose a novel method to rank credit authors in the lv system we first propose a keacm graph to describe the article citation structure in the lv system and then we build a weighted graph model kucm graph to reveal the implicit relationship between authors hidden behind the citations among their articles furthermore we design a graphbased ranking algorithm the credit author ranking car algorithm which can be applied to rank nodes in a graph with negative edges finally we perform experimental evaluations by simulations the results of evaluations illustrate that the proposed method works pretty well on ranking the credibility of users in the lv system
modeling user behavior in recommender systems based on maximum entropy modeling user behavior in recommender systems based on maximum entropy we propose a model for user purchase behavior in online stores that provide recommendation services we model the purchase probability given recommendations for each user based on the maximum entropy principle using features that deal with recommendations and user interests the proposed model enable us to measure the effect of recommendations on user purchase behavior and the effect can be used to evaluate recommender systems we show the validity of our model using the log data of an online cartoon distribution service and measure the recommendation effects for evaluating the recommender system
parallel crawling for online social networks parallel crawling for online social networks given a huge online social network how do we retrieve information from it through crawling even better how do we improve the crawling performance by using parallel crawlers that work independent of each other in this paper we present the framework of parallel crawlers for online social networks utilizing a centralized queue to show how this works in practice we describe our implementation of the crawlers for an online auction website the crawlers work independently therefore the failing of one crawler does not affect the others at all the framework ensures that no redundant crawling would occur using the crawlers that we built we visited a total of approximately million auction users about of which were completely crawled
personalized social amp realtime collaborative search personalized social amp realtime collaborative search this paper presents adaptive web search aws a novel search technique that combines personalized social and realtime collaborative search preliminary empirical results from on a small sample suggest that an aws prototype built on wamp platform using yahoo web search api generates more relevant results and allows faster discovery of information
towards extracting flickr tag semantics towards extracting flickr tag semantics we address the problem of extracting semantics of tags short unstructured textlabels assigned to resources on the web based on each tags metadata patterns in particular we describe an approach for extracting place and event semantics for tags that are assigned to photos on flickr a popular photo sharing website supporting time and location latitudelongitude metadata the approach can be generalized to other domains where text terms can be extracted and associated with metadata patterns such as geoannotated web pages
autoperf an automated load generator and performance measurement tool for multitier software systems autoperf an automated load generator and performance measurement tool for multitier software systems we present a load generator and performance measurement tool autoperf which requires minimal input and configuration from the user and produces a comprehensive capacity analysis as well as serverside resource usage profile of a webbased distributed system in an automated fashion the tool requires only the workload and deployment description of the distributed system and automatically sets typical parameters that load generator programs need such as maximum number of users to be emulated number of users for each experiment warmup time etc the tool also does all the coordination required to generate a critical type of measure namely resource usage per transaction or per user for each software server this is a necessary input for creating a performance model of a software system
construction by linking the linkbase method construction by linking the linkbase method the success of many innovative web applications is not based on the content they produce but on how they combine and link existing content older web engineering methods lack flexibility in a sense that they rely strongly on apriori knowledge of existing content structures and do not take into account initially unknown content sources we propose the adoption of principles that are also found in componentbased software engineering to assemble highly extensible solutions from reusable artifacts the main contribution of our work is a support system consisting of a central service that manages nm relationships between arbitrary web resources and of web application components that realize navigation presentation and interaction for the linked content
image collector iii a web imagegathering system with bagofkeypoints image collector iii a web imagegathering system with bagofkeypoints we propose a new system to mine visual knowledge on the web thereare huge image data as well as text data on the web however miningimage data from the web is paid less attention than mining text datasince treating semantics of images are much more difficult in thispaper we propose introducing a latest image recognition techniquewhich is the bagofkeypoints representation into webimagegathering task by the experiments we show the proposed systemoutperforms our previous systems and google image search greatly
mirror site maintenance based on evolution associations of web directories mirror site maintenance based on evolution associations of web directories mirroring web sites is a wellknown technique commonly used in theweb community a mirror site should be updated frequently toensure that it reflects the content of the original site existingmirroring tools apply pagelevel strategies to check eachpage of a site which is inefficient and expensive in this paperwe propose a novel sitelevel mirror maintenancestrategy our approach studies the evolution of web directorystructures and mines association rules between ancestordescendantweb directories discovered rules indicate the evolutioncorrelations between web directories thus when maintaining themirror of a web site directory we can optimally skipsubdirectories which are negatively correlated with it inundergoing significant changes the preliminary experimentalresults show that our approach improves the efficiency of themirror maintenance process significantly while sacrificingslightly in keeping the freshness of the mirrors
on building graphs of documents with artificial ants on building graphs of documents with artificial ants we present an incremental algorithm for building a neighborhood graph from a set of documents this algorithm is based on a population of artificial agents that imitate the way real ants build structures with selfassembly behaviors we show that our method outperforms standard algorithms for building such neighborhood graphs up to times faster on the tested databases with equal quality and how the user may interactively explore the graph
towards a scalable search and query engine for the web towards a scalable search and query engine for the web current search engines do not fully leverage semantically rich datasets or specialise in indexing just one domainspecific dataset the search engine described in this paper uses the rdf data model to enable interactive query answering over large amounts of richly structured and interlinked data collected from many disparate sources on the web
web mashup scripting language web mashup scripting language the web mashup scripting language wmsl enables an enduser you working from his browser eg not needing any other infrastructure to quickly write mashups that integrate any two or more web services on the web the enduser accomplishes this by writing a web page that combines html metadata in the form of mapping relations and small piece of code or script the mapping relations enable not only the discovery and retrieval of the wmsl pages but also affect a new programming paradigm that abstracts many programming complexities from the script writer furthermore the wmsl web pages or scripts that disparate endusers you write can be harvested by crawlers to automatically generate the concepts needed to build lightweight ontologies containing local semantics of a web service and its data model to extend context ontologies or middle ontologies and to develop links or mappings between these ontologies this enables an opensource model of building ontologies based on the wmsl web page or scripts that end users you write
a password stretching method using user specific salts a password stretching method using user specific salts in this paper we present a password stretching method based on user specific salt our scheme takes a similar time to stretch a password as a recent password stretching algorithm but the complexity of precomputation attack increases by times and also the storage to store precomputation result increases by times over a recent password stretching algorithm
simple authentication for the web simple authentication for the web automated emailbased password reestablishment ebpr is an efficient costeffective means to deal with forgotten passwords in this technique email providers authenticate users on behalf of web sites this method works because web sites trust email providers to deliver messages to their intended recipients simple authentication for the web saw improves upon this basic approach to user authentication to create an alternative to passwordbased logins saw removes the setup and management costs of passwords at sites that accept the risks of ebpr provides single signon without a specialized identity provider thwarts all passive attacks
a kernel based structure matching for web services search a kernel based structure matching for web services search this paper describes a kernel based web services abbreviated asservice matching mechanism for service discovery and integrationthe matching mechanism tries to exploit the latent semantics by thestructure of services using textual similarity and nspectrumkernel values as features of lowlevel and midlevel we build up amodel to estimate the functional similarity between services whoseparameters are learned by a rankingsvm the experiment resultsshowed that several metrics for the retrieval of services have beenimproved by our approach
a novel collaborative filteringbased framework for personalized services in mcommerce a novel collaborative filteringbased framework for personalized services in mcommerce with the rapid growth of wireless technologies and handheld devices mcommerce is becoming a promising research area personalization is especially important to the success of mcommerce this paper proposes a novel collaborative filteringbased framework for personalized services in mcommerce the framework extends our previous works by using olap to represent the relationships among user content and context information and adopting a multidimensional collaborative filtering model to perform inference it provides a powerful and wellfounded mechanism to personalization for mcommerce it is implemented in an existing mcommerce platform and experimental results demonstrate its feasibility and correctness
crawling multiple uddi business registries crawling multiple uddi business registries as web services proliferate size and magnitude of uddi business registries ubrs are likely to increase the ability to discover web services of interest then across multiple ubrs becomes a major challenge specially when using primitive search methods provided by existing uddi apis clients do not have the time to endlessly search accessible ubrs for finding appropriate services particularly when operating via mobile devices although there have been numerous standards that have the potential of enhancing the discovery of web services searching for relevant web services across multiple ubrs raises a number of concerns such as performance efficiency reliability and most importantly quality of returned results finding services of interest should be time effective and highly productive this paper addresses issues relating to the efficient access and discovery of web services across multiple ubrs and introduces a novel exploration engine the web service crawler engine wsce wsce is capable of crawling multiple ubrs and enables for the establishment of a centralized web services repository that can be used for discovering web services much more efficiently the paper presents experimental validation results and analysis of the proposed ideas
discovering the best web service discovering the best web service major research challenges in discovering web services include provisioning of services across multiple or heterogeneous registries differentiating between services that share similar functionalities improving endtoend quality of service qos and enabling clients to customize the discovery process proliferation and interoperability of this multitude of web services have lead to the emergence of new standards on how services can be published discovered or used ie uddi wsdl soap such standards can potentially provide many of these features and much more however there are technical challenges associated with existing standards one of these challenges is the clients ability to control the discovery process across accessible service registries for finding services of interest this work proposes a solution to this problem and introduces the web service relevancy function wsrf used for measuring the relevancy ranking of a particular web service based on qos metrics and client preferences we present experimental validation results and analysis of the presented ideas
mobile shopping assistant integration of mobile applications and web services mobile shopping assistant integration of mobile applications and web services the goal of this poster is to describe our implementation of a newarchitecture enabling efficient integration between mobile phoneapplications and web services using this architecture we haveimplemented a mobile shopping assistant described further in orderto build this architecture we designed an innovative xmlcompression mechanism to facilitate data exchange between mobilephones and web services we also designed a smart connection managerto control asynchronous communication for all possible channels of amobile phone in addition we used diverse input modes in order toextend users access to web services
on automated composition for web services on automated composition for web services the main objective of composing web services is to identify usable web services through discovery and to orchestrate or assemble selected services according to the goal specification we formulate and study a framework to compose web services through discovery and orchestration for a given goal service composition algorithms without or with a goal service invocation request are developed two strategies are developed to tighten the goal service two notions of completeness are defined to measure the ability of how thorough the algorithms can find a composition
providing session management as core business service providing session management as core business service it is extremely hard for a global organization with services over multiple channels to capture a consistent and unified view of its data services and interactions while soa and web services are addressing integration and interoperability problems it is painful for an operational organization with legacy systems to quickly switch to servicebased methods we need methods to combine advantages of traditional ie web desktop or mobile application development environments and servicebased deployments
towards automating regression test selection for web services towards automating regression test selection for web services this paper reports a safe regression test selection rts approach that is designed for verifying web services in an endtoend manner the safe rts technique has been integrated into a systematic method that monitors distributed code modifications and automates the rts and rt processes
towards environment generated media objectparticipationtype weblog in home sensor network towards environment generated media objectparticipationtype weblog in home sensor network the environment generated media egm are defined here as being generated from a massive amount of andor incomprehensible environmental data by compressing them into averages or representative values andor by converting them into such userfriendly media as text figurescharts and animationsas an application of egman objectparticipationtype weblog is introduced whereanthropomorphic indoor objects with sensor nodes post weblog entries and comments about what happened to them in a sensor networked environment
towards service pool based approach for services discovery and subscription towards service pool based approach for services discovery and subscription there are many function identical web services in the internet or some largescale organizations they provide consumers with more choices according to their personalized qos requirements however in current web service discovery and subscription consumers pay too much time on manually selection and cannot easily benefit from the wide qos spectrum brought by the proliferating services in this paper we propose a qosaware discovery and subscription approach to free consumers from timeconsuming human computer interactions while helping them negotiate qos with multiple service providers the core of this approach is the service pool which is a virtual service grouping function identical services together and dispatching consumer requests to the proper service in terms of qos requirements based on our previous work on the service pool this paper does two main contributions one is we investigate the pool construction mechanisms by the similarity retrieval and formalize the representation of the service pool another is we propose a formalized qos model from consumers perspective and design a qosaware discovery algorithm to select the most adequate provider from the service pool according to consumers qos requirements
modeling anchor text and classifying queries to enhance web document retrieval modeling anchor text and classifying queries to enhance web document retrieval several types of queries are widely used on the world wide web and the expected retrieval method can vary depending on the query type we propose a method for classifying queries into informational and navigational types because terms in navigational queries often appear in anchor text for links to other pages we analyze the distribution of query terms in anchor texts on the web for query classification purposes while contentbased retrieval is effective for informational queries anchorbased retrieval is effective for navigational queries our retrieval system combines the results obtained with the contentbased and anchorbased retrieval methods in which the weight for each retrieval result is determined automatically depending on the result of the query classification we also propose a method for improving anchorbased retrieval our retrieval method which computes the probability that a document is retrieved in response to the given query identifies synonyms of query terms in the anchor texts on the web and uses these synonyms for smoothing purposes in the probability estimation we use the ntcir test collections and show the effectiveness of individual methods and the entire web retrieval system experimentally
unsupervised query segmentation using generative language models and wikipedia unsupervised query segmentation using generative language models and wikipedia in this paper we propose a novel unsupervised approach to query segmentation an important task in web search we use a generative query model to recover a querys underlying concepts that compose its original segmented form the models parameters are estimated using an expectationmaximization em algorithm optimizing the minimum description length objective function on a partial corpus that is specific to the query to augment this unsupervised learning we incorporate evidence from wikipediaexperiments show that our approach dramatically improves performance over the traditional approach that is based on mutual information and produces comparable results with a supervised method in particular the basic generative language model contributes a improvement over the mutual information based method measured by segment f on the intersection test set em optimization further improves the performance by additional knowledge from wikipedia provides another improvement of adding up to a total of improvement from to 
spatial variation in search engine queries spatial variation in search engine queries local aspects of web search associating web content andqueries with geography is a topic of growing interest however the underlying question of how spatial variationis manifested in search queries is still not well understoodhere we develop a probabilistic framework for quantifyingsuch spatial variation on complete yahoo query logs we find that our modelis able to localize large classes of queries to within a few milesof their natural centers based only on the distributionof activity for the queryour model provides not only an estimate of a querys geographiccenter but also a measure of its spatial dispersion indicatingwhether it has highly local interest or broader regional or national appealwe also show how variations on our model can track geographically shiftingtopics over time annotate a map with each locations distinctivequeries and delineate the spheres of influencefor competing queries in the same general domain
mining indexing and searching for textual chemical molecule information on the web mining indexing and searching for textual chemical molecule information on the web current search engines do not support user searches for chemical entities chemical names and formulae beyond simple keyword searches usually a chemical molecule can be represented in multiple textual ways a simple keyword search would retrieve only the exact match and not the others we show how to build a search engine that enr symbol in this paper we investigate whathappens when both searchers and authors are dynamicallychoosing terms to match the other side with each side trying toanticipate the other does a terminological convention everemerge or do searchers and providers continue to miss potentialpartners through mismatch of terms we use a gametheoreticsetup to frame questions and learning theory to make predictionsabout whether and which term will emerge as a convention
feature weighting in content based recommendation system using social network analysis feature weighting in content based recommendation system using social network analysis we propose a hybridization of collaborative filtering and content based recommendation system attributes used for content based recommendations are assigned weights depending on their importance to users the weight values are estimated from a set of linear regression equations obtained from a social network graph which captures human judgment about similarity of items 
king functions are introduced for chemical name searches experiments show that our approaches to chemical entity tagging perform well furthermore we show that index pruning can reduce the index size and query time without changing the returned ranked results significantly finally experiments show that our approaches outperform traditional methods for document search with ambiguous chemical terms
valuedriven design for infosuasive web applications valuedriven design for infosuasive web applications an infosuasive web application is mainly intended to be at the same time informative and persuasive ie it aims at supporting knowledge needs and it has also the declared or not declared goal of influencing users opinions attitudes and behaviors most web applications in fact are infosuasive except those whose aim is mainly operational in this paper we investigate the complex set of elements that informs the very early design of infosuasive web applications we propose a conceptual framework aimed at supporting the actors involved in this process to integrate their different viewpoints to organize the variety of issues that need to be analyzed to find a direction in the numerous design options and to represent the results of this activity in an effective wayour approach is valuedriven since it is centered around the concept of communication value regarded as a vehicle to fulfill communication goals on specific communication targets we place the analysis of these aspects in the wider context of web requirements analysis highlighting their relationships with business values analysis and user needs analysis we pinpoint how values and communication goals impact on various design dimensions of infosuasive web application contents information architecture interaction operations and layout our approach is multidisciplinary and was inspired to goalbased and valuebased requirements engineering often used in web engineering to brand design often used in marketing and to valuecentered design frameworksas proposed by the hci community a case study exemplifies our methodological proposal discussing a large project in which we are currently involved
organizing and sharing distributed personal webservice data organizing and sharing distributed personal webservice data the migration from desktop applications to webbased services isscattering personal data across a myriad of web sites such asgoogle flickr youtube and amazon s this dispersal poses newchallenges for users making it more difficult for them to organize search and archive their data much of which is nowhosted by web sites create heterogeneous multiwebserviceobject collections and share them in a protected way and manipulate their data with standard applications or scriptsin this paper we show that a webservice interfacesupporting standardized naming protection and objectaccess servicescan solve these problems and can greatly simplify the creation of anew generation of objectmanagement services for the web we describethe implementation of menagerie a proofofconcept prototype thatprovides these services for webbased applications at a high levelmenagerie creates an integrated file and object system fromheterogeneous personal webservice objects dispersed across theinternet we present several objectmanagement applications wedeveloped on menagerie to show the practicality and benefits of ourapproach
structured objects in owl representation and reasoning structured objects in owl representation and reasoning applications of semantic technologies often require therepresentation of and reasoning with structured objectsthat is objects composed of parts connected in complex ways although owl is a general and powerful language its class descriptions and axioms cannot be used to describe arbitrarily connected structures an owl representation of structured objects can thus be underconstrained which reduces the inferences that can be drawn and causes performance problems in reasoning to address these problems we extend owl with description graphs which allow for the description of structured objects in a simple and precise way to represent conditional aspects of the domain we also allow for swrllike rules over description graphs based on an observation about the nature of structured objects we ensure decidability of our formalism we also present a hypertableaubased decision procedure which we implemented in the hermit reasoner to evaluate its performance we have extracted description graphs from the galen and fma ontologies classified them successfully and even detected a modeling error in galen
computing minimum cost diagnoses to repair populated dlbased ontologies computing minimum cost diagnoses to repair populated dlbased ontologies ontology population is prone to cause inconsistency because the populating process is imprecise or the populated data may conflict with the original data by assuming that the intensional part of the populated dlbased ontology is fixed and each removable abox assertion is given a removal cost we repair the ontology by deleting a subset of removable abox assertions in which the sum of removal costs is minimum we call such subset a minimum cost diagnosis we show that unless pnp the problem of finding a minimum cost diagnosis for a dllite ontology is insolvable in ptime wrt data complexity in spite of that we present a feasible computational method for more general ie shiq ontologies it transforms a shiq ontology to a set of disjoint propositional programs thus reducing the original problem into a set of independent subproblems each such subproblem computes an optimal model and is solvable in logarithmic calls to a sat solver experimental results show that the method can handle moderately complex ontologies with over thousands of abox assertions where all abox assertions can be assumed removable
scalable querying services over fuzzy ontologies scalable querying services over fuzzy ontologies fuzzy ontologies are envisioned to be useful in the semantic web existing fuzzy ontology reasoners are not scalableenough to handle the scale of data that the web providesin this paper we propose a framework of fuzzy query languages for fuzzy ontologies and present query answeringalgorithms for these query languages over fuzzy dllite ontologies moreover this paper reports on implementationof our approach in the fuzzy dllite query engine in theontosearch system and preliminary but encouragingbenchmarking results to the best of our knowledge this isthe first ever scalable query engine for fuzzy ontologies
statistical analysis of the social network and discussion threads in slashdot statistical analysis of the social network and discussion threads in slashdot we analyze the social network emerging from the user comment activity on the website slashdot the network presentscommon features of traditional social networks such as a giant component small average path length and high clustering but differs from them showing moderate reciprocity andneutral assortativity by degree using kolmogorovsmirnovstatistical tests we show that the degree distributions arebetter explained by lognormal instead of powerlaw distributions we also study the structure of discussion threadsusing an intuitive radial tree representation threads showstrong heterogeneity and selfsimilarity throughout the different nesting levels of a conversation we use these resultsto propose a simple measure to evaluate the degree of controversy provoked by a post
yes there is a correlation from social networks to personal behavior on the web yes there is a correlation from social networks to personal behavior on the web characterizing the relationship that exists between a persons social group and hisher personal behavior has been a long standing goal of social network analysts in this paper we apply data mining techniques to study this relationship for a population of over million people by turning to online sources of data the analysis reveals that people who chat with each other using instant messagingare more likely to share interests their web searches arethe same or topically similar the more time they spend talking the stronger this relationship is people who chat with each other are also more likely to share other personal characteristics such as their age and location and they are likely to be of opposite gender similar findings hold for people who do not necessarily talk to each other but do have a friend in common our analysis is based on a welldefined mathematical formulation of the problem and isthe largest such study we are aware of
knowledge sharing and yahoo answers everyone knows something knowledge sharing and yahoo answers everyone knows something yahoo answers ya is a large and diverse questionanswer forum acting not only as a medium for sharing technical knowledge but as a place where one can seek advice gather opinions and satisfy ones curiosity about a countless number of things in this paper we seek to understand yas knowledge sharing and activity we analyze the forum categories and cluster them according to content characteristics and patterns of interaction among the users while interactions in some categories resemble expertise sharing forums others incorporate discussion everyday advice and support with such a diversity of categories in which one can participate we find that some users focus narrowly on specific topics while others participate across categories this not only allows us to map related categories but to characterize the entropy of the users interests we find that lower entropy correlates with receiving higher answer ratings but only for categories where factual expertise is primarily sought after we combine both user attributes and answer characteristics to predict within a given category whether a particular answer will be chosen as the best answer by the asker
querysets using implicit feedback and query patterns to organize web documents querysets using implicit feedback and query patterns to organize web documents in this paper we present a new document representation model based on implicit user feedback obtained from search engine queries the main objective of this model is to achieve better results in nonsupervised tasks such as clustering and labeling through the incorporation of usagedata obtained from search engine queries this type of model allows us to discover the motivations of users when visiting a certain document the terms used in queries can provide a better choice of features from the users point of view for summarizing the web pages that were clicked from these queries in this work we extend and formalize as query model an existing but not very well known idea of query view for document representation furthermore we create a novel model based on frequent query patterns called the queryset model our evaluation shows that both querybased models outperform the vectorspace model when used for clustering and labeling documents in a website in our experiments the queryset model reduces by more than the number of features needed to represent a set of documents and improves by over the quality of the results we believe that this can be explained because our model chooses better features and provides more accurate labels according to the users expectations
mining the search trails of surfing crowds identifying relevant websites from user activity mining the search trails of surfing crowds identifying relevant websites from user activity the paper proposes identifying relevant information sources from the history of combined searching and browsing behavior of many web users while it has been previously shown that user interactions with search engines can be employed to improve document ranking browsing behavior that occurs beyond search result pages has been largely overlooked in prior work the paper demonstrates that users postsearch browsing activity strongly reflects implicit endorsement of visited pages which allows estimating topical relevance of web resources by mining largescale datasets of search trails we present heuristic and probabilistic algorithms that rely on such datasets for suggesting authoritative websites for search queries experimental evaluation shows that exploiting complete postsearch browsing trails outperforms alternatives in isolation eg clickthrough logs and yields accuracy improvements when employed as a feature in learning to rank for web search
using the wisdom of the crowds for keyword generation using the wisdom of the crowds for keyword generation in the sponsored search model search engines are paid by businesses that areinterested in displaying ads for their site alongside the search resultsbusinesses bid for keywords and their ad is displayed when thekeyword is queried to the search engine an important problem in this processis emphkeyword generation given a business that is interested in launchinga campaign suggest keywords that are related to that campaign we address thisproblem by making use of the query logs of the search engine we identifyqueries related to a campaign by exploiting the associations between queriesand urls as they are captured by the users clicks these queries form goodkeyword suggestions since they capture the wisdom of the crowd as to whatis related to a site we formulate the problem as a semisupervised learningproblem and propose algorithms within the markov random field model weperform experiments with real query logs and we demonstrate that ouralgorithms scale to large query logs and produce meaningful results
learning deterministic regular expressions for the inference of schemas from xml data learning deterministic regular expressions for the inference of schemas from xml data inferring an appropriate dtd or xml schema definition xsd for a given collection of xml documents essentially reduces to learning emphdeterministic regular expressions from sets of positive example words unfortunately there is no algorithm capable of learning the complete class of deterministic regular expressions from positive examples only as we will show the regular expressions occurring in practical dtds and xsds however are such that every alphabet symbol occurs only a small number of times as such in practice it suffices to learn the subclass of regular expressions in which each alphabet symbol occurs at most k times for some small k we refer to such expressions as koccurrence regular expressions kores for short motivated by this observation we provide a probabilistic algorithm that learns kores for increasing values of k and selects the one that best describes the sample based on a minimum description length argumentthe effectiveness of the method is empirically validated both on real world and synthetic data furthermore the method is shown to be conservative over the simpler classes of expressions considered in previous work
efficient evaluation of generalized path pattern queries on xml data efficient evaluation of generalized path pattern queries on xml data finding the occurrences of structural patterns in xml data is a key operation in xml query processing existing algorithms for this operation focus almost exclusively on pathpatterns or treepatterns requirements in flexible querying of xml data have motivated recently the introduction of query languages that allow a partial specification of pathpatterns in a query in this paper we focus on the efficient evaluation of partial path queries a generalization of path pattern queries our approach explicitly deals with repeated labels that is multiple occurrences of the same label in a query we show that partial path queries can be represented as rooted dags for which a topological ordering of the nodes exists we present three algorithms for the efficient evaluation of these queries under the indexed streaming evaluation model the first one exploits a structural summary of data to generate a set of pathpatterns that together are equivalent to a partial path query to evaluate these pathpatterns we extend pathstack so that it can work on pathpatterns with repeated labels the second one extracts a spanning tree from the query dag uses a stackbased algorithm to find the matches of the roottoleaf paths in the tree and mergejoins the matches to compute the answer finally the third one exploits multiple pointers of stack entries and a topological ordering of the query dag to apply a stackbased holistic technique an analysis of the algorithms and extensive experimental evaluation shows that the holistic algorithm outperforms the other ones
on incremental maintenance of hop labeling of graphs on incremental maintenance of hop labeling of graphs recent interests on xml semantic web and web ontology among other topics have sparked a renewed interest on graphstructured databases a fundamental query on graphs is the reachability test of nodes recently it hop labeling has been proposed to index large collections of xml andor graphs for efficient reachability tests however there has been few work on updates of it hop labeling this is compounded by the fact that web data changes over time in response to these this paper studies the incremental maintenance of it hop labeling we identify the main reason for the inefficiency of updates of existing it hop labels we propose two updatable it hop labelings hybrids of it hop labeling and their incremental maintenance algorithms the proposed it hop labeling is derived from graph connectivities as opposed to sc set cover which is used by em all previous work our experimental evaluation illustrates the space efficiency and update performance of various kinds of it hop labeling the main conclusion is that there is a natural way to spare some index size for update performance in it hop labeling
externalities in online advertising externalities in online advertising most models for online advertising assume that an advertisers valuefrom winning an ad auction which depends on the clickthrough rate orconversion rate of the advertisement is independent of otheradvertisements served alongside it in the same sessionthis ignores an important em externality effect as the advertisingaudience has a limited attention span a highquality ad on a page candetract attention from other ads on the same pagethat is the utility to a winner in such an auction also depends onthe set of other winnersin this paper we introduce the problem of modeling externalities inonline advertising and study the winner determination problem inthese models our models are based on choice modelson the audience sidewe show that in the most general case the winner determinationproblem is hard even to approximate however we give an approximation algorithm for this problem with anapproximation factor that is logarithmic in the ratio of the maximumto the minimum bid furthermore we show that there are someinteresting special cases such as the case where the audiencepreferences are single peaked where the problem can be solved exactlyin polynomial time for all these algorithms we prove that thewinner determination algorithm can be combined with vcgstyle paymentsto yield truthful mechanisms
a combinatorial allocation mechanism with penalties for banner advertising a combinatorial allocation mechanism with penalties for banner advertising most current banner advertising is sold through negotiation thereby incurring large transaction costs and possibly suboptimal allocations we propose a new automated system for selling banner advertising in this system each advertiser specifies a collection of host webpages which are relevant to his product a desired total quantity ofimpressions on these pages and a maximum perimpression price the system selects a subset of advertisers as em winners and maps each winner to a set of impressions on pages within his desired collection the distinguishing feature of our system as opposed to current combinatorial allocation mechanisms is that mimicking the current negotiation system we guarantee that winners receive at least as many advertising opportunities as they requested or else receive ample compensation in the form of a monetary payment by the host such guarantees are essential in markets like banner advertising where a major goal of the advertising campaign is developing brand recognitionas we show the problem of selecting a feasible subset of advertisers with maximum total value is inapproximable we thus present two greedy heuristics and discuss theoretical techniques to measure their performances our first algorithm iteratively selects advertisers and corresponding sets of impressions which contribute maximum marginal perimpression profit to the current solution we prove a bicriteria approximation for this algorithm showing that it generates approximately as much value as the optimum algorithm on a slightly harder problem however his algorithm might perform poorly on instances in which the value of the optimum solution is quite large a clearly undesirable failure mode hence we present an adaptive greedy algorithm which again iteratively selects advertisers with maximum marginal perimpression profit but additionally reassigns impressions at each iteration for this algorithm we prove a structural approximation result a newly defined framework for evaluating heuristics we thereby prove that this algorithm has a better performance guarantee than the simple greedy algorithm
dynamic costperaction mechanisms and applications to online advertising dynamic costperaction mechanisms and applications to online advertising we study the costperaction or costperacquisition cpa charging scheme in online advertising in this scheme instead of paying per click the advertisers pay only when a user takes a specific actioneg fills out a form or completes a transaction on their websiteswe focus on designing efficient and incentive compatible mechanisms that use this charging scheme we describe a mechanism based on a samplingbased learning algorithm that under suitable assumptions is asymptotically individually rational asymptotically bayesian incentive compatible and asymptotically exante efficientin particular we demonstrate our mechanism for the case where the utility functions of the advertisers are independent and identicallydistributed random variables as well as the case where they evolve like independent reflected brownian motions
genealogical trees on the web a search engine user perspective genealogical trees on the web a search engine user perspective this paper presents an extensive study about the evolution of textual content on the web which shows how some new pages are created from scratch while others are created using already existing content we show that a significant fraction of the web is a byproduct of the latter case we introduce the concept of web genealogical tree in which every page in a web snapshot is classified into a component we study in detail these components characterizing the copies and identifying the relation between a source of content and a search engine by comparing page relevancemeasures documents returned by real queries performed in the past and clickthrough data we observe that sources of copies are more frequently returned by queries and more clicked than other documents
a graphtheoretic approach to webpage segmentation a graphtheoretic approach to webpage segmentation we consider the problem of segmenting a webpage into visually andsemantically cohesive pieces our approach is based on formulating anappropriate optimization problem on weighted graphs where the weightscapture if two nodes in the dom tree should be placed together orapart in the segmentation we present a learning framework to learnthese weights from manually labeled data in a principled manner ourwork is a significant departure from previous heuristic and rulebasedsolutions to the segmentation problem the results of our empiricalanalysis bring out interesting aspects of our framework includingvariants of the optimization problem and the role of learning
performance of compressed inverted list caching in search engines performance of compressed inverted list caching in search engines due to the rapid growth in the size of the web web search engines are facingenormous performance challenges the larger engines in particular have to beable to process tens of thousands of queries per second on tens of billionsof documents making query throughput a critical issue to satisfy this heavyworkload search engines use a variety of performance optimizations includingindex compression caching and early terminationwe focus on two techniques inverted index compression and index cachingwhich play a crucial rule in web search engines as well as otherhighperformance information retrieval systems we perform a comparison andevaluation of several inverted list compression algorithms including newvariants of existing algorithms that have not been studied before we thenevaluate different inverted list caching policies on large query traces andfinally study the possible performance benefits of combining compression andcaching the overall goal of this paper is to provide an updated discussionand evaluation of these two techniques and to show how to select the bestset of approaches and settings depending on parameter such as disk speedand main memory cache size
personalized web exploration with task models personalized web exploration with task models personalized web search has emerged as one of the hottest topicsfor both the web industry and academic researchers however themajority of studies on personalized search focused on a rathersimple type of search which leaves an important research topic the personalization in exploratory searches as an understudiedarea in this paper we present a study of personalization in taskbasedinformation exploration using a system called tasksievetasksieve is a web search system that utilizes a relevance feedbackbased profile called a task model for personalization itsinnovations include flexible and user controlled integration ofqueries and task models taskinfused text snippet generation andonscreen visualization of task models through an empirical studyusing human subjects conducting taskbased exploration searcheswe demonstrate that tasksieve pushes significantly more relevantdocuments to the top of search result lists as compared to atraditional search system tasksieve helps users select significantlymore accurate information for their tasks allows the users to do sowith higher productivity and is viewed more favorably by subjectsunder several usability related characteristics
validating the use and role of visual elements of web pages in navigation with an eyetracking study validating the use and role of visual elements of web pages in navigation with an eyetracking study this paper presents an eyetracking study that examines how people use the visual elements of web pages to complete certain tasks whilst these elements are available to play their role in these tasks for sighted users it is not the case for visually disabled users this lack of access to some visual elements of a page means that visually disabled users are hindered in accomplishing these tasks our previous work has introduced a framework that identifies these elements and then reengineers web pages such that these elements can play their intended roles in an audio as well as visual presentation to further improve our understanding of how these elements are used and to validate our framework we track the eye movements of sighted users performing a number of different tasks the resulting gaze data show that there is a strong relationship between the aspects of a page that receive visual attention and the objects identified by our framework the study also shows some limitations as well as yielding information to address these shortcomings perhaps the most important result is the support provided for a particular kind of object called a way edgethe visual construct used to group content into sections there is a significant effect of way edges on the distribution of attention across tasks this is a result that not only provides strong evidence for the utility of reengineering but also has consequences for our understanding of how people allocate attention to different parts of a page we speculate that the phenomenon of banner blindness owes as much to way edges as it does to colour and font size
improving relevance judgment of web search results with image excerpts improving relevance judgment of web search results with image excerpts current web search engines return result pages containing mostly text summary even though the matched web pages may contain informative pictures a text excerpt ie snippet is generated by selecting keywords around the matched query terms for each returned page to provide context for users relevance judgment however in many scenarios we found that the pictures in web pages if selected properly could be added into search result pages and provide richer contextual description because a picture is worth a thousand words such new summary is named as image excerpts by well designed user study we demonstrate image excerpts can help users make much quicker relevance judgment of search results for a wide range of query types to implement this idea we propose a practicable approach to automatically generate image excerpts in the result pages by considering the dominance of each picture in each web page and the relevance of the picture to the query we also outline an efficient way to incorporate image excerpts in web search engines web search engines can adopt our approach by slightly modifying their index and inserting a few low cost operations in their workflow our experiments on a large web dataset indicate the performance of the proposed approach is very promising
keysurf a character controlled browser for people with physical disabilities keysurf a character controlled browser for people with physical disabilities for many users with a physical or motor disability using a computer mouse or other pointing device to navigate the web is cumbersome or impossible due to problems with pointing accuracy at the same time web accessibility using a keyboard in major browsers is rudimentary requiring many key presses to select links or other elements we introduce keysurf a character controlled web navigation system which addresses this situation by presenting an interface which allows a user to activate any web page element with only two or three keystrokes through an implementation of a usercentric incremental search algorithm elements are matched according to user expectation as characters are entered into the interface we show how our interface can be integrated with a speech recognition input as well as with specialized onscreen keyboards for people with disabilities using the users browsing history we improve the efficiency of the selection process and find potentially interesting page links for the user within the current web page we present the results from a pilot study evaluating the performance of various components of our system
optimal marketing strategies over social networks optimal marketing strategies over social networks we discuss the use of social networks in implementing viralmarketing strategies while influence maximization hasbeen studied in this context see chapter of westudy revenue maximization arguably a more natural objectivein our model a buyers decision to buy an item isinfluenced by the set of other buyers that own the item andthe price at which the item is offeredbr br we focus on algorithmic question of finding revenue maximizingmarketing strategies when the buyers are completelysymmetric we can find the optimal marketing strategyin polynomial time in the general case motivated byhardness results we investigate approximation algorithmsfor this problem we identify a family of strategies calledinfluenceandexploit strategies that are based on the followingidea initially influence the population by giving theitem for free to carefully a chosen set of buyers then extractrevenue from the remaining buyers using a greedy pricingstrategy we first argue why such strategies are reasonableand then show how to use recently developed setfunctionmaximization techniques to find the right set of buyers toinfluence
trustbased recommendation systems an axiomatic approach trustbased recommendation systems an axiomatic approach highquality personalized recommendations are a key feature in many onlinesystems since these systems often have explicit knowledge of socialnetwork structures the recommendations may incorporate this informationthis paper focuses on networks that represent trust and recommendationsystems that incorporate these trust relationships the goal of a trustbasedrecommendation system is to generate personalized recommendations by aggregating the opinionsof other users in the trust networkin analogy to prior work on voting and ranking systems we use the axiomaticapproach from the theory of social choice we develop a set of five naturalaxioms that a trustbased recommendation system might be expected to satisfythen we show that no system can simultaneously satisfy all the axiomshowever for any subset of four of the five axioms we exhibit arecommendation system that satisfies those axioms next we consider various ways ofweakening the axioms one of which leads to a unique recommendation systembased on random walks we consider other recommendation systems includingsystems based on personalized pagerank majority of majorities and minimumcuts and search for alternative axiomatizations that uniquely characterizethese systemsfinally we determine which of these systems are incentivecompatible meaning that groups of agents interested inmanipulating recommendations can not induce others to share their opinion bylying about their votes or modifying their trust linksthis is an important property for systems deployed ina monetized environment 
secure or insure a gametheoretic analysis of information security games secure or insure a gametheoretic analysis of information security games despite general awareness of the importance of keeping ones system secure and widespread availability of consumer security technologies actual investment in security remains highly variable across the internet population allowing attacks such as distributed denialofservice ddos and spam distribution to continue unabated by modeling security investment decisionmaking in established eg weakestlink bestshot and novel games eg weakesttarget and allowing expenditures in selfprotection versus selfinsurance technologies we can examine how incentives may shift between investment in a public good protection and a private good insurance subject to factors such as network size type of attack loss probability loss magnitude and cost of technology we can also characterize nash equilibria and social optima for different classes of attacks and defenses in the weakesttarget game an interesting result is that for almost all parameter settings more effort is exerted at nash equilibrium than at the social optimum we may attribute this to the strategic uncertainty of players seeking to selfprotect at just slightly above the lowest protection level
tagbased social interest discovery tagbased social interest discovery the success and popularity of social network systems suchas delicious facebook myspace and youtube have generatedmany interesting and challenging problems to the researchcommunity among others discovering social interestsshared by groups of users is very important because ithelps to connect people with common interests and encouragespeople to contribute and share more contents themain challenge to solving this problem comes from the difficulty of detecting and representing the interest of the usersthe existing approaches are all based on the online connectionsof users and so unable to identify the common interestof users who have no online connectionsin this paper we propose a novel social interest discoveryapproach based on usergenerated tags our approachis motivated by the key observation that in a social networkhuman users tend to use descriptive tags to annotatethe contents that they are interested in our analysis ona large amount of realworld traces reveals that in generalusergenerated tags are consistent with the web content theyare attached to while more concise and closer to the understandingand judgments of human users about the contentthus patterns of frequent cooccurrences of user tags canbe used to characterize and capture topics of user interestswe have developed an internet social interest discovery systemisid to discover the common user interests and clusterusers and their saved urls by different interest topics ourevaluation shows that isid can effectively cluster similardocuments by interest topics and discover user communitieswith common interests no matter if they have any onlineconnections
facetnet a framework for analyzing communities and their evolutions in dynamic networks facetnet a framework for analyzing communities and their evolutions in dynamic networks we discover communities from social network data and analyze the community evolution these communities are inherent characteristics of human interaction in online social networks as well as paper citation networks also communities may evolve over time due to changes to individuals roles and social status in the network as well as changes to individuals research interests we present an innovative algorithm that deviates from the traditional twostep approach to analyze community evolutions in the traditional approach communities are first detected for each time slice and then compared to determine correspondences we argue that this approach is inappropriate in applications with noisy data in this paper we propose facetnet for analyzing communities and their evolutions through a robust unified process in this novel framework communities not only generate evolutions they also are regularized by the temporal smoothness of evolutions as a result this framework will discover communities that jointly maximize the fit to the observed data and the temporal evolution our approach relies on formulating the problem in terms of nonnegative matrix factorization where communities and their evolutions are factorized in a unified way then we develop an iterative algorithm with proven low time complexity which is guaranteed to converge to an optimal solution we perform extensive experimental studies on both synthetic datasets and real datasets to demonstrate that our method discovers meaningful communities and provides additional insights not directly obtainable from traditional methods
statistical properties of community structure in large social and information networks statistical properties of community structure in large social and information networks a large body of work has been devoted to identifying community structure innetworks a community is often though of as a set of nodes that has moreconnections between its members than to the remainder of the network in thispaper we characterize as a function of size the statistical and structuralproperties of such sets of nodes we define the network communityprofile plot which characterizes the best possiblecommunityaccording to the conductance measureover a wide range of sizescales and we study over large sparse realworld networks taken from awide range of application domains our results suggest a significantly morerefined picture of community structure in large realworld networks than hasbeen appreciated previouslyour most striking finding is thatin nearly every network dataset we examined we observe tight but almosttrivial communities at very small scales and at larger size scales the bestpossible communities gradually blend in with the rest of the network andthus become less communitylike this behavior is not explained even ata qualitative level by any of the commonlyused network generation modelsmoreover this behavior is exactly the opposite of what one would expectbased on experience with and intuition from expander graphs from graphs thatare wellembeddable in a lowdimensional structure and from small socialnetworks that have served as testbeds of community detection algorithms wehave found however that a generative model in which new edges are addedvia an iterative forest fire burning process is able to produce graphsexhibiting a network community structure similar to our observations
ranking refinement and its application to information retrieval ranking refinement and its application to information retrieval we consider the problem of ranking renement ie toimprove the accuracy of an existing ranking function witha small set of labeled instances we are particularly interested in learning a better ranking function using two complementary sources of information ranking information givenby the existing ranking function ie the base ranker andthat obtained from users feedbacks this problem is veryimportant in information retrieval where feedbacks are gradually collected the key challenge in combining the twosources of information arises from the fact that the ranking information presented by the base ranker tends to beimperfect and the ranking information obtained from usersfeedbacks tends to be noisy we present a novel boostingalgorithm for ranking renement that can eectively leverage the uses of the two sources of information our empirical study shows that the proposed algorithm is eective forranking renement and furthermore it signicantly outperforms the baseline algorithms that incorporate the outputsfrom the base ranker as an additional feature
learning to rank relational objects and its application to web search learning to rank relational objects and its application to web search learning to rank is a new statistical learning technology on creating a ranking model for sorting objects the technology has been successfully applied to web search and is becoming one of the key machineries for building search engines existing approaches to learning to rank however did not consider the cases in which there exists relationship between the objects to be ranked despite of the fact that such situations are very common in practice for example in web search given a query certain relationships usually exist among the the retrieved documents eg url hierarchy similarity etc and sometimes it is necessary to utilize the information in ranking of the documents this paper addresses the issue and formulates it as a novel learning problem referred to as learning to rank relational objects in the new learning task the ranking model is defined as a function of not only the contents features of objects but also the relations between objects the paper further focuses on one setting of the learning problem in which the way of using relation information is predetermined it formalizes the learning task as an optimization problem in the setting the paper then proposes a new method to perform the optimization task particularly an implementation based on svm experimental results show that the proposed method outperforms the baseline methods for two ranking tasks pseudo relevance feedback and topic distillation in web search indicating that the proposed method can indeed make effective use of relation information and content information in ranking 
contextual advertising by combining relevance with click feedback contextual advertising by combining relevance with click feedback contextual advertising supports much of the webs ecosystem todayuser experience and revenue shared by the site publisher ad the adnetwork depend on the relevance of the displayed ads to thepage content as with other document retrieval systems relevance isprovided by scoring the match between individual ads documentsand the content of the page where the ads are shown query in thispaper we show how this match can be improved significantly byaugmenting the adpage scoring function with extra parameters from alogistic regression model on the words in the pages and ads a keyproperty of the proposed model is that it can be mapped to standardcosine similarity matching and is suitable for efficient and scalableimplementation over inverted indexes the model parameter values arelearnt from logs containing ad impressions and clicks with shrinkageestimators being used to combat sparsity to scale our computations totrain on an extremely large training corpus consisting of severalgigabytes of data we parallelize our fitting algorithm in a hadoop framework experimental evaluation is provided showing improved clickprediction over a holdout set of impression and click events from alarge scale realworld ad placement engine our best model achieves a lift in precision relative to a traditional informationretrieval model which is based on cosine similarity for recalling of the clicks in our test data
xml data dissemination using automata on top of structured overlay networks xml data dissemination using automata on top of structured overlay networks we present a novel approach for filtering xml documents using nondeterministic finite automata and distributed hash tables our approach differs architecturally from recent proposals that deal with distributed xml filtering they assume an xml broker architecture whereas our solution is built on top of distributed hash tables the essence of our work is a distributed implementation of yfilter a stateoftheart automatabased xml filtering system on top of chord we experimentally evaluate our approach and demonstrate that our algorithms can scale to millions of xpath queries under various filtering scenarios and also exhibit very good load balancing properties
utilitydriven load shedding for xml stream processing utilitydriven load shedding for xml stream processing because of the high volume and unpredictable arrival rate streamprocessing systems may not always be able to keep up with the inputdata streams resulting in buffer overflow and uncontrolled lossof data load shedding the prevalent strategy for solving thisoverflow problem has so far only been considered for relationalstream processing but not for xml shedding applied to xml streamprocessing brings new opportunities and challenges due to complexnested nature of xml structures in this paper we tackle thisunsolved xml shedding problem using a threepronged approach firstwe develop an xquery preference model that enables users to specifythe relative importance of preserving different subpatterns in thexml result structure this transforms shedding into the problem ofrewriting the user query into shed queries that return approximatequery answers with utility as measured by the given user preferencemodel second we develop a cost model to compare the performance ofalternate shed queries third we develop two shedding algorithmsoptshed and fastshed optshed guarantees to find an optimal solutionhowever at the cost of exponential complexity fastshed asconfirmed by our experiments achieves a closetooptimal result ina wide range of test cases finally we describe the inautomatonshedding mechanism for xquery stream engines the experiments showthat our proposed utilitydriven shedding solutions consistentlyachieve higher utility results compared to the existing relationalshedding techniques
sewnet a framework for creating services utilizing telecom functionality sewnet a framework for creating services utilizing telecom functionality with telecom market reaching saturation in many geographies and revenues from voice calls decreasing telecom operators are trying to identify new sources of revenue for this purpose these operators can take advantage of their core functionalities like location call control etc by exposing them as services to be composed by developers with third party offerings available over the web to hide the complexity of underlying telecom protocols from application developers the operators are steadily adopting service oriented architecture soa and reference standards like parlayx and ims however a number of challenges still remain in rapid utilization of telecom functionalities for creating new applications existence of multiple protocols different classes of developers and the need to coordinate and manage usage of these functionalities in this paper we present sewnet a framework for creating applications exploiting telecom functionality exposed over a converged ip network more specifically sewnet a provides an abstraction model for encapsulating invocation coordination and enrichment of the telecom functionalities b renders a service creation environment on top of this model and c caters to various different categories of developers with the help of two usecase scenarios we demonstrate how sewnet can create services utilizing rich telecom functionality
privacyenhanced sharing of personal content on the web privacyenhanced sharing of personal content on the web publishing personal content on the web is gaining increased popularity with dramatic growth in social networking websites and availability of cheap personal domain names and hosting services although the internet enables easy publishing of any content intended to be generally accessible restricting personal content to a selected group of contacts is more difficult social networking websites partially enable users to restrict access to a selected group of users of the same network by explicitly creating a friends list while this limited restriction supports users privacy on those few selected websites personal websites must still largely be protected manually by sharing passwords or obscure links our focus is the general problem of privacyenabled web content sharing from any userchosen web server by leveraging the existing circle of trust in popular instant messaging im networks we propose a scheme called imbased privacyenhanced content sharing impecs for personal web content sharing impecs enables a publishing users personal data to be accessible only to her im contacts a user can put her personal web page on any web server she wants vs being restricted to a specific social networking website and maintain privacy of her content without requiring sitespecific passwords our prototype of impecs required only minor modifications to an im server and php scripts on a web server the general idea behind impecs extends beyond im and im circles of trust any equivalent scheme ideally containing prearranged groups could similarly be leveraged
detecting image spam using visual features and near duplicate detection detecting image spam using visual features and near duplicate detection email spam is a much studied topic but even though current email spam detecting software has been gaining a competitive edge against text based email spam new advances in spam generation have posed a new challenge imagebased spam image based spam is email which includes embedded images containing the spam messages but in binary format in this paper we study the characteristics of image spam to propose two solutions for detecting imagebased spam while drawing a comparison with the existing techniques the first solution which uses the visual features for classification offers an accuracy of about ie an improvement of at least compared to existing solutions svms support vector machines are used to train classifiers using judiciously decided color texture and shape features the second solution offers a novel approach for near duplication detection in images it involves clustering of image gmms gaussian mixture models based on the agglomerative information bottleneck aib principle using jensenshannon divergence js as the distance measure
better abstractions for secure serverside scripting better abstractions for secure serverside scripting it is notoriously difficult to program a solid web application besides addressing web interactions state maintenance and whimsical user navigation behaviors programmers must also avoid a minefield of security vulnerabilities the problem is twofold first we lack a clear understanding of the new computation model underlying web applications second we lack proper abstractions for hiding common and subtle coding details that are orthogonal to the business functionalities of specific web applicationsthis paper addresses both issues first we present a language bass for declarative serverside scripting bass allows programmers to work in an ideal world using new abstractions to tackle common but problematic aspects of web programming the meta properties of bass provide useful security guarantees second we present a language moss reflecting realistic web programming concepts and scenarios thus articulating the computation model behind web programming finally we present a translation from bass to moss demonstrating how the ideal programming model and security guarantees of bass can be implemented in practice
networked graphs a declarative mechanism for sparql rules sparql views and rdf data integration on the web networked graphs a declarative mechanism for sparql rules sparql views and rdf data integration on the web easy reuse and integration of declaratively described information in a distributed setting is one of the main motivations for building the semantic web despite of this claim reuse and recombination of rdf data today is mostly done using data replication and procedural code a simple declarative mechanism for reusing and combining rdf data would help users to generate content for the semantic web having such a mechanism the semantic web could better benefit from user generated content as it is broadly present in the so called web but also from better linkage of existing contentwe propose networked graphs which allow users to define rdf graphs both by extensionally listing content but also by using views on other graphs these views can be used to include parts of other graphs to transform data before including it and to denote rules the relationships between graphs are described declaratively using sparql queries and an extension of the sparql semantics networked graphs are easily exchangeable between and interpretable on different computers using existing protocols networked graphss can be evaluated in a distributed setting
sparql basic graph pattern optimization using selectivity estimation sparql basic graph pattern optimization using selectivity estimation in this paper we formalize the problem of basic graph pattern bgp optimization for sparql queries and main memory graph implementations of rdf data we define and analyze the characteristics of heuristics for selectivitybased static bgp optimization the heuristics range from simple triple pattern variable counting to more sophisticated selectivity estimation techniques customized summary statistics for rdf data enable the selectivityestimation of joined triple patterns and the development of efficient heuristics using the lehigh university benchmark lubm we evaluate the performance of the heuristics for the queries provided by the lubm and discuss some of them in more details
saling rdf with time saling rdf with time the world wide web consortiums rdf standard primarily consists of subjectpropertyobject triples that specify the value that a given subject has for a given property however it is frequently the case that even for a fixed subject and property the valuevaries with time as a consequence efforts have been made to annotate rdf triples with valid time intervals however to date no proposals exist for efficient indexing of such temporal rdfdatabases it is clearly beneficial to store rdf data in arelational db however standard relational indexes are inadequately equipped to handle rdfs graph structure in this paper we propose the tgrin index structure that builds a specialized index for temporal rdf that is physically stored in an rdbms past efforts to store rdf in relational stores include jena from hp sesame from openrdforg and store from the university of southampton we show that even when these efforts are augmented with well known temporal indexes like r trees srtrees stindex and map the tgrin index exhibits superior performance in terms of index build time tgrin takes two thirds or less of the time used by any other system and it uses a comparable amount of memory and less disk space than jena sesame and store more importantly tgrin can answer queries three to six times faster for average query graph patterns and five to ten times faster for complex queries than these systems
floatcascade learning for fast imbalanced web mining floatcascade learning for fast imbalanced web mining this paper is concerned with the problem of imbalancedclassification ic in web mining which often arises on the webdue to the matthew effect as web ic applications usually needto provide online service for user and deal with large volume of dataclassification speed emerges as an important issue to be addressedin face detection asymmetric cascade is used to speed upimbalanced classification by building a cascade structure of simpleclassifiers but it often causes a loss of classification accuracy due tothe iterative feature addition in its learning procedure in this paperwe adopt the idea of cascade classifier in imbalanced web miningfor fast classification and propose a novel asymmetric cascadelearning method called floatcascade to improve the accuracy tothe end floatcascade selects fewer yet more effective features ateach stage of the cascade classifier in addition a decisiontreescheme is adopted to enhance feature diversity and discriminationcapability for floatcascade learning we evaluate floatcascadethrough two typical ic applications in web mining web pagecategorization and citation matching experimental resultsdemonstrate the effectiveness and efficiency of floatcascadecomparing to the stateoftheart ic methods like asymmetriccascade asymmetric adaboost and weighted svm
recommending questions using the mdlbased tree cut model recommending questions using the mdlbased tree cut model the paper is concerned with the problem of question recommendation specifically given a question as query we are to retrieve and rank other questions according to their likelihood of being good recommendations of the queried question a good recommendation provides alternative aspects around users interest we tackle the problem of question recommendation in two steps first represent questions as graphs of topic terms and then rank recommendations on the basis of the graphs we formalize both steps as the treecutting problems and then employ the mdl minimum description length for selecting the best cuts experiments have been conducted with the real questions posted at yahoo answers the questions are about two domains travel and computers amp internet experimental results indicate that the use of the mdlbased tree cut model can significantly outperform the baseline methods of wordbased vsm or phrasebased vsm the results also show that the use of the mdlbased tree cut model is essential to our approach
learning to classify short and sparse text amp web with hidden topics from largescale data collections learning to classify short and sparse text amp web with hidden topics from largescale data collections this paper presents a general framework for building classifiers that deal with short and sparse text amp web segments by making the most of hidden topics discovered from largescale data collections the main motivation of this work is that many classification tasks working with short segments of text amp web such as search snippets forum amp chat messages blog amp news feeds product reviews and book amp movie summaries fail to achieve high accuracy due to the data sparseness we therefore come up with an idea of gaining external knowledge to make the data more related as well as expand the coverage of classifiers to handle future data better the underlying idea of the framework is that for each classification task we collect a largescale external data collection called universal dataset and then build a classifier on both a small set of labeled training data and a rich set of hidden topics discovered from that data collection the framework is general enough to be applied to different data domains and genres ranging from web search results to medical text we did a careful evaluation on several hundred megabytes of wikipedia m words and medline m words with two tasks web search domain disambiguation and disease categorization for medical text and achieved significant quality enhancement
matching independent global constraints for composite web services matching independent global constraints for composite web services service discovery employs matching techniques to select services by comparing their descriptions against user constraints semanticbased matching approaches achieve higher recall than syntacticbased ones as they employ ontological reasoning mechanisms to match syntactically heterogeneous descriptions however semanticbased approaches still have problems eg lack of scalability as an exhaustive search is often performed to located services conforming to constraints this paper proposes two approaches that deal with the problem of scalabilityperformance for composite service location first services are indexed based on the values they assign to their restricted attributes the attributes restricted by a given constraint then services that assign conforming values to those attributes are combined to form composite services the first proposed approach extends a local optimisation technique to perform the latter since identifying such values is nphard however this approach returns false negatives since the local optimisation technique does not consider all the values hence a second approach that derives conforming values using domain rules is defined the used rules are returned with each composite service so that a user can understand the context in which it is retrieved results obtained from the experiments that varied the number of available services demonstrate that the performance of the local optimisationbased approach is better than existing semanticbased approaches and recall is higher than syntacticbased approaches
wishful search interactive composition of data mashups wishful search interactive composition of data mashups with the emergence of yahoo pipes and several similar services data mashup tools have started to gain interest ofbusiness users making these tools simple and accessible tousers with no or little programming experience has become apressing issue in this paper we introduce mario mashupautomation with runtime orchestration and invocationa new tool that radically simplifies data mashup composition we have developed an intelligent automatic composition engine in mario together with a simple user interfaceusing an intuitive wishful search abstraction it thus allows users to explore the space of potentially composabledata mashups and preview composition results as they iteratively refine their wishes ie mashup composition goalsit also lets users discover and make use of system capabilities without having to understand the capabilities of individual components and instantly reflects changes made tothe components by presenting an aggregate view of changedcapabilities of the entire system we describe our experience with using mario to compose flows of yahoo pipescomponents
extending the compatibility notion for abstract wsbpel processes extending the compatibility notion for abstract wsbpel processes wsbpel defines a standard for executable processes executable processes are business processes which can be automated through an it infrastructure the wsbpel specification also introduces the concept of abstract processes in contrast to their executable siblings abstract processes are not executable and can have parts where business logic is disguised nevertheless the wsbpel specification introduces a notion of compatibility between such an underspecified abstract process and a fully specified executable one basically this compatibility notion defines a set of syntactical rules that can be augmented or restricted by profiles so far there exist two of such profiles the abstract process profile for observable behavior and the abstract process profile for templates none of these profiles defines a concept of behavioral equivalence therefore both profiles are too strict with respect to the rules they impose when deciding whether an executable process is compatible to an abstract one in this paper we propose a novel profile that extends the existing abstract process profile for observable behavior by defining a behavioral relationship we also show that our novel profile allows for more flexibility when deciding whether an executable and an abstract process are compatible
sessionlock securing web sessions against eavesdropping sessionlock securing web sessions against eavesdropping typical web sessions can be hijacked easily by a network eavesdropper in attacks that have come to be designated sidejacking the rise of ubiquitous wireless networks often unprotected at the transport layer has significantly aggravated this problem while ssl can protect against eavesdropping its usability disadvantages often make it unsuitable when the data is not considered highly confidential most webbased email services for example use ssl only on their login page and are thus vulnerable to sidejackingwe propose sessionlock a simple approach to securing web sessions against eavesdropping without extending the use of ssl sessionlock is easily implemented by web developers using only javascript and simple serverside logic its performance impact is negligible and all major web browsers are supported interestingly it is particularly easy to implement on singlepage ajax web applications eg gmail or yahoo mail with approximately lines of javascript and lines of serverside verification code
forcehttps protecting highsecurity web sites from network attacks forcehttps protecting highsecurity web sites from network attacks as wireless networks proliferate web browsers operate in an increasingly hostile network environment the https protocol has the potential to protect web users from network attackers but realworld deployments must cope with misconfigured servers causing imperfect web sites and users to compromise browsing sessions inadvertently forcehttps is a simple browser security mechanism that web sites or users can use to opt in to stricter error processing improving the security of https by preventing network attacks that leverage the browsers lax error processing by augmenting the browser with a database of custom url rewrite rules forcehttps allows sophisticated users to transparently retrofit security onto some insecure sites that support https we provide a prototype implementation of forcehttps as a firefox browser extension
smash secure component model for crossdomain mashups on unmodified browsers smash secure component model for crossdomain mashups on unmodified browsers mashup applications mix and merge content data and code from multiple content providers in a users browser to provide highvalue web applications that can rival the user experience provided by desktop applications current browser security models were not designed to support such applications and they are therefore implemented with insecure workarounds in this paper we present a secure component model where components are provided by different trust domains and can interact using a communication abstraction that allows ease of specification of a security policy we have developed an implementation of this model that works currently in all major browsers and addresses challenges of communication integrity and framephishing an evaluation of the performance of our implementation shows that this approach is not just feasible but also practical
compoweb a componentoriented web architecture compoweb a componentoriented web architecture in this paper clientsite web mashups are studied from componentoriented perspective and compoweb a componentoriented web architecture is proposed in compoweb a web application is decomposed into web components called gadgets a gadget is an abstraction of functional or logical web component it is isolated from other gadgets for security and reliability contractbased channels are the only way to interact with each other an abstraction of contractbased channels supported or required by a gadget is also presented it enables binding of gadgets at deployment and promotes interchangeable gadgets unlike the model of a normal function call where the function logic is executed in callers context compoweb ensures that the function logic is executed in callees context so that both the caller and callee are protected implementation of a prototype compoweb system and its performance are also presented
generating diverse and representative image search results for landmarks generating diverse and representative image search results for landmarks can we leverage the communitycontributed collections of rich media on the web to automatically generate representative and diverse views of the worlds landmarks we use a combination of context and contentbased tools to generate representative sets of images for locationdriven features and landmarks a common search task to do that we using location and other metadata as well as tags associated with images and the images visual features we present an approach to extracting tags that represent landmarks we show how to use unsupervised methods to extract representative views and images for each landmark this approach can potentially scale to provide better search and representation for landmarks worldwide we evaluate the system in the context of image search using a reallife dataset of images from the san francisco area
pagerank for product image search pagerank for product image search in this paper we cast the imageranking problem into the task of identifying authority nodes on an inferred visual similarity graph and propose an algorithm to analyze the visual link structure that can be created among a group of images through an iterative procedure based on the pagerank computation a numerical weight is assigned to each image this measures its relative importance to the other images being considered the incorporation of visual signals in this process differs from the majority of largescale commercialsearch engines in use today commercial searchengines often solely rely on the text clues of the pages in which images are embedded to rank images and often entirely ignore the content of theimages themselves as a ranking signal to quantify the performance of our approach in a realworld system we conducted a series of experiments based on the task of retrieving images for of the most popular products queries our experimental results show significant improvement in terms of user satisfaction and relevancyin comparison to the most recent google image search results
graph theoretical framework for simultaneously integrating visual and textual features for efficient web image clustering graph theoretical framework for simultaneously integrating visual and textual features for efficient web image clustering with the explosive growth of web and the recent development in digital media technology the number of images on the web has grown tremendously consequently web image clustering has emerged as an important application some of the initial efforts along this direction revolved around clustering web images based on the visual features of images or textual features by making use of the text surrounding the images however not much work has been done in using multimodal information for clustering web images in this paper we propose a graph theoretical framework for simultaneouslyintegrating visual and textual features for efficient web image clustering specifically we model visual features images and words from surrounding text using a tripartite graph partitioning this graph leads to clustering of the web images although graph partitioning approach has been adopted before the main contribution of this work lies in a new algorithm that we propose consistent isoperimetric highorder coclustering cihc for partitioning thetripartite graph computationally cihc is very quick as it requires a simple solution to a sparse system of linear equations our theoretical analysis and extensive experiments performed on real web images demonstrate the performance of cihc in terms of the quality efficiency and scalability in partitioning the visual featureimageword tripartite graph
flickr tag recommendation based on collective knowledge flickr tag recommendation based on collective knowledge online photo services such as flickr and zooomr allow users to share their photos with family friends and the online community at large an important facet of these services is that users manually annotate their photos using so called tags which describe the contents of the photo or provide additional contextual and semantical information in this paper we investigate how we can assist users in the tagging phase the contribution of our research is twofold we analyse a representative snapshot of flickr and present the results by means of a tag characterisation focussing on how users tags photos and what information is contained in the tagging based on this analysis we present and evaluate tag recommendation strategies to support the user in the photo annotation task by recommending a set of tags that can be added to the photo the results of the empirical evaluation show that we can effectively recommend relevant tags for a variety of photos with different levels of exhaustiveness of original tagging
topic modeling with network regularization topic modeling with network regularization in this paper we formally define the problem of topic modeling with network structure tmn we propose a novel solution to this problem which regularizes a statistical topic model with a harmonic regularizer based on a graph structure in the data the proposed method bridges topic modeling and social network analysis which leverages the power of both statistical topic models and discrete regularization the output of this model well summarizes topics in text maps a topic on the network and discovers topical communities with concrete selection of a topic model and a graphbased regularizer our model can be applied to text mining problems such as authortopic analysis community discovery and spatial text mining empirical experiments on two different genres of data show that our approach is effective which improves textoriented methods as well as networkoriented methods the proposed model is general it can be applied to any text collections with a mixture of topics and an associated network structure
modeling online reviews with multigrain topic models modeling online reviews with multigrain topic models in this paper we present a novel framework for extracting the ratable aspects of objects from online user reviews extracting such aspects is an important challenge in automatically mining product opinions from the web and in generating opinionbased summaries of user reviews our models are based on extensions to standard topic modeling methods such as lda and plsa to induce multigrain topics we argue that multigrain models are more appropriate forour task since standard models tend to produce topics that correspond to global properties of objects eg the brand of a product type rather than the aspects of an object that tend to be rated by a user the models we present not only extract ratable aspects but also cluster them into coherent topics eg waitress and bartender are part of the same topic staff for restaurants this differentiates it from much of the previous work which extracts aspects through term frequency analysis with minimal clustering we evaluate the multigrain models both qualitatively and quantitatively to show that they improve significantly upon standard topic models
opinion integration through semisupervised topic modeling opinion integration through semisupervised topic modeling web technology has enabled more and more people to freely express their opinions on the web making the web an extremely valuable source for mining user opinions about all kinds of topics in this paper we study how to automatically integrate opinions expressed in a wellwritten expert review with lots of opinions scattering in various sources such as blogspaces and forums we formally define this new integration problem and propose to use semisupervised topic models to solve the problem in a principled way experiments on integrating opinions about two quite different topics a product and a political figure show that the proposed method is effective for both topics and can generate useful aligned integrated opinion summaries the proposed method is quite general it can be used to integrate a well written review with opinions in an arbitrary text collection about any topic to potentially support many interesting applications in multiple domains
supporting anonymous location queries in mobile environments with privacygrid supporting anonymous location queries in mobile environments with privacygrid this paper presents privacygrid a framework for supporting anonymous locationbased queries in mobile information delivery systems the privacygrid framework offers three unique capabilities first it provides a locationprivacy protection preference profile model called locationpp which allows mobile users to explicitly define their preferred location privacy requirements in terms of both location hiding measures eg location kanonymity and location ldiversity and location service quality measures egmaximum spatial resolution and maximum temporal resolution second it provides fast and effective location cloakingalgorithms for location kanonymity and location ldiversityin a mobile environment we develop dynamic bottomup and topdown grid cloaking algorithms with the goal ofachieving high anonymization success rate and efficiency interms of both time complexity and maintenance cost ahybrid approach that carefully combines the strengths ofboth bottomup and topdown cloaking approaches to further reduce the average anonymization time is also developed last but not the least privacygrid incorporatestemporal cloaking into the location cloaking process to further increase the success rate of location anonymizationwe also discuss privacygrid mechanisms for supportinganonymous location queries experimental evaluation showsthat the privacygrid approach can provide close to optimal location kanonymity as defined by per user locationpp without introducing significant performance penalties
learning transportation mode from raw gps data for geographic applications on the web learning transportation mode from raw gps data for geographic applications on the web geographic information has spawned many novel web applications where global positioning system gps plays important roles in bridging the applications and end users learning knowledge from users raw gps data can provide rich context information for both geographic and mobile applications however so far raw gps data are still used directly without much understanding in this paper an approach based on supervised learning is proposed to automatically infer transportation mode from raw gps data the transportation mode such as walking driving etc implied in a users gps data can provide us valuable knowledge to understand the user it also enables contextaware computing based on users present transportation mode and design of an innovative user interface for web users our approach consists of three parts a change pointbased segmentation method an inference model and a postprocessing algorithm based on conditional probability the change pointbased segmentation method was compared with two baselines including uniform duration based and uniform length based methods meanwhile four different inference models including decision tree bayesian net support vector machine svm and conditional random field crf are studied in the experiments we evaluated the approach using the gps data collected by users over six months period as a result beyond other two segmentation methods the change point based method achieved a higher degree of accuracy in predicting transportation modes and detecting transitions between them decision tree outperformed other inference models over the change point based segmentation method
deciphering mobile search patterns a study of yahoo mobile search queries deciphering mobile search patterns a study of yahoo mobile search queries in this paper we study the characteristics of search queries submitted from mobile devices using various yahoo mobile onesearch applications during a months period in the second half of and report the query patterns derived from million english sample queries submitted by users in us canada europe and asia we examine the query distribution and topical categories the queries belong to in order to find new trends we compare and contrast the search patterns between us vs international queries and between queries from various search interfaces xhtmlwap java widgets and sms we also compare our results with previous studies wherever possible either to confirm previous findings or to find interesting differences in the query distribution and pattern
serviceoriented data denormalization for scalable web applications serviceoriented data denormalization for scalable web applications many techniques have been proposed to scale web applications however the data interdependencies between the database queries and transactions issued by the applications limit their efficiency we claim that major scalability improvements can be gained by restructuring the web application data into multiple independent data services with exclusive access to their private data store while this restructuring does not provide performance gains by itself the implied simplification of each database workload allows a much more efficient use of classical techniques we illustrate the data denormalization process on three benchmark applications tpcw rubis and rubbos we deploy the resulting serviceoriented implementation of tpcw across an node cluster and show that restructuring its data can provide at least an order of magnitude improvement in the maximum sustainable throughput compared to masterslave database replication while preserving strong consistency and transactional properties
anycast cdns revisited anycast cdns revisited because it is an integral part of the internet routing apparatus and because it allows multiple instances of the same service to be naturally discovered ip anycast has many attractive features for any service that involve the replication of multiple instances across the internetwhile briefly considered as an enabler when content distribution networks cdns first emerged the use of ip anycast was deemed infeasible in that environment the main reasons for this decision were the lack of load awareness of ip anycast and unwanted side effects of internet routing changes on the ip anycast mechanism prompted by recent developments in route control technology as well as a better understanding of the behavior of ip anycast in operational settings we revisit this decision and propose a loadaware ip anycastcdn architecture that addresses these concerns while benefiting from inherent ip anycast features our architecture makes use of route control mechanisms to take server and network load into account to realize loadaware anycast we show that the resulting redirection requirements can be formulated as a generalized assignment problem and present practical algorithms that address these requirements while at the same time limiting session disruptions that plague regular ip anycast we evaluate our algorithms through trace based simulation using tracesobtained from an operation cdn network
a comparative analysis of web and peertopeer traffic a comparative analysis of web and peertopeer traffic peertopeer pp applications continue to grow in popularity andhave reportedly overtaken web applications as the single largestcontributor to internet traffic using traces collected from a largeedge network we conduct an extensive analysis of pp trafficcompare pp traffic with web traffic and discuss the implicationsof increased pp traffic in addition to studying the aggregate pptraffic we also analyze and compare the two main constituents ofpp traffic in our data namely bittorrent and gnutella the resultspresented in the paper may be used for generating syntheticworkloads gaining insights into the functioning of ppapplications and developing network management strategies forexample our results suggest that new models are necessary forinternet traffic as a first step we present flowleveldistributional models for web and pp traffic that may be used innetwork simulation and emulation experiments
irlbot scaling to billion pages and beyond irlbot scaling to billion pages and beyond this paper shares our experience in designing a web crawler that candownload billions of pages using a singleserver implementation andmodels its performance we show that with the quadratically increasingcomplexity of verifying url uniqueness bfs crawl order and fixedperhost ratelimiting current crawling algorithms cannot effectivelycope with the sheer volume of urls generated in large crawlshighlybranching spam legitimate multimillionpage blog sites andinfinite loops created by serverside scripts we offer a set oftechniques for dealing with these issues and test their performance in animplementation we call irlbot in our recent experiment that lasted days irlbot running on a single server successfully crawled billion valid html pages billion connection requests andsustained an average download rate of mbs pagessunlike our prior experiments with algorithms proposed in related workthis version of irlbot did not experience any bottlenecks andsuccessfully handled content from over million hosts parsed out billion links and discovered a subset of the web graph with billion unique nodes
recrawl scheduling based on information longevity recrawl scheduling based on information longevity it is crucial for a web crawler to distinguish between ephemeral and persistent content ephemeral content eg quote of the day is usually not worth crawling because by the time it reaches the index it is no longer representative of the web page from which it was acquiredon the other hand content that persists across multiple page updates eg recent blog postings may be worth acquiring because it matches the pages true content for a sustained period of timein this paper we characterize the longevity of information found on the webvia both empirical measurements and a generative model that coincides with these measurementswe then develop new recrawl scheduling policies that take longevity into accountas we show via experiments over real web data our policies obtain better freshnessat lower cost compared with previous approaches
irobot an intelligent crawler for web forums irobot an intelligent crawler for web forums we study in this paper the web forum crawling problem which isa very fundamental step in many web applications such as searchengine and web data mining as a typical usercreated contentucc web forum has become an important resource on the webdue to its rich information contributed by millions of internetusers every day however web forum crawling is not a trivialproblem due to the indepth link structures the large amount ofduplicate pages as well as many invalid pages caused by loginfailure issues in this paper we propose and build a prototype ofan intelligent forum crawler irobot which has intelligence tounderstand the content and the structure of a forum site and thendecide how to choose traversal paths among different kinds ofpages to do this we first randomly sample download a fewpages from the target forum site and introduce the page contentlayout as the characteristics to group those presampled pages andreconstruct the forums sitemap after that we select an optimalcrawling path which only traverses informative pages and skipsinvalid and duplicate ones the extensive experimental results onseveral forums show the performance of our system in the followingaspects effectiveness compared to a generic crawlerirobot significantly decreases the duplicate and invalid pages efficiency with a small cost of presampling a few pages forlearning the necessary knowledge irobot saves substantial networkbandwidth and storage as it only fetches informative pagesfrom a forum site and long threads that are divided into multiplepages can be reconcatenated and archived as a whole threadwhich is of great help for further indexing and data mining
efficient similarity joins for near duplicate detection efficient similarity joins for near duplicate detection with the increasing amount of data and the need to integrate data from multiple data sources a challenging issue is to find near duplicate records efficiently in this paper we focus on efficient algorithms to find pairs of records such that their similarities are above a given threshold several existing algorithms rely on the prefix filtering principle to avoid computing similarity values for all possible pairs of records we propose new filtering techniques by exploiting the ordering information they are integrated into the existing methods and drastically reduce the candidate sizes and hence improve the efficiency experimental results show that our proposed algorithms can achieve up to xx speedup over previous algorithms on several real datasets and provide alternative solutions to the near duplicate web page detection problem
learning multiple graphs for document recommendations learning multiple graphs for document recommendations the web offers rich relational data with different semantics in this paper we address the problem of document recommendation in a digital library where the documents in question are networked by citations and are associated with other entities by various relations due to the sparsity of a single graph and noise in graph construction we propose a new method for combining multiple graphs to measure document similarities where different factorization strategies are used based on the nature of different graphs in particular the new method seeks a single lowdimensional embedding of documents that captures their relative similarities in a latent space based on the obtained embedding a new recommendation framework is developed using semisupervised learning on graphs in addition we address the scalability issue and propose an incremental algorithm the new incremental method significantly improves the efficiency by calculating the embedding for new incoming documents only the new batch and incremental methods are evaluated on two real world datasets prepared from citeseer experiments demonstrate significant quality improvement for our batch method and significant efficiency improvement with tolerable quality loss for our incremental method
enhanced hierarchical classification via isotonic smoothing enhanced hierarchical classification via isotonic smoothing hierarchical topic taxonomies have proliferated on the world wide web and exploiting the output space decompositions they induce in automated classification systems is an active area of research in many domains classifiers learned on a hierarchy of classes have been shown to outperform those learned on a flat set of classes in this paper we argue that the hierarchical arrangement of classes leads to intuitive relationships between the corresponding classifiers output scores and that enforcing these relationships as a postprocessing step after classification can improve its accuracy we formulate the task of smoothing classifier outputs as a regularized isotonic tree regression problem and present a dynamic programming based method that solves it optimally this new problem generalizes the classic isotonic tree regression problem and both the new formulation and algorithm might be of independent interest in our empirical analysis of two realworld text classification scenarios we show that our approach to smoothing classifier outputs results in improved classification accuracy
why web is good for learning and for research principles and prototypes why web is good for learning and for research principles and prototypes the term web is used to describe applications that distinguish themselves from previous generations of software by a number of principlesexisting work shows that web applications can be successfully exploited for technologyenhance learning however indepth analyses of the relationship between web technology on the one hand and teaching and learning on the other hand are still rarein this article we will analyze the technological principles of the web and describe their pedagogical implications on learningwe will furthermore show that web is not only well suited for learning but also for research on learning the wealth of services that is available and their openness regarding api and data allow to assemble prototypes of technologysupported learning applications in amazingly small amount of time these prototypes can be used to evaluate research hypotheses quickly we will present two example prototypes and discuss the lessons we learned from building and using these prototypes 
exploring social annotations for information retrieval exploring social annotations for information retrieval social annotation has gained increasing popularity in many webbased applications leading to an emerging research area in text analysis and information retrieval this paper is concerned with developing probabilistic models and computational algorithms for social annotations we propose a unified framework to combine the modeling of social annotations with the language modelingbased methods for information retrieval the proposed approach consists of two steps discovering topics in the contents and annotations of documents while categorizing the users by domains and enhancing document and query language models by incorporating user domain interests as well as topical background models differences in user domain expertise are also considered when combining the discovered user domain interests in particular we propose a new general generative model for social annotations which is then simplified to a computationally tractable hierarchical bayesian network then we apply smoothing techniques in a risk minimization framework to incorporate the topical information to language models experiments are carried out on a realworld annotation data set sampled from delicious our results demonstrate significant improvements over the alternative approaches without consideration of topical information social annotations user expertise or simple incorporation of topic analysis
lockfree consistency control for web applications lockfree consistency control for web applications online collaboration and sharing is the central theme of many webbasedservices that create the socalled web phenomena usingthe internet as a computing platform many web applicationsset up mirror sites to provide largescale availability and to achieveload balance however in the age of web where every user isalso a writer and publisher the deployment of mirror sites makesconsistency maintenance aweb scale problem traditional concurrencycontrol methods eg two phase lock serialization etc arenot up to the task for several reasons first large network latencybetween mirror sites will make two phase locking a throughput bottlenecksecond locking will block a large portion of concurrentoperations which makes it impossible to provide largescale availabilityon the other hand most web operations do not needstrict serializability it is not the intention of a user who is correctinga typo in a shared document to block another who is addinga comment as long as consistency can still be achieved thusin order to enable maximal online collaboration and sharing weneed a lockfree mechanism that can maintain consistency amongmirror sites on the web in this paper we propose a flexible and efficientmethod to achieve consistency maintenance in the web world our experiments show its good performance improvementcompared with existing methods based on distributed lock
analyzing search engine advertising firm behavior and crossselling in electronic markets analyzing search engine advertising firm behavior and crossselling in electronic markets the phenomenon of sponsored search advertising is gaining ground as the largest source of revenues for search engines firms across different industries have are beginning to adopt this as the primary form of online advertising this process works on an auction mechanism in which advertisers bid for different keywords and final rank for a given keyword is allocated by the search engine but how different are firms actual bids from their optimal bids moreover what are other ways in which firms can potentially benefit from sponsored search advertising based on the model and estimates from prior work we conduct a number of policy simulations in order to investigate to what extent an advertiser can benefit from bidding optimally for its keywords further we build a hierarchical bayesian modeling framework to explore the potential for crossselling or spillovers effects from a given keyword advertisement across multiple product categories and estimate the model using markov chain monte carlo mcmc methods our analysis suggests that advertisers are not bidding optimally with respect to maximizing profits we conduct a detailed analysis with product level variables to explore the extent of crossselling opportunities across different categories from a given keyword advertisement we find that there exists significant potential for crossselling through search keyword advertisements in that consumers often end up buying products from other categories in addition to the product they were searching for latency the time it takes for consumer to place a purchase order after clicking on the advertisement and the presence of a brand name in the keyword are associated with consumer spending on product categories that are different from the one they were originally searching for on the internet
online learning from click data for sponsored search online learning from click data for sponsored search sponsored search is one of the enabling technologies for todays web search engines it corresponds to matching and showing ads related to theuser query on the search engine results page users are likely toclick on topically related ads and the advertisers pay only when auser clicks on their ad hence it is important to be able to predict if an ad is likely to be clicked and maximize the number of clicks we investigate the sponsored search problem from a machine learning perspective with respect to three main subproblems how to use click data for training and evaluation which learning framework is more suitable for the task and which features are useful for existingmodels we perform a large scale evaluation based on data from a commercial web search engine results show that it ispossible to learn and evaluate directly and exclusively on click dataencoding pairwise preferences following simple and conservativeassumptions we find that online multilayer perceptron learning basedon a small set of features representing content similarity ofdifferent kinds significantly outperforms an information retrievalbaseline and other learning models providing a suitable framework forthe sponsored search task 
automatic online news issue construction in web environment automatic online news issue construction in web environment in many cases rather than a keyword search people intend to seewhat is going on through the internet then the integratedcomprehensive information on news topics is necessary which wecalled news issues including the background history currentprogress different opinions and discussions etc traditionallynews issues are manually generated by website editors it is quitea timeconsuming hard work and hence realtime update isdifficult to perform in this paper a threestep automatic onlinealgorithm for news issue construction is proposed the first step isa topic detection process in which newly appearing stories areclustered into new topic candidates the second step is a topictracking process where those candidates are compared withprevious topics either merged into old ones or generating a newone in the final step news issues are constructed by thecombination of related topics and updated by the insertion of newtopics an automatic online news issue construction process underpractical web circumstances is simulated to perform news issueconstruction experiments fmeasure of the best results is eitherabove topic detection or close to topic detection and tracking four news issue construction results are successfullygenerated in different time granularities one meets the needs likewhats new and the other three will answer questions likewhats hot or whats going on through the proposedalgorithm news issues can be effectively and automaticallyconstructed with realtime update and lots of human efforts willbe released from tedious manual work
finding the right facts in the crowd factoid question answering over social media finding the right facts in the crowd factoid question answering over social media community question answering has emerged as a popular and effective paradigm for a wide range of information needs for example to find out an obscure piece of trivia it is now possible and even very effective to post a question on a popular community qa site such as yahoo answers and to rely on other users to provide answers often within minutes the importance of such community qa sites is magnified as they create archives of millions of questions and hundreds of millions of answers many of which are invaluable for the information needs of other searchers however to make this immense body of knowledge accessible effective answer retrieval is required in particular as any user can contribute an answer to a question the majority of the content reflects personal often unsubstantiated opinions a ranking that combines both relevance and quality is required to make such archives usable for factual information retrieval this task is challenging as the structure and the contents of community qa archives differ significantly from the web setting to address this problem we present a general ranking framework for factual information retrieval from social media results of a large scale evaluation demonstrate that our method is highly effective at retrieving wellformed factual answers to questions as evaluated on a standard factoid qa benchmark we also show that our learning framework can be tuned with the minimum of manual labeling finally we provide result analysis to gain deeper understanding of which features are significant for social media search and retrieval our system can be used as a crucial building block for combining results from a variety of social media content with general web search results and to better integrate social media content for effective information access
personalized interactive faceted search personalized interactive faceted search faceted search is becoming a popular method to allow users to interactively search and navigate complex information spaces a faceted search system presents users with keyvalue metadata that is used for query refinement while popular in ecommerce and digital libraries not much research has been conducted on which metadata to present to a user in order to improve the search experience nor are there repeatable benchmarks for evaluating a faceted search engine this paper proposes the use of collaborative filtering and personalization to customize the search interface to each users behavior this paper also proposes a utility based framework to evaluate the faceted interface in order to demonstrate these ideas and better understand personalized faceted search several faceted search algorithms are proposed and evaluated using the novel evaluation methodology
investigating web services on the world wide web investigating web services on the world wide web searching for web service access points is no longer attached to service registries as web search engines have become a new major source for discovering web services in this work we conduct a thorough analytical investigation on the plurality of web service interfaces that exist on the web today using our web service crawler engine wsce we collect metadata service information on retrieved interfaces through accessible ubrs service portals and search engines we use this data to determine web service statistics and distribution based on object sizes types of technologies employed and the number of functioning services this statistical data can be used to help determine the current status of web services we determine an intriguing result that of the available web services on the web are considered to be active we further use our findings to provide insights on improving the service retrieval process
restful web services vs big web services making the right architectural decision restful web services vs big web services making the right architectural decision recent technology trends in the web services ws domain indicate that a solution eliminating the presumed complexity of the ws standards may be in sight advocates of representational state transfer rest have come to believe that their ideas explaining why the world wide web works are just as applicable to solve enterprise application integration problems and to simplify the plumbing required to build serviceoriented architectures in this paper we objectify the ws vs rest debate by giving a quantitative technical comparison based on architectural principles and decisions we show that the two approaches differ in the number of architectural decisions that must be made and in the number of available alternatives this discrepancy between freedomfromchoice and freedomofchoice explains the complexity difference perceived however we also show that there are significant differences in the consequences of certain decisions in terms of resulting development and maintenance costs our comparison helps technical decision makers to assess the two integration styles and technologies more objectively and select the one that best fits their needs rest is well suited for basic ad hoc integration scenarios ws is more flexible and addresses advanced quality of service requirements commonly occurring in enterprise computing
nonintrusive monitoring and service adaptation for wsbpel nonintrusive monitoring and service adaptation for wsbpel web service processes currently lack monitoring and dynamic runtime adaptation mechanisms in highly dynamic processes services frequently need to be exchanged due to a variety of reasons in this paper we present viedame a system which allows monitoring of bpel processes according to quality of service qos attributes and replacement of existing partner services based on various pluggable replacement strategies the chosen replacement services can be syntactically or semantically equivalent to the bpel interface services can be automatically replaced duringruntime without any downtime of the overall system we implemented our solution with an aspectoriented approach by intercepting soap messages and allow services to be exchanged during runtime with little performance penalty costs as shown in our experiments thereby makingour approach suitable for highavailability bpel environments
wiki content templating wiki content templating wiki content templating enables reuse of content structures among wiki pages in this paper we present a thorough study of this widespread feature showing how its two state of the art models functional and creational templating are suboptimal we then propose a third better model called lightly constrained lc templating and show its implementation in the moin wiki engine we also show how lc templating implementations are the appropriate technologies to push forward semantically rich web pages on the lines of lowercase semantic web and microformats
querying for meta knowledge querying for meta knowledge the semantic web is based on accessing and reusing rdf datafrom many different sources which one may assign different levelsof authority and credibility existing semantic web query languageslike sparql have targeted the retrieval combination andreuse of facts but have so far ignored all aspects of meta knowledgesuch as origins authorship recency or certainty of data toname but a fewin this paper we present an original generic formalized and implementedapproach for managing many dimensions of meta knowledgelike source authorship certainty and others the approachreuses existing rdf modeling possibilities in order to representmeta knowledge then it extends sparql query processing insuch a way that given a sparql query for data one may requestmeta knowledge without modifying the original query thus ourapproach achieves highly flexible and automatically coordinatedquerying for data and meta knowledge while completely separatingthe two areas of concern
automatically refining the wikipedia infobox ontology automatically refining the wikipedia infobox ontology the combined efforts of human volunteers have recently extractednumerous facts fromwikipedia storing them as machineharvestableobjectattributevalue triples inwikipedia infoboxes machine learningsystems such as kylin use these infoboxes as training dataaccurately extracting even more semantic knowledge from naturallanguage text but in order to realize the full power of this informationit must be situated in a cleanlystructured ontology this paperintroduces kog an autonomous system for refining wikipediasinfoboxclass ontology towards this end we cast the problem ofontology refinement as a machine learning problem and solve itusing both svms and a more powerful jointinference approachexpressed in markov logic networks we present experimentsdemonstrating the superiority of the jointinference approach andevaluating other aspects of our system using these techniques webuild a rich ontology integratingwikipedias infoboxclass schematawithwordnet we demonstrate how the resulting ontology may beused to enhance wikipedia with improved query processing andother features
guanxi in the chinese web a study of mutual linking guanxi in the chinese web a study of mutual linking guanxi is a type of dyadic social interaction based on feelingsqing and trust xin long studied by scholarsof chinese origin it has recently drawn the attention of researchersoutside of china we define the concept of guanxias applied to the interaction between web sites we exploremethods to identify guanxi in the chinese web show theunique characteristics of the chinese web which result fromit and introduce a mechanism for simulating guanxi in aweb graph model
topigraphy visualization for largescale tag clouds topigraphy visualization for largescale tag clouds this paper proposes a new method for displaying largescale tag clouds we use a topographical image that helps users to grasp the relationship among tags intuitively as a background to the tag clouds we apply this interface to a blog navigation system and show that the proposed method enables users to find the desired tags easily even if the tag clouds are very large and above tags our approach is also effective for understanding the overall structure of a large amount of tagged documents
efficient mining of frequent sequence generators efficient mining of frequent sequence generators sequential pattern mining has raised great interest in data mining research field in recent years however to our best knowledge no existing work studies the problem of frequent sequence generator mining in this paper we present a novel algorithm feat abbr frequent sequence generator miner to perform this task experimental results show that feat is more efficient than traditional sequential pattern mining algorithms but generates more concise result set and is very effective for classifying web product reviews 
what do they think aggregating local views about news events and topics what do they think aggregating local views about news events and topics the web has become an important medium for news delivery and consumption fresh content about a variety of topics and events is constantly being created and published on the web by many sources as intuitively understood by readers and studied in journalism news articles produced by different social groups present different attitudes towards and interpretations of the same news issues in this paper we propose a new paradigm for aggregating news articles according to the news sources related to the stakeholders of the news issues we implement this paradigm in a prototype system called localsavvy the system provides users the capability to aggregate and browse various local views about the news issues in which they are interested
a generic framework for collaborative multiperspective ontology acquisition a generic framework for collaborative multiperspective ontology acquisition the research objective of this work is to develop a general framework that incorporates collaborative social tagging with a novel ontology scheme conveying multiple perspectives we propose a framework where multiple users tag the same object an image in our case and an ontology is extended based on these tags while being tolerant about different points of view we are not aware of any other work that attempted to devise such an environment and to study its dynamics the proposed framework characterizes the underlying processes for controlled collaborative development of a multiperspective ontology and its application to improve image annotation searching and browsing our case study experiment with a set of selected annotated images indicates the soundness of the proposed ontological model
personalized search and exploration with mytag personalized search and exploration with mytag 
emergence of terminological conventions as an authorsearcher coordination game emergence of terminological conventions as an authorsearcher coordination game all information exchange on the internet whether through fulltext controlled vocabularies ontologies or other mechanisms ultimately requires that that an information provider and seekeruse the same word or symbol in this paper we investigate whathappens when both searchers and authors are dynamicallychoosing terms to match the other side with each side trying toanticipate the other does a terminological convention everemerge or do searchers and providers continue to miss potentialpartners through mismatch of terms we use a gametheoreticsetup to frame questions and learning theory to make predictionsabout whether and which term will emerge as a convention
feature weighting in content based recommendation system using social network analysis feature weighting in content based recommendation system using social network analysis we propose a hybridization of collaborative filtering and content based recommendation system attributes used for content based recommendations are assigned weights depending on their importance to users the weight values are estimated from a set of linear regression equations obtained from a social network graph which captures human judgment about similarity of items 
personalized tag suggestion for flickr personalized tag suggestion for flickr we present a system for personalized tag suggestion for flickr while the user is enteringselecting new tags for a particular picture the system is suggesting related tags to her based on the tags that she or other people have used in the past along with some of the tags already entered the suggested tags are dynamically updated with every additional tag enteredselected we describe three algorithms which can be applied to this problem in experiments our bestperforming method yields an improvement in precision of over a baseline method very similar to the system currently used by flickr our system is accessible at httpltaaepflchflickrtagsto the best of our knowledge this is the first study on tag suggestion in a setting where i no full text information is available such as for blogs ii no item has been tagged by more than one person such as for social bookmarking sites and iii suggestions are dynamically updated requiring efficient yet effective algorithms
a domainspecific language for the modeldriven construction of advanced webbased dialogs a domainspecific language for the modeldriven construction of advanced webbased dialogs complex dialogs with comprehensive underlying data models are gaining increasing importance in todays web applications this in turn accelerates the need for highly dynamic dialogs offering guidance to the users and thus reducing cognitive overload beyond that requirements from the fields of aesthetics web accessibility platformindependence and web service integration arise to this end we present an evolutionary extensible approach for the modeldriven construction of advanced dialogs it is based on a domainspecific language dsl focusing on simplicity and fostering collaboration with stakeholders
contextbased page unit recommendation for webbased sensemaking tasks contextbased page unit recommendation for webbased sensemaking tasks sensemaking tasks require users to perform complex research behaviors to gather and comprehend information from many sources such tasks are common and include for example researching vacation destinations or deciding how to invest in this paper we present an algorithm and interface that provides contextbased page unit recommendation to assist in connection discovery during sensemaking tasks we exploit the natural notetaking activity common to sensemaking behavior as the basis for a taskspecific context model each web page visited by a user is dynamically analyzed to determine the most relevant content fragments which are then recommended to the user our initial evaluations indicate that our approach improves user performance
web user deidentification in personalization web user deidentification in personalization it is a kind of privacy infraction in personalized web service if the user profile submitted to one web site transferred to another site without user permission that can cause the second web site easily reidentify to whom these personal data belong no matter whether the transfer is under control or by hacking this paper presents a portable solution for users to bind their sensitive web data under the appointed domain such data including query logs user accounts click stream etc could be used to identify the sensitive information of the particular user by our domain stretching deidentification method if personal data leak from domain a to b the web user could still not be identified even though he logins to sites under domain b using the same name and password in the experiment implemented by javascript we show the flexibility and efficiency of our deidentification approach
information retrieval and knowledge discovery on the semantic web of traditional chinese medicine information retrieval and knowledge discovery on the semantic web of traditional chinese medicine we conduct the first systematical adoption of the semantic web solution in the integration management and utilization of tcm information and knowledge resources as the results the largest tcm semantic web ontology is engineered as the uniform knowledge representation mechanism the ontologybased query and search engineis deployed mapping legacy and heterogeneous relational databases to the semantic web layer for query and search across database boundaries the first global herbdrug interaction network is mapped through semantic integration and the semantic graph mining methodology is implemented for discovering and interpretinginteresting patterns from this network the platform and underlying methodology are proved effective in tcmrelated drug usage discovery and safety analysis
influencers and their barriers to technology influencers and their barriers to technology modern day marco polos young influencers in emerging markets who have more access to technology than their peers often act as the gateway to new websites and technology in their respective countries however as they influence their peers through account gifting they are often met with barriers such as language
mining for personal name aliases on the web mining for personal name aliases on the web we propose a novel approach to find aliases of a given name from the webwe exploit a set of known names and their aliasesas training data and extract lexical patterns that convey information related to aliases ofnames from text snippets returned by a web search enginethe patterns are then used to find candidate aliases of a given name we use anchor texts and hyperlinks to design a word cooccurrence model and define numerous ranking scores to evaluate the association between a name and its candidate aliasesthe proposed method outperforms numerous baselines and previous work on alias extraction on a dataset of personal names achieving a statistically significant mean reciprocal rank of moreover the aliases extracted using theproposed method improve recall by in a relationdetection task
determining users interest in real time determining users interest in real time most of the search engine optimization techniques attempt to predict users interest by learning from the past information collected from different sources but a users current interest often depends on many factors which are not captured in the past information in this paper we attempt to identify users current interest in real time from the information provided by the user in the current query session by identifying users interest in real time the engine could adapt differently to different users in real time experimental verification indicates that our approach is encouraging for short queries
how to influence my customers the impact of electronic market design how to influence my customers the impact of electronic market design this paper investigates the strategic decisions of online vendors for offering different mechanism such as sampling and online reviews of information products to increase their online sales focusing on measuring the effectiveness of electronic market design offering reviews sampling or both our study shows that online markets behavior as communication markets and consumers learn product quality information both passively reading online reviews and actively but subjectively listening to music sampling using data from amazon first we show that sampling along is a strong product quality signal that reduces the product uncertainty after controlling for halo effect in general products with sampling option enjoy a higher conversion rate which leads to better sales than those without sampling because sampling decreases the uncertainty of consuming experience goods second the impact of online reviews on sales conversion rate is lower for experience goods with a sampling option than those without third when the uncertainty of the societal reviews is higher sampling plays a more important role because it mitigates such uncertainty introduced by online reviews
model bloggers interests based on forgetting mechanism model bloggers interests based on forgetting mechanism blogs have been expanded at an incredible speed in recent yearsplentiful personal information makes blogs a popular way mininguser profiles in this paper we propose a novel bloggers interestsmodeling approach based on forgetting mechanism a newforgetting function is introduced to track interest drift based onthat the short term interest models stim and long terminterest models ltim are constructed to describe bloggersshortterm and longterm interests the experiments show thatboth models can identify bloggers preferences well respectively
psst a webbased system for tracking political statements psst a webbased system for tracking political statements determining candidates views on important issues is critical in deciding whom to support and vote for but finding their statements and votes on an issue can be laborious in this paper we present psst political statement and support tracker a search engine to facilitate analysis of political statements and votes over time we show that prior tools for text analysis can be combined with minimal manual processing to provide a first step in the full automation of this process
groupme where information meets groupme where information meets this paper presents the groupme system a resource sharing system with advanced tagging functionality groupme provides a novel user interface which enables users to organize and arrange arbitrary web resources into groups the content of such groups can be overlooked and inspected immediately as resources are visualized in a multimediabased fashion in this paper we furthermore introduce new folksonomybased ranking strategies that exploit the group structure shipped with groupme folksonomies experiments show that those strategies significantly improve the performance of such ranking algorithms
histrace building a search engine of historical events histrace building a search engine of historical events in this paper we describe an experimental search engine on our chinese web archive since the original data set contains nearly billion chinese web pages crawled from past years from the collection million articlelike pages are selected and then partitioned into million sets of similar pages the titles and publication dates are determined for the pages an index is built when searching the system returns related pages in a chronological order this way if a user is interested in news reports or commentaries for certain previously happened event heshe will be able to find a quite rich set of highly related pages in a convenient way
online change detection in individual web user behaviour online change detection in individual web user behaviour based on our field studies and consultations with field experts we identified three main problems that are of key importance to online web personalization and customer relationship management detecting changes in individual behaviour reporting on user actions that may need special care and detecting changes in visitation frequencywe propose solutions to these problems and experiment on realworld data from an investment bank collected over years of web traffic these solutions can be applied on any domains where individuals tend to revisit the website and can be identified accurately
guanxi in the chinese web a study of mutual linking guanxi in the chinese web a study of mutual linking guanxi is a type of dyadic social interaction based on feelingsqing and trust xin long studied by scholarsof chinese origin it has recently drawn the attention of researchersoutside of china we define the concept of guanxias applied to the interaction between web sites we exploremethods to identify guanxi in the chinese web show theunique characteristics of the chinese web which result fromit and introduce a mechanism for simulating guanxi in aweb graph model
towards robust trust establishment in webbased social networks with socialtrust towards robust trust establishment in webbased social networks with socialtrust we propose the socialtrust framework for tamperresilient trust establishment in online social networks two of the salient features of socialtrust are its dynamic revision of trust by i distinguishing relationship quality from trust and ii incorporating a personalized feedback mechanism for adapting as the social network evolves 
plurality a contextaware personalized tagging system plurality a contextaware personalized tagging system we present the design of plurality an interactive tagging system pluralitys modular architecture allows users to automatically generate highquality tags over web content as well as over archival and personal content typically beyond the reach of existing web social tagging systems three of the salient features of plurality are i its selflearning and feedbacksensitive capabilities based on a users personalized tagging style ii its leveraging of the collective intelligence of existing social tagging services and iii its contextawareness for optimizing tag suggestions eg based on spatial or temporal features 
defection detection predicting search engine switching defection detection predicting search engine switching searchers have a choice about which web search engine they use when looking for information online if they are unsuccessful on one engine users may switch to a different engine to continue their search by predicting when switches are likely to occur the search experience can be modified to retain searchers or ensure a quality experience for incoming searchers in this poster we present research on a technique for predicting search engine switches our findings show that prediction is possible at a reasonable level of accuracy particularly when personalization or user grouping is employed these findings have implications for the design of applications to support more effective online searching
mining tag clouds and emoticons behind community feedback mining tag clouds and emoticons behind community feedback in this paper we describe our mining system which automatically mines tags from feedback text in an ecommerce scenario it renders these tags in a visually appealing manner further emoticons are attached to mined tags to add sentiment to the visual aspect 
user oriented link function classification user oriented link function classification currently most linkrelated applications treat all links in the same web page to be identical one linkrelated application usually requires one certain property of hyperlinks but actually not all links have this property or they have this property on different levels based on a study of how human users judge the links the idea of the link function classification lfc is introduced in this paper the link functions reflect the purpose that links are created by web page designers and the way they are used by viewers links in a certain function class imply one certain relationship between the adjacent pages and thus they can be assumed to have similar properties an algorithm is proposed to analyze the link functions based on both vision and structure features which simulates the reaction on the links of human users current applications can be enhanced by lfc with a more accurate modeling of the web graph new mining methods can be also developed by making more and stronger assumptions on links within each function class due to the purer property set they share
extraction and mining of an academic social network extraction and mining of an academic social network this paper addresses several key issues in extraction and mining of an academic social network extraction of a researcher social network from the existing web integration of the publications from existing digital libraries expertise search on a given topic and association search between researchers we developed a social network system called arnetminer based on proposed methods to the above problems in total researcher profiles and publications were extractedintegrated after the system having been in operation for two years the paper describes the architecture and main features of the system it also briefly presents the experimental results of the proposed methods
a framework for fast community extraction of largescale networks a framework for fast community extraction of largescale networks most of the faster community extraction algorithms are based on the clauset newman and moore cnm which is employed for networks with sizes up to nodes the modification proposed by danon diaz and arenas dda obtains better modularity among cnm and its variations but there is no improvement in speed as its authors expressed in this paper we identify some inefficiencies in the data structure employed by former algorithms we propose a new framework for the algorithm and a modification of the dda to make it applicable to largescale networks for instance the community extraction of a network with million nodes and million edges was performed in about minutes in contrast to former cnm that required hours times the former cnm obtaining better modularitythe scalability of our improvements is shown by applying it to networks with sizes up to million nodes obtaining the best modularity and execution time compared to the former algorithms
enabling secure digital marketplace enabling secure digital marketplace the fast development of the web provides new ways for effective distribution of networkbaseddigital goods a digital marketplace provides a platform to enable web users toeffectively acquire share marketand distribute digital content however the success of thedigital marketplace business models hinges on securely managing the digital rights and usageof the digital content for example the digital content should be only consumable bypaid users this paper describes a webbased system that enables the secure exchange of digital contentbetween web users and prevents users from illegally resell of the digital contentpart of our solution is based on broadcast encryption technology
mashups for semantic user profiles mashups for semantic user profiles in this paper we discuss challenges and provide solutions for capturing and maintaining accurate models of user profiles using semantic web technologies by aggregating and sharing distributed fragments of user profile information spread over multiple services our framework for profile management allows for evolvable extensible and expressive user profiles we have implemented a prototype targeting the retail domain on the hp labs retail store assistant
finding core members in virtual communities finding core members in virtual communities finding the core members of a virtual community is an important problem in community analysis here we presented an simulated annealing algorithm to solve this problem by optimizing the user interests concentration ratio in user groups as an example we test this algorithm on a virtual community site and evaluate its results using human gold standard method
improving personalized services in mobile commerce by a novel multicriteria rating approach improving personalized services in mobile commerce by a novel multicriteria rating approach with the rapid growth of wireless technologies and mobile devices there is a great demand for personalized services in mcommerce collaborative filtering cf is one of successful techniques to produce personalized recommendations for users this paper proposes a novel approach to improve cf algorithms where the contextual information of a user and the multicriteria ratings of an item are considered besides the typical information on users and items the multilinear singular value decomposition msvd technique is utilized to explore both explicit relations and implicit relations among user item and criterion we implement the approach in an existing mcommerce platform and encouraging experimental results demonstrate its effectiveness
ruin doing what you like with people whom you like ruin doing what you like with people whom you like this paper presents ruin a social networking application that leverages web and imsbased converged networks technologies to create a rich nextgeneration service ruin allows a user to search in realtime and solicit participation of likeminded partners for an activity of mutual interest eg a rock concert a soccer game or a movie it is an example of a situational mashup application that exploits content and capabilities of a telecom operator blended with web technologies to provide an enhanced valueadded service experience
social and semantics analysis via nonnegative matrix factorization social and semantics analysis via nonnegative matrix factorization social media such as web forum often have dense interactions betweenuser and content where network models are often appropriate foranalysis joint nonnegative matrix factorization model ofparticipation and content data can be viewed as a bipartite graphmodel between users and media and is proposed for analysis socialmedia the factorizations allow simultaneous automatic discovery ofleaders and subcommunities in the web forum as well as the corelatent topics in the forum results on topic detection of web forumsand cluster analysis show that social features are highly effectivefor forum analysis
personalized multimedia web summarizer for tourist personalized multimedia web summarizer for tourist in this paper we highlight the use of multimedia technology in generating intrinsic summaries of tourism related information the system utilizes an automated process to gather filter and classify information on various tourist spots on the web the end result present to the user is a personalized multimedia summary generated with respect to users queries filled with text image video and realtime news made retrievable for mobile devices preliminary experiments demonstrate the superiority of our presentation scheme to traditional methods
user behavior oriented web spam detection user behavior oriented web spam detection combating web spam has become one of the top challenges for web search engines stateoftheart spam detection techniques are usually designed for specific known types of web spam and are incapable and inefficient for recentlyappeared spam with user behavior analyses into web access logs we propose a spam page detection algorithm based on bayes learning preliminary experiments on web access data collected by a commercial web site containing over billion user clicks in months show the effectiveness of the proposed detection framework and algorithm 
web people search results of the first evaluation and plan for the second web people search results of the first evaluation and plan for the second this paper presents the motivation resources and results for thefirst web people search task which was organized as part of thesemeval evaluation exercise also we will describe asurvey and proposal for a new task attribute extraction whichis planned for inclusion in the second evaluation planned forautumn 
finding similar pages in a social tagging repository finding similar pages in a social tagging repository social tagging describes a community of users labeling web content with tags it is a simple activity that enriches our knowledge about resources on the web for a computer to help users search the tagged repository it must know when tags are good or bad we describe tagscore a scoring function that rates the goodness of tags the tags and their ratings give us a succinct synopsis for a page we find similar pages in delicious by comparing synopses our approach gives good correlation to the full cosine similarity but is hundreds of times faster
the seamless browser enhancing the speed of web browsing by zooming and preview thumbnails the seamless browser enhancing the speed of web browsing by zooming and preview thumbnails in this paper we present a new web browsing system seamless browser for fast link traversal on a large screen like tv in navigating web users mainly suffer from cognitive overhead of determining whether or not to follow links this overhead can be reduced by providing preview information of the destination of links and also by providing semantic cues on the nearest location in relation to the anchor in order to reduce disorientation and annoyance from the preview information we propose that users will focus on the small area nearside around a pointer and a small number of hyperlink previews in that focused area will appear differently depending on the distances between the pointer and the hyperlinks the nearer the distance is the richer the content of the information scent is we also propose that users can navigate the link paths by controlling the pointer and the zooming interface so that users may go backward and forward seamlessly along several possible link paths we found that combining the pointer and a zoom significantly improved performance for navigational tasks
personalized viewbased search and visualization as a means for deepsemantic web data access personalized viewbased search and visualization as a means for deepsemantic web data access effective access to and navigation in information stored in deep web ontological repositories or relational databases has yet to be realized due to issues with usability of user interfaces and the overall scope and complexity of information as well as the nature of exploratory user taskswe propose the integration and adaptation of novel navigation and visualization approaches to faceted browsing such as visual depiction of facets and restrictions visual navigation in clusters of search results and graph like exploration of individual search results properties
as we may perceive finding the boundaries of compound documents on the web as we may perceive finding the boundaries of compound documents on the web this paper considers the problem of identifying on the web compound documents cdocs groups of web pages that in aggregate constitute semantically coherent information entities examples of cdocs are a news article consisting of several html pages or a set of pages describing specifications price and reviews of a digital camera being able to identify cdocs would be useful in many applications including web and intranet search user navigation automated collection generation and information extractionin the past several heuristic approaches have been proposed to identify cdocs however heuristics fail to capture the variety of types styles and goals of information on the web and do not account for the fact that the definition of a cdoc often depends on the context this paper presents an experimental evaluation of three machine learningbased algorithms for cdoc discovery these algorithms are responsive to the varying structure of cdocs and adaptive to their applicationspecific nature based on our previous work this paper proposes a different scenario for discovering cdocs and compares in this new setting the local machine learned clustering algorithm from to a global purely graph based approach and a conditional markov network approach previously applied to noun coreference task the results show that the approach of outperforms the other algorithms suggesting that global relational characteristics of web sites are too noisy for cdoc identification purposes 
system a hypergraph based native rdf repository system a hypergraph based native rdf repository to manage the increasing amount of rdf data an rdf repository should provide not only necessary scalability and efficiency but also sufficient inference capabilities in this paper we propose a native rdf repository system to pursue a better tradeoff among the above requirements system takes the hypergraph representation for rdf as the data model for its persistent storage which effectively avoids the costs of data model transformation when accessing rdf data in addition a set of efficient semantic query processing techniques are designed the results of performance evaluation on the lubm benchmark show that system has a better combined metric value than the other comparable systems
extracting spam blogs with cocitation clusters extracting spam blogs with cocitation clusters this paper reports the estimated number of spam blogs in order to assess their current state in the blogosphere to extract spam blogs i developed a traversal method among cocitation clusters of blogs from a spam seed spam seeds were collected in terms of high outdegree and spam keyword according to the experiment a mixed seed set composed of high outdegree and spam keyword seeds is more effective than individual seed sets in terms of fmeasure in conclusion mixed seeds from different methods are effective in improving the fmeasure results of spam extraction with cocitation clusters
the scalefree nature of semantic web ontology the scalefree nature of semantic web ontology semantic web ontology languages such as owl have been widely used for knowledge representation through empirical analysis of realworld ontologies we discover that like many natural and social phenomenon the semantic web ontology is also scalefree 
efficiently querying rdf data in triple stores efficiently querying rdf data in triple stores efficiently querying rdf data is being an important factor in applying semantic web technologies to realworld applications in this context many efforts have been made to store and query rdf data in relational database using particular schemas in this paper we propose a new scheme to store index and query rdf data in triple stores graph feature of rdf data is taken into considerations which might help reduce the join costs on the vertical database structure we would partition rdf triples into overlapped groups store them in a triple table with one more column of group identity and build up a signature tree to index them based on this infrastructure acomplex rdf query is decomposed into multiple pieces ofsubqueries which could be easily filtered into some rdf groups using signature tree index and finally is evaluated with a composed and optimized sql with specific constraints we compare the performance of our method with prior art on typical queries over a large scaled lubm and uobm benchmark data more than million triples for some extreme cases they can promote to orders of magnitude
collaborative knowledge semantic graph image search collaborative knowledge semantic graph image search in this paper we propose a collaborative knowledge semantic graphs image search cksgis system it provides a novel way to conduct image search by utilizing the collaborative nature in wikipedia and by performing network analysis to form semantic graphs for searchterm expansion the collaborative article editing process used by wikipedias contributors is formalized as bipartite graphs that are folded into networks between terms when a user types in a search term cksgis automatically retrieves an interactive semantic graph of related terms that allow users to easily find related images not limited to a specific search term interactive semantic graph then serve as an interface to retrieve images through existing commercial search engines this method significantly saves users time by avoiding multiple search keywords that are usually required in generic search engines it benefits both nave users who do not possess a large vocabulary and professionals who look for images on a regular basis in our experiments of the participants favored cksgis system rather than commercial search engines
a logical framework for modeling and reasoning about semantic web services contract a logical framework for modeling and reasoning about semantic web services contract in this paper we incorporate concrete domain and action theory into a very expressive description logic dl called alcqo notably this extension can significantly augment the expressive power for modeling and reasoning about dynamic aspects of services contracting meanwhile the original nature and advantages of classical dls are also preserved to the extent possible
sailer an effective search engine for unified retrieval of heterogeneous xml and web documents sailer an effective search engine for unified retrieval of heterogeneous xml and web documents this paper studies the problem of unified ranked retrieval of heterogeneous xml documents and web data we propose an effective search engine called sailer to adaptively and versatilely answer keyword queries over the heterogenous data we model the web pages and xml documents as graphs we propose the concept of pivotal trees to effectively answer keyword queries and present an effective method to identify the topk pivotal trees with the highest ranks from the graphs moreover we proposeeffective indexes to facilitate the effective unified ranked retrieval we have conducted an extensive experimental study using real datasets and the experimental results show that sailer achieves both high search efficiency and accuracy and outperforms the existing approaches significantly
larger is better seed selection in linkbased antispamming algorithms larger is better seed selection in linkbased antispamming algorithms seed selection is of significant importance for the biasedpagerank algorithms such as trustrank to combat linkspamming previous work usually uses a small seed setwhich has a big problem that the top ranking results havea strong bias towards seeds in this paper we analyze therelationship between the result bias and the number of seeds furthermore we experimentally show that an automatically selected large seed set can work better than a carefully selected small seed set
ajax for mobility mobileweaver ajax framework ajax for mobility mobileweaver ajax framework this paper provides a brief description of a research project on using ajax to enhance mobile web applications for enterprise use the project known as mobileweaver ajax framework leverages enterprise soa service oriented architecture and the latest web technologies on mobile devices
cmpmi improved webbased association measure with contextual label matching cmpmi improved webbased association measure with contextual label matching webpmi is a popular webbased association measure to evaluatethe semantic similarity between two queries ie words or entitiesby leveraging search results returned by search engines this paperproposes a novel measure named cmpmi to evaluate querysimilarity at a finer granularity than webpmi under theassumption that a query is usually associated with more than oneaspect and two queries are deemed semantically related if theirassociated aspect sets are highly consistent with each othercmpmi first extracts contextual labels from search results torepresent the aspects of a query and then uses the optimalmatching method to assess the consistency between the aspects oftwo queries experimental results on the benchmark millercharles dataset demonstrate the good effectiveness of theproposed cmpmi measure moreover we further fuse webpmiand cmpmi to obtain improved results
topigraphy visualization for largescale tag clouds topigraphy visualization for largescale tag clouds this paper proposes a new method for displaying largescale tag clouds we use a topographical image that helps users to grasp the relationship among tags intuitively as a background to the tag clouds we apply this interface to a blog navigation system and show that the proposed method enables users to find the desired tags easily even if the tag clouds are very large and above tags our approach is also effective for understanding the overall structure of a large amount of tagged documents
folksoviz a subsumptionbased folksonomy visualization using wikipedia texts folksoviz a subsumptionbased folksonomy visualization using wikipedia texts in this paper targeting delicious tag data we propose a method folksoviz for deriving subsumption relationships between tags by using wikipedia texts and visualizing a folksonomy to fulfill this method we propose a statistical model for deriving subsumption relationships based on the frequency of each tag on the wikipedia texts as well as the tsd tag sense disambiguation method for mapping each tag to a corresponding wikipedia text the derived subsumption pairs are visualized effectively on the screen the experiment shows that the folksoviz manages to find the correct subsumption pairs with high accuracy
falcons searching and browsing entities on the semantic web falcons searching and browsing entities on the semantic web as of today the amount of data on the semantic web has grown considerably the services for searching and browsing entities on the semantic web are in demand to provide such services we developed the falcons system in this poster we present the features of the falcons system
a semantic layer for publishing and localizing xml data for a pp xquery mediator a semantic layer for publishing and localizing xml data for a pp xquery mediator in this poster we present the pp xml data localization layer of xlive an xquery mediation system developed at university of versailles cite xlive a major challenge in the evaluation of xquery over a pp network in the context of multiple xml sources is to abstract from structural heterogeneity most existing approaches have mainly exploited varied types of semantic mappings between peer schemas andor ontologies complex query algorithms are required to exploit these semantic links between peers in contrast our approach focuses on a simple semantic layer it is built on a chord dht for indexing semantic descriptions of mediated data sources a mapping process transforms an xml view of a mediator each peer is equipped with a mediator to a semantic form that is published into the pp network and stored in the dht for a given user query a search in the dht retrieves all relevant data sources able to contribute to the result elaboration then every peer that contains relevant data is directly queried to collect data for elaborating the final answer
application of bitmap index to information retrieval application of bitmap index to information retrieval we developed the hsbitmap index for efficient information retrieval the hsbitmap index is a hierarchical documentterm matrix the original documentterm matrix is called the leaf matrix and an upper matrix is the summary of its lower matrix our experiment results show the hsbitmap index performs better than the inverted index with a minor space overhead
pivotbrowser a tagspace image searching prototype pivotbrowser a tagspace image searching prototype we propose a novel iterative searching and refining prototype fortagged images this prototype named pivotbrowser capturessemantically similar tag sets in a structure called pivot byconstructing a pivot for a textual query pivotbrowser first selectscandidate images possibly relevant to the query the tags containedin these candidate images are then selected in terms of their tagrelevances to the pivot the shortlisted tags are clustered and oneof the tag clusters is used to select the results from the candidateimages ranking of the images in each partition is based on theirrelevance to the tag cluster with the guidance of the tag clusterspresented a user is able to perform searching and iterative queryrefinement
improving web spam detection with reextracted features improving web spam detection with reextracted features web spam detection has become one of the top challenges for the internet search industry instead of using some heuristic rules we propose a feature reextraction strategy to optimize the detection result based on the predicted spamicity obtained by the preliminary detection through the host level web graph three types of features are extracted experiments on webspamuk benchmark show that with this strategy the performance of web spam detection can be improved evidently
semantic similarity based on compact concept ontology semantic similarity based on compact concept ontology this paper presents a new method of calculating the semantic similarity between two articles based on wordnet to further improve the performance of the proposed method we build a new compact concept ontology cco from wordnet by combining the words with similar semantic meanings the experimental results show that our approach significantly outperforms a recent proposal of computing semantic similarity and demonstrate the superiority of the proposed cco method
temporal views over rdf data temporal views over rdf data supporting fast access to large rdf stores has been one of key challenges for enabling use of the semantic web in reallife applications more so in sensorbased systems where large amounts of historic data needs to be stored we propose a semanticsbased temporal view mechanism that enables faster access to timevarying data by caching into memory only the required subset of rdf triples we describe our experience of implementing such a framework in the context of a wide area network monitoring system our preliminary results show that our solution significantly improves client access time and scales well for moderate data sets
collaborative filtering on skewed datasets collaborative filtering on skewed datasets many real life datasets have skewed distributions of events when the probability of observing few events far exceeds the others in this paper we observed that in skewed datasets the state of the art collaborative filtering methods perform worse than a simple probabilistic model our test bench includes a real ad click stream dataset which is naturally skewed the same conclusion is obtained even from the popular movie rating dataset when we pose a binary prediction problem of whether a user will give maximum rating to a movie or not
lip the step towards the webd lip the step towards the webd the world wide web allows users to create and publish a varietyof resources including multimedia ones most of thecontemporary best practices for designing web interfaceshowever do not take into account the d techniquesin this paper we present a novel approach for designinginteractive web applications layer interface paradigm lipthe background layer of the liptype user interface is a dscene which a user cannot directly interact with the foregroundlayer is html content only taking an action on this content egpressing a hyperlink scrolling a page can affect the d scenewe introduce a reference implementation of lip copernicus the virtual d encyclopedia which shows one of the potentialpaths of the evolution of wikipedia towards web based onthe evaluation of copernicus we prove that designing webinterfaces according to lip provides users a better browsingexperience without harming the interaction
exploiting semantic web technologies to model web form interactions exploiting semantic web technologies to model web form interactions form mapping is the key problem that needs to be solved in order to get access to the hidden web currently available solutions for fully automatic mapping are not ready for commercial metasearch engines which still have to rely on hand crafted code and are hard to maintainwe believe that a thorough formal description of the problem with semantic web technologies provides a promising perspective to develop a new class of vertical search engines that is more robust and easier to maintain than existing solutionsin this paper instead of trying to tackle the mapping problem we model the interaction necessary to fill out a web form first during a userassisted phase the connection from the visible elements on the form to the domain concepts is established then with help from background knowledge about the possible interaction steps a plan for filling out the form is derived
an initial investigation on evaluating semantic web instance data an initial investigation on evaluating semantic web instance data many emerging semantic web applications include ontologies from one set of authors and instance data from another often much larger set of authors often ontologies are reused and instance data is integrated in manners unanticipated by their authors not surprisingly many instance data rich applications encounter instance data that is not compatible with the expectations of the original ontology authors this line of work focuses on issues related to semantic expectation mismatches in instance data our initial results include a customizable and extensible serviceoriented evaluation architecture and a domain implementation called pmlvalidator which checks instance data using the corresponding ontologies and additional style requirements
investigation of partial query proximity in web search investigation of partial query proximity in web search proximity of query terms in a document is an important criterion in ir however no investigation has been made to determine the most useful term sequences for which proximity should be considered in this study we test the effectiveness of using proximity of partial term sequences ngrams for web search we observe that the proximity of sequences of to terms is most effective for long queries while shorter or longer sequences appear less useful this suggests that combinations of to terms can best capture the intention in user queries in addition we also experiment with weighing the importance of query subsequences using query log frequencies our preliminary tests show promising empirical results
identifying regional sensitive queries in web search identifying regional sensitive queries in web search in web search ranking the expected results for some queries couldvary greatly depending upon location of the user we name suchqueries regional sensitive queries identifying regional sensitivityof queries is important to meet users needs the objective of thiswork is to identify whether a user expects only regional results fora query we present three novel features generated from search logsand build a meta query classifier to identify regional sensitive queryexperimental results show that the proposed method achieves high accuracyin identifying regional sensitive queries
keyword extraction for contextual advertisement keyword extraction for contextual advertisement as the largest online marketplace ebay strives to promote its inventory throughout the web via different types of online advertisement contextually relevant links to ebay assets on third party sites is one example of such advertisement avenues keyword extraction is the task at the core of any contextual advertisement system in this paper we explore a machine learning approach to this problem the proposed solution uses linear and logistic regression models learnt from human labeled data combined with document text and ebay specific features in addition we propose a solution to identify the prevalent category of ebay items in order to solve the problem of keyword ambiguity
which apple are you talking about which apple are you talking about in a higher level task such as clustering of web results orword sense disambiguation knowledge of all possible distinct concepts in which an ambiguous word can be expressed would be advantageous for instance in determining the number of clusters in case of clustering web search results we propose an algorithm to generate such a ranked list of distinct concepts associated with an ambiguous word concepts which are popular in terms of usage are ranked higherwe evaluate the coverage of the concepts inferred from ouralgorithm on the results retrieved by querying the ambiguousword using a major search engine and show a coverageof for top documents averaged over all keywords
a unified framework for name disambiguation a unified framework for name disambiguation name ambiguity problem has been a challenging issue for a long history in this paper we intend to make a thorough investigation of the whole problem specifically we formalize the name disambiguation problem in a unified framework the framework can incorporate both attribute and relationship into a probabilistic model we explore a dynamic approach for automatically estimating the person number k and employ an adaptive distance measure to estimate the distance between objects experimental results show that our proposed framework can significantly outperform the baseline method
kc browser semantic mashup and linkfree browsing kc browser semantic mashup and linkfree browsing this paper proposes a general framework of for a system with a semantic browsing and visualization interface called knowledge communication collaboration and creation browser kc browser integrates multimedia contests and web services on the grid networks and makes a semantic mashup called knowledge workspace kworkspace with various visual gadgets according to users contexts eg their interests purpose and computational environments kc browser also achieves a linkfree browsing for seamless knowledge access by generating semantic links based on an arbitrary knowledge models such as ontology and vector space models it assists users to look down and to figure out various social and natural events from the web contents we have implemented a prototype of kc browser and tested it to an international project on risk intelligence against natural disaster 
generating hypotheses from the web generating hypotheses from the web hypothesis generation is a crucial initial step for making scientific discoveries this paper addresses the problem of automatically discovering interesting hypotheses from the web given a query containing one or two entities of interest our algorithm automatically generates a semantic profile describing the specified entity or provides the potential connections between two entities of interest we implemented a prototype on top of the google search engine and the experimental results demonstrate the effectiveness of our algorithms
visualizing historical content of web pages visualizing historical content of web pages recently along with the rapid growth of the web the preservation efforts have also increased as a consequence large amounts of past web data are stored in web archives this historical data can be used for better understanding of longterm page topics and characteristics in this paper we propose an interactive visualization system called page history explorer for exploring page histories it allows for roughly portraying evolution of pages and summarizing their content over time we use a temporal term cloud as a structure for visualizing prevailing and active terms appearing on pages in the past
integrating the iac neural network in ontology mapping integrating the iac neural network in ontology mapping ontology mapping seeks to find semantic correspondences between similar elements of different ontologies this paper proposes a neural network based approach to search for a global optimal solution that best satisfies ontology constraints experiments on oaei benchmark tests show it dramatically improves the performance of preliminary mapping results
reasoning about similarity queries in text retrieval tasks reasoning about similarity queries in text retrieval tasks in many text retrieval tasks it is highly desirable to obtain a similarity profile of the document collection for a given query we propose samplingbased techniques to address this need using calibration techniques to improve the accuracy experimental results confirm the effectiveness of the proposed approaches
automatic web image selection with a probabilistic latent topic model automatic web image selection with a probabilistic latent topic model we propose a new method to select relevant images to thegiven keywords from images gathered from theweb based onthe probabilistic latent semantic analysis plsa modelwhich is a probabilistic latent topic model originally proposedfor text document analysis the experimental resultsshows that the results by the proposed method is almostequivalent to or outperforms the results by existing methodsin addition it is proved that our method can selectmore various images compared to the existing svmbasedmethods
behavioral classification on the click graph behavioral classification on the click graph a bipartite queryurl graph where an edge indicates that a document wasclicked for a query is a useful construct for finding groups of relatedqueries and urls here we use this behavior graph for classification wechoose a click graph sampled from two weeks of image search activity and thetask of adult filtering identifying content in the graph that isinappropriate for minors we show how to perform classification using randomwalks on this graph and two methods for estimating classifier parameters
incremental web page template detection incremental web page template detection most template detection methods process web pages in batches that anewly crawled page can not be processed until enough pages have beencollected this results in large storage consumption and a hugedelay of data refreshing in this paper we present an incrementalframework to detect templates in which a page is processed as soonas it has been crawled in this framework we dont need to cacheany web page experiments show that our framework consumes less than storage than traditional methods and also the speed of datarefreshing is accelerated because of the incremental manner
efficient vectorial operators for processing xml twig queries efficient vectorial operators for processing xml twig queries this paper proposes several vectorial operators for processing xmltwig queries which are easy to be performed and inherentlyefficient for both ancestordescendant ad and parentchild pcrelationships we develop optimizations on the vectorial operatorsto improve the efficiency of answering twig queries in holistic wepropose an algorithm to answer gtp queries based on our vectorialoperators
race finding and ranking compact connected trees for keyword proximity search over xml documents race finding and ranking compact connected trees for keyword proximity search over xml documents in this paper we study the problem of keyword proximity search over xml documents and leverage the efficiency and effectiveness we take the disjunctive semantics among input keywords into consideration and identify meaningful compact connected trees as the answers of keyword proximity queries we introduce the notions of compactlowest common ancestor clca and maximal clca mclca and propose compact connected trees cctrees and maximal cctrees mcctrees to efficiently and effectively answer keyword queries we propose a novel ranking mechanism textsfrace to rank compact connected trees by taking into consideration both the structural similarity and the textual similarity our extensive experimental study showsthat our method achieves both high search efficiency andeffectiveness and outperforms existing approaches significantly
asymmetrical query recommendation method based on bipartite network resource allocation asymmetrical query recommendation method based on bipartite network resource allocation this paper presents a new query recommendation method that generates recommended query list by mining largescale user logs starting from the user logs of clickthrough data we construct a bipartite network where the nodes on one side correspond to unique queries on the other side to unique urls inspired by the bipartite network based resource allocation method we try to extract the hidden information from the queryurl bipartite network the recommended queries generated by the method are asymmetrical which means two related queries may have different strength to recommend each other to evaluate the method we use one week user logs from chinese search engine sogou the method is not only content ignorant but also can be easily implemented in a paralleled manner which is feasible for commercial search engines to handle large scale user logs
efficient mining of frequent sequence generators efficient mining of frequent sequence generators sequential pattern mining has raised great interest in data mining research field in recent years however to our best knowledge no existing work studies the problem of frequent sequence generator mining in this paper we present a novel algorithm feat abbr frequent sequence generator miner to perform this task experimental results show that feat is more efficient than traditional sequential pattern mining algorithms but generates more concise result set and is very effective for classifying web product reviews 
dissemination of heterogeneous xml data dissemination of heterogeneous xml data a lot of recent research has focused on the contentbased dissemination of xml data however due to the heterogeneous data schemas used by different data publishers even for data in the same domain an important challenge is how to efficiently and effectively disseminate relevant data to subscribers whose subscriptions might be specified based on schemas that are different from those used by the data publishers this paper examines the options to resolve this schema heterogeneity problem in xml data dissemination and proposes a novel paradigm that is based on data rewriting our experimental results demonstrate the effectiveness of the data rewriting paradigm and identifies the tradeoffs of the various approaches
using subspace analysis for event detection from web clickthrough data using subspace analysis for event detection from web clickthrough data although most of existing research usually detects events byanalyzing the content or structural information of web documentsa recent direction is to study the usage data in this paper wefocus on detecting events from web textitclickthrough datagenerated by web search engines we propose a novel approach whicheffectively detects events from clickthrough data based on robustsubspace analysis we first transform clickthrough data to thed polar space next an algorithm based on generalizedprincipal component analysis gpca is used to estimate subspacesof transformed data such that each subspace contains querysessions of similar topics then we prune uninteresting subspaceswhich do not contain query sessions corresponding to real eventsby considering both the semantic certainty and the temporalcertainty of query sessions in each subspace finally variousevents are detected from interesting subspaces by utilizing anonparametric clustering technique compared with existingapproaches our experimental results based on reallifeclickthrough data have shown that the proposed approach is moreaccurate in detecting real events and more effective indetermining the number of events
web page rank prediction with markov models web page rank prediction with markov models in this paper we propose a method for predicting the ranking position of a web page assuming a set of successive past topk rankings we study the evolution of web pages in terms of ranking trend sequences used for markov models training which are in turn used to predict future rankings the predictions are highly accurate for all experimental setups and similarity measures
a systematic approach for cellphone worm containment a systematic approach for cellphone worm containment cell phones are increasingly becoming attractive targets of various worms which cause the leakage of user privacy extra service charges and depletion of battery power in this work we study propagation of cellphone worms which exploit multimedia messaging service mms andor bluetooth for spreading we then propose a systematic countermeasureagainst the worms at the terminal level we adopt graphic turing test and identitybased signature to block unauthorized messages from leaving compromised phones at the network level we propose a pushbased automated patching scheme for cleansing compromised phones throughexperiments on phone devices and a wide variety of networks we show that cellular systems taking advantage of our defense can achieve a low infection rate eg less than within hours even under severe attacks 
gspexr gsp protocol with an exclusive right for keyword auctions gspexr gsp protocol with an exclusive right for keyword auctions we propose a keyword auction protocol called the generalized second price with an exclusive right gspexr in existing keyword auctions the number of displayed advertisements is determined in advance thus we consider adjusting the number of advertisements dynamically based on bids in the gspexr the number of slots can be either or k when k slots are displayed the protocol is identical to the gsp if the value per click of the highest ranked bidder is large enough then this bidder can exclusively display her advertisement by paying a premium thus this pricing scheme is relatively simple and seller revenue is at least as good as the gsp also in the gspexr the highest ranked bidder has no incentive to change the number of slots by overunderbidding as long as she retains the top position 
size matters word count as a measure of quality on wikipedia size matters word count as a measure of quality on wikipedia wikipedia the free encyclopedia now contains over two million english articles and is widely regarded as a highquality authoritative encyclopedia some wikipedia articles however are of questionable quality and it is not always apparent to the visitor which articles are good and which are bad we propose a simple metric word count for measuring article quality in spite of its striking simplicity we show that this metric significantly outperforms the more complex methods described in related work
understanding internet video sharing site workload a view from data center design understanding internet video sharing site workload a view from data center design in this paper we measured and analyzed the workload on yahoo videothe nd largest us video sharing site to understand itsnature and theimpact on online video data center design we discovered interestingstatistical properties on both static and temporal dimensions ofthe workload including file duration and popularity distributionsarrival rate dynamics and predictability and workload stationarity and burstinesscomplemented with queueingtheoretic techniques we furtherextended our understanding on the measurement data with a virtualdesign on the workload and capacity management components of adata center assuming the same workload as measured which revealskey results regarding the impact of service level agreements slas and workload scheduling schemeson the design and operations of such largescale videodistribution systems
representing a web page as sets of named entities of multiple types a model and some preliminary applications representing a web page as sets of named entities of multiple types a model and some preliminary applications as opposed to representing a document as a bag of words in mostinformation retrieval applications we propose a model ofrepresenting a web page as sets of named entities of multiple typesspecifically four types of named entities are extracted namelyperson geographic location organization and time moreover therelations among these entities are also extracted weightedclassified and marked by labels on top of this model someinteresting applications are demonstrated in particular we introducea notion of personactivity which contains four different elementsperson location time and activity with this notion and based on areasonably large set of web pages we are able to show how onepersons activities can be attributed by time and location whichgives a good idea of the mobility of the person under question
measuring extremal dependencies in web graphs measuring extremal dependencies in web graphs we analyze dependencies in power law graph data web sample wikipedia sample and a preferential attachment graph using statistical inference for multivariate regular variation the well developed theory of regular variation is widely applied in extreme value theory telecommunications and mathematical finance and it provides a natural mathematical formalism for analyzing dependencies between variables with power laws however most of the proposed methods have never been used in the web graph data mining the present work fills this gap the new insights this yields are striking the three abovementioned data sets are shown to have a totally different dependence structure between different graph parameters such as indegree and pagerank
the world wide telecom web browser the world wide telecom web browser as the number of telephony voice applications grow there will be a need for a browser to surf the web of interconnected voice applications called as voicesites these voicesites are accessed through a telephone over an audio channel we present the concept and architecture of tweb browser a world wide telecom web browser that enables browsing the web of voice applications through an ordinary phone this browser will support rich browsing features such as history and bookmarking
voikiosk increasing reachability of kiosks in developing regions voikiosk increasing reachability of kiosks in developing regions one of the several initiatives to bridge the digital divide in developing countries has been the deployment of information kiosks or knowledge centers in villages in rural parts of the country these kiosks provide services ranging from email chat and browsing to distance education programs agricultural services and egovernance services a kiosk typically comprises of a computer with printer web cam multimedia system and internet connectivity and isowned by a local entrepreneur moving away from the pc based kiosk model we present an alternative platform to create and host such information kiosks in the telephony network we call these as voikiosks and they are accessible through voice interaction over an ordinary phone call
layman tuning of websites facing change resilience layman tuning of websites facing change resilience client scripting permits end users to customize content layout orstyle of their favourite websites but current scripting suffers froma tight coupling with the website if the page changes all thescripting can fall apart the problem is that websites are reckonedto evolve frequently and this can jeopardize all the scriptingefforts to avoid this situation this work enriches websiteswith a modding interface in an attempt to decouple laymansscript from website upgrades from the website viewpoint thisinterface ensures safe scripting ie scripts that do not break thepage from a scripter perspective this interface limits tuningbut increases change resilience the approach tries to find abalance between openness scripter free inspection and modularityscripter isolation from website design decisions that permitsscripting to scale up as a mature software practice the approach isrealized for greasemonkey scripts
a teapot graph and its hierarchical structure of the chinese web a teapot graph and its hierarchical structure of the chinese web the shape of the web in terms of its graphical structure has been a widely interested topic two graphs bow tie and daisy have stood out from previous research in this work we take a different approach by viewing the web as a hierarchy of three levels namely page level host level and domain level such structures are analyzed and compared with a snapshot of chinese web in early involving million pages million hosts and million domains some interesting results have emerged for example the chinese web appears more like a teapot with a large size of scc a medium size of in and a small size of out at page level than the classic bow tie or daisy shape some challenging phenomena are also observed for example the ins become much smaller than outs at host and domain levels future work will tackle these puzzles
protecting web services from remote exploit code a static analysis approach protecting web services from remote exploit code a static analysis approach we propose still a signaturefree remote exploit binary code injection attack blocker to protect web servers and web applications still is robust to almost all antisignature antistaticanalysis and antiemulation obfuscation
composing and optimizing data providing web services composing and optimizing data providing web services in this paper we propose a new approach to automatically compose data providing web services our approach exploits existing mature works done in data integration systems specifically data providing services are modeled as rdf parameterized views over mediated ontologies then an rdf oriented algorithm to compose services based on query rewriting techniques was devised we apply also an optimization algorithm on the generated composition to speed up its execution the results of our experiments show that the algorithms scale up very well up to a large number of services covering thus most realistic applications
microscale evolution of web pages microscale evolution of web pages we track a large set of rapidly changing web pages and examine the assumption that the arrival of content changes follows a poisson process on a microscale we demonstrate that there are significant differences in the behavior of pages that can be exploited to maintain freshness in a web corpus
web page sectioning using regexbased template web page sectioning using regexbased template this work aims to provide a novel sitespecific web page segmentation and section importance detection algorithm which leverages structural content and visual information the structural and content information is leveraged via template a generalized regular expression learnt over set of pages the template along with visual information results into high sectioning accuracy the experimental results demonstrate the effectiveness of the approach
towards a programming language for services computing towards a programming language for services computing services computing is emerging as a new discipline the acceptance of web services technology stems from the fact that services enable easy integration and interoperation of enterprise level distributed systems however currently software developers are forced to translate business level service requirements and encode them into programs using low level abstractions such as objects we propose to introduce language constructs for service oriented programming that would enable raising programming abstractions from objects to services
webanywhere enabling a screen reading interface for the web on any computer webanywhere enabling a screen reading interface for the web on any computer eople often use computers other than their own to access web content but blind users are restricted to using computers equipped with expensive specialpurpose screen reading programs that they use to access the web webanywhere is a webbased selfvoicing web application that enables blind web users to access the web from almost any computer that can produce sound without installing new software webanywhere could serve as a convenient lowcost solution for blind users onthego for blind users unable to afford another screen reader and for web developers targeting accessible design this paper describes the implementation of webanywhere overviews an evaluation of it by blind web users and summarizes a survey of public terminals that shows it can run on most public computers
web graph similarity for anomaly detection poster web graph similarity for anomaly detection poster web graphs are approximate snapshots of the web created by searchengines their creation is an errorprone procedure that relies on theavailability of internet nodes and the faultless operation of multiplesoftware and hardware units checking the validity of a web graphrequires a notion of graph similarity web graph similarity helpsmeasure the amount and significance of changes in consecutive webgraphs these measurements validate how well searchengines acquire content from the web in this paper we study fivesimilarity schemes three of them adapted from existing graphsimilarity measures and two adapted from wellknown document andvector similarity methods we compare and evaluate all five schemesusing a sequence of web graphs for yahoo and study if the schemes canidentify anomalies that may occur due to hardware or other problems
static query result caching revisited static query result caching revisited query result caching is an important mechanism for search engine efficiency in this study we first review several query features that are used to determine the contents of a static result cache next we introduce a new feature that more accurately represents the popularity of a query by measuring the stability of query frequency over a set of time intervals experimental results show that this new feature achieves hit ratios better than those of the previously proposed features
a larger scale study of robotstxt a larger scale study of robotstxt a website can regulate search engine crawler access to its content using the robots exclusion protocol specified in its robotstxt file the rules in the protocol enable the site to allow or disallow part or all of its content to certain crawlers resulting in a favorable or unfavorable bias towards some of them a survey on the robotstxt usage of about sites found some evidence of suchbiases the news of which led to widespread discussions on the web in this paper we report on our survey of about million sites our survey tries to correct the shortcomings of the previous survey and shows the lack of any significant preferences towards any particular search engine
algorithm for stochastic multiplechoice knapsack problem and application to keywords bidding algorithm for stochastic multiplechoice knapsack problem and application to keywords bidding we model budgetconstrained keyword bidding in sponsoredsearch auctions as a stochastic multiplechoice knapsack problem smckp and design an algorithm to solve smckpand the corresponding bidding optimization problem ouralgorithm selects items online based on a threshold function which can be builtupdated using historical data our algorithm achieved about performance compared to the offline optimum when applied to a real bidding dataset with synthetic dataset and iid itemsets its performance ratio against the offline optimum converges to one empirically with increasing number of periods
simrank query rewriting through link analysis of the clickgraph poster simrank query rewriting through link analysis of the clickgraph poster we focus on the problem of query rewriting for sponsored searchwe base rewrites on a historical click graph that records the adsthat have been clicked on in response to past user queries givena query q we first consider simrank asa way to identify queries similar to q ie queries whose adsa user may be interested in we argue that simrank fails toproperly identify query similarities in our application and wepresent two enhanced versions of simrank one that exploitsweights on click graph edges and another that exploitsevidence we experimentally evaluate our new schemes againstsimrank using actual click graphs and queries form yahoo andusing a variety of metrics our results show that the enhancedmethods can yield more and better query rewrites
offline matching approximation algorithms in exchangemarkets offline matching approximation algorithms in exchangemarkets motivated by several marketplace applications on rapidlygrowing online social networks we study the problem of efficient offline matching algorithms for online exchange markets we consider two main models of oneshot marketsand exchange markets over time for oneshot marketswe study three main variants of the problem onetooneexchange market problem exchange market problem withshort cycles and probabilistic exchange market problemwe show that all the above problems are nphard andpropose heuristics and approximation algorithms for theseproblems experiments show that the number of items exchanged will increase when exchanges through cycles are allowed exploring algorithms for markets over time is aninteresting direction for future work
an efficient twophase service discovery mechanism an efficient twophase service discovery mechanism we bring forward a twophase semantic service discovery mechanism which supports both the operation matchmaking and operationcomposition matchmaking a serial of experiments on a service management framework show that the mechanism gains better performance on both discovery recall rate and precision than a traditional matchmaker
making bpel flexible adapting in the context of coordination constraints using wsbpel making bpel flexible adapting in the context of coordination constraints using wsbpel although wsbpel is emerging as the prominent language for modeling executable business processes its support for designing flexible processes is limited an important need of many adaptive processes is for concurrent activities in the process to respect em coordination constraints these require that concurrent activities coordinate their behaviors in response to exogenous events otherwise the process may become inconsistent we show how coordination inducing constraints may be represented in wsbpel and use generalized adaptation and constraint enforcement models to transform the traditional bpel process to an adaptive one the final outcome is an executable wsbpel process without extensions able to adapt to events while respecting coordination constraints between activities
speeding up web service composition with volatile information speeding up web service composition with volatile information this paper introduces a novel method for composing web services in the presence of external volatile information our approach which we call the informedpresumptive is compared to previous stateoftheart approaches for web service composition in volatile environments we show empirically that the informedpresumptive strategy produces compositions in significantly less time than the other strategies with lesser backtracks
contextsensitive qos model a rulebased approach to web service composition contextsensitive qos model a rulebased approach to web service composition generally web services are provided with different qos values so they can be selected dynamically in service composition process however the conventional context free composition qos model does not consider the changeability of qos values and the context sensitive constraints during composition process in this paper we propose a rule based context sensitive qos model to support the changeability of qos values and the context sensitive constraints by considering context in the qos model web service composition can be used widely and flexibly in the real world business
lowload server crawler design and evaluation lowload server crawler design and evaluation this paper proposes a method of crawling web servers connectedto the internet without imposing a high processingload we are using the crawler for a field survey of thedigital divide including the ability to connect to the networkrather than employing normal web page crawlingalgorithm which usually collect all pages found on the targetserver we have developed server crawling algorithmwhich collect only minimum pages from the same server andachieved lowload and highspeed crawling of servers
using graphics processors for highperformance ir query processing using graphics processors for highperformance ir query processing web search engines are facing formidable performance challenges as they need to process thousands of queries per second over billions of documents to deal with this heavy workload current engines use massively parallelarchitectures of thousands of machines that require large hardware investmentswe investigate new ways to build such highperformance irsystems based on graphical processing units gpus gpus were originally designed to accelerate computer graphics applications through massive onchip parallelism recently a number of researchers have studied how to use gpus for other problem domains including databases and scientificcomputing but we are not aware of previous attempts to use gpus for largescale web search our contribution here is to design a basic system architecture for gpubased highperformance ir and to describe how to perform highly efficient query processing within such an architecture preliminary experimental results based on a prototype implementation suggest that significant gains in query processing performance might be obtainable with such an approach
extracting xml schema from multiple implicit xml documents based on inductive reasoning extracting xml schema from multiple implicit xml documents based on inductive reasoning we propose a method of classifying xml documents and extracting xml schema from xml by inductive inference based on constraint logic programming the goal of this work is to type a large collection of xml approximately but efficiently this can also process xml code written in a different schema or even code which is schemaless our approach is intended to achieve identification based on the syntax and semantics of the xml documents by information extraction using ontology and to support retrieval and data management our approach has three steps the first step is xml to predicates the second step is to compare predicates and classifies structures which represent similar meanings in different structures and the last step is predicates to rules by using ontology and to maintain xml schema we evaluate similarity of data type and data range by using an ontology dictionary and xml schema is made from results of second and last step 
fast algorithms for topk personalized pagerank queries fast algorithms for topk personalized pagerank queries in entityrelation er graphs ve nodes v represent typed entities and edges e represent typed relations for dynamic personalized pagerank queries nodes are ranked by their steadystate probabilities obtained using the standard random surfer model in this work we propose a framework to answer topk graph conductance queries our topk ranking technique leads to a x speedup and overall our system executes queries x faster than wholegraph pagerank some queries might contain hard predicates ie predicates that must be satisfied by the answer nodes eg we may seek authoritative papers on public key cryptography but only those written during we extend our system to handle hard predicates our system achieves these substantial query speedups while consuming only of the space taken by a regular text index
using cep technology to adapt messages exchanged by web services using cep technology to adapt messages exchanged by web services web service may be unable to interact with each other because of incompatibilities between their interfaces in this paper we present an event driven approach which aims at adapting messages exchanged during service interactions the proposed framework relies on the complex event processing cep technology which provides an environment for the development of applications that need to continuously process analyse and respond to event streams our main contribution is a system that enables developers to design and implement cepbased adapters these latter are deployed in a cep engine which is responsible for continuously receiving messages and processing them according to rules implemented by the adapters resulting transformed messages are thus forwarded to their original service recipient 
budget constrained bidding in keyword auctions and online knapsack problems budget constrained bidding in keyword auctions and online knapsack problems we consider the budgetconstrained bidding optimizationproblem for sponsored search auctions and model it as anonline multiplechoice knapsack problem we design bothdeterministic and randomized algorithms for the online multiplechoiceknapsack problems achieving a provably optimal competitiveratio this translates back to fully automatic biddingstrategies maximizing either profit or revenue for thebudgetconstrained advertiser our bidding strategy for revenuemaximization is oblivious ie without knowledge ofother bidders prices andor clickthroughrates for thosepositions we evaluate our bidding algorithms using bothsynthetic data and real bidding data gathered manuallyand also discuss a sniping heuristic that strictly improvesbidding performance with sniping and parameter tuningenabled our bidding algorithms can achieve a performanceratio above against the optimum by the omniscient bidder
rogue access point detection using segmental tcp jitter rogue access point detection using segmental tcp jitter rogue access points raps pose serious security threats to local networks an analytic model of prior probability distribution of segmental tcp jitter stj is deduced from the mechanism of ieee mac distributed coordinated function dcf and used to differentiate the types of wire and wlan connections which is the crucial step for raps detecting stj as the detecting metric can reflect more the characteristic of mac than ackpair since it can eliminate the delay caused by packet transmission the experiment on an operated network shows the average detection ratio of the algorithm with stj is more than and the average detection time is less than s with improvement of and over the detecting approach of ackpair respectively farther more no wlan training trace is needed in the detecting algorithm
